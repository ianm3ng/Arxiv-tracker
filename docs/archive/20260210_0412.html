<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-10 04:12</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260210_0412</div>
    <div class="row"><div class="card">
<div class="title">MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</div>
<div class="meta-line">Authors: Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</div>
<div class="meta-line">First: 2026-02-06T18:59:59+00:00 · Latest: 2026-02-06T18:59:59+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures and 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://genmilab.github.io/MedMO-Page">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO&#x27;s broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedMO：医学图像多模态大语言模型的基础与理解</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）迅速发展，但在医学领域的应用仍受限于领域覆盖、模态对齐和基础推理的差距。在本研究中，我们介绍了MedMO，一个基于通用MLLM架构构建的医学基础模型，专门在大规模领域特定数据上进行训练。MedMO遵循多阶段训练方案：（i）跨模态预训练，以将异构视觉编码器与医学语言骨干对齐；（ii）在涵盖图像描述、视觉问答、报告生成、检索和基于边界框的疾病定位的多任务监督下进行指令调优；（iii）结合事实检查与框级GIoU奖励的可验证奖励进行强化学习，以增强复杂临床场景中的空间基础和逐步推理。MedMO在多个模态和任务上始终优于强大的开源医学MLLMs。在视觉问答基准测试中，MedMO的平均准确率比基线提高了13.7%，并在1.9%内接近SOTA Fleming-VL。在基于文本的问答中，它比基线提高了6.9%，比Fleming-VL提高了14.5%。在医学报告生成中，MedMO在语义和临床准确性上均有显著提升。此外，它展现出强大的基础能力，IoU比基线提高了40.4%，比Fleming-VL提高了37.0%，突显其强大的空间推理和定位性能。在放射学、眼科学和病理显微镜学的评估中，确认了MedMO的广泛跨模态泛化能力。我们发布了MedMO的两个版本：4B和8B。项目可在https://genmilab.github.io/MedMO-Page获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the application of multimodal large language models (MLLMs) in the medical field, addressing limitations in domain coverage, modality alignment, and grounded reasoning. The authors introduce MedMO, a medical foundation model developed through a multi-stage training process that includes cross-modal pretraining, instruction tuning across various medical tasks, and reinforcement learning with verifiable rewards. Experimental results show that MedMO significantly outperforms existing medical MLLMs, achieving an average accuracy improvement of +13.7% on visual question answering benchmarks and notable gains in text-based question answering and medical report generation, along with a +40.4% improvement in spatial grounding capability, confirming its effectiveness across multiple medical domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强多模态大语言模型（MLLM）在医学领域的应用，解决领域覆盖、模态对齐和基础推理的局限性。作者提出了MedMO，这是一种医学基础模型，通过多阶段训练过程开发，包括跨模态预训练、针对各种医学任务的指令调优和带有可验证奖励的强化学习。实验结果表明，MedMO显著优于现有医学MLLM，在视觉问答基准测试中平均准确率提高了13.7%，在基于文本的问答和医学报告生成中也取得了显著提升，同时在空间对齐方面的交并比提高了40.4%，确认了其在多个医学领域的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning a Generative Meta-Model of LLM Activations</div>
<div class="meta-line">Authors: Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</div>
<div class="meta-line">First: 2026-02-06T18:59:56+00:00 · Latest: 2026-02-06T18:59:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06964v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06964v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://generative-latent-prior.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating &quot;meta-models&quot; that learn the distribution of a network&#x27;s internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model&#x27;s learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model&#x27;s neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习生成性元模型的LLM激活</div>
<div class="mono" style="margin-top:8px">现有的神经网络激活分析方法，如PCA和稀疏自编码器，依赖于强结构假设。生成模型提供了一种替代方案：它们可以在没有这些假设的情况下揭示结构，并作为提高干预保真度的先验。我们通过在十亿个残差流激活上训练扩散模型，探索这一方向，创建“元模型”，学习网络内部状态的分布。我们发现，扩散损失随着计算量的增加而平稳下降，并可靠地预测下游效用。特别是，将元模型学习的先验应用于引导干预可以提高流畅性，损失减少时增益更大。此外，元模型的神经元越来越多地将概念隔离为单独的单元，稀疏探测分数随着损失的减少而增加。这些结果表明，生成性元模型提供了一条可扩展的可解释性路径，而不需要限制性的结构假设。项目页面：https://generative-latent-prior.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the analysis of neural network activations, which traditionally relies on strong structural assumptions. The authors propose a novel method by training diffusion models on one billion residual stream activations to create generative meta-models that learn the distribution of a network&#x27;s internal states. The key experimental findings indicate that the diffusion loss decreases smoothly with increased computational resources and reliably predicts downstream utility, with improved fluency in interventions when applying the learned prior, and that the meta-model&#x27;s neurons effectively isolate concepts into individual units as loss decreases.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善神经网络激活的分析，传统方法依赖于强结构假设。作者提出了一种使用扩散模型的方法，该模型在十亿个残差流激活上进行训练，以创建学习网络内部状态分布的生成元模型。关键发现表明，扩散损失随着计算资源的增加而减少，准确预测下游效用，并在应用学习的先验时增强干预的流畅性，随着损失的减少，元模型中的神经元更有效地隔离概念。</div>
</details>
</div>
<div class="card">
<div class="title">InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen</div>
<div class="meta-line">First: 2026-02-06T18:59:27+00:00 · Latest: 2026-02-06T18:59:27+00:00</div>
<div class="meta-line">Comments: Project Page: https://zju-real.github.io/InftyThink-Plus Code: https://github.com/ZJU-REAL/InftyThink-Plus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06960v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06960v1">PDF</a> · <a href="https://github.com/ZJU-REAL/InftyThink-Plus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zju-real.github.io/InftyThink-Plus">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InftyThink+: 通过强化学习实现有效且高效的无限期推理</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理时的思维链实现强大性能，但这一范式存在二次成本、上下文长度限制以及因中途丢失效应导致的推理退化。迭代推理通过定期总结中间思路来缓解这些问题，但现有方法依赖于监督学习或固定启发式，未能优化何时总结、保留什么以及如何恢复推理。我们提出了InftyThink+，一个端到端的强化学习框架，优化整个迭代推理轨迹，基于模型控制的迭代边界和明确的总结。InftyThink+采用两阶段训练方案，先进行监督冷启动，然后进行轨迹级强化学习，使模型能够学习战略性总结和继续决策。在DeepSeek-R1-Distill-Qwen-1.5B上的实验表明，InftyThink+在AIME24上提高了21%的准确率，并明显优于传统的长思维链强化学习，同时在分布外基准上也表现出更好的泛化能力。此外，InftyThink+显著减少了推理延迟，加速了强化学习训练，展示了更强的推理效率和更好的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of large reasoning models, which face issues such as quadratic costs and degraded reasoning due to lost-in-the-middle effects. The authors propose InftyThink+, an end-to-end reinforcement learning framework that optimizes iterative reasoning by controlling iteration boundaries and summarizing intermediate thoughts. Experimental results demonstrate that InftyThink+ improves accuracy by 21% on the AIME24 benchmark, outperforms traditional long chain-of-thought reinforcement learning methods, generalizes better to out-of-distribution benchmarks, and significantly reduces inference latency while enhancing reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型推理模型在推理过程中面临的二次成本和上下文长度限制。作者提出了InftyThink+，一种端到端的强化学习框架，通过引入模型控制的迭代边界和显式摘要来优化迭代推理过程。实验结果表明，InftyThink+在AIME24基准上提高了21%的准确率，相比传统的长链推理强化学习显著减少了推理延迟并提高了训练效率，展示了更强的性能和改进的推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine</div>
<div class="meta-line">Authors: Reza E. Fazel, Arash Bakhtiary, Siavash A. Bigdeli</div>
<div class="meta-line">First: 2026-02-06T18:56:17+00:00 · Latest: 2026-02-06T18:56:17+00:00</div>
<div class="meta-line">Comments: 22 pages, 5 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06955v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature selection, and preprocessing refinement. Rather than relying on conventional sampling techniques that may introduce bias or cause information loss, the optimized EBM achieves an effective balance between accuracy and interpretability, enabling precise detection of fraudulent transactions while providing actionable insights into feature importance and interaction effects. Furthermore, the Taguchi method is employed to optimize both the sequence of data scalers and model hyperparameters, ensuring robust, reproducible, and systematically validated performance improvements. Experimental evaluation on benchmark credit card data yields an ROC-AUC of 0.983, surpassing prior EBM baselines (0.975) and outperforming Logistic Regression, Random Forest, XGBoost, and Decision Tree models. These results highlight the potential of interpretable machine learning and data-driven optimization for advancing trustworthy fraud analytics in financial systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过优化可解释增强机器提高信用卡欺诈检测</div>
<div class="mono" style="margin-top:8px">解决类别不平衡是信用卡欺诈检测中的一个核心挑战，因为它直接影响现实金融系统中的预测可靠性。为此，本研究提出了一种基于可解释增强机器（EBM）的增强工作流程——一种透明的、最先进的GA2M算法实现，通过系统的超参数调优、特征选择和预处理优化。与可能引入偏差或导致信息损失的传统采样技术不同，优化后的EBM在准确性和可解释性之间实现了有效平衡，能够精确检测欺诈交易，同时提供有关特征重要性和交互效应的可操作见解。此外，采用田口方法优化数据缩放器的顺序和模型超参数，确保稳健、可重复和系统验证的性能提升。在基准信用卡数据上的实验评估显示ROC-AUC为0.983，超过了之前的EBM基线（0.975），并优于逻辑回归、随机森林、XGBoost和决策树模型。这些结果突显了可解释机器学习和数据驱动优化在推动金融系统中可信欺诈分析方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of class imbalance in credit card fraud detection, which affects predictive reliability in financial systems. The study employs an optimized Explainable Boosting Machine (EBM) that integrates systematic hyperparameter tuning, feature selection, and preprocessing refinement to enhance performance without the biases of traditional sampling techniques. Experimental results demonstrate that the optimized EBM achieves an ROC-AUC of 0.983, outperforming previous EBM baselines and other models such as Logistic Regression and Random Forest, thereby showcasing the effectiveness of interpretable machine learning in improving fraud detection accuracy and providing insights into feature importance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决信用卡欺诈检测中的类别不平衡问题，这影响了金融系统中的预测可靠性。研究采用了一种优化的可解释增强机器（EBM），通过超参数调优、特征选择和预处理改进来增强，避免了可能引入偏差的传统采样技术。实验结果表明，优化后的EBM达到了0.983的ROC-AUC，超越了之前的EBM基线和传统模型如逻辑回归和随机森林，从而展示了可解释机器学习在提高欺诈检测准确性和提供特征重要性洞察方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Unitarity Bias in Tensor Factorization: A Theoretical Framework for Symmetry Group Discovery</div>
<div class="meta-line">Authors: Dongsung Huh, Halyun Jeong</div>
<div class="meta-line">First: 2025-11-28T12:58:13+00:00 · Latest: 2026-02-06T18:51:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.23152v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.23152v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While modern neural architectures typically generalize via smooth interpolation, it lacks the inductive biases required to uncover algebraic structures essential for systematic generalization. We present the first theoretical analysis of HyperCube, a differentiable tensor factorization architecture designed to bridge this gap. This work establishes an intrinsic geometric property of the HyperCube formulation: we prove that the architecture mediates a fundamental equivalence between geometric alignment and algebraic structure. Independent of the global optimization landscape, we show that the condition of geometric alignment imposes rigid algebraic constraints, proving that the feasible collinear manifold is non-empty if and only if the target is isotopic to a group. Within this manifold, we characterize the objective as a rank-maximizing potential that unconditionally drives factors toward full-rank, unitary representations. Finally, we propose the Collinearity Dominance mechanism to link these structural results to the global landscape. Supported by empirical scaling laws, we establish that global minima are achieved exclusively by unitary regular representations of group isotopes. This formalizes the HyperCube objective as a differentiable proxy for associativity, demonstrating how rigid geometric constraints enable the discovery of latent algebraic symmetry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>张量分解中的隐式单位性偏差：对对称群发现的理论框架</div>
<div class="mono" style="margin-top:8px">现代神经架构通常通过平滑插值进行泛化，但缺乏揭示系统性泛化所需的归纳偏差。我们首次对HyperCube进行理论分析，这是一种可微分的张量分解架构，旨在弥补这一差距。本研究确立了HyperCube公式的内在几何特性：我们证明该架构在几何对齐与代数结构之间介导了基本等价关系。独立于全局优化景观，我们展示了几何对齐的条件施加了严格的代数约束，证明了可行的共线流形非空当且仅当目标同胚于一个群。在该流形内，我们将目标表征为一个无条件驱动因子朝向满秩单位表示的秩最大化潜力。最后，我们提出了共线性主导机制，将这些结构结果与全局景观联系起来。通过经验缩放法则的支持，我们确立了全局最小值仅通过群同胚的单位正则表示实现。这将HyperCube目标形式化为结合性的一种可微代理，展示了严格的几何约束如何促进潜在代数对称性的发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of modern neural architectures in uncovering algebraic structures necessary for systematic generalization. The authors introduce HyperCube, a differentiable tensor factorization architecture, and provide a theoretical analysis demonstrating its intrinsic geometric properties that connect geometric alignment with algebraic structure. Key findings reveal that geometric alignment imposes strict algebraic constraints, leading to the conclusion that the feasible collinear manifold is non-empty if the target is isotopic to a group, and that global minima are achieved by unitary regular representations of group isotopes, thus establishing HyperCube as a differentiable proxy for associativity and facilitating the discovery of latent algebraic symmetry.</div>
<div class="mono" style="margin-top:8px">本研究解决了现代神经架构在发现系统性泛化所需的代数结构方面的局限性。作者提出了HyperCube，这是一种可微分的张量分解架构，并提供了理论分析，证明其内在几何特性将几何对齐与代数结构联系起来。他们的研究结果表明，几何对齐施加了严格的代数约束，确保当目标与一个群同构时，存在可行的共线流形，并将目标特征化为一个最大化秩的潜力，导致单位表示的出现，实证证据表明全局最小值对应于这些表示。</div>
</details>
</div>
<div class="card">
<div class="title">DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</div>
<div class="meta-line">Authors: Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K. R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi &quot;Jim&quot; Fan</div>
<div class="meta-line">First: 2026-02-06T18:49:43+00:00 · Latest: 2026-02-06T18:49:43+00:00</div>
<div class="meta-line">Comments: Project page: https://dreamdojo-world.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06949v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dreamdojo-world.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamDojo：来自大规模人类视频的通用机器人世界模型</div>
<div class="mono" style="margin-top:8px">能够在多样环境中模拟行动结果将彻底改变通用智能体的大规模发展。然而，建模这些世界动态，特别是对于灵巧机器人任务，由于数据覆盖有限和动作标签稀缺，面临重大挑战。为此，我们推出了DreamDojo，一个基础世界模型，从44,000小时的自我中心人类视频中学习多样的交互和灵巧控制。我们的数据混合代表了迄今为止最大的世界模型预训练视频数据集，涵盖了各种日常场景，包含多样的物体和技能。为了解决动作标签稀缺的问题，我们引入了连续潜在动作作为统一的代理动作，增强了从未标记视频中转移交互知识的能力。在小规模目标机器人数据上进行后训练后，DreamDojo展示了对物理的强理解和精确的动作可控性。我们还设计了一个蒸馏管道，使DreamDojo加速到10.81 FPS的实时速度，并进一步改善上下文一致性。我们的工作使基于生成世界模型的多个重要应用成为可能，包括实时遥操作、策略评估和基于模型的规划。在多个具有挑战性的分布外（OOD）基准上的系统评估验证了我们的方法在模拟开放世界、接触丰富任务中的重要性，为通用机器人世界模型铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance the development of generalist agents capable of simulating actions in diverse environments, which is particularly challenging due to limited data and action labels in dexterous robotics tasks. The authors introduce DreamDojo, a foundational world model that learns from 44,000 hours of egocentric human videos, representing the largest dataset for world model pretraining. Key experimental findings show that after post-training on smaller robot datasets, DreamDojo exhibits strong physics understanding and precise action control, achieving a real-time processing speed of 10.81 FPS and demonstrating effectiveness in various applications such as live teleoperation and model-based planning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于推动通用机器人代理的发展，使其能够在多样化环境中模拟动作，而这一过程受到数据和动作标签不足的制约。作者提出了DreamDojo，一个基础世界模型，从44,000小时的自我中心人类视频中学习，代表了世界模型预训练中最大的数据库。主要发现表明，DreamDojo在小规模机器人数据的后期训练后，能够强烈理解物理学和精确的动作可控性，并以10.81帧每秒的实时速度运行，展示了其在模拟复杂任务中的有效性，并支持实时遥操作和基于模型的规划等应用。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Uncertainty Reveals Agentic Overconfidence</div>
<div class="meta-line">Authors: Jean Kaddour, Srijan Patel, Gbètondji Dovonon, Leo Richter, Pasquale Minervini, Matt J. Kusner</div>
<div class="meta-line">First: 2026-02-06T18:49:35+00:00 · Latest: 2026-02-06T18:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06948v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理不确定性揭示代理过度自信</div>
<div class="mono" style="margin-top:8px">AI代理能否预测自己在任务中是否会成功？我们通过在任务执行前、中、后引导成功概率估计来研究代理不确定性。所有结果都表现出代理过度自信：一些成功率仅为22%的代理预测成功率为77%。反直觉的是，执行前的评估在信息严格较少的情况下往往比标准的执行后评审产生更好的区分，尽管差异并不总是显著。对抗性提示将评估重新框定为缺陷发现实现了最佳校准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the concept of agentic uncertainty in AI agents by examining their ability to predict their success rates on tasks. The researchers employed a method that involved eliciting success probability estimates from the agents before, during, and after task execution. The key findings reveal that many agents exhibit agentic overconfidence, with some predicting a 77% success rate despite only achieving success 22% of the time, and that pre-execution assessments, despite having less information, often provide better discrimination than post-execution reviews, although the differences are not always statistically significant. Additionally, reframing assessments as bug-finding through adversarial prompting resulted in the best calibration of predictions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了人工智能代理的代理不确定性，特别关注它们预测任务成功能力的情况。研究方法包括在任务执行前、中、后收集代理的成功概率估计。主要发现表明，代理表现出代理过度自信，有些代理在成功率仅为22%的情况下预测成功率为77%，而且执行前的评估往往比执行后的评估提供更好的区分，尽管这种差异并不总是显著。此外，通过对抗性提示将评估重新框定为错误查找，能够提高校准效果。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches</div>
<div class="meta-line">Authors: Saber Omidi, Rene Akupan Ebunle, Se Young Yoon</div>
<div class="meta-line">First: 2026-02-06T18:42:01+00:00 · Latest: 2026-02-06T18:42:01+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures. Preprint; manuscript under journal review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06944v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the design and implementation of data-driven optimal derivative feedback controllers for an active magnetic levitation system. A direct, model-free control design method based on the reinforcement learning framework is compared with an indirect optimal control design derived from a numerically identified mathematical model of the system. For the direct model-free approach, a policy iteration procedure is proposed, which adds an iteration layer called the epoch loop to gather multiple sets of process data, providing a more diverse dataset and helping reduce learning biases. This direct control design method is evaluated against a comparable optimal control solution designed from a plant model obtained through the combined Dynamic Mode Decomposition with Control (DMDc) and Prediction Error Minimization (PEM) system identification. Results show that while both controllers can stabilize and improve the performance of the magnetic levitation system when compared to controllers designed from a nominal model, the direct model-free approach consistently outperforms the indirect solution when multiple epochs are allowed. The iterative refinement of the optimal control law over the epoch loop provides the direct approach a clear advantage over the indirect method, which relies on a single set of system data to determine the identified model and control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动磁悬浮系统的最优导数反馈控制：基于数据驱动方法的实验研究</div>
<div class="mono" style="margin-top:8px">本文介绍了主动磁悬浮系统的数据驱动最优导数反馈控制器的设计与实现。直接的无模型控制设计方法基于强化学习框架，与从系统的数值识别数学模型推导出的间接最优控制设计方法进行了比较。对于直接的无模型方法，提出了一种策略迭代程序，增加了一个称为周期循环的迭代层，以收集多组过程数据，提供更为多样化的数据集，并帮助减少学习偏差。该直接控制设计方法与通过结合动态模式分解与控制（DMDc）和预测误差最小化（PEM）系统识别获得的植物模型设计的可比最优控制解决方案进行了评估。结果表明，尽管两种控制器在与基于名义模型设计的控制器相比时都能稳定并改善磁悬浮系统的性能，但在允许多个周期的情况下，直接的无模型方法始终优于间接解决方案。通过周期循环对最优控制律的迭代优化使得直接方法相较于依赖单一系统数据集确定识别模型和控制的间接方法具有明显优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the performance of active magnetic levitation systems through optimal derivative feedback control using data-driven approaches. The study compares a direct, model-free control design method based on reinforcement learning with an indirect optimal control design derived from a mathematical model of the system. Experimental results indicate that while both control methods stabilize the system, the direct model-free approach significantly outperforms the indirect method when multiple epochs of data are utilized, demonstrating the advantages of iterative refinement in control law optimization.</div>
<div class="mono" style="margin-top:8px">本研究探讨了主动磁悬浮系统的最优导数反馈控制器的设计，动机是为了在此类系统中寻找有效的控制方法。研究人员比较了一种基于强化学习的直接无模型控制设计与一种基于系统数学模型的间接最优控制。实验结果表明，尽管这两种控制器在与名义模型相比时都能提高系统的稳定性和性能，但直接无模型方法在利用多个数据周期时表现出更优的性能，这得益于其迭代优化过程减少了学习偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay</div>
<div class="meta-line">Authors: Duygu Altinok</div>
<div class="meta-line">First: 2026-02-06T18:41:14+00:00 · Latest: 2026-02-06T18:41:14+00:00</div>
<div class="meta-line">Comments: Submitted to Cambridge NLP journal, all rights belong to them</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06942v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer&#x27;s training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a &quot;subwords manifest&quot;, that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this &quot;subwords manifest&quot; delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模土耳其语子词优化策略：数据、词汇、形态学相互作用的系统评估</div>
<div class="mono" style="margin-top:8px">分词是神经语言建模在形态丰富语言（MRLs）如土耳其语中的关键设计选择，生产性粘合对词汇效率和形态学保真度提出了挑战。先前的研究探讨了分词器家族和词汇大小，但通常（i）在不系统控制分词器训练语料的情况下变化词汇，（ii）提供有限的内在诊断，以及（iii）评估狭窄的下游任务切片。我们提出了首个全面、原则性的土耳其语子词分词研究；一个“子词清单”，共同变化词汇大小和分词器训练语料大小（数据和词汇耦合），在匹配参数预算下比较多个分词器家族（WordPiece、形态学水平和字符基线），并在语义（NLI、STS、情感分析、NER）、句法（POS、依赖解析）和形态敏感探针中进行评估。为了说明分词器成功或失败的原因，我们引入了一种形态学感知诊断工具包，超越粗略聚合，达到边界级微/宏F1、解耦的词根原子性与表面边界命中、过/欠分割指数、字符/单词编辑距离（CER/WER）、延续率以及词缀类型覆盖和标记级原子性。我们的贡献有四个方面：（i）对词汇-语料-成功三元组的系统调查；（ii）一个统一的、形态学感知的评估框架，将内在诊断与外在结果联系起来；（iii）控制比较，识别何时字符级和形态级分词带来收益；以及（iv）评估代码、分词器管道和模型的开源发布。作为首个此类工作，这个“子词清单”为在MRLs中构建有效的分词器提供了可操作的指导，并为未来研究建立了可重复的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of tokenization in morphologically rich languages like Turkish, where agglutination complicates vocabulary efficiency and morphological accuracy. The authors conducted a systematic evaluation of Turkish subword tokenization by varying vocabulary size and training corpus size, comparing multiple tokenizer families, and assessing performance across various semantic and syntactic tasks. Key findings indicate that the interplay between vocabulary and corpus size significantly impacts tokenizer effectiveness, and the introduction of a morphology-aware diagnostic toolkit provides deeper insights into the success and failure of different tokenization strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决形态丰富语言（如土耳其语）中的分词挑战，因其粘合性影响词汇效率和形态准确性。作者通过改变词汇大小和训练语料库大小，系统评估了土耳其语子词分词，同时比较了包括WordPiece和形态级分词器在内的不同分词器家族，涵盖了多种语义、句法和形态敏感任务。主要发现表明，词汇大小与训练语料库之间的相互作用显著影响分词成功，并且引入的形态感知诊断工具包提供了对分词器性能的更深入见解，最终为形态丰富语言中有效分词器的开发提供了全面框架。</div>
</details>
</div>
<div class="card">
<div class="title">Endogenous Resistance to Activation Steering in Language Models</div>
<div class="meta-line">Authors: Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, Mike Vaiana, Diogo de Lucena, Judd Rosenblatt, Michael S. A. Graziano</div>
<div class="meta-line">First: 2026-02-06T18:41:12+00:00 · Latest: 2026-02-06T18:41:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06941v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06941v1">PDF</a> · <a href="http://github.com/agencyenterprise/endogenous-steering-resistance">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型中的内源性激活引导抵抗</div>
<div class="mono" style="margin-top:8px">大型语言模型在推理过程中能够抵抗与任务不一致的激活引导，有时在生成过程中恢复，从而产生更好的响应，即使引导仍然处于活跃状态。我们将其称为内源性引导抵抗（ESR）。通过使用稀疏自编码器（SAE）潜变量来引导模型激活，我们发现Llama-3.3-70B表现出显著的ESR，而Llama-3和Gemma-2系列中的较小模型则较少出现这种现象。我们识别出26个在离题内容中差异性激活的SAE潜变量，并与Llama-3.3-70B中的ESR存在因果关联。零消融这些潜变量使多次尝试率降低25%，为专门的内部一致性检查电路提供了因果证据。我们证明ESR可以通过提示和训练故意增强：指导模型自我监控的元提示使Llama-3.3-70B的多次尝试率增加4倍，而在自我纠正示例上进行微调成功诱导了较小模型中的ESR类行为。这些发现具有双重意义：ESR可以防止对抗性操控，但也可能干扰依赖于激活引导的有益安全干预。理解和控制这些抵抗机制对于开发透明和可控的人工智能系统至关重要。代码可在github.com/agencyenterprise/endogenous-steering-resistance获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the phenomenon of Endogenous Steering Resistance (ESR) in large language models, motivated by the observation that these models can resist task-misaligned activation steering during inference. The study employs sparse autoencoder (SAE) latents to steer model activations, revealing that the Llama-3.3-70B model exhibits significant ESR, while smaller models from the Llama-3 and Gemma-2 families show this effect less frequently. Key findings include the identification of 26 SAE latents linked to ESR, with zero-ablating these latents resulting in a 25% reduction in multi-attempt rates, and demonstrating that ESR can be enhanced through meta-prompts and fine-tuning, which has implications for both adversarial resistance and safety interventions in AI systems.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型中的内源性引导抵抗（ESR）现象，动机是观察到这些模型在推理过程中能够抵抗与任务不一致的激活引导。研究采用稀疏自编码器（SAE）潜变量来引导模型激活，发现Llama-3.3-70B模型表现出显著的ESR，而较小的模型则较少出现这种行为。关键实验结果显示，零消融26个与离题内容相关的SAE潜变量会使多次尝试率降低25%，而且通过特定的提示和训练方法可以增强ESR，使Llama-3.3-70B的多次尝试率增加四倍，这突显了理解这些抵抗机制对可控AI系统开发的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">From Core to Detail: Unsupervised Disentanglement with Entropy-Ordered Flows</div>
<div class="meta-line">Authors: Daniel Galperin, Ullrich Köthe</div>
<div class="meta-line">First: 2026-02-06T18:41:03+00:00 · Latest: 2026-02-06T18:41:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning unsupervised representations that are both semantically meaningful and stable across runs remains a central challenge in modern representation learning. We introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that orders latent dimensions by their explained entropy, analogously to PCA&#x27;s explained variance. This ordering enables adaptive injective flows: after training, one may retain only the top C latent variables to form a compact core representation while the remaining variables capture fine-grained detail and noise, with C chosen flexibly at inference time rather than fixed during training. EOFlows build on insights from Independent Mechanism Analysis, Principal Component Flows and Manifold Entropic Metrics. We combine likelihood-based training with local Jacobian regularization and noise augmentation into a method that scales well to high-dimensional data such as images. Experiments on the CelebA dataset show that our method uncovers a rich set of semantically interpretable features, allowing for high compression and strong denoising.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从核心到细节：基于熵排序流的无监督解缠</div>
<div class="mono" style="margin-top:8px">学习既具有语义意义又在多次运行中稳定的无监督表示仍然是现代表示学习中的一个核心挑战。我们引入了熵排序流（EOFlows），这是一种通过其解释熵对潜在维度进行排序的归一化流框架，类似于主成分分析（PCA）的解释方差。这种排序使得自适应单射流成为可能：训练后，可以仅保留前C个潜在变量以形成紧凑的核心表示，而其余变量则捕捉细粒度的细节和噪声，C在推理时灵活选择，而不是在训练期间固定。EOFlows建立在独立机制分析、主成分流和流形熵度量的见解之上。我们将基于似然的训练与局部雅可比正则化和噪声增强相结合，形成一种适用于高维数据（如图像）的方法。在CelebA数据集上的实验表明，我们的方法揭示了一组丰富的语义可解释特征，允许高压缩和强去噪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of learning unsupervised representations that are both semantically meaningful and stable across different runs. The authors introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that organizes latent dimensions based on their explained entropy, similar to PCA&#x27;s explained variance. Experimental results on the CelebA dataset demonstrate that EOFlows effectively reveal a diverse range of semantically interpretable features, enabling significant compression and robust denoising capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决学习既具有语义意义又在不同运行中保持稳定的无监督表示的挑战。作者提出了熵排序流（EOFlows），这是一种基于熵解释的潜在维度组织的归一化流框架，允许仅保留最重要的潜在变量以形成紧凑的核心表示，同时用剩余变量捕捉更细致的细节。对CelebA数据集的实验结果表明，EOFlows有效地揭示了一系列语义可解释的特征，实现了高压缩率和强去噪能力。</div>
</details>
</div>
<div class="card">
<div class="title">code_transformed: The Influence of Large Language Models on Code</div>
<div class="meta-line">Authors: Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen</div>
<div class="meta-line">First: 2025-06-13T17:59:39+00:00 · Latest: 2026-02-06T18:40:02+00:00</div>
<div class="meta-line">Comments: EACL 2026 Findings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12014v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12014v2">PDF</a> · <a href="https://github.com/ignorancex/LLM_code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 20,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake_case function names in Python code increased from 40.7% in Q1 2023 to 49.8% in Q3 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Our experimental results may provide the first large-scale empirical evidence that LLMs affect real-world programming style. We release all the experimental dataset and source code at: https://github.com/ignorancex/LLM_code</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代码转变：大型语言模型对代码的影响</div>
<div class="mono" style="margin-top:8px">编码仍然是人类与机器之间最基本的交互方式之一。随着大型语言模型（LLMs）的快速发展，代码生成能力开始显著重塑编程实践。这个发展引发了一个核心问题：LLMs是否改变了代码风格，这种转变如何被表征？在本文中，我们呈现了一项开创性的研究，调查LLMs对代码风格的影响，重点关注命名约定、复杂性、可维护性和相似性。通过分析与2020年至2025年间发表的arXiv论文相关的超过20,000个GitHub代码库，我们识别出与LLM生成代码特征一致的编码风格演变的可测量趋势。例如，Python代码中snake_case函数名称的比例从2023年第一季度的40.7%增加到2025年第三季度的49.8%。此外，我们通过检查LLMs的推理过程，研究它们如何处理算法问题。我们的实验结果可能提供了LLMs影响现实世界编程风格的首个大规模实证证据。我们在以下网址发布所有实验数据集和源代码：https://github.com/ignorancex/LLM_code</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to explore how Large Language Models (LLMs) are reshaping coding practices, particularly in terms of code style. The researchers analyzed code from over 20,000 GitHub repositories associated with arXiv papers published between 2020 and 2025 to identify trends in naming conventions, complexity, maintainability, and similarity. The findings reveal a significant increase in the use of snake_case function names in Python, rising from 40.7% in Q1 2023 to 49.8% in Q3 2025, providing empirical evidence that LLMs influence real-world programming style.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）对编码实践的影响，特别关注命名约定和复杂性等代码风格方面。研究人员分析了与2020年至2025年间发布的arXiv论文相关的超过20,000个GitHub代码库，以识别编码风格演变的趋势。主要发现表明，Python中使用snake_case函数名称的比例显著增加，从2023年第一季度的40.7%上升到2025年第三季度的49.8%，这表明LLMs正在影响现实世界的编程风格。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</div>
<div class="meta-line">Authors: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe</div>
<div class="meta-line">First: 2026-01-26T18:46:56+00:00 · Latest: 2026-02-06T18:38:32+00:00</div>
<div class="meta-line">Comments: Blog post: https://ssundaram21.github.io/soar/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18778v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18778v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ssundaram21.github.io/soar/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>教模型自我学习：在可学习性边缘的推理</div>
<div class="mono" style="margin-top:8px">模型能否学会逃离自身的学习停滞？强化学习方法在微调大型推理模型时，在初始成功率低的数据集上停滞，因此训练信号微弱。我们探讨一个基本问题：预训练的LLM能否利用潜在知识为无法解决的问题生成自动化课程？为此，我们设计了SOAR：一个通过元强化学习揭示这些教学信号的自我改进框架。模型的教师副本为学生副本提出合成问题，并根据其在一小部分难题上的改进获得奖励。关键是，SOAR将课程建立在测量的学生进展上，而不是内在的代理奖励。我们对最难的数学基准子集（0/128成功）进行的研究揭示了三个核心发现。首先，我们展示了实现双层元强化学习的可能性，该方法通过增强预训练模型生成有用的垫脚石的潜在能力，在稀疏的二元奖励下解锁学习。其次，基于进展的奖励优于先前LLM自我对弈中使用的内在奖励机制，可靠地避免了它们通常表现出的不稳定性和多样性崩溃模式。第三，分析生成的问题表明，结构质量和良好表述性对学习进展比解决正确性更为关键。我们的结果表明，生成有用的垫脚石的能力并不需要预先具备解决难题的能力，为在没有额外策划数据的情况下逃离推理停滞铺平了原则性道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of reinforcement learning methods stalling on datasets with low initial success rates, which limits the training signal for large reasoning models. The authors propose SOAR, a self-improvement framework that utilizes meta-reinforcement learning to enable a pretrained language model to generate an automated curriculum for unsolvable problems by leveraging latent knowledge. The key findings indicate that bi-level meta-RL can facilitate learning under sparse rewards, that grounded rewards are more effective than intrinsic rewards in promoting stability, and that the structural quality of generated questions is more important for learning progress than their correctness, suggesting a new approach to overcoming reasoning plateaus without needing additional curated data.</div>
<div class="mono" style="margin-top:8px">本研究探讨了预训练的大型语言模型（LLM）是否能够自主生成课程，以克服在强化学习场景中面临的学习停滞，特别是在初始成功率较低的情况下。作者提出了SOAR，一个自我改进框架，利用元强化学习使教师模型能够为学生模型提出合成问题，并根据学生在挑战性任务上的进步来奖励教师。主要发现表明，双层元强化学习可以在稀疏奖励下促进学习，基于实际进展的奖励比内在奖励更有效，且生成问题的结构质量对学习进展比其正确性更为重要。</div>
</details>
</div>
<div class="card">
<div class="title">Dataset Distillation as Pushforward Optimal Quantization</div>
<div class="meta-line">Authors: Hong Ye Tan, Emma Slade</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-13T20:41:52+00:00 · Latest: 2026-02-06T18:38:03+00:00</div>
<div class="meta-line">Comments: ICLR 2026, https://openreview.net/forum?id=FMSp8AUF3m</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.07681v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.07681v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose Dataset Distillation by Optimal Quantization, based on clustering in a latent space. Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better performance and inter-model generalization on the ImageNet-1K dataset with trivial additional computation, and SOTA performance in higher image-per-class settings. Using the distilled noise initializations in a stronger diffusion transformer model, we obtain SOTA distillation performance on ImageNet-1K and its subsets, outperforming diffusion guidance methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据集蒸馏作为推前最优量化</div>
<div class="mono" style="margin-top:8px">数据集蒸馏旨在找到一个合成训练集，使得在合成数据上训练的性能与在真实数据上训练的性能相似，同时计算需求减少几个数量级。现有方法大致可分为两类：一种是将神经网络训练启发式作为下层问题的双层优化问题，另一种是通过匹配数据分布来绕过双层优化的解耦方法。后者在训练和蒸馏数据集的规模方面具有速度和可扩展性的主要优势。我们证明，当配备编码器-解码器结构时，经验上成功的解耦方法可以重新表述为最优量化问题，其中通过最小化期望投影距离找到有限的点集来近似基础概率测度。特别地，我们将现有的解耦数据集蒸馏方法与经典的最优量化和Wasserstein重心问题联系起来，展示了蒸馏数据集在基于扩散的生成先验中的一致性。我们提出了基于潜在空间聚类的最优量化数据集蒸馏。与之前的SOTA方法D⁴M相比，我们在ImageNet-1K数据集上实现了更好的性能和模型间泛化，且额外计算量微不足道，在每类图像设置中实现了SOTA性能。使用蒸馏噪声初始化在更强的扩散变换器模型中，我们在ImageNet-1K及其子集上获得了SOTA蒸馏性能，超越了扩散引导方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve dataset distillation, which seeks to create synthetic training sets that replicate the performance of real datasets while significantly reducing computational costs. The authors propose a novel method that reformulates existing disentangled dataset distillation techniques as an optimal quantization problem, utilizing an encoder-decoder structure to minimize expected projection distances. Experimental results show that their approach, Dataset Distillation by Optimal Quantization, outperforms the previous state-of-the-art method D4M on the ImageNet-1K dataset, achieving superior performance and inter-model generalization with minimal additional computation, and setting new records in distillation performance for diffusion transformer models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善数据集蒸馏，旨在创建合成训练集，使其在计算资源显著减少的情况下与真实数据的性能相当。作者提出了一种方法，将现有的解耦数据集蒸馏技术重新表述为最优量化问题，利用编码器-解码器结构来最小化期望投影距离。实验结果表明，他们提出的基于最优量化的数据集蒸馏方法在ImageNet-1K数据集上优于之前的最先进方法D4M，实现了更好的性能和模型间泛化，且仅需极少的额外计算，在每类图像数量较高的设置中创造了新的性能记录。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Ali, Rachael Shaw, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-02-06T18:36:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向预测的深度强化学习在奶牛场高效电力负荷调度中的应用</div>
<div class="mono" style="margin-top:8px">奶牛养殖是一个能源密集型行业，严重依赖电网电力。随着可再生能源的不断整合，可持续能源管理已成为减少对电网依赖和支持联合国可持续发展目标7（可负担和清洁能源）的关键。然而，可再生能源的间歇性特征在实时平衡供需方面带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面显示出潜力。然而，大多数基于RL的调度方法假设对未来价格或发电有完全的了解，这在动态环境中是不现实的。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，通常导致在可变电价下训练不稳定。为了解决这些挑战，本研究提出了一种深度强化学习框架，用于奶牛场的高效负荷调度，重点关注在现实操作约束下的电池储存和水加热。所提出的面向预测的PPO结合了基于日时和月份的残差校准的短期需求和可再生发电预测，而PID KL PPO变体则采用比例-积分-微分控制器自适应地调节KL散度以实现稳定的策略更新。该方法在真实奶牛场数据上训练，电力成本比PPO低1%，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO将电网进口减少了13.1%，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance energy management in dairy farming, which is heavily reliant on grid electricity and faces challenges due to the intermittent nature of renewable energy sources. The study introduces a Deep Reinforcement Learning framework, specifically the Forecast Aware PPO, which incorporates short-term forecasts of demand and renewable generation, alongside a PID KL PPO variant that stabilizes policy updates. Experimental results indicate that this approach can reduce electricity costs by up to 1% compared to standard PPO, 4.8% compared to DQN, and 1.5% compared to SAC, while also achieving a 13.1% reduction in grid imports for battery scheduling, demonstrating its effectiveness in sustainable energy management.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善奶牛养殖业的能源管理，该行业是一个能源密集型领域，面临可再生能源间歇性带来的挑战。研究提出了一种深度强化学习框架，特别是预测感知PPO，利用需求和可再生发电的短期预测来改善负载调度，同时解决传统强化学习方法假设对未来条件完全了解的局限性。实验结果表明，该方法在电力成本上比标准PPO低1%，比DQN低4.8%，比SAC低1.5%，同时在电池调度方面减少了13.1%的电网进口，证明了其在奶牛养殖业可持续能源管理中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics</div>
<div class="meta-line">Authors: Zuyuan Zhang, Sizhe Tang, Tian Lan</div>
<div class="meta-line">First: 2026-02-06T18:35:41+00:00 · Latest: 2026-02-06T18:35:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06939v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06939v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越马尔可夫动态的时间差信号的协同视角</div>
<div class="mono" style="margin-top:8px">非马尔可夫动态在现实环境中常见，因其存在长程依赖、部分可观测性和记忆效应。作为强化学习（RL）核心支柱的贝尔曼方程在非马尔可夫情况下仅近似有效。现有工作通常集中于实用算法设计，对关键问题的理论处理有限，例如哪些动态确实可以被贝尔曼框架捕捉，以及如何激发具有最佳近似的新算法类别。本文提出了一种基于时间差（TD）的强化学习的新拓扑视角。我们展示了TD误差可以视为状态转移的拓扑空间中的1-协同，而马尔可夫动态则被解释为拓扑可积性。这一新视角使我们能够通过贝尔曼-德拉姆投影将TD误差分解为可积分量和拓扑残差。我们进一步提出了HodgeFlow策略搜索（HFPS），通过拟合潜在网络来最小化RL中的非可积投影残差，实现稳定性/敏感性保证。在数值评估中，HFPS在非马尔可夫情况下显著提高了RL性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenges posed by non-Markovian dynamics in real-world environments, which complicate the application of the Bellman equation in reinforcement learning. The authors introduce a topological perspective on temporal-difference (TD) learning, interpreting TD errors as 1-cochains in the topological space of state transitions and establishing a connection between Markov dynamics and topological integrability. Their method, HodgeFlow Policy Search (HFPS), involves fitting a potential network to minimize the non-integrable projection residual, and experimental results demonstrate that HFPS significantly enhances reinforcement learning performance in non-Markovian settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决贝尔曼方程在强化学习（RL）中应用于非马尔可夫动态时的局限性，这在现实世界场景中普遍存在。作者引入了对时间差（TD）信号的拓扑视角，将TD误差解释为状态转移的拓扑空间中的1-共链，并建立了这些误差的霍奇型分解。所提出的霍奇流策略搜索（HFPS）方法通过最小化不可积投影残差，在数值评估中显示出在非马尔可夫环境中显著提高了RL性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reliable Mislabel Detection for Video Capsule Endoscopy Data</div>
<div class="meta-line">Authors: Julia Werner, Julius Oexle, Oliver Bause, Maxime Le Floch, Franz Brinkmann, Hannah Tolle, Jochen Hampe, Oliver Bringmann</div>
<div class="meta-line">First: 2026-02-06T18:33:12+00:00 · Latest: 2026-02-06T18:33:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06938v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06938v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频胶囊内窥镜数据的可靠误标检测</div>
<div class="mono" style="margin-top:8px">深度神经网络的分类性能在很大程度上依赖于获取大型、准确标注的数据集。然而，在医学影像中，获取这样的数据集尤其具有挑战性，因为标注必须由专业医生提供，这严重限制了标注者的池。此外，类别边界往往模糊或难以定义，这进一步复杂化了基于机器学习的分类。在本文中，我们旨在解决这个问题，并提出一个用于医学数据集误标检测的框架。该框架在两个最大的公开可用的视频胶囊内窥镜数据集上进行了验证，这是一种基于低分辨率图像视频流检查胃肠道的重要成像程序。此外，我们的管道识别的潜在误标样本由三位经验丰富的胃肠病专家进行了审查和重新标注。我们的结果表明，所提出的框架成功检测到错误标记的数据，并在清理数据集后，相较于当前基线，改善了异常检测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the challenge of obtaining large, accurately annotated datasets for deep learning in medical imaging, particularly for Video Capsule Endoscopy, where annotations require specialized expertise. The authors propose a framework for detecting mislabeled data in medical datasets, which they validate using two large publicly available Video Capsule Endoscopy datasets. The key experimental findings indicate that their framework effectively identifies incorrectly labeled samples, and subsequent cleaning of the datasets leads to enhanced anomaly detection performance compared to existing baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于医学影像中获取大型准确标注数据集的挑战，特别是由于需要专业医生的标注和类别边界的模糊性。作者提出了一种专门针对视频胶囊内窥镜数据集的错误标注检测框架，这对于胃肠道检查至关重要。对两个大型公开数据集的实验验证表明，该框架有效识别错误标注样本，随后由胃肠病专家重新标注后，异常检测性能相比现有方法得到了改善。</div>
</details>
</div>
<div class="card">
<div class="title">Reciprocal Latent Fields for Precomputed Sound Propagation</div>
<div class="meta-line">Authors: Hugo Seuté, Pranai Vasudev, Etienne Richan, Louis-Xavier Buffoni</div>
<div class="meta-line">First: 2026-02-06T18:31:11+00:00 · Latest: 2026-02-06T18:31:11+00:00</div>
<div class="meta-line">Comments: Temporary pre-print, will be updated. In review at a conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06937v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06937v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预计算声传播的互惠潜在场</div>
<div class="mono" style="margin-top:8px">真实的声传播对于虚拟场景的沉浸感至关重要，但物理准确的基于波的模拟在实时应用中仍然计算成本高昂。波编码方法通过预计算和压缩给定场景的脉冲响应，将其转化为一组标量声学参数，从而解决了这一限制，但在具有多个源-接收对的大型环境中，这些参数的大小可能变得难以管理。我们引入了互惠潜在场（RLF），这是一个高效的内存框架，用于编码和预测这些声学参数。RLF框架采用可训练的潜在嵌入的体积网格，通过对称函数解码，确保声学的互惠性。我们研究了多种解码器，并表明利用黎曼度量学习可以更好地再现复杂场景中的声学现象。实验验证表明，RLF在减少内存占用几个数量级的同时，保持了复制质量。此外，类似MUSHRA的主观听觉测试表明，通过RLF渲染的声音在感知上与真实模拟无差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to achieve realistic sound propagation in virtual scenes while overcoming the computational limitations of wave-based simulations for real-time applications. The authors propose a novel framework called Reciprocal Latent Fields (RLF), which utilizes a volumetric grid of trainable latent embeddings decoded with a symmetric function to efficiently encode and predict acoustic parameters. Experimental results show that RLF significantly reduces memory usage while maintaining high replication quality of acoustic phenomena, and subjective listening tests reveal that sounds rendered using RLF are perceptually indistinguishable from ground-truth simulations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于实现虚拟环境中的真实声传播，同时克服波动模拟在实时应用中的计算挑战。作者提出了一种新颖的框架，称为互惠潜在场（RLF），该框架利用可训练的潜在嵌入的体积网格，通过对称函数有效地编码和预测声学参数。实验结果表明，RLF显著减少了内存使用，同时保持了声学现象的高复制质量，主观听觉测试表明，RLF产生的声音在感知上与传统的真实模拟无差异。</div>
</details>
</div>
<div class="card">
<div class="title">Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI</div>
<div class="meta-line">Authors: Ehud Shapiro</div>
<div class="meta-line">First: 2026-02-06T18:30:11+00:00 · Latest: 2026-02-06T18:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.
  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.
  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a concurrent logic programming language that supports distributed systems, particularly for grassroots platforms like smartphones. The authors developed Grassroots Logic Programs (GLP) and its multiagent variant (maGLP), defining their operational semantics through transition systems. Key findings include the successful implementation of deterministic operational semantics for both single-agent and multiagent GLP, with the dGLP and madGLP frameworks being validated against their respective operational semantics, enabling AI-driven implementations on workstations and smartphones.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现草根逻辑程序（GLP），这是一种为分布式系统设计的并发逻辑编程语言，特别用于智能手机上的点对点通信。作者使用转移系统开发了单代理（dGLP）和多代理（madGLP）版本的确定性操作语义，并确保其与原始操作语义的一致性。主要发现包括成功实现了用于工作站应用的dGLP和用于智能手机应用的madGLP，证明了使用人工智能创建指导这些实现的形式规范的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">When RL Meets Adaptive Speculative Training: A Unified Training-Serving System</div>
<div class="meta-line">Authors: Junxiong Wang, Fengxiang Bie, Jisen Li, Zhongzhu Zhou, Zelei Shao, Yubo Wang, Yinghui Liu, Qingyang Wu, Avner May, Sri Yanamandra, Yineng Zhang, Ce Zhang, Tri Dao, Percy Liang, Ben Athiwaratkun, Shuaiwen Leon Song, Chenfeng Xu, Xiaoxia Wu</div>
<div class="meta-line">First: 2026-02-06T18:28:54+00:00 · Latest: 2026-02-06T18:28:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06932v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06932v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.
  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当强化学习遇上自适应推测训练：统一的训练-服务系统</div>
<div class="mono" style="margin-top:8px">推测解码可以显著加速大语言模型的服务，但目前大多数部署将推测器训练与服务分开，视推测器训练为独立的离线建模问题。我们表明，这种解耦的形式引入了显著的部署和适应延迟：（1）高服务时间，因为推测器必须在部署前离线训练相当长的时间；（2）延迟的效用反馈，因为真正的端到端解码加速只有在训练后才能得知，且无法仅通过接受率可靠推断，因为存在模型架构和系统级的开销；（3）领域漂移退化，因为目标模型被重新用于新领域，推测器变得过时且效果降低。为了解决这些问题，我们提出了Aurora，一个统一的训练-服务系统，通过直接从实时推理轨迹中持续学习推测器来闭合循环。Aurora将在线推测器学习重新框架为一个异步强化学习问题：接受的标记提供正反馈，而被拒绝的推测器提案提供隐式负反馈，我们利用这些反馈来提高样本效率。我们的设计将基于SGLang的推理服务器与异步训练服务器集成，使得推测器更新可以热交换而不影响服务。关键是，Aurora支持零日部署：推测器可以立即提供服务并快速适应实时流量，提高系统性能，同时提供即时的效用反馈。在实验中，Aurora在最近发布的前沿模型（例如，MiniMax M2.1 229B和Qwen3-Coder-Next 80B）上实现了1.5倍的零日加速。Aurora还有效适应用户流量中的分布变化，在广泛使用的模型（例如，Qwen3和Llama3）上提供了比经过良好训练但静态的推测器额外1.25倍的加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies in deploying speculative decoding for large language models (LLMs), which often leads to high latency and adaptation issues due to the decoupling of speculator training from serving. The authors propose Aurora, a unified training-serving system that utilizes asynchronous reinforcement learning to continuously learn from live inference traces, allowing for immediate deployment and adaptation of speculators. Experimental results demonstrate that Aurora achieves a 1.5x speedup on day-0 deployment of frontier models and an additional 1.25x speedup in adapting to distribution shifts in user traffic compared to static speculators.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLM）中投机解码部署的低效问题，这种低效通常导致延迟和有效性降低，因为投机训练与服务的脱钩。作者提出了Aurora，一个统一的训练-服务系统，利用异步强化学习从实时推理轨迹中持续学习，允许投机者的即时部署和适应。实验结果表明，Aurora在新模型的第0天部署中实现了1.5倍的加速，并在适应用户流量的分布变化时，相较于静态投机者实现了1.25倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation</div>
<div class="meta-line">Authors: Davide Ettori, Nastaran Darabi, Sureshkumar Senthilkumar, Amit Ranjan Trivedi</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-19T07:53:55+00:00 · Latest: 2026-02-06T18:25:36+00:00</div>
<div class="meta-line">Comments: 5 pages, submitted to ICASSP 2026, September 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15724v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.15724v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RMT-KD：随机矩阵理论因果知识蒸馏</div>
<div class="mono" style="margin-top:8px">大型深度学习模型如BERT和ResNet在性能上达到最先进水平，但由于其规模和计算需求，在边缘部署时成本高昂。我们提出了RMT-KD，这是一种利用随机矩阵理论（RMT）进行知识蒸馏的压缩方法，旨在迭代减少网络规模。RMT-KD不采用剪枝或启发式秩选择，而是保留通过隐藏表示的谱特性识别的有信息方向。基于RMT的因果降维逐层应用自蒸馏，以保持稳定性和准确性。在GLUE和CIFAR-10上，RMT-KD实现了高达80%的参数减少，仅损失2%的准确性，推理速度提高2.8倍，功耗几乎减半。这些结果确立了RMT-KD作为一种数学基础的网络蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the deployment challenges of large deep learning models like BERT and ResNet, which are resource-intensive. The authors introduce RMT-KD, a novel compression method that utilizes Random Matrix Theory for knowledge distillation, focusing on preserving only the informative directions of hidden representations rather than traditional pruning methods. Experimental results on the GLUE and CIFAR-10 datasets demonstrate that RMT-KD can achieve up to 80% reduction in model parameters with only a 2% loss in accuracy, resulting in 2.8 times faster inference and nearly 50% lower power consumption.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型深度学习模型（如BERT和ResNet）在部署时面临的资源消耗问题。作者提出了一种新颖的压缩方法RMT-KD，该方法利用随机矩阵理论进行知识蒸馏，重点保留网络中的信息方向，而不是传统的剪枝技术。实验结果表明，在GLUE和CIFAR-10数据集上，RMT-KD可以实现高达80%的模型参数减少，仅损失2%的准确率，同时实现2.8倍的推理速度提升和近50%的功耗降低。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous-time reinforcement learning: ellipticity enables model-free value function approximation</div>
<div class="meta-line">Authors: Wenlong Mou</div>
<div class="meta-line">First: 2026-02-06T18:25:33+00:00 · Latest: 2026-02-06T18:25:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06930v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics.
  Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续时间强化学习：椭圆性使得无模型值函数逼近成为可能</div>
<div class="mono" style="margin-top:8px">我们研究了用于控制具有离散时间观测和动作的连续时间马尔可夫扩散过程的离策略强化学习。我们考虑无模型算法，通过函数逼近直接从数据中学习值函数和优势函数，而不对动态过程做不切实际的结构假设。
利用扩散过程的椭圆性，我们建立了一类新的希尔伯特空间正定性和有界性性质，适用于贝尔曼算子。基于这些性质，我们提出了Sobolev-近似拟合$q$-学习算法，该算法通过迭代求解最小二乘回归问题来学习值函数和优势函数。我们推导了估计误差的oracle不等式，受以下因素控制：(i) 函数类的最佳逼近误差，(ii) 它们的局部复杂性，(iii) 指数衰减的优化误差，以及(iv) 数值离散化误差。这些结果表明，椭圆性是一个关键的结构性质，使得对于马尔可夫扩散的函数逼近强化学习与监督学习同样简单。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of off-policy reinforcement learning in controlling continuous-time Markov diffusion processes with discrete-time observations and actions, motivated by the need for model-free algorithms that can learn value and advantage functions directly from data. The authors introduce the Sobolev-prox fitted q-learning algorithm, which utilizes the ellipticity of the diffusions to establish new properties for Bellman operators and iteratively solves least-squares regression problems to learn these functions. Key experimental findings include the derivation of oracle inequalities for estimation error, demonstrating that the complexity of reinforcement learning with function approximation for Markov diffusions is comparable to that of supervised learning, thus highlighting the significance of ellipticity in this context.</div>
<div class="mono" style="margin-top:8px">本研究探讨了用于管理具有离散时间观测和动作的连续时间马尔可夫扩散过程的离线强化学习，动机在于需要能够直接从数据中学习价值和优势函数的无模型算法。作者提出了Sobolev-prox拟合q学习算法，该算法通过迭代解决最小二乘回归问题来近似这些函数，利用扩散的椭圆性建立了贝尔曼算子的新的性质。主要发现包括估计误差的oracle不等式，这些不等式依赖于近似误差和优化误差等因素，表明椭圆性简化了在强化学习中应用函数近似的过程，使其与监督学习相当。</div>
</details>
</div>
<div class="card">
<div class="title">EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</div>
<div class="meta-line">Authors: Davide Ettori, Nastaran Darabi, Sina Tayebati, Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, Amit Ranjan Trivedi</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-19T08:05:28+00:00 · Latest: 2026-02-06T18:25:25+00:00</div>
<div class="meta-line">Comments: 5 pages, submitted to ICASSP 2026, September 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15735v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.15735v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenTrack：用于LLMs和VLMs中的幻觉和分布外检测的谱激活特征跟踪</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）具有广泛的实用性，但仍然容易出现幻觉和分布外（OOD）错误。我们提出了EigenTrack，这是一种可解释的实时检测器，利用隐藏激活的谱几何，这是模型动态的紧凑全局特征。通过将熵、特征值间隙和KL散度等协方差谱统计信息从随机基线流入轻量级递归分类器，EigenTrack跟踪表示结构中的时间变化，提前识别幻觉和OOD漂移，防止表面错误的出现。与黑箱和灰箱方法不同，它只需一次前向传递而无需重采样。与现有的白箱检测器不同，它保留时间上下文，聚合全局信号，并提供可解释的准确性-延迟权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issues of hallucination and out-of-distribution errors in large language models (LLMs). The authors propose a method called EigenTrack, which utilizes the spectral geometry of hidden activations to create a real-time detector that analyzes covariance-spectrum statistics. Key experimental findings indicate that EigenTrack can effectively track temporal shifts in representation structure, allowing it to detect potential errors before they manifest, while maintaining a lightweight design that requires only a single forward pass and preserves temporal context.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLMs）中的幻觉和分布外错误问题。作者提出了EigenTrack，这是一种实时检测方法，利用隐藏激活的谱几何来监测模型动态。主要实验结果表明，EigenTrack能够有效跟踪表示结构的时间变化，从而在保持可解释的准确性和延迟权衡的同时，提前检测幻觉和分布外漂移，仅需一次前向传递。</div>
</details>
</div>
<div class="card">
<div class="title">WAFT: Warping-Alone Field Transforms for Optical Flow</div>
<div class="meta-line">Authors: Yihan Wang, Jia Deng</div>
<div class="meta-line">First: 2025-06-26T17:47:59+00:00 · Latest: 2026-02-06T18:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21526v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.21526v3">PDF</a> · <a href="https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT">Code1</a> · <a href="https://github.com/princeton-vl/WAFT">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Warping-Alone Field Transforms (WAFT), a simple and effective method for optical flow. WAFT is similar to RAFT but replaces cost volume with high-resolution warping, achieving better accuracy with lower memory cost. This design challenges the conventional wisdom that constructing cost volumes is necessary for strong performance. WAFT is a simple and flexible meta-architecture with minimal inductive biases and reliance on custom designs. Compared with existing methods, WAFT ranks 1st on Spring, Sintel, and KITTI benchmarks, achieves the best zero-shot generalization on KITTI, while being 1.3-4.1x faster than existing methods that have competitive accuracy (e.g., 1.3x than Flowformer++, 4.1x than CCMR+). Code and model weights are available at \href{https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WAFT：用于光流的单独变形场变换</div>
<div class="mono" style="margin-top:8px">我们介绍了单独变形场变换（WAFT），这是一种简单有效的光流方法。WAFT 类似于 RAFT，但用高分辨率变形替代了成本体积，以更低的内存成本实现更好的准确性。该设计挑战了构建成本体积是强大性能所必需的传统观念。WAFT 是一种简单灵活的元架构，具有最小的归纳偏见和对自定义设计的依赖。与现有方法相比，WAFT 在 Spring、Sintel 和 KITTI 基准测试中排名第一，在 KITTI 上实现了最佳的零样本泛化，同时比现有具有竞争力准确性的算法快 1.3-4.1 倍（例如，比 Flowformer++ 快 1.3 倍，比 CCMR+ 快 4.1 倍）。代码和模型权重可在 \href{https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT} 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to improve optical flow estimation by challenging the necessity of constructing cost volumes for achieving strong performance. The authors introduce Warping-Alone Field Transforms (WAFT), a method that utilizes high-resolution warping instead of cost volumes, leading to better accuracy and lower memory usage. Experimental results show that WAFT outperforms existing methods, ranking first on the Spring, Sintel, and KITTI benchmarks, achieving the best zero-shot generalization on KITTI, and demonstrating a speed advantage of 1.3-4.1 times over competitive methods with similar accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过挑战传统对成本体的依赖来改善光流估计。作者提出了仅依赖变形场变换（WAFT）的方法，该方法利用高分辨率变形而不是成本体，从而提高了准确性并减少了内存使用。实验结果表明，WAFT在Spring、Sintel和KITTI基准测试中表现优异，获得第一名，在KITTI上实现了最佳的零样本泛化，并且比具有相似准确性的竞争方法快1.3到4.1倍。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Group Relative Policy Optimization</div>
<div class="meta-line">Authors: Roger Girgis, Rodrigue de Schaetzen, Luke Rowe, Azalée Robitaille, Christopher Pal, Liam Paull</div>
<div class="meta-line">First: 2026-02-05T16:44:23+00:00 · Latest: 2026-02-06T18:22:59+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05863v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受限组相对策略优化</div>
<div class="mono" style="margin-top:8px">虽然组相对策略优化（GRPO）已成为一种可扩展的无评论策略学习框架，但将其扩展到具有明确行为约束的设置仍然未被充分探索。我们引入了受限GRPO，这是GRPO在受限策略优化中的拉格朗日扩展。约束通过指示成本函数指定，允许通过拉格朗日松弛直接优化违反率。我们表明，优势估计中的简单多组件处理可能会破坏受限学习：不匹配的组件标准差扭曲了不同目标项的相对重要性，从而破坏了拉格朗日信号，阻碍了有意义的约束执行。我们正式推导了这一效应，以激励我们的标量优势构造，保持奖励和约束项之间的预期权衡。在一个玩具网格世界中的实验确认了预测的优化病理，并展示了标量化优势恢复了稳定的约束控制。此外，我们在机器人任务上评估了受限GRPO，它在提高任务成功率的同时改善了约束满足，为在日益依赖大型多模态基础模型的具身AI领域中的受限策略优化建立了简单有效的方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to extend Group Relative Policy Optimization (GRPO) to settings with explicit behavioral constraints, which has not been thoroughly explored. The authors introduce Constrained GRPO, a Lagrangian-based method that allows for constrained policy optimization by using indicator cost functions to directly optimize violation rates. Experimental results in a toy gridworld demonstrate that naive multi-component advantage estimation can disrupt constrained learning, while their proposed scalarized advantage construction effectively restores stable constraint control. Additionally, evaluations on robotics tasks show that Constrained GRPO enhances constraint satisfaction and increases task success, providing a practical approach for constrained policy optimization in embodied AI applications reliant on large multimodal foundation models.</div>
<div class="mono" style="margin-top:8px">本研究解决了将群体相对策略优化（GRPO）扩展到具有明确行为约束的场景的挑战，这一领域尚未得到充分探索。作者提出了约束GRPO，这是一种基于拉格朗日的方法，利用指示成本函数来优化违反率。玩具网格世界中的实验结果表明，不当的优势估计会破坏约束学习，但所提出的标量化优势构造有效地恢复了稳定的约束控制。此外，在机器人任务上的测试表明，约束GRPO提高了约束满足率，同时改善了任务成功率，为依赖大型多模态模型的具身人工智能应用提供了一种实用的约束策略优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Robustness Beyond Known Groups with Low-rank Adaptation</div>
<div class="meta-line">Authors: Abinitha Gourabathina, Hyewon Jeong, Teya Bergamaschi, Marzyeh Ghassemi, Collin Stultz</div>
<div class="meta-line">First: 2026-02-06T18:18:13+00:00 · Latest: 2026-02-06T18:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06924v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning models trained to optimize average accuracy often exhibit systematic failures on particular subpopulations. In real world settings, the subpopulations most affected by such disparities are frequently unlabeled or unknown, thereby motivating the development of methods that are performant on sensitive subgroups without being pre-specified. However, existing group-robust methods typically assume prior knowledge of relevant subgroups, using group annotations for training or model selection. We propose Low-rank Error Informed Adaptation (LEIA), a simple two-stage method that improves group robustness by identifying a low-dimensional subspace in the representation space where model errors concentrate. LEIA restricts adaptation to this error-informed subspace via a low-rank adjustment to the classifier logits, directly targeting latent failure modes without modifying the backbone or requiring group labels. Using five real-world datasets, we analyze group robustness under three settings: (1) truly no knowledge of subgroup relevance, (2) partial knowledge of subgroup relevance, and (3) full knowledge of subgroup relevance. Across all settings, LEIA consistently improves worst-group performance while remaining fast, parameter-efficient, and robust to hyperparameter choice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越已知群体的鲁棒性与低秩适应</div>
<div class="mono" style="margin-top:8px">深度学习模型在优化平均准确率时，往往在特定子群体上表现出系统性失败。在现实世界中，受此类差异影响最大的子群体通常是未标记或未知的，这促使开发在敏感子群体上表现良好的方法，而无需预先指定。然而，现有的群体鲁棒性方法通常假设对相关子群体有先验知识，使用群体注释进行训练或模型选择。我们提出了低秩误差信息适应（LEIA），这是一种简单的两阶段方法，通过识别表示空间中模型错误集中所在的低维子空间来提高群体鲁棒性。LEIA通过对分类器logits进行低秩调整，将适应限制在这个误差信息子空间，直接针对潜在的失败模式，而无需修改主干网络或要求群体标签。使用五个真实世界数据集，我们在三种设置下分析群体鲁棒性：（1）对子群体相关性完全没有知识，（2）对子群体相关性部分了解，以及（3）对子群体相关性完全了解。在所有设置中，LEIA始终提高最差群体的表现，同时保持快速、参数高效，并对超参数选择具有鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the systematic failures of deep learning models on specific subpopulations, particularly when these subpopulations are unlabeled or unknown. The authors propose a novel method called Low-rank Error Informed Adaptation (LEIA), which is a two-stage approach that enhances group robustness by identifying a low-dimensional subspace in the representation space where model errors are concentrated. Experimental results across five real-world datasets demonstrate that LEIA significantly improves worst-group performance in scenarios with no, partial, and full knowledge of subgroup relevance, while also being efficient in terms of speed and parameter usage.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度学习模型在特定子群体上系统性失败的问题，尤其是在这些子群体未标记或未知的情况下。作者提出了一种名为低秩误差信息适应（LEIA）的两阶段方法，通过识别表示空间中模型错误集中所在的低维子空间来增强群体鲁棒性。五个真实世界数据集的实验结果表明，LEIA在不同的子群体知识水平下始终能提高最差群体的表现，同时保持高效性和对超参数选择的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers</div>
<div class="meta-line">Authors: Ziming Liu, Sophia Sanborn, Surya Ganguli, Andreas Tolias</div>
<div class="meta-line">First: 2026-02-06T18:17:37+00:00 · Latest: 2026-02-06T18:17:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06923v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06923v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on &quot;world models&quot; -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous &quot;AI Physicist&quot; approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively &quot;bake in&quot; the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从开普勒到牛顿：归纳偏见引导变压器中的学习世界模型</div>
<div class="mono" style="margin-top:8px">通用人工智能架构能否超越预测，发现支配宇宙的物理法则？真正的智能依赖于“世界模型”——因果抽象，使代理不仅能预测未来状态，还能理解潜在的支配动态。虽然之前的“人工智能物理学家”方法成功恢复了这些法则，但它们通常依赖于强大的领域特定先验，有效地“烘焙”了物理。相反，Vafa等人最近表明，通用变压器未能获取这些世界模型，尽管预测准确性高，但未能捕捉潜在的物理法则。我们通过系统地引入三种最小归纳偏见来弥补这一差距。我们表明，确保空间平滑性（通过将预测表述为连续回归）和稳定性（通过在嘈杂上下文中训练以减轻误差积累）使通用变压器超越之前的失败，学习到一致的开普勒世界模型，成功地将椭圆拟合到行星轨迹上。然而，真正的物理洞察需要第三种偏见：时间局部性。通过将注意力窗口限制在最近的过去——施加简单假设，即未来状态仅依赖于局部状态而非复杂历史——我们迫使模型放弃曲线拟合，发现牛顿力的表示。我们的结果表明，简单的架构选择决定了人工智能是成为曲线拟合者还是物理学家，标志着自动化科学发现的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether general-purpose AI architectures can move beyond mere prediction to uncover the physical laws of the universe, addressing limitations in previous AI Physicist approaches that relied on strong domain-specific priors. The authors introduce three minimal inductive biases into generic Transformers: ensuring spatial smoothness through continuous regression, stability by training with noisy contexts, and temporal locality by restricting the attention window to the immediate past. The findings reveal that these biases enable the Transformers to learn a coherent Keplerian world model and accurately fit ellipses to planetary trajectories, while also facilitating the discovery of Newtonian force representations, highlighting the impact of architectural choices on the AI&#x27;s ability to engage in scientific discovery.</div>
<div class="mono" style="margin-top:8px">本研究探讨了通用人工智能架构是否能够发展出理解宇宙物理法则的世界模型，而不仅仅是进行预测。作者为通用变换器引入了三种最小归纳偏置：通过连续回归确保空间平滑性，通过噪声上下文训练增强稳定性，以及通过限制注意力仅关注最近状态来强制时间局部性。实验结果表明，这些偏置使变换器能够学习一致的开普勒世界模型，并成功拟合行星轨迹的椭圆，同时也使模型发现牛顿力的表示，突显了人工智能在科学发现中架构选择的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</div>
<div class="meta-line">Authors: Samir Abdaljalil, Parichit Sharma, Erchin Serpedin, Hasan Kurban</div>
<div class="meta-line">First: 2026-02-06T18:16:09+00:00 · Latest: 2026-02-06T18:16:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06920v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06920v1">PDF</a> · <a href="https://huggingface.co/datasets/sabdalja/HalluVerse-M3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Halluverse-M^3：一个多任务多语言的LLM幻觉基准</div>
<div class="mono" style="margin-top:8px">大型语言模型中的幻觉仍然是一个持续的挑战，特别是在多语言和生成环境中，事实一致性难以维持。尽管最近的模型在以英语为中心的基准上表现强劲，但它们在不同语言、任务和幻觉类型上的表现尚不清楚。在本研究中，我们介绍了Halluverse-M^3，这是一个旨在系统分析多语言、多生成任务和多幻觉类别的幻觉数据集。Halluverse-M^3涵盖四种语言：英语、阿拉伯语、印地语和土耳其语，并支持两个生成任务：问答和对话摘要。该数据集明确区分实体级、关系级和句子级幻觉。幻觉输出通过受控编辑过程构建，并由人工注释者验证，确保原始内容与幻觉生成之间的清晰对齐。使用该数据集，我们评估了一组多样的现代开源和专有语言模型在细粒度幻觉检测上的表现。我们的结果表明，问答任务始终比对话摘要任务更容易，而句子级幻觉即使对于最强的模型也仍然具有挑战性。英语的表现最高，而在资源较少的语言中表现下降，印地语的检测准确率最低。总体而言，Halluverse-M^3为研究多语言、多任务环境中的幻觉提供了一个现实且具有挑战性的基准。我们发布该数据集以支持未来关于幻觉检测和缓解的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the persistent issue of hallucinations in large language models, particularly in multilingual and generative contexts where maintaining factual consistency is challenging. The authors introduce Halluverse-M^3, a dataset designed for systematic analysis of hallucinations across four languages—English, Arabic, Hindi, and Turkish—while supporting two generation tasks: question answering and dialogue summarization. Key findings indicate that question answering is generally easier than dialogue summarization, with sentence-level hallucinations proving difficult even for advanced models, and performance varies significantly across languages, with Hindi showing the lowest detection accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型中持续存在的幻觉问题，特别是在多语言和生成环境中，保持事实一致性具有挑战性。作者介绍了Halluverse-M^3，这是一个旨在系统分析四种语言（英语、阿拉伯语、印地语和土耳其语）中的幻觉的数据集，涵盖了两个生成任务：问答和对话摘要。主要发现表明，问答通常比对话摘要更容易，而句子级幻觉即使对于表现最好的模型也构成重大挑战；在英语中表现最佳，而在低资源语言中表现下降，印地语的检测准确率最低。</div>
</details>
</div>
<div class="card">
<div class="title">Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy</div>
<div class="meta-line">Authors: Sumit Kumar, Suraj Jaiswal, Parampreet Singh, Vipul Arora</div>
<div class="meta-line">First: 2026-02-06T18:15:36+00:00 · Latest: 2026-02-06T18:15:36+00:00</div>
<div class="meta-line">Comments: Under Review at Transactions of Audio Speech and Language Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06917v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>音乐教育中唱歌错误的自动检测与分析</div>
<div class="mono" style="margin-top:8px">音频分析中机器学习的进步为技术增强的音乐教育开辟了新可能。本文介绍了一个在音乐教育背景下自动检测唱歌错误的框架，支持一个新整理的数据集。该数据集包含同步的教师与学习者的声乐录音，并标注了学习者所犯的不同类型的错误。利用该数据集，我们开发了不同的深度学习模型用于错误检测并进行了基准测试。为了比较错误检测系统的有效性，提出了一种新的评估方法。实验表明，所提出的基于学习的方法优于基于规则的方法。对错误的系统研究和跨教师研究揭示了可用于各种音乐应用的音乐教育见解。这项工作为音乐教育的研究开辟了新方向。代码和数据集已公开可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the potential of machine learning to enhance music education through technology. The authors introduce a framework for automatic detection of singing mistakes, utilizing a newly curated dataset of synchronized vocal recordings from teachers and learners, annotated with various mistake types. Experimental results demonstrate that the proposed deep learning models for mistake detection outperform traditional rule-based methods, providing valuable insights into music pedagogy that can inform future applications in the field.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过技术增强音乐教育，实现自动检测唱歌错误。作者开发了一个框架，利用新创建的同步教师-学习者声乐录音数据集，其中包含各种错误类型的注释。实验结果表明，所提出的深度学习模型在错误检测方面优于传统的基于规则的方法，为音乐教育提供了有价值的见解，可应用于不同的音乐应用。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Beyond Redundancy: Task Complexity&#x27;s Role in Vision Token Specialization in VLLMs</div>
<div class="meta-line">Authors: Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</div>
<div class="meta-line">First: 2026-02-06T18:13:01+00:00 · Latest: 2026-02-06T18:13:01+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06914v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越冗余的视野：任务复杂性在视觉大语言模型中的视觉标记专业化角色</div>
<div class="mono" style="margin-top:8px">视觉大语言模型（VLLMs）在视觉能力上始终落后于其语言能力。特别是，许多基准研究表明，当需要细粒度视觉信息或空间推理时，VLLMs表现不佳。然而，我们尚不清楚为什么VLLMs在这些任务上相较于其他任务如此挣扎。一些研究将视觉冗余作为解释，认为高层次视觉信息均匀分布在多个标记中，而特定的细粒度视觉信息被丢弃。在本研究中，我们更详细地探讨这一前提，旨在更好地理解模型如何处理各种类型的视觉信息以及哪些类型的视觉信息被丢弃。为此，我们引入了一个简单的合成基准数据集，专门构建用于探测各种视觉特征，并提供了一套用于测量视觉冗余的指标，使我们能够更好地理解它们之间的细微关系。然后，我们探索在多个复杂视觉任务上微调VLLMs，以更好地理解冗余和压缩如何根据模型训练的数据复杂性而变化。我们发现任务复杂性与视觉压缩之间存在联系，这意味着拥有足够比例的高复杂性视觉数据对于改变VLLMs分配其视觉表示的方式至关重要，从而提高其在复杂视觉任务上的表现。我们希望这项工作能为训练下一代VLLMs提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the underperformance of vision large language models (VLLMs) in processing fine-grained visual information and spatial reasoning compared to their linguistic capabilities. The authors introduce a synthetic benchmark dataset designed to assess various visual features and develop metrics for measuring visual redundancy. The key findings indicate that task complexity influences visual compression in VLLMs, suggesting that a higher ratio of complex visual data is essential for enhancing the models&#x27; visual representation and performance on intricate visual tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉大型语言模型（VLLMs）在处理细粒度视觉信息和空间推理方面的表现不佳，尤其是与其语言能力相比。作者引入了一个合成基准数据集，旨在分析各种视觉特征，并开发了评估视觉冗余的指标。主要发现表明，任务复杂性显著影响视觉压缩，暗示更高比例的复杂视觉数据对于提升VLLMs在复杂视觉任务中的表现至关重要。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
