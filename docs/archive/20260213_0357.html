<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 03:57</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0357</div>
    <div class="row"><div class="card">
<div class="title">Diffusion-Pretrained Dense and Contextual Embeddings</div>
<div class="meta-line">Authors: Sedigheh Eslami, Maksim Gaiduk, Markus Krimmel, Louis Milliken, Bo Wang, Denis Bykov</div>
<div class="meta-line">First: 2026-02-11T18:59:08+00:00 · Latest: 2026-02-11T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models&#x27; effectiveness in production environments where retrieval quality and efficiency are critical at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散预训练的稠密和上下文嵌入</div>
<div class="mono" style="margin-top:8px">在本报告中，我们介绍了pplx-embed，一个多语言嵌入模型系列，采用多阶段对比学习，基于扩散预训练的语言模型骨干进行网络规模检索。通过利用扩散预训练的双向注意力，我们的模型捕捉到段落内的全面双向上下文，使得使用均值池化和后期分块策略更好地保留长文档中的全局上下文。我们发布了两种模型类型：pplx-embed-v1用于标准检索，pplx-embed-context-v1用于将全局文档上下文纳入段落表示的上下文化嵌入。pplx-embed-v1在MTEB（多语言，v2）、MTEB（代码）、MIRACL、BERGEN和ToolRet检索基准上表现出色，而pplx-embed-context-v1在ConTEB基准上创下新纪录。除了公共基准外，pplx-embed-v1在我们的内部评估套件中表现强劲，该套件专注于数千万文档的真实世界大规模搜索场景。这些结果验证了模型在检索质量和效率至关重要的生产环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multilingual embedding models for web-scale retrieval by utilizing a diffusion-pretrained language model. The authors introduce pplx-embed, which employs multi-stage contrastive learning and bidirectional attention to capture comprehensive context in passages, using mean pooling and a late chunking strategy to maintain global context in long documents. The experimental results show that pplx-embed-v1 achieves competitive performance on various retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark, demonstrating strong effectiveness in real-world large-scale search scenarios involving millions of documents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多语言嵌入模型以实现有效的网络规模检索。作者介绍了pplx-embed，该模型利用多阶段对比学习和扩散预训练语言模型来捕捉段落中的全面双向上下文。实验结果表明，pplx-embed-v1在多个检索基准上表现出竞争力，而pplx-embed-context-v1在ConTEB基准上创下新纪录，展示了其在涉及数百万文档的真实大规模搜索场景中的强大能力。</div>
</details>
</div>
<div class="card">
<div class="title">YOR: Your Own Mobile Manipulator for Generalizable Robotics</div>
<div class="meta-line">Authors: Manan H Anjaria, Mehmet Enes Erciyes, Vedant Ghatnekar, Neha Navarkar, Haritheja Etukuru, Xiaole Jiang, Kanad Patel, Dhawal Kabra, Nicholas Wojno, Radhika Ajay Prayage, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui</div>
<div class="meta-line">First: 2026-02-11T18:59:00+00:00 · Latest: 2026-02-11T18:59:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://www.yourownrobot.ai/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR&#x27;s capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>YOR：您自己的通用机器人移动操控器</div>
<div class="mono" style="margin-top:8px">最近机器人学习的进展引发了对能够最终接近人类水平能力的平台的重大兴趣。这种兴趣，加上执行器的商品化，推动了低成本机器人平台的增长。然而，移动操控的最佳形态，尤其是在预算有限的情况下，仍然是一个未解的问题。我们介绍了YOR，一个开源、低成本的移动操控器，集成了全向底盘、伸缩垂直升降机和两个带夹具的手臂，以实现全身的移动和操控。我们的设计强调模块化、使用现成组件的组装简便性和经济性，材料成本低于10,000美元。我们通过完成需要协调全身控制、双手操控和自主导航的任务来展示YOR的能力。总体而言，YOR以现有平台成本的一小部分提供了竞争性的移动操控功能。项目网站：https://www.yourownrobot.ai/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a low-cost mobile manipulator that can achieve human-level competence in robotics, addressing the need for affordable platforms in the field. The authors introduce YOR, an open-source mobile manipulator featuring an omnidirectional base, a telescopic vertical lift, and two arms with grippers, designed for modularity and ease of assembly using readily available components. Experimental results demonstrate YOR&#x27;s effectiveness in performing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation, showcasing its competitive functionality at a significantly lower cost than existing robotic platforms.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种低成本的移动操纵器，以实现机器人领域的人类水平能力，满足移动操纵中对经济实惠平台的需求。作者介绍了YOR，这是一种开源移动操纵器，具有全向底座、可伸缩的垂直升降装置和两个带夹具的手臂，旨在实现模块化和使用现成组件的组装简便性。实验结果表明，YOR在执行需要协调全身控制、双手操作和自主导航的任务方面表现出色，展示了其在成本显著低于现有机器人平台的情况下的竞争功能。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling</div>
<div class="meta-line">Authors: Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao, Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo</div>
<div class="meta-line">First: 2026-02-11T18:57:29+00:00 · Latest: 2026-02-11T18:57:29+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/HKUST-C4G/diffusion-rm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11146v1">PDF</a> · <a href="https://github.com/HKUST-C4G/diffusion-rm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越基于VLM的奖励：扩散原生潜在奖励建模</div>
<div class="mono" style="margin-top:8px">扩散和流匹配模型的偏好优化依赖于既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLM）已成为主要的奖励提供者，利用其丰富的多模态先验来指导对齐。然而，它们的计算和内存成本可能相当高，通过像素空间奖励优化潜在扩散生成器会引入领域不匹配，复杂化对齐。在本文中，我们提出了DiNa-LRM，一种扩散原生潜在奖励模型，直接在噪声扩散状态上进行偏好学习。我们的方法引入了噪声校准的Thurstone似然，具有依赖于扩散噪声的不确定性。DiNa-LRM利用预训练的潜在扩散骨干网络，配备时间步条件的奖励头，并支持推理时的噪声集成，为测试时的扩散原生机制提供了缩放和鲁棒奖励。在图像对齐基准测试中，DiNa-LRM显著超越现有的基于扩散的奖励基线，并在计算成本的很小一部分下实现与最先进的VLM竞争的性能。在偏好优化中，我们证明了DiNa-LRM改善了偏好优化动态，使模型对齐更快且资源更高效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance preference optimization for diffusion and flow-matching models by developing a more efficient reward function that addresses the limitations of existing Vision-Language Models (VLMs). The authors propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states, utilizing a noise-calibrated Thurstone likelihood. Experimental results show that DiNa-LRM significantly outperforms current diffusion-based reward baselines in image alignment tasks and achieves competitive performance with state-of-the-art VLMs while being more computationally efficient, thus improving the dynamics of preference optimization for faster model alignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过开发一种既稳健又计算高效的奖励函数来增强扩散和流匹配模型的偏好优化，解决现有视觉语言模型（VLM）在计算和内存方面的局限性。作者提出了DiNa-LRM，一种扩散原生的潜在奖励模型，直接在噪声扩散状态上制定偏好学习，利用噪声校准的Thurstone似然和预训练的潜在扩散骨干网络与时间步条件的奖励头。实验结果表明，DiNa-LRM在图像对齐基准测试中显著优于现有的基于扩散的奖励基线，并在计算成本上更具资源效率，同时与最先进的VLMs的性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">SCRAPL: Scattering Transform with Random Paths for Machine Learning</div>
<div class="meta-line">Authors: Christopher Mitcheltree, Vincent Lostanlen, Emmanouil Benetos, Mathieu Lagrange</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-11T18:57:08+00:00 · Latest: 2026-02-11T18:57:08+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Code, audio samples, and Python package provided at https://christhetree.github.io/scrapl/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11145v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://christhetree.github.io/scrapl/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose &quot;Scattering transform with Random Paths for machine Learning&quot; (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCRAPL：用于机器学习的随机路径散射变换</div>
<div class="mono" style="margin-top:8px">小波散射变换系数（称为路径）之间的欧几里得距离为计算机视觉、语音和音频处理中的深度逆问题的感知质量评估提供了信息丰富的梯度。然而，由于路径数量众多，这些变换在作为随机梯度下降的可微损失函数时计算开销较大，显著限制了它们在神经网络训练中的应用。针对这一问题，我们提出了“用于机器学习的随机路径散射变换”（SCRAPL）：一种高效评估多变量散射变换的随机优化方案。我们为联合时频散射变换（JTFS）实现了SCRAPL，该变换在多个尺度和速率下解调声谱时间模式，允许对间歇性听觉纹理进行精细表征。我们将SCRAPL应用于可微数字信号处理（DDSP），具体来说，是对颗粒合成器和Roland TR-808鼓机的无监督声音匹配。我们还提出了一种基于重要性采样的初始化启发式方法，使SCRAPL适应数据集的感知内容，从而改善神经网络的收敛性和评估性能。我们提供了代码和音频样本，并将SCRAPL作为Python包发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of using wavelet scattering transform coefficients as differentiable loss functions in neural network training due to their computational expense. The authors introduce SCRAPL, a stochastic optimization method designed for efficient evaluation of multivariable scattering transforms, specifically implementing it for the joint time-frequency scattering transform (JTFS) to analyze auditory textures. Experimental results demonstrate that SCRAPL enhances the performance of unsupervised sound matching tasks, such as matching a granular synthesizer with the Roland TR-808 drum machine, and the proposed initialization heuristic based on importance sampling improves convergence and evaluation outcomes.</div>
<div class="mono" style="margin-top:8px">本研究解决了小波散射变换系数的高计算成本问题，这限制了它们在神经网络训练中用于感知质量评估的应用，尤其是在计算机视觉和音频处理等领域。作者提出了一种新方法，称为机器学习中的随机路径散射变换（SCRAPL），该方法提供了一种随机优化方案，以高效评估多变量散射变换。实验结果表明，SCRAPL在无监督声音匹配任务中应用于联合时频散射变换时，显著提高了神经网络的收敛性和评估性能，特别是结合了基于重要性采样的初始化启发式方法，该方法针对数据集的感知内容进行了调整。</div>
</details>
</div>
<div class="card">
<div class="title">GENIUS: Generative Fluid Intelligence Evaluation Suite</div>
<div class="meta-line">Authors: Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang</div>
<div class="meta-line">First: 2026-02-11T18:55:54+00:00 · Latest: 2026-02-11T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11144v1">PDF</a> · <a href="https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$">Code1</a> · <a href="https://github.com/arctanxarc/GENIUS">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GENIUS：生成流体智能评估套件</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在视觉生成方面取得了显著进展。然而，现有基准主要评估$\textit{晶体智能}$，这依赖于回忆积累的知识和学习的模式。这种关注忽视了$\textit{生成流体智能（GFI）}$：即在瞬息万变的场景中诱导模式、通过约束推理和适应新情境的能力。为了严格评估这一能力，我们引入了$\textbf{GENIUS}$（$\textbf{GEN}$流体$\textbf{I}$ntelligence评估$\textbf{U}$套件）。我们将$\textit{GFI}$形式化为三种原语的综合。这些原语包括$\textit{诱导隐式模式}$（例如，推断个性化视觉偏好）、$\textit{执行临时约束}$（例如，视觉化抽象隐喻）和$\textit{适应上下文知识}$（例如，模拟反直觉物理）。这些原语共同挑战模型解决完全基于即时上下文的问题。我们对12个代表性模型的系统评估揭示了这些任务中的显著性能缺陷。关键是，我们的诊断分析解开了这些失败模式，表明缺陷源于有限的上下文理解，而非内在生成能力不足。为了弥补这一差距，我们提出了一种无训练的注意力干预策略。最终，$\textbf{GENIUS}$为$\textit{GFI}$建立了严格的标准，引导该领域超越知识利用，迈向动态的通用推理。我们的数据集和代码将发布于：$\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing benchmarks that primarily evaluate Crystallized Intelligence in Unified Multimodal Models, neglecting the assessment of Generative Fluid Intelligence (GFI), which involves pattern induction and reasoning in novel contexts. The authors introduce GENIUS, a comprehensive evaluation suite that formalizes GFI through three key primitives: inducing implicit patterns, executing ad-hoc constraints, and adapting to contextual knowledge. Their evaluation of 12 models reveals significant performance deficits in GFI tasks, attributed to limited context comprehension rather than a lack of generative capability, leading to the proposal of a training-free attention intervention strategy to enhance model performance in dynamic reasoning scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有基准主要评估统一多模态模型中的晶体智力的局限性，而忽视了生成流体智力（GFI）的评估，后者对于动态推理和适应至关重要。作者提出了GENIUS，一个新的评估套件，通过三种原语形式化GFI：诱导隐含模式、执行临时约束和适应上下文知识。他们对12个模型的系统评估揭示了在需要GFI的任务中存在显著的性能缺陷，诊断分析表明这些缺陷源于有限的上下文理解，而非生成能力的不足。为了解决这个问题，他们提出了一种无训练的注意力干预策略，确立了GENIUS作为评估GFI的严格标准，推动动态推理能力的进步。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows</div>
<div class="meta-line">Authors: Shaswat Garg, Matin Moezzi, Brandon Da Silva</div>
<div class="meta-line">First: 2026-02-11T18:54:48+00:00 · Latest: 2026-02-11T18:54:48+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, IEEE International Conference on Robotics and Automation 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11142v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于归一化流的数据高效层次目标条件强化学习</div>
<div class="mono" style="margin-top:8px">层次目标条件强化学习（H-GCRL）通过将复杂的长期任务分解为结构化的子目标，提供了一个强大的框架。然而，其实际应用受到数据效率低下和策略表达能力有限的阻碍，尤其是在离线或数据稀缺的情况下。在本研究中，提出了一种基于归一化流的层次隐式Q学习（NF-HIQL）新框架，该框架在层次的高低层次上用表达能力强的归一化流策略替代单模高斯策略。该设计使得可处理的对数似然计算、高效采样以及建模丰富的多模态行为成为可能。推导出新的理论保证，包括对实值非体积保持（RealNVP）策略的显式KL散度界限和PAC风格的样本效率结果，表明NF-HIQL在提高泛化能力的同时保持稳定性。在实证方面，NF-HIQL在OGBench的运动、运球和多步操作等多种长期任务中进行了评估。NF-HIQL始终优于先前的目标条件和层次基线，展示了在有限数据下的卓越鲁棒性，并突显了基于流的架构在可扩展、数据高效的层次强化学习中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the practicality of hierarchical goal-conditioned reinforcement learning (H-GCRL) for complex tasks, which often suffer from poor data efficiency and limited policy expressivity. The authors introduce a novel framework called Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), which utilizes normalizing flow policies instead of unimodal Gaussian policies to improve performance at both high- and low-levels of the hierarchy. Experimental results demonstrate that NF-HIQL outperforms existing goal-conditioned and hierarchical methods across various long-horizon tasks, showing improved robustness and generalization, particularly in data-scarce environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高层次目标条件强化学习（H-GCRL）在复杂任务中的实际应用，因其常常受到数据效率和策略表现力的限制。作者提出了基于归一化流的层次隐式Q学习（NF-HIQL）框架，该框架利用归一化流策略来改善高低层策略的表现力。实验结果表明，NF-HIQL在多种长时间跨度任务中优于现有的目标条件和层次方法，展示了在数据稀缺环境下的更强鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">LCIP: Loss-Controlled Inverse Projection of High-Dimensional Image Data</div>
<div class="meta-line">Authors: Yu Wang, Frederik L. Dennig, Michael Behrisch, Alexandru Telea</div>
<div class="meta-line">First: 2026-02-11T18:52:46+00:00 · Latest: 2026-02-11T18:52:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11141v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Projections (or dimensionality reduction) methods $P$ aim to map high-dimensional data to typically 2D scatterplots for visual exploration. Inverse projection methods $P^{-1}$ aim to map this 2D space to the data space to support tasks such as data augmentation, classifier analysis, and data imputation. Current $P^{-1}$ methods suffer from a fundamental limitation -- they can only generate a fixed surface-like structure in data space, which poorly covers the richness of this space. We address this by a new method that can `sweep&#x27; the data space under user control. Our method works generically for any $P$ technique and dataset, is controlled by two intuitive user-set parameters, and is simple to implement. We demonstrate it by an extensive application involving image manipulation for style transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LCIP：高维图像数据的损失控制逆投影</div>
<div class="mono" style="margin-top:8px">投影（或降维）方法 $P$ 旨在将高维数据映射到通常的二维散点图中以进行视觉探索。逆投影方法 $P^{-1}$ 旨在将该二维空间映射回数据空间，以支持数据增强、分类器分析和数据插补等任务。目前的 $P^{-1}$ 方法存在一个根本性限制——它们只能在数据空间中生成固定的表面结构，无法很好地覆盖该空间的丰富性。我们通过一种新方法解决了这个问题，该方法可以在用户控制下“扫过”数据空间。我们的方法对任何 $P$ 技术和数据集通用，由两个直观的用户设置参数控制，且易于实现。我们通过涉及图像操作以进行风格迁移的广泛应用来演示它。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve inverse projection methods, which currently generate limited surface-like structures in high-dimensional data space, hindering their effectiveness for tasks like data augmentation and classifier analysis. The authors propose a new method called LCIP that allows for user-controlled sweeping of the data space, applicable to any projection technique and dataset, using two intuitive parameters. Experimental results demonstrate the method&#x27;s effectiveness through extensive applications in image manipulation for style transfer, showcasing its ability to better capture the richness of high-dimensional data space.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善当前只能生成有限表面结构的逆投影方法，这些方法无法充分代表高维数据空间的复杂性。作者提出了一种名为LCIP的新方法，该方法允许用户控制对数据空间的“扫掠”，并适用于任何投影技术和数据集。实验结果表明，LCIP在图像处理任务中，特别是在风格迁移方面的有效性，展示了其相比现有方法更好地捕捉高维数据丰富性的能力。</div>
</details>
</div>
<div class="card">
<div class="title">AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models</div>
<div class="meta-line">Authors: R E Zera Marveen Lyngkhoi, Chirag Chawla, Pratinav Seth, Utsav Avaiya, Soham Bhattacharjee, Mykola Khandoga, Rui Yuan, Vinay Kumar Sankarapu</div>
<div class="meta-line">First: 2026-02-10T10:08:51+00:00 · Latest: 2026-02-11T18:51:19+00:00</div>
<div class="meta-line">Comments: Library opensource and available at https://github.com/Lexsi-Labs/aligntune</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09621v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09621v2">PDF</a> · <a href="https://github.com/Lexsi-Labs/aligntune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlignTune：大型语言模型后训练对齐的模块化工具包</div>
<div class="mono" style="margin-top:8px">后训练对齐是部署大型语言模型（LLMs）的核心，但实际工作流程仍然分散在特定后端工具和临时粘合代码中，使实验难以重现。我们将后端干扰、奖励碎片化和不可重现的管道视为对齐研究的主要障碍。我们介绍了AlignTune，一个模块化工具包，提供统一的接口用于监督微调（SFT）和RLHF风格的优化，支持可互换的TRL和Unsloth后端。AlignTune标准化配置，提供可扩展的奖励层（基于规则和学习），并集成对标准基准和自定义任务的评估。通过将特定后端逻辑隔离在单一工厂边界后，AlignTune实现了受控比较和可重现的对齐实验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in post-training alignment of large language models (LLMs), which are hindered by backend interference, reward fragmentation, and irreproducible workflows. The authors present AlignTune, a modular toolkit that offers a unified interface for supervised fine-tuning and reinforcement learning from human feedback (RLHF) using interchangeable backends. Key experimental findings demonstrate that AlignTune standardizes configurations, provides an extensible reward layer, and facilitates reproducible alignment experiments, thereby improving the reliability of comparisons across different alignment methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）后训练对齐中的挑战，这些挑战受到特定后端工具和不可重复工作流程的阻碍。作者提出了AlignTune，一个模块化工具包，提供了一个统一的接口，用于监督微调和基于人类反馈的强化学习，同时允许可互换的后端。主要实验结果表明，AlignTune标准化了配置，提供了可扩展的奖励层，并通过在各种基准和任务中进行受控比较，促进了可重复的对齐实验。</div>
</details>
</div>
<div class="card">
<div class="title">TabICLv2: A better, faster, scalable, and open tabular foundation model</div>
<div class="meta-line">Authors: Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan</div>
<div class="meta-line">First: 2026-02-11T18:51:02+00:00 · Latest: 2026-02-11T18:51:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11139v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11139v1">PDF</a> · <a href="https://github.com/soda-inria/tabicl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TabICLv2：一个更好、更快、可扩展的开放表格基础模型</div>
<div class="mono" style="margin-top:8px">表格基础模型，如TabPFNv2和TabICL，最近在预测基准中取代了梯度提升树，展示了上下文学习在表格数据中的价值。我们介绍了TabICLv2，这是一个新的最先进的回归和分类基础模型，建立在三个支柱上：（1）一个新颖的合成数据生成引擎，旨在实现高预训练多样性；（2）各种架构创新，包括一种新的可扩展softmax注意力机制，改善了对更大数据集的泛化，而无需昂贵的长序列预训练；（3）优化的预训练协议，特别是用Muon优化器替代AdamW。在TabArena和TALENT基准上，TabICLv2在没有任何调优的情况下超越了当前最先进的模型RealTabPFN-2.5（经过超参数调优、集成和在真实数据上微调）。仅用适度的预训练计算，TabICLv2在50GB GPU内存下有效地泛化到百万规模的数据集，同时明显快于RealTabPFN-2.5。我们提供了广泛的消融研究来量化这些贡献，并承诺开放研究，首先在https://github.com/soda-inria/tabicl发布推理代码和模型权重，合成数据引擎和预训练代码随后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of tabular foundation models, which have recently outperformed traditional methods like gradient-boosted trees in predictive tasks. The authors introduce TabICLv2, which incorporates a novel synthetic data generation engine, architectural innovations including a scalable softmax for better generalization, and optimized pretraining protocols using the Muon optimizer instead of AdamW. Experimental results show that TabICLv2 outperforms the current state-of-the-art model, RealTabPFN-2.5, on the TabArena and TALENT benchmarks without any tuning, while effectively generalizing to large datasets with limited computational resources and demonstrating significantly faster performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高表格基础模型的性能，这些模型最近在预测任务中超越了传统的梯度提升树方法。作者介绍了TabICLv2，这是一种新模型，结合了用于多样化预训练的合成数据生成引擎、可扩展的注意力软最大值等架构创新，以及使用Muon优化器的优化预训练协议。实验结果表明，TabICLv2在TabArena和TALENT基准测试中未经过任何调优就超越了当前的最先进模型RealTabPFN-2.5，展示了在有限计算资源下对大规模数据集的有效泛化，同时速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification</div>
<div class="meta-line">Authors: Tiantian Yang, Zhiqian Chen</div>
<div class="meta-line">First: 2025-08-10T19:35:53+00:00 · Latest: 2026-02-11T18:50:44+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07465v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.07465v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease. However, the high dimensionality of multi-omics data, the heterogeneity across modalities, and the lack of reliable biological interaction networks make meaningful integration challenging. In addition, many existing models rely on handcrafted similarity graphs, are vulnerable to class imbalance, and often lack built-in interpretability, limiting their usefulness in biomedical applications. We propose Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), a novel and interpretable framework for binary disease classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) for omics-specific supervised graph construction, followed by modality-specific Graph Neural Networks (GNNs) for hierarchical representation learning, and a deep feedforward network for cross-omics integration. Across three real-world disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance. The model maintains computational efficiency through the use of sparse graphs and provides built-in interpretability, revealing both top-ranked biomarkers and the relative contributions of each omics modality. These results highlight the potential of MOTGNN to improve both predictive accuracy and interpretability in multi-omics disease modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MOTGNN：用于多组学疾病分类的可解释图神经网络</div>
<div class="mono" style="margin-top:8px">整合多组学数据，如DNA甲基化、mRNA表达和微小RNA（miRNA）表达，提供了对疾病背后生物机制的全面视角。然而，多组学数据的高维性、模态间的异质性以及缺乏可靠的生物相互作用网络使得有意义的整合变得具有挑战性。此外，许多现有模型依赖于手工制作的相似性图，易受类别不平衡的影响，并且通常缺乏内置可解释性，限制了它们在生物医学应用中的实用性。我们提出了基于树生成图神经网络的多组学整合（MOTGNN），这是一个新颖且可解释的二元疾病分类框架。MOTGNN采用极端梯度提升（XGBoost）进行组学特定的监督图构建，随后使用模态特定的图神经网络（GNN）进行层次表示学习，并通过深度前馈网络进行跨组学整合。在三个真实世界的疾病数据集上，MOTGNN在准确性、ROC-AUC和F1-score上比最先进的基线提高了5-10%，并且在严重类别不平衡的情况下仍然保持稳健。该模型通过使用稀疏图保持计算效率，并提供内置可解释性，揭示了排名最高的生物标志物及每个组学模态的相对贡献。这些结果突显了MOTGNN在提高多组学疾病建模中的预测准确性和可解释性方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of integrating multi-omics data for disease classification, particularly due to high dimensionality, modality heterogeneity, and the absence of reliable biological interaction networks. The authors propose a novel framework called Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), which utilizes eXtreme Gradient Boosting for constructing omics-specific graphs, followed by modality-specific Graph Neural Networks for representation learning and a deep feedforward network for integration. Experimental results demonstrate that MOTGNN outperforms existing models by 5-10% in accuracy, ROC-AUC, and F1-score across three disease datasets, while also providing interpretability and robustness against class imbalance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多组学数据整合在疾病分类中的挑战，特别是由于高维度、模态异质性和缺乏可靠的相互作用网络。作者提出了一种新颖的框架，称为基于树生成的多组学图神经网络（MOTGNN），该框架利用极端梯度提升（XGBoost）为每种组学类型构建特定的监督图，然后通过模态特定的图神经网络进行层次学习，并使用深度前馈网络进行整合。实验结果表明，MOTGNN在三个疾病数据集上比现有模型的准确性、ROC-AUC和F1-score提高了5-10%，同时提供了解释性和对类别不平衡的鲁棒性，揭示了关键生物标志物及不同组学模态的贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Weight Decay Improves Language Model Plasticity</div>
<div class="meta-line">Authors: Tessa Han, Sebastian Bordt, Hanlin Zhang, Sham Kakade</div>
<div class="meta-line">First: 2026-02-11T18:49:26+00:00 · Latest: 2026-02-11T18:49:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model&#x27;s validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay&#x27;s mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>权重衰减改善语言模型的可塑性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）开发的主流范式是预训练基础模型，然后进行进一步训练以提高性能和模型行为。然而，超参数优化和规模法则主要从基础模型的验证损失角度进行研究，忽视了下游适应性。在本研究中，我们从模型可塑性的角度研究预训练，即基础模型通过微调成功适应下游任务的能力。我们关注权重衰减的作用，这是预训练期间的一个关键正则化参数。通过系统实验，我们表明，使用较大权重衰减值训练的模型更具可塑性，这意味着它们在下游任务微调时表现出更大的性能提升。这种现象可能导致反直觉的权衡，即在预训练后表现较差的基础模型在微调后可能表现更好。对权重衰减对模型行为的机制效应的进一步研究表明，它鼓励线性可分表示，正则化注意力矩阵，并减少对训练数据的过拟合。总之，本研究展示了在超参数优化中使用超越交叉熵损失的评估指标的重要性，并阐明了单一优化超参数在塑造模型行为中的多方面作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of the current approach to large language model (LLM) development, which primarily focuses on validation loss without considering downstream adaptability. The authors investigate the impact of weight decay, a regularization parameter, on model plasticity, defined as the ability to adapt to downstream tasks through fine-tuning. Their systematic experiments reveal that models with higher weight decay values exhibit greater plasticity, leading to improved performance on downstream tasks, even if they initially perform worse after pretraining. Additionally, the study uncovers that weight decay promotes linearly separable representations, regularizes attention matrices, and mitigates overfitting, highlighting the need for diverse evaluation metrics in hyperparameter optimization.</div>
<div class="mono" style="margin-top:8px">本研究解决了当前大型语言模型开发范式的局限性，该范式主要关注验证损失，而忽视了下游适应性。作者研究了权重衰减这一正则化参数对模型可塑性的影响，定义为预训练模型有效适应下游任务的能力。通过系统实验，他们发现具有较高权重衰减值的模型表现出更大的可塑性，导致在微调后性能改善，即使其预训练性能较低。此外，研究表明权重衰减促进了线性可分表示，正则化了注意力矩阵，并减轻了过拟合，强调了在超参数优化中需要更广泛的评估指标。</div>
</details>
</div>
<div class="card">
<div class="title">Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models</div>
<div class="meta-line">Authors: Jialiang Wang, Hanmo Liu, Shimin Di, Zhili Wang, Jiachuan Wang, Lei Chen, Xiaofang Zhou</div>
<div class="meta-line">Venue: WSDM 2026</div>
<div class="meta-line">First: 2024-08-13T08:22:01+00:00 · Latest: 2026-02-11T18:49:00+00:00</div>
<div class="meta-line">Comments: Accepted at WSDM 2026. Title changed from &quot;Computation-friendly graph neural network design by accumulating knowledge on large language models&quot; to &quot;Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models&quot;</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.06717v3">Abs</a> · <a href="https://arxiv.org/pdf/2408.06717v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-level automation is increasingly critical in AI, driven by rapid advances in large language models (LLMs) and AI agents. However, LLMs, despite their general reasoning power, struggle significantly in specialized, data-sensitive tasks such as designing Graph Neural Networks (GNNs). This difficulty arises from (1) the inherent knowledge gaps in modeling the intricate, varying relationships between graph properties and suitable architectures and (2) the external noise from misleading descriptive inputs, often resulting in generic or even misleading model suggestions. Achieving proficiency in designing data-aware models -- defined as the meta-level capability to systematically accumulate, interpret, and apply data-specific design knowledge -- remains challenging for existing automated approaches, due to their inefficient construction and application of meta-knowledge. To achieve meta-level proficiency, we propose DesiGNN, a knowledge-centered framework that systematically converts past model design experience into structured, fine-grained knowledge priors well-suited for meta-learning with LLMs. To account for the inherent variability and external noise, DesiGNN aligns empirical property filtering from extensive benchmarks with adaptive elicitation of literature insights via LLMs. By constructing a solid meta-knowledge between unseen graph understanding and known effective architecture patterns, DesiGNN can deliver top-5.77% initial model proposals for unseen datasets within seconds and achieve consistently superior performance with minimal search cost compared to baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过积累大语言模型知识实现高效图神经网络设计</div>
<div class="mono" style="margin-top:8px">高水平的自动化在人工智能中变得越来越重要，这得益于大语言模型（LLMs）和人工智能代理的快速进展。然而，尽管LLMs具有一般推理能力，但在设计图神经网络（GNNs）等专业、数据敏感的任务中却面临显著挑战。这种困难源于（1）在建模图属性与合适架构之间复杂、多变关系时固有的知识差距，以及（2）来自误导性描述输入的外部噪声，常常导致通用或甚至误导性的模型建议。实现数据感知模型设计的熟练度——定义为系统性积累、解释和应用数据特定设计知识的元级能力——对现有自动化方法仍然具有挑战性，因为它们在元知识的构建和应用上效率低下。为了实现元级熟练度，我们提出了DesiGNN，一个以知识为中心的框架，系统地将过去的模型设计经验转化为适合与LLMs进行元学习的结构化、细粒度知识先验。为了考虑固有的变异性和外部噪声，DesiGNN将来自广泛基准的经验属性过滤与通过LLMs自适应引导的文献洞察相结合。通过在未见图理解与已知有效架构模式之间构建坚实的元知识，DesiGNN能够在几秒钟内为未见数据集提供前5.77%的初始模型建议，并在与基线相比的情况下以最小的搜索成本实现持续优越的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for high-level automation in AI, particularly in the design of Graph Neural Networks (GNNs), where large language models (LLMs) face challenges due to knowledge gaps and external noise. The authors propose DesiGNN, a framework that transforms past model design experiences into structured knowledge priors for meta-learning with LLMs. Experimental results demonstrate that DesiGNN can generate initial model proposals that rank in the top 5.77% for unseen datasets within seconds and consistently outperforms baseline methods with reduced search costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于利用大型语言模型（LLMs）的能力来增强图神经网络（GNNs）的设计，而LLMs在专门任务中通常由于知识差距和外部噪声而表现不佳。作者提出了一种名为DesiGNN的框架，该框架系统地将过去的模型设计经验转化为结构化的知识先验，以促进与LLMs的元学习。实验结果表明，DesiGNN能够在几秒钟内为未见数据集生成排名前5.77%的初始模型提案，同时与现有方法相比，具有更低的搜索成本和更优越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight</div>
<div class="meta-line">Authors: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu</div>
<div class="meta-line">First: 2026-02-11T18:48:11+00:00 · Latest: 2026-02-11T18:48:11+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11136v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FormalJudge：一种用于代理监督的神经符号范式</div>
<div class="mono" style="margin-top:8px">随着基于LLM的代理在具有现实后果的高风险领域中越来越多地运作，确保其行为安全变得至关重要。主导的监督范式LLM作为法官面临一个根本性困境：概率系统如何可靠地监督其他概率系统而不继承其失败模式？我们认为形式验证提供了一个原则性的解决方案，但其采用受到一个关键瓶颈的阻碍：从自然语言需求到形式规范的翻译。本文通过提出一个神经符号框架来弥合这一差距，该框架采用双向的思维形式架构：LLM作为规范编译器，从上到下将高层次的人类意图分解为原子、可验证的约束，然后从下到上使用Dafny规范和Z3理论求解进行合规性证明，从而产生数学保证而非概率分数。我们在三个基准上进行了验证，涵盖行为安全、多领域约束遵循和代理向上欺骗检测。对7个代理模型的实验表明，该方法在LLM作为法官基线的基础上平均提高了16.6%，实现了从弱到强的泛化，其中一个70亿参数的法官在检测72亿代理的欺骗时达到了90%以上的准确率，并通过迭代优化提供了近线性的安全性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for reliable oversight of LLM-based agents operating in high-stakes environments, addressing the limitations of the current LLM-as-a-Judge paradigm. The authors propose a neuro-symbolic framework called FormalJudge, which utilizes a bidirectional Formal-of-Thought architecture to translate natural language requirements into formal specifications, enabling LLMs to decompose human intent into verifiable constraints and prove compliance using Dafny and Z3. Experimental results across three benchmarks show that FormalJudge improves performance by an average of 16.6% over LLM-as-a-Judge baselines, achieves over 90% accuracy in detecting deception from larger agents, and demonstrates significant safety improvements through iterative refinement.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要对在高风险环境中运行的基于LLM的代理进行可靠的监督，解决现有LLM-as-a-Judge范式的局限性。作者提出了一种名为FormalJudge的神经符号框架，利用双向的Formal-of-Thought架构将自然语言需求转化为正式规范，从而实现对代理行为的形式验证。实验结果表明，FormalJudge在LLM-as-a-Judge基线之上平均提高了16.6%的性能，在检测来自更大代理的欺骗时达到了超过90%的准确率，并在三个基准测试中通过迭代改进展示了显著的安全性提升。</div>
</details>
</div>
<div class="card">
<div class="title">Just on Time: Token-Level Early Stopping for Diffusion Language Models</div>
<div class="meta-line">Authors: Zahar Kohut, Severyn Shykula, Dmytro Khamula, Mykola Vysotskyi, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-02-11T18:44:04+00:00 · Latest: 2026-02-11T18:44:04+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model&#x27;s predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>及时：扩散语言模型的令牌级提前停止</div>
<div class="mono" style="margin-top:8px">扩散语言模型通过迭代精炼生成文本，这一过程通常计算效率低下，因为许多令牌在最终去噪步骤之前就已达到稳定。我们提出了一种无训练的令牌级提前停止方法，能够独立识别每个位置的收敛。我们的方法利用从模型预测和局部上下文中提取的轻量信号，动态确定何时可以最终确定单个令牌。这实现了自适应的逐令牌冻结，无需特定任务的微调，显著减少所需的扩散步骤总数。在涵盖数学推理、一般问答和科学理解的多样基准测试中，我们的方法在保持生成质量的同时，实现了最先进的效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the computational efficiency of diffusion language models, which often require many iterations to generate stable tokens. The authors propose a training-free, token-level early stopping method that assesses convergence for each token independently using lightweight signals from the model&#x27;s predictions and local context. Experimental results demonstrate that this approach significantly reduces the number of diffusion steps needed while maintaining high generation quality across various benchmarks, including mathematical reasoning and scientific understanding.</div>
<div class="mono" style="margin-top:8px">本研究解决了扩散语言模型在生成文本时的计算低效问题，这些模型通常需要过多的步骤来生成每个标记。作者提出了一种无训练的标记级早停方法，该方法利用模型预测和局部上下文中的轻量信号，独立评估每个标记的收敛性。实验结果表明，该方法显著减少了所需的扩散步骤，同时在数学推理和一般问答等多个基准测试中保持了高质量的文本生成。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-11T18:43:26+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈扩展强化学习的能力</div>
<div class="mono" style="margin-top:8px">强化学习在大规模语言模型后训练中的成功源于一个不合理的信息来源：每次回合仅提供一个二进制奖励或偏好标签的信息。在另一极端，蒸馏提供了密集的监督，但需要演示，这既昂贵又难以扩展。我们研究文本反馈作为一种中间信号：比标量奖励更丰富，但比完整演示更便宜。文本反馈是人类互动的自然方式，并且在许多现实世界场景中已经很丰富，用户、注释者和自动评审者经常批评大规模语言模型的输出。为了在大规模上利用文本反馈，我们形式化了一个多轮强化学习设置，即文本反馈强化学习（RLTF），其中在训练期间可以获得文本反馈，但在推理时不可用。因此，模型必须学习内化反馈，以提高其测试时的单轮性能。为此，我们提出了两种方法：自我蒸馏（RLTF-SD），训练单轮策略以匹配其自身反馈条件的第二轮生成；以及反馈建模（RLTF-FM），将反馈预测作为辅助目标。我们对这两种方法进行了理论分析，并在推理难题、竞赛数学和创意写作任务上进行了实证评估。我们的结果表明，这两种方法在基准测试中始终优于强基线，突显了在大规模上使用额外丰富监督源的强化学习的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of traditional reinforcement learning (RL) methods that rely on sparse binary rewards or costly demonstrations. The authors propose using text feedback as a more informative and scalable intermediate signal for training RL models. They introduce a multi-turn RL framework called RL from Text Feedback (RLTF) and develop two methods: Self Distillation (RLTF-SD) and Feedback Modeling (RLTF-FM). Experimental results demonstrate that both methods significantly outperform strong baselines on reasoning puzzles, competition math, and creative writing tasks, indicating the effectiveness of incorporating text feedback into RL training processes.</div>
<div class="mono" style="margin-top:8px">本研究解决了强化学习（RL）在大型语言模型（LLM）后训练中的局限性，传统方法依赖于稀疏的二元奖励或昂贵的示范。作者提出使用文本反馈作为一种更具信息量和可扩展性的中间信号，正式化了一个名为文本反馈强化学习（RLTF）的多轮RL框架。他们引入了两种方法，自我蒸馏（RLTF-SD）和反馈建模（RLTF-FM），以在训练过程中利用这种反馈，并通过在推理难题、竞赛数学和创意写作任务上的实证评估，证明这两种方法显著超越了既有基准，表明在RL中引入更丰富的监督信号的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">End to End Collaborative Synthetic Data Generation</div>
<div class="meta-line">Authors: Sikha Pentyala, Geetha Sitaraman, Trae Claar, Martine De Cock</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-12-04T23:10:51+00:00 · Latest: 2026-02-11T18:43:08+00:00</div>
<div class="meta-line">Comments: Accepted at PPAI Workshop, AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.03766v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.03766v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of AI is based on the availability of data to train models. While in some cases a single data custodian may have sufficient data to enable AI, often multiple custodians need to collaborate to reach a cumulative size required for meaningful AI research. The latter is, for example, often the case for rare diseases, with each clinical site having data for only a small number of patients. Recent algorithms for federated synthetic data generation are an important step towards collaborative, privacy-preserving data sharing. Existing techniques, however, focus exclusively on synthesizer training, assuming that the training data is already preprocessed and that the desired synthetic data can be delivered in one shot, without any hyperparameter tuning. In this paper, we propose an end-to-end collaborative framework for publishing of synthetic data that accounts for privacy-preserving preprocessing as well as evaluation. We instantiate this framework with Secure Multiparty Computation (MPC) protocols and evaluate it in a use case for privacy-preserving publishing of synthetic genomic data for leukemia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>端到端协作合成数据生成</div>
<div class="mono" style="margin-top:8px">人工智能的成功基于可用于训练模型的数据。在某些情况下，单个数据管理者可能拥有足够的数据来支持人工智能，但通常需要多个管理者合作，以达到进行有意义的人工智能研究所需的累积数据量。这种情况在罕见疾病中尤为常见，每个临床站点的数据仅涵盖少数患者。最近的联邦合成数据生成算法是实现协作、保护隐私的数据共享的重要一步。然而，现有技术仅专注于合成器训练，假设训练数据已经过预处理，并且期望的合成数据可以一次性交付，而无需任何超参数调整。本文提出了一种端到端的协作框架，用于合成数据的发布，考虑了保护隐私的预处理和评估。我们用安全多方计算（MPC）协议实例化该框架，并在保护隐私的白血病合成基因组数据发布的用例中进行了评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for collaborative data sharing among multiple custodians to generate sufficient data for AI, particularly in the context of rare diseases where individual sites have limited data. The authors propose an end-to-end collaborative framework for synthetic data generation that incorporates privacy-preserving preprocessing and evaluation, utilizing Secure Multiparty Computation (MPC) protocols. Experimental results demonstrate the framework&#x27;s effectiveness in securely publishing synthetic genomic data for leukemia, addressing the limitations of existing federated synthetic data generation techniques that do not account for preprocessing and hyperparameter tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人工智能中数据稀缺的问题，特别是在罕见疾病的背景下，多个数据持有者必须合作生成足够的训练数据。作者提出了一种端到端的协作合成数据生成框架，该框架结合了隐私保护的预处理和评估，利用安全多方计算（MPC）协议。实验结果表明，该框架在安全发布白血病合成基因组数据方面的有效性，突显了其在促进协作数据共享同时维护隐私的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">MIND: Benchmarking Memory Consistency and Action Control in World Models</div>
<div class="meta-line">Authors: Yixuan Ye, Xuanyu Lu, Yuxin Jiang, Yuchao Gu, Rui Zhao, Qiwei Liang, Jiachun Pan, Fengda Zhang, Weijia Wu, Alex Jinpeng Wang</div>
<div class="meta-line">First: 2026-02-08T15:57:23+00:00 · Latest: 2026-02-11T18:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08025v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08025v2">PDF</a> · <a href="https://github.com/CSU-JPG/MIND">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Code: https://github.com/CSU-JPG/MIND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIND：世界模型中的记忆一致性和动作控制基准测试</div>
<div class="mono" style="margin-top:8px">世界模型旨在理解、记忆和预测动态视觉环境，但缺乏统一的基准来评估其基本能力。为了解决这一问题，我们推出了MIND，这是第一个开放领域的闭环基准，用于评估世界模型中的记忆一致性和动作控制。MIND包含250个高质量的1080p和24 FPS视频，包括100个（第一人称）和100个（第三人称）视频片段，均在共享动作空间下，以及25 + 25个跨越不同动作空间的片段，涵盖八个多样化场景。我们设计了一个高效的评估框架，以测量两个核心能力：记忆一致性和动作控制，捕捉视角间的时间稳定性和上下文连贯性。此外，我们设计了各种动作空间，包括不同的角色移动速度和相机旋转角度，以评估在共享场景下跨不同动作空间的动作泛化能力。为了促进未来在MIND上的性能基准测试，我们引入了MIND-World，一个新颖的互动视频到世界基线。大量实验表明MIND的完整性，并揭示了当前世界模型中的关键挑战，包括维持长期记忆一致性和在动作空间间泛化的困难。代码：https://github.com/CSU-JPG/MIND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the lack of a unified benchmark for evaluating the fundamental abilities of world models in understanding and predicting dynamic visual environments. To address this, the authors introduce MIND, an open-domain closed-loop benchmark that includes 250 high-quality videos to assess memory consistency and action control. Experimental results highlight the challenges faced by current world models, particularly in maintaining long-term memory consistency and generalizing across different action spaces, indicating areas for further improvement in model capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是缺乏一个统一的基准来评估世界模型在理解和预测动态视觉环境方面的基本能力。为了解决这个问题，作者提出了MIND，一个综合性的基准，包含250个高质量视频，旨在评估记忆一致性和动作控制。实验结果突显了当前世界模型面临的重大挑战，特别是在维持长期记忆一致性和在不同动作空间中进行泛化方面，作者通过所提出的框架和各种动作场景进行了广泛评估。</div>
</details>
</div>
<div class="card">
<div class="title">Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis</div>
<div class="meta-line">First: 2026-02-11T18:39:42+00:00 · Latest: 2026-02-11T18:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可验证奖励的强化学习中的非对称提示加权</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习推动了最近在LLM后训练中的进展，特别是在推理方面。策略优化算法为给定提示生成多个响应，然后根据奖励有效地加权相应的梯度。最流行的算法包括GRPO、DAPO和RLOO，专注于模糊提示，即成功概率中等的提示，同时降低非常简单和非常困难提示的梯度。在本文中，我们考虑非对称提示加权，赋予低甚至零经验成功概率的提示更高的权重。我们发现，非对称加权特别有利于从零开始的强化学习（如R1-Zero），在这种情况下，训练跨越了广泛的准确性范围，而在后SFT强化学习中则不那么明显，因为模型已经从高准确性开始。我们还提供了理论，描述了在固定更新预算下，最小化将成功概率从初始水平提高到目标准确性所需时间的提示权重。在低成功率的情况下，信息性响应稀少且响应成本占主导地位，这些最优权重变得非对称，增加低成功概率的权重，从而加速有效时间收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance reinforcement learning (RL) with verifiable rewards, particularly in the context of large language models (LLMs) for reasoning tasks. The authors propose a method of asymmetric prompt weighting that assigns greater importance to prompts with low or zero empirical success probabilities, contrasting with existing algorithms that focus on ambiguous prompts. Experimental results demonstrate that this approach significantly improves from-scratch RL performance, especially in scenarios with low success rates, by optimizing prompt weights to accelerate convergence towards target accuracy under fixed update budgets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善具有可验证奖励的强化学习（RL），特别是在大型语言模型（LLM）的推理任务中。作者提出了一种非对称提示加权的方法，该方法对成功概率低或为零的提示赋予更高的权重，这与现有算法关注模糊提示的做法形成对比。实验结果表明，这种方法显著提高了从零开始的RL性能，特别是在低成功率的情况下，通过加速收敛到目标准确性，而在后SFT RL场景中，模型已经以高准确性开始时，影响较小。</div>
</details>
</div>
<div class="card">
<div class="title">The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization</div>
<div class="meta-line">Authors: Stephanie Holly, Alexandru-Ciprian Zăvoianu, Siegfried Silber, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">First: 2026-02-11T18:38:40+00:00 · Latest: 2026-02-11T18:38:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11126v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线前沿转移：诊断生成多目标优化中的分布限制</div>
<div class="mono" style="margin-top:8px">离线多目标优化（MOO）旨在从有限的静态数据集中恢复帕累托最优设计。最近的生成方法，包括扩散模型，在超体积下表现出色，但它们在其他已建立的MOO指标下的表现尚不清楚。我们表明，生成方法在其他指标（如代际距离）方面系统性地表现不如进化替代方法。我们将这种失败模式与离线前沿转移相关联，即离线数据集与帕累托前沿的位移，这在离线MOO中构成了根本限制。我们认为，克服这一限制需要在目标空间中进行分布外采样（通过积分概率度量），并实证观察到生成方法仍然保守地接近离线目标分布。我们的结果将离线MOO定位为一个受分布转移限制的问题，并提供了一个诊断视角，以理解生成优化方法何时以及为何失败。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the limitations of generative multi-objective optimization (MOO) methods, particularly their performance under various metrics beyond hypervolume. The authors employ a comparative analysis between generative approaches and evolutionary alternatives, revealing that generative methods consistently underperform in metrics like generational distance due to the offline-frontier shift, which indicates a misalignment between the offline dataset and the Pareto front. The study concludes that addressing this issue necessitates out-of-distribution sampling in objective space, highlighting the constraints of offline MOO and providing insights into the conditions under which generative optimization methods may fail.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成多目标优化（MOO）方法的局限性，特别是在超体积以外的各种指标下的表现。作者通过比较分析生成方法与进化替代方法，揭示了生成方法在代际距离等指标上持续表现不佳，这归因于离线前沿转移，表明离线数据集与帕累托前沿之间的不匹配。研究得出结论，解决这一局限性需要在目标空间进行分布外采样，强调生成方法往往过于接近离线目标分布，从而将离线MOO框定为一个受分布转移限制的问题，并提供了对生成优化技术失败的见解。</div>
</details>
</div>
<div class="card">
<div class="title">PhyCritic: Multimodal Critic Models for Physical AI</div>
<div class="meta-line">Authors: Tianyi Xiong, Shihao Wang, Guilin Liu, Yi Dong, Ming Li, Heng Huang, Jan Kautz, Zhiding Yu</div>
<div class="meta-line">First: 2026-02-11T18:35:39+00:00 · Latest: 2026-02-11T18:35:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11124v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhyCritic：用于物理人工智能的多模态评判模型</div>
<div class="mono" style="margin-top:8px">随着大型多模态模型的快速发展，可靠的评判和批评模型已成为开放式评估和偏好对齐的必要条件，提供成对偏好、数值评分和解释性理由，以评估模型生成的响应。然而，现有的评判模型主要在通用视觉领域（如图像描述或图像问答）进行训练，导致涉及感知、因果推理和规划的物理人工智能任务在很大程度上未被探索。我们介绍了PhyCritic，这是一种通过两阶段RLVR管道优化的多模态评判模型：一个增强物理导向感知和推理的物理技能热身阶段，随后是自我参考评判微调阶段，在此阶段，评判模型生成自己的预测作为内部参考，然后再判断候选响应，从而提高判断的稳定性和物理正确性。在物理和通用多模态评判基准测试中，PhyCritic在开源基线之上实现了显著的性能提升，并且在作为策略模型应用时，进一步改善了物理基础任务中的感知和推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for reliable judge and critic models in the evaluation of model-generated responses, particularly in the context of physical AI tasks that require perception, causal reasoning, and planning. The authors introduce PhyCritic, a multimodal critic model developed through a two-stage RLVR pipeline, which includes a physical skill warmup stage to enhance perception and reasoning, followed by self-referential critic finetuning. Experimental results demonstrate that PhyCritic significantly outperforms existing open-source baselines on both physical and general-purpose multimodal judge benchmarks, and when utilized as a policy model, it further enhances perception and reasoning in tasks grounded in physical reality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于物理人工智能领域对可靠评判和批评模型的需求，因为现有模型主要集中在一般视觉任务上。作者提出了PhyCritic，这是一种多模态批评模型，采用两阶段的RLVR管道，包括物理技能热身阶段以增强感知和推理，随后进行自我参考的批评微调以提高判断的稳定性。实验结果表明，PhyCritic在物理和通用多模态评判基准上均优于开源基线，并且作为策略模型时，进一步增强了物理任务中的感知和推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent</div>
<div class="meta-line">Authors: Genmao Zhuang, Amir Barati Farimani</div>
<div class="meta-line">First: 2026-02-11T18:34:24+00:00 · Latest: 2026-02-11T18:34:24+00:00</div>
<div class="meta-line">Comments: 22 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11123v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D &gt; 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从自然语言到材料发现：材料知识导航代理</div>
<div class="mono" style="margin-top:8px">加速高性能材料的发现仍然是能源、电子和航空航天技术中的一个核心挑战，传统工作流程在很大程度上依赖于专家直觉和计算成本高昂的模拟。在这里，我们介绍材料知识导航代理（MKNA），这是一个语言驱动的系统，将自然语言科学意图转化为可执行的数据库检索、属性预测、结构生成和稳定性评估的操作。除了自动化工具调用外，MKNA还自主从文献和数据库证据中提取定量阈值和化学意义明确的设计模式，从而实现基于数据的假设形成。应用于高德拜温度陶瓷的搜索，该代理识别出一个文献支持的筛选标准（Theta_D &gt; 800 K），重新发现了经典的超刚性材料，如金刚石、SiC、SiN和BeO，并提出了热力学稳定的、以前未报告的富铍化合物，这些化合物分布在稀疏探索的1500-1700 K范围内。这些结果表明，MKNA不仅找到稳定的候选材料，还重建了可解释的设计启发式，为自主的、语言引导的材料探索建立了一个可推广的平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to accelerate the discovery of high-performance materials, which is traditionally reliant on expert intuition and costly simulations. The authors introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that converts natural language scientific queries into actionable tasks for retrieving data, predicting properties, generating structures, and evaluating stability. The experimental results show that MKNA successfully identifies a screening criterion for high-Debye-temperature ceramics, rediscovers known ultra-stiff materials, and proposes new thermodynamically stable Be-C-rich compounds, thus demonstrating its capability to facilitate autonomous materials exploration and generate interpretable design heuristics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决加速高性能材料发现的挑战，这一过程传统上依赖于专家直觉和昂贵的模拟。作者介绍了材料知识导航代理（MKNA），这是一种语言驱动的系统，可以将自然语言科学查询转化为可执行的数据检索、属性预测、结构生成和稳定性评估任务。实验结果表明，MKNA成功识别出高德拜温度陶瓷的筛选标准，重新发现已知的超刚性材料，并提出新的热力学稳定化合物，展示了其促进自主材料探索和生成可解释设计启发式的能力。</div>
</details>
</div>
<div class="card">
<div class="title">A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Computers</div>
<div class="meta-line">Authors: Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman</div>
<div class="meta-line">First: 2025-07-14T20:02:40+00:00 · Latest: 2026-02-11T18:32:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10775v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10775v2">PDF</a> · <a href="https://github.com/RiceD2KLab/SWiM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA&#x27;s TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Our dataset includes images with several real-world challenges, including noise, camera distortions, glare, varying lighting conditions, varying field of view, partial spacecraft visibility, brightly-lit city backgrounds, densely patterned and confounding backgrounds, aurora borealis, and a wide variety of spacecraft geometries. Finally, we finetuned YOLOv8 and YOLOv11 models for spacecraft segmentation to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA&#x27;s inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at https://github.com/RiceD2KLab/SWiM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实时航天器分割的新数据集和性能基准</div>
<div class="mono" style="margin-top:8px">部署在外太空的航天器经常受到各种形式的损害，因为它们暴露在危险环境中。此外，通过人类出舱活动或机器人操作进行的太空维修过程存在重大风险，导致可观的运营成本。最近的图像分割发展可能使得可靠且具有成本效益的自主检查系统的开发成为可能。虽然这些模型通常需要大量的训练数据以达到令人满意的结果，但公开可用的标注航天器分割数据非常稀缺。在这里，我们展示了一个新的数据集，包含近64k张使用真实航天器模型创建的标注航天器图像，这些图像叠加在使用NASA的TTALOS管道生成的真实和合成背景的混合上。为了模拟真实世界图像采集中的相机畸变和噪声，我们还向图像中添加了不同类型的噪声和畸变。我们的数据集包括具有多种现实挑战的图像，包括噪声、相机畸变、眩光、不同的光照条件、不同的视场、部分航天器可见性、明亮的城市背景、密集的图案和混淆背景、极光以及各种航天器几何形状。最后，我们对YOLOv8和YOLOv11模型进行了微调，以进行航天器分割，并在明确的硬件和推理时间限制下生成数据集的性能基准，以模拟NASA检查航天器在太空中实时图像分割挑战的真实场景。在这些限制下测试的模型达到了0.92的Dice分数、0.69的Hausdorff距离和约0.5秒的推理时间。数据集和性能基准模型可在https://github.com/RiceD2KLab/SWiM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for reliable and cost-effective autonomous inspection systems for spacecraft, which face damage from hazardous environments and costly repair processes. The authors developed a new dataset comprising nearly 64,000 annotated spacecraft images, created using real spacecraft models against a mix of real and synthetic backgrounds, while incorporating various noise and distortion types to simulate real-world conditions. The fine-tuning of YOLOv8 and YOLOv11 models for spacecraft segmentation yielded performance benchmarks, achieving a Dice score of 0.92, a Hausdorff distance of 0.69, and an inference time of approximately 0.5 seconds, demonstrating the models&#x27; effectiveness under realistic constraints for onboard applications in space.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于对可靠且具有成本效益的航天器自主检查系统的需求，因为航天器在危险环境中面临损坏，并且在维修过程中会产生高昂的运营成本。作者创建了一个新的数据集，包含近64,000张标注的航天器图像，利用真实的航天器模型和真实与合成背景的组合，以解决公开可用数据稀缺的问题。他们对YOLOv8和YOLOv11模型进行了微调，以实现航天器分割，取得了0.92的Dice分数和0.69的Hausdorff距离，推理时间约为0.5秒，从而为太空中的实时机载应用建立了性能基准。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations</div>
<div class="meta-line">Authors: Kadircan Aksoy, Protim Bhattacharjee, Peter Jung</div>
<div class="meta-line">First: 2026-01-28T10:46:44+00:00 · Latest: 2026-02-11T18:32:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20477v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20477v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经网络表示中的隐式假设检验与散度保持</div>
<div class="mono" style="margin-top:8px">我们通过二元假设检验的视角研究神经分类器的监督训练动态。我们将分类建模为表示的类条件分布之间的一组二元测试，并实证表明，在训练轨迹中，良好泛化的网络通过与错误率指数相关的KL散度的单调改善，越来越与Neyman-Pearson最优决策规则对齐。最后，我们讨论这如何为不同类别的神经网络提供解释和可能的训练或正则化策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the training dynamics of neural classifiers by framing classification as binary hypothesis testing between class-conditional distributions. The authors employ empirical analysis to demonstrate that well-generalizing networks align with Neyman-Pearson optimal decision rules during training, as indicated by monotonic improvements in KL divergence that correlate with error rate exponents. The findings provide insights into training and regularization strategies for various types of neural networks.</div>
<div class="mono" style="margin-top:8px">本研究通过将分类视为二元假设检验，探讨神经分类器的训练动态，旨在理解良好泛化网络如何改善其决策过程。作者采用模型分析表示的类条件分布与Neyman-Pearson最优决策规则的对齐情况，研究训练过程中KL散度的变化。主要发现表明，随着训练的进行，网络在KL散度上表现出单调改善，这与错误率的降低相关，为不同神经网络架构的训练和正则化策略提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion</div>
<div class="meta-line">Authors: Di Chang, Ji Hou, Aljaz Bozic, Assaf Neuberger, Felix Juefei-Xu, Olivier Maury, Gene Wei-Chin Lin, Tuur Stuyck, Doug Roble, Mohammad Soleymani, Stephane Grabli</div>
<div class="meta-line">First: 2026-02-11T18:31:47+00:00 · Latest: 2026-02-11T18:31:47+00:00</div>
<div class="meta-line">Comments: Website: https://boese0601.github.io/hairweaver/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://boese0601.github.io/hairweaver/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject&#x27;s photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HairWeaver：基于模拟到真实引导的视频扩散的少量真实感头发运动合成</div>
<div class="mono" style="margin-top:8px">我们提出了HairWeaver，这是一种基于扩散的管道，可以为单个人类图像动画化出逼真且富有表现力的头发动态。虽然现有方法成功控制身体姿势，但缺乏对头发的具体控制，因此未能捕捉复杂的头发运动，导致动画僵硬且不真实。HairWeaver通过两个专门模块克服了这一限制：一个是Motion-Context-LoRA，用于整合运动条件；另一个是Sim2Real-Domain-LoRA，用于在不同数据域中保持主体的真实感外观。这些轻量级组件旨在引导视频扩散主干，同时保持其核心生成能力。通过在从CG模拟器生成的动态人类运动的专门数据集上进行训练，HairWeaver实现了对头发运动的精细控制，并最终学会生成对运动自然反应的高度真实的头发。全面评估表明，我们的方法设定了新的技术标准，生成具有动态细节的栩栩如生的人类头发动画。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind HairWeaver is to improve the realism and expressiveness of hair motion in animated human images, addressing the limitations of existing methods that fail to capture intricate hair dynamics. The method employs a diffusion-based pipeline enhanced by two specialized modules: Motion-Context-LoRA for integrating motion conditions and Sim2Real-Domain-LoRA for maintaining photorealistic appearance across data domains. Experimental results indicate that HairWeaver achieves a new state of the art in generating lifelike hair animations that respond naturally to movement, demonstrating significant advancements in the control and realism of hair dynamics in animations.</div>
<div class="mono" style="margin-top:8px">HairWeaver的研究动机在于解决现有动画方法无法捕捉人类运动中真实发丝动态的局限性。作者开发了一种基于扩散的管道，结合了两个专门模块：Motion-Context-LoRA用于整合运动条件，Sim2Real-Domain-LoRA用于保持跨数据域的真实外观。实验结果表明，HairWeaver在一个来自CG模拟器的动态人类运动数据集上训练，达到了生成自然响应运动的逼真发丝动画的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Compose for Cross-domain Agentic Workflow Generation</div>
<div class="meta-line">Authors: Jialiang Wang, Shengxiang Xu, Hanmo Liu, Jiachuan Wang, Yuyu Luo, Shimin Di, Min-Ling Zhang, Lei Chen</div>
<div class="meta-line">First: 2026-02-11T18:27:22+00:00 · Latest: 2026-02-11T18:27:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11114v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨域自主工作流生成的学习</div>
<div class="mono" style="margin-top:8px">自动生成自主工作流——可执行的操作图或代码，协调推理、验证和修复——已成为解决复杂任务的实用方法，超出了单次生成的LLM所能可靠处理的范围。然而，什么构成一个好的工作流在很大程度上依赖于任务分布和可用操作符。在领域转移下，当前系统通常依赖于迭代工作流优化，从庞大的工作流空间中发现可行的工作流，这会产生高昂的迭代成本，并导致不稳定的领域特定行为。为此，我们将分解-重组-决策机制内化到一个开源LLM中，以实现跨域工作流生成。为了分解，我们学习了一组可重用的紧凑工作流能力，涵盖不同领域。为了重组，我们将每个输入任务映射到这些基础上的稀疏组合，以单次生成特定任务的工作流。为了决策，我们将工作流生成的成功或失败归因于学习能力的反事实贡献，从而捕捉哪些能力通过其边际效应实际推动成功。在严格的多域、跨域和未见域评估中，我们的单次生成器超越了消耗20次迭代的SOTA优化基线，同时显著降低了生成延迟和成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of agentic workflows, which are essential for executing complex tasks that exceed the capabilities of single-pass large language models (LLMs). The authors propose a method that incorporates a decompose-recompose-decide mechanism into an open-source LLM, allowing for the learning of reusable workflow capabilities across various domains. Experimental results demonstrate that their one-pass workflow generator outperforms state-of-the-art refinement baselines, which require 20 iterations, significantly reducing both generation latency and cost while maintaining effectiveness across multiple domains, including unseen ones.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善代理工作流的生成，这对于执行超出单次大型语言模型（LLM）生成能力的复杂任务至关重要。作者提出了一种将分解-重组-决策机制融入开源LLM的方法，允许识别跨多个领域的可重用工作流能力，将输入任务映射到这些能力上，以单次生成任务特定的工作流。关键实验结果表明，该单次生成器在多个评估场景中，包括严格的多领域和未见领域测试中，超越了通常需要20次迭代的最先进的精炼基线，同时显著降低了生成延迟和成本。</div>
</details>
</div>
<div class="card">
<div class="title">Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection</div>
<div class="meta-line">Authors: Albert Dorador</div>
<div class="meta-line">First: 2026-02-11T18:22:59+00:00 · Latest: 2026-02-11T18:22:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11107v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called \textit{relaxation}. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ``One-Standard-Error&#x27;&#x27; rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Renet：通过动态目标选择实现弹性网的原则性和高效放松</div>
<div class="mono" style="margin-top:8px">我们介绍了Renet，这是对弹性网估计量的放松Lasso的原则性推广。一方面，$\ell_1$正则化是高维环境中变量选择的标准工具；另一方面，$\ell_2$惩罚通过严格的凸性提供了稳定性和解的唯一性。然而，标准的弹性网仍然存在收缩偏差，常常导致次优的预测准确性。我们提出通过一个称为\textit{放松}的框架来解决这一限制。现有的放松实现依赖于惩罚和未惩罚解的简单线性插值，忽略了表征整个正则化路径的非线性几何，并可能违反Karush-Kuhn-Tucker条件。Renet通过一种自适应放松程序强制符号一致性，动态调度凸混合和高效子路径重拟合，从而解决了这些限制。此外，我们识别并形式化了放松与“一标准误”规则之间的独特协同：放松作为一种稳健的去偏机制，使从业者能够利用1-SE规则的简约性，而不损失传统的预测保真度。我们的理论框架结合了超高维环境的自动稳定性保障，并通过20个合成和真实世界数据集的全面基准测试支持，证明Renet在高维、低信噪比和高多重共线性环境中始终优于标准弹性网，并提供了比自适应弹性网更稳健的替代方案。通过利用自适应求解器后端，Renet在提供这些统计增益的同时，保持了与最先进的坐标下降实现的竞争性计算性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation behind Renet is to improve the Elastic Net&#x27;s prediction accuracy, which suffers from shrinkage bias despite its utility in high-dimensional variable selection. The authors propose a novel framework called relaxation that enhances the Elastic Net by enforcing sign consistency through an adaptive relaxation procedure, allowing for dynamic adjustments between convex blending and efficient sub-path refitting. Experimental results across 20 synthetic and real-world datasets demonstrate that Renet consistently outperforms the standard Elastic Net and offers a more robust alternative to the Adaptive Elastic Net, particularly in high-dimensional settings with low signal-to-noise ratios and high multicollinearity, while maintaining a competitive computational efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高弹性网的预测准确性，尽管其在高维变量选择中具有实用性，但仍存在收缩偏差。作者提出了Renet，这一新框架通过自适应松弛程序增强了弹性网，允许在凸混合和高效子路径重拟合之间动态选择。对20个合成和真实数据集的实验结果表明，Renet在高维环境下，尤其是在低信噪比和高多重共线性情况下，始终优于标准弹性网，并提供了比自适应弹性网更稳健的替代方案，同时保持了竞争力的计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference</div>
<div class="meta-line">Authors: Divya Jyoti Bajpai, Dhruv Bhardwaj, Soumya Roy, Tejas Duseja, Harsh Agarwal, Aashay Sandansing, Manjesh Kumar Hanawal</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-11T18:21:11+00:00 · Latest: 2026-02-11T18:21:11+00:00</div>
<div class="meta-line">Comments: Accepted at International Conference on Learning Representations (ICLR) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11105v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11105v1">PDF</a> · <a href="https://github.com/Div290/FastFlow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FastFlow：通过强盗推理加速生成流匹配模型</div>
<div class="mono" style="margin-top:8px">流匹配模型在图像和视频生成中提供了最先进的保真度，但固有的顺序去噪过程使其速度较慢。现有的加速方法如蒸馏、轨迹截断和一致性方法是静态的，需要重新训练，并且往往无法跨任务泛化。我们提出了FastFlow，一个即插即用的自适应推理框架，能够加速流匹配模型的生成。FastFlow识别出仅对去噪路径产生微小调整的去噪步骤，并在不使用用于速度预测的完整神经网络模型的情况下对其进行近似。该近似利用来自先前预测的有限差分速度估计来有效外推未来状态，从而以零计算成本加快沿去噪路径的进展。这使得可以跳过中间步骤的计算。我们将安全跳过多少步骤的决策建模为多臂强盗问题。强盗学习最佳跳过策略，以平衡速度与性能。FastFlow与现有管道无缝集成，并在图像生成、视频生成和编辑任务中具有广泛的泛化能力。实验表明，在保持高质量输出的同时，速度提升超过2.6倍。该工作的源代码可以在https://github.com/Div290/FastFlow找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the slow generation speed of flow-matching models in image and video generation due to their sequential denoising process. The authors propose FastFlow, an adaptive inference framework that accelerates these models by identifying and approximating denoising steps that make minor adjustments, thus allowing for faster advancements without full neural network computations. Experimental results show that FastFlow achieves over 2.6 times speedup while maintaining high-quality outputs across various tasks, demonstrating its effectiveness and generalizability in existing pipelines.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于加速用于图像和视频生成的流匹配模型，这些模型受到缓慢的序列去噪过程的限制。作者提出了FastFlow，这是一种自适应推理框架，通过识别和近似对整体输出影响较小的去噪步骤，从而加速这些模型，利用来自先前预测的有限差分速度估计。实验结果表明，FastFlow在生成过程中实现了超过2.6倍的加速，同时在各种任务中保持高质量输出，证明了其有效性和在现有管道中的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He</div>
<div class="meta-line">First: 2026-02-10T18:55:41+00:00 · Latest: 2026-02-11T18:20:25+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10090v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10090v2">PDF</a> · <a href="https://github.com/Snowflake-Labs/agent-world-model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理世界模型：用于代理强化学习的无限合成环境</div>
<div class="mono" style="margin-top:8px">最近大型语言模型（LLM）的进展使自主代理能够执行需要与工具和环境进行多轮交互的复杂任务。然而，代理训练的规模受到缺乏多样化和可靠环境的限制。本文提出了代理世界模型（AWM），一个完全合成的环境生成管道。通过该管道，我们扩展到1,000个涵盖日常场景的环境，代理可以与丰富的工具集（每个环境平均35个工具）进行交互并获得高质量的观察。值得注意的是，这些环境是代码驱动的，并由数据库支持，提供比LLM模拟的环境更可靠和一致的状态转移。此外，与从现实环境中收集轨迹相比，它们还使代理交互更高效。为了证明这一资源的有效性，我们对多轮工具使用代理进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还可以设计可靠的奖励函数。在三个基准上的实验表明，仅在合成环境中训练，而不是特定基准的环境，能够产生强大的分布外泛化。代码可在 https://github.com/Snowflake-Labs/agent-world-model 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations in scaling agent training due to the lack of diverse and reliable environments for autonomous agents. The authors propose the Agent World Model (AWM), a synthetic environment generation pipeline that creates 1,000 diverse environments with rich toolsets for agents to interact with. Key experimental findings indicate that training agents exclusively in these synthetic environments leads to strong out-of-distribution generalization, outperforming traditional benchmark-specific training methods, and demonstrating the effectiveness of the AWM in facilitating multi-turn tool-use tasks in reinforcement learning scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于缺乏多样化和可靠环境而限制自主智能体训练规模的问题。作者提出了代理世界模型（AWM），这是一种合成环境生成管道，可以创建1000个多样化的环境，供智能体与丰富的工具集进行交互。实验结果表明，智能体仅在这些合成环境中训练，能够实现强大的超出分布的泛化，优于传统的基准特定训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估智能体能力</div>
<div class="mono" style="margin-top:8px">尽管编码智能体的进展迅速，但其多模态对应物的进展却滞后。一个关键挑战是缺乏结合软件开发复杂性与深度多模态理解需求的评估测试平台。游戏开发提供了这样的测试平台，因为智能体必须在处理大型、密集的代码库时，同时操作如着色器、精灵和动画等内在多模态资产。我们提出了GameDevBench，这是第一个用于评估智能体在游戏开发任务中的基准。GameDevBench包含132个源自网络和视频教程的任务。这些任务需要显著的多模态理解，并且复杂性较高——平均解决方案所需的代码行数和文件更改量是先前软件开发基准的三倍以上。智能体在游戏开发中仍然面临挑战，最佳智能体仅解决了54.5%的任务。我们发现感知任务难度与多模态复杂性之间存在强相关性，成功率从游戏玩法导向任务的46.9%下降到2D图形任务的31.6%。为了提高多模态能力，我们引入了两种简单的基于图像和视频的反馈机制。尽管这些方法简单，但它们始终能提高性能，最大的变化是Claude Sonnet 4.5的性能从33.3%提高到47.7%。我们公开发布GameDevBench，以支持对智能体游戏开发的进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lag in multimodal capabilities of coding agents, particularly in the context of game development, which presents unique challenges due to its complex and multimodal nature. The authors introduce GameDevBench, a benchmark consisting of 132 tasks derived from web and video tutorials that require significant multimodal understanding and are more complex than previous software development benchmarks. Experimental results show that the best agent only solves 54.5% of the tasks, with a notable drop in success rates from 46.9% for gameplay-oriented tasks to 31.6% for 2D graphics tasks, highlighting the correlation between task difficulty and multimodal complexity. To enhance agent performance, two simple image and video-based feedback mechanisms were implemented, resulting in a performance increase for Claude Sonnet 4.5 from 33.3% to 47.7%. GameDevBench is publicly released to facilitate further research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决编码智能体在多模态能力方面的滞后，特别是在游戏开发的背景下，因为游戏开发由于其复杂的软件和多模态资产而面临独特的挑战。作者介绍了GameDevBench，这是一个基准测试，包含132个来自网络和视频教程的任务，这些任务需要显著的多模态理解，并且比以前的基准更复杂。实验结果表明，表现最好的智能体仅解决了54.5%的任务，成功率在面向游戏玩法的任务中为46.9%，而在2D图形任务中降至31.6%，突显了任务难度与多模态复杂性之间的相关性。为了提高性能，作者提出了简单的基于图像和视频的反馈机制，这显著改善了智能体的表现，以Claude Sonnet 4.5为例，其成功率从33.3%提高到47.7%。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Learning Analysis of Physics-Informed Neural Networks</div>
<div class="meta-line">Authors: David A. Barajas-Solano</div>
<div class="meta-line">First: 2026-02-11T18:09:29+00:00 · Latest: 2026-02-11T18:09:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11097v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11097v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理信息神经网络的统计学习分析</div>
<div class="mono" style="margin-top:8px">我们从统计学习的角度研究物理信息学习在初值和边值问题（IBVP）中的训练和性能，使用物理信息神经网络（PINNs）。具体而言，我们将自己限制在具有严格初始和边界条件约束的参数化上，并将估计PINN参数的问题重新表述为一个统计学习问题。从这个角度来看，IBVP残差上的物理惩罚可以更好地理解为一个无限的间接数据源，而学习过程则是通过最小化真实数据生成分布与PINN分布之间的Kullback-Leibler散度，将PINN残差分布$p(y \mid x, t, w) q(x, t)$拟合到真实数据生成分布$δ(0) q(x, t)$。此外，这一分析表明，使用PINNs的物理信息学习是一个奇异学习问题，我们采用奇异学习理论工具，即所谓的局部学习系数（Lau等，2025）来分析通过随机优化获得的热方程IBVP的PINN参数估计。最后，我们讨论了这一分析对PINNs预测不确定性量化和外推能力的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the training and performance of physics-informed neural networks (PINNs) for initial and boundary value problems (IBVP) through a statistical learning lens. The authors reformulate the estimation of PINN parameters as a statistical learning problem, treating the physics penalty on IBVP residuals as an infinite source of indirect data and minimizing the Kullback-Leibler divergence between the true and PINN distributions. The findings reveal that physics-informed learning with PINNs is a singular learning problem, and the application of singular learning theory, particularly the Local Learning Coefficient, provides insights into the parameter estimates obtained through stochastic optimization for a heat equation IBVP, highlighting implications for predictive uncertainty and extrapolation capacity of PINNs.</div>
<div class="mono" style="margin-top:8px">本研究从统计学习的角度探讨了物理信息神经网络（PINNs）在初始和边界值问题（IBVP）中的训练和性能，旨在提高对PINN参数估计的理解。作者将PINN参数的估计重新表述为一个统计学习问题，将IBVP残差的物理惩罚视为无限间接数据源，并通过最小化真实分布与PINN分布之间的Kullback-Leibler散度来进行学习。主要发现表明，物理信息学习与PINNs是一个奇异学习问题，使用奇异学习理论，特别是局部学习系数，为通过随机优化获得的热方程IBVP的参数估计提供了见解，并对PINNs的预测不确定性和外推能力具有重要意义。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
