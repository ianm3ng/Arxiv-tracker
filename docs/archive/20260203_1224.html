<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-03 12:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260203_1224</div>
    <div class="row"><div class="card">
<div class="title">Reward-free Alignment for Conflicting Objectives</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin</div>
<div class="meta-line">First: 2026-02-02T18:59:52+00:00 · Latest: 2026-02-02T18:59:52+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02495v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无奖励对齐冲突目标</div>
<div class="mono" style="margin-top:8px">直接对齐方法越来越多地用于将大型语言模型（LLMs）与人类偏好对齐。然而，许多现实世界的对齐问题涉及多个冲突目标，简单的偏好聚合可能导致不稳定的训练和较差的权衡。特别是，加权损失方法可能无法识别同时改善所有目标的更新方向，而现有的多目标方法通常依赖于显式奖励模型，增加了额外的复杂性并扭曲用户指定的偏好。本文的贡献有两个方面。首先，我们提出了一种无奖励对齐冲突目标的框架（RACO），直接利用成对偏好数据，并通过一种新颖的剪切变体的冲突规避梯度下降来解决梯度冲突。我们提供了收敛保证，以尊重用户指定的目标权重的帕累托临界点，并进一步表明，在双目标设置中，剪切可以严格提高收敛速度。其次，我们使用一些启发式方法改进了我们的算法，并进行实验以证明所提框架在LLM对齐中的兼容性。在多个LLM家族（Qwen 3、Llama 3、Gemma 3）上的多目标摘要和安全对齐任务的定性和定量评估表明，我们的方法在帕累托权衡方面始终优于现有的多目标对齐基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of aligning large language models (LLMs) with human preferences in scenarios involving multiple conflicting objectives, where traditional methods may lead to unstable training outcomes. The authors propose a Reward-free Alignment framework for Conflicted Objectives (RACO), which utilizes pairwise preference data and introduces a novel clipped variant of conflict-averse gradient descent to resolve gradient conflicts. Experimental results demonstrate that RACO provides convergence guarantees to Pareto-critical points while achieving better Pareto trade-offs in multi-objective summarization and safety alignment tasks across various LLM families compared to existing multi-objective alignment methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在涉及多个冲突目标的场景中，将大型语言模型（LLMs）与人类偏好对齐所面临的挑战，传统方法可能导致不稳定和不良结果。作者提出了一种无奖励冲突目标对齐框架（RACO），该框架利用成对偏好数据和一种新颖的剪切变体的冲突规避梯度下降法来解决梯度冲突。实验结果表明，RACO在收敛到Pareto临界点方面提供了保证，同时提高了收敛速度，并且在多目标摘要和安全对齐等任务中，RACO在各种LLM家族中始终优于现有的多目标对齐基准，表现出更好的Pareto权衡。</div>
</details>
</div>
<div class="card">
<div class="title">MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training</div>
<div class="meta-line">Authors: Dulhan Jayalath, Oiwi Parker Jones</div>
<div class="meta-line">First: 2026-02-02T18:59:50+00:00 · Latest: 2026-02-02T18:59:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02494v1">PDF</a> · <a href="https://github.com/neural-processing-lab/MEG-XL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEG-XL：通过长上下文预训练实现数据高效的脑到文本</div>
<div class="mono" style="margin-top:8px">临床脑到文本接口旨在为无法提供大量训练录音的瘫痪患者服务。预训练通过学习跨个体的统计先验来提高数据高效的泛化能力，但这些先验严重依赖于上下文。虽然自然语言可能在几分钟内逐渐展开，但大多数方法仅使用几秒的上下文进行预训练。因此，我们提出了MEG-XL，这是一种每个样本预训练2.5分钟MEG上下文的模型，比之前的工作长5-300倍，相当于191k个标记，捕捉扩展的神经上下文。在脑数据的词解码任务上进行微调，MEG-XL以较少的数据（例如1小时对比50小时）匹配监督性能，并超越脑基础模型。我们发现，使用更长上下文预训练的模型学习到的表示在词解码中转移效果更好。我们的结果表明，长上下文预训练有助于利用其他方法不必要丢弃的扩展神经上下文。代码、模型权重和说明可在https://github.com/neural-processing-lab/MEG-XL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance brain-to-text interfaces for paralyzed patients who have limited ability to provide training data. The authors introduce MEG-XL, a model that is pre-trained using 2.5 minutes of MEG context per sample, significantly longer than previous methods that used only a few seconds. Experimental results show that MEG-XL achieves comparable performance to supervised models while requiring substantially less data for fine-tuning, demonstrating that longer context pre-training improves the transferability of learned representations for word decoding tasks from brain data.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善为无法提供大量训练录音的瘫痪患者设计的脑-文本接口。作者提出了MEG-XL，这是一种每个样本预训练2.5分钟MEG上下文的模型，显著长于以前的方法，以通过捕捉扩展的神经上下文来提高数据效率的泛化能力。实验结果表明，MEG-XL在仅使用少量数据的情况下达到了与监督模型相当的性能，证明了更长的上下文预训练有助于更好地转移表示以进行单词解码任务。</div>
</details>
</div>
<div class="card">
<div class="title">PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</div>
<div class="meta-line">Authors: Zehong Ma, Ruihan Xu, Shiliang Zhang</div>
<div class="meta-line">First: 2026-02-02T18:59:42+00:00 · Latest: 2026-02-02T18:59:42+00:00</div>
<div class="meta-line">Comments: Project Pages: https://zehong-ma.github.io/PixelGen/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02493v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02493v1">PDF</a> · <a href="https://github.com/Zehong-Ma/PixelGen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zehong-ma.github.io/PixelGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PixelGen：像素扩散超越潜在扩散与感知损失</div>
<div class="mono" style="margin-top:8px">像素扩散以端到端的方式直接在像素空间生成图像，避免了两阶段潜在扩散中由变分自编码器引入的伪影和瓶颈。然而，优化包含许多感知无关信号的高维像素流形是具有挑战性的，这使得现有的像素扩散方法落后于潜在扩散模型。我们提出了PixelGen，一个简单的像素扩散框架，具有感知监督。PixelGen引入了两个互补的感知损失，以引导扩散模型学习更有意义的感知流形，而不是建模完整的图像流形。LPIPS损失有助于学习更好的局部模式，而基于DINO的感知损失则增强了全局语义。在感知监督下，PixelGen超越了强大的潜在扩散基线。在ImageNet-256上，它在仅使用80个训练周期的情况下实现了5.11的FID，并在大规模文本到图像生成中展示了良好的扩展性能，GenEval得分为0.79。PixelGen不需要变分自编码器、潜在表示和辅助阶段，提供了一个更简单但更强大的生成范式。代码可在https://github.com/Zehong-Ma/PixelGen公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image generation by addressing the limitations of existing pixel diffusion methods, which struggle with optimizing high-dimensional pixel manifolds. The authors propose PixelGen, a pixel diffusion framework that utilizes perceptual supervision through two complementary perceptual losses: an LPIPS loss for enhancing local patterns and a DINO-based loss for improving global semantics. Experimental results show that PixelGen outperforms strong latent diffusion baselines, achieving an FID of 5.11 on ImageNet-256 with only 80 training epochs and demonstrating effective scaling performance in large-scale text-to-image generation with a GenEval score of 0.79.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有像素扩散方法在高维像素流形上的局限性来改善图像生成。作者提出了PixelGen，这是一种利用感知监督的像素扩散框架，通过两种互补的感知损失：LPIPS用于局部模式，基于DINO的损失用于全局语义。实验结果表明，PixelGen超越了强大的潜在扩散基线，在仅80个训练周期内在ImageNet-256上实现了5.11的FID，并在大规模文本到图像生成中展示了有效的扩展能力，GenEval得分为0.79。</div>
</details>
</div>
<div class="card">
<div class="title">RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</div>
<div class="meta-line">Authors: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</div>
<div class="meta-line">First: 2026-02-02T18:59:04+00:00 · Latest: 2026-02-02T18:59:04+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Gen-Verse/Open-AgentRL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02488v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02488v1">PDF</a> · <a href="https://github.com/Gen-Verse/Open-AgentRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLAnything：在完全动态的强化学习系统中锻造环境、策略和奖励模型</div>
<div class="mono" style="margin-top:8px">我们提出了RLAnything，一个通过闭环优化动态锻造环境、策略和奖励模型的强化学习框架，增强学习信号并强化任何LLM或智能场景的整体RL系统。具体而言，策略通过逐步和结果信号的综合反馈进行训练，而奖励模型则通过一致性反馈共同优化，从而进一步改善策略训练。此外，我们基于理论的自动环境适应通过利用来自每个模型的评论反馈改善奖励和策略模型的训练，实现从经验中学习。实证结果表明，每个新增组件都持续改善整体系统，RLAnything在各种代表性的LLM和智能任务中带来了显著提升，在OSWorld上提升Qwen3-VL-8B-Thinking 9.1%，在AlfWorld和LiveBench上分别提升Qwen2.5-7B-Instruct 18.7%和11.9%。我们还发现，优化的奖励模型信号优于依赖人类标签的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance reinforcement learning systems by dynamically integrating environment, policy, and reward models through closed-loop optimization. The proposed framework, RLAnything, employs a method that combines step-wise and outcome feedback for policy training while optimizing the reward model with consistency feedback, which collectively improves the learning process. Experimental results demonstrate that the inclusion of these components leads to significant performance improvements, with RLAnything achieving gains of 9.1% on OSWorld and 18.7% and 11.9% on AlfWorld and LiveBench, respectively, for different models, while also showing that optimized reward signals are more effective than those based on human labels.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过动态整合环境、策略和奖励模型来增强强化学习（RL）系统，采用闭环优化方法。作者提出了RLAnything框架，该框架利用来自逐步和结果信号的综合反馈进行策略训练，同时通过一致性反馈优化奖励模型，以进一步增强策略训练。实验结果表明，每个组件的添加都能带来持续的改进，RLAnything在多个任务上取得了显著的性能提升，包括Qwen3-VL-8B-Thinking在OSWorld上提高了9.1%，Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提高了18.7%和11.9%，同时还显示优化的奖励模型信号优于基于人工标签的结果。</div>
</details>
</div>
<div class="card">
<div class="title">RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents</div>
<div class="meta-line">Authors: Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, Ruichun Ma, Bei Liu, Yifan Yang, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Xin Geng, Baining Guo</div>
<div class="meta-line">First: 2026-02-02T18:58:07+00:00 · Latest: 2026-02-02T18:58:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02486v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02486v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RE-TRAC：深度搜索代理的递归轨迹压缩</div>
<div class="mono" style="margin-top:8px">基于LLM的深度研究代理主要建立在ReAct框架上。这种线性设计使得重新访问早期状态、分支到替代搜索方向或在长上下文中保持全局意识变得困难，常常导致局部最优、冗余探索和低效搜索。我们提出了Re-TRAC，一个代理框架，通过在每个轨迹后生成结构化状态表示来进行跨轨迹探索，以总结证据、不确定性、失败和未来计划，并在此状态表示的基础上调整后续轨迹。这使得迭代反思和全局知情规划成为可能，将研究重新框架为一个渐进的过程。实证结果表明，Re-TRAC在前沿LLM的BrowseComp上始终比ReAct表现优越15-20%。对于较小的模型，我们引入了Re-TRAC感知的监督微调，在可比规模上实现了最先进的性能。值得注意的是，Re-TRAC在各轮中显示出工具调用和令牌使用的单调减少，表明通过跨轨迹反思驱动的逐步针对性探索，而非冗余搜索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of LLM-based deep search agents, particularly their inability to revisit earlier states and maintain global awareness, which can lead to inefficient search and local optima. The authors propose Re-TRAC, a framework that facilitates cross-trajectory exploration by creating a structured state representation after each trajectory, allowing for iterative reflection and informed planning. Experimental results demonstrate that Re-TRAC outperforms the existing ReAct framework by 15-20% on the BrowseComp dataset with advanced LLMs, and for smaller models, it achieves state-of-the-art performance through Re-TRAC-aware supervised fine-tuning, while also showing reduced tool calls and token usage across iterations, indicating more efficient exploration strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于LLM的深度研究代理在使用ReAct框架时的局限性，该框架在重新访问早期状态和在长上下文中保持全局意识方面存在困难。作者提出了Re-TRAC框架，通过在每个轨迹后创建结构化状态表示来增强跨轨迹探索，从而允许迭代反思和知情规划。实验结果表明，Re-TRAC在BrowseComp上比ReAct提高了15-20%的性能，并通过Re-TRAC感知的监督微调在较小模型中实现了最先进的性能，同时在迭代中显示出工具调用和令牌使用的一致减少，表明探索效率更高。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-02T18:56:56+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈扩展强化学习的能力</div>
<div class="mono" style="margin-top:8px">强化学习在大规模语言模型后训练中的成功源于一个不合理的不信息来源：每次回合一个信息位作为二元奖励或偏好标签。在另一个极端，蒸馏提供了密集的监督，但需要演示，这既昂贵又难以扩展。我们研究文本反馈作为一种中间信号：比标量奖励更丰富，但比完整演示更便宜。文本反馈是人类互动的自然模式，并且在许多现实世界场景中已经很丰富，用户、注释者和自动评审者经常批评大规模语言模型的输出。为了在规模上利用文本反馈，我们形式化了一个多轮强化学习设置，即文本反馈强化学习（RLTF），其中在训练期间可以获得文本反馈，但在推理时不可用。因此，模型必须学习内化反馈，以提高其测试时的单轮性能。为此，我们提出了两种方法：自蒸馏（RLTF-SD），训练单轮策略以匹配其自身反馈条件的第二轮生成；以及反馈建模（RLTF-FM），将反馈预测为辅助目标。我们对这两种方法进行了理论分析，并在推理难题、竞赛数学和创意写作任务上进行了实证评估。我们的结果表明，这两种方法在基准测试中始终优于强基线，突显了在规模上利用额外丰富监督源的强化学习的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of traditional reinforcement learning (RL) methods that rely on sparse binary rewards, which are often uninformative, and the challenges associated with using dense supervision through demonstrations. The authors propose a novel approach called RL from Text Feedback (RLTF), which utilizes text feedback as a richer and more scalable intermediate signal during training. They introduce two methods, Self Distillation (RLTF-SD) and Feedback Modeling (RLTF-FM), and demonstrate through empirical evaluations on reasoning puzzles, competition math, and creative writing tasks that both methods significantly outperform strong baselines, indicating the effectiveness of incorporating text feedback into RL frameworks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于传统强化学习（RL）方法依赖稀疏的二元奖励和使用演示进行密集监督的挑战。作者提出了一种新方法，称为文本反馈强化学习（RLTF），该方法在训练过程中利用文本反馈作为一种更丰富且可扩展的中间信号，而在推理时不可用。他们引入了两种方法，自我蒸馏（RLTF-SD）和反馈建模（RLTF-FM），并通过在推理难题、竞赛数学和创意写作任务上的实证评估，证明这两种方法显著优于强基线，表明将文本反馈纳入RL框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Flow Policy Gradients for Robot Control</div>
<div class="meta-line">Authors: Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong, Carmelo Sferrazza, Yi Ma, Rocky Duan, Pieter Abbeel, Guanya Shi, Karen Liu, Angjoo Kanazawa</div>
<div class="meta-line">First: 2026-02-02T18:56:49+00:00 · Latest: 2026-02-02T18:56:49+00:00</div>
<div class="meta-line">Comments: Project webpage: https://hongsukchoi.github.io/fpo-control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02481v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02481v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hongsukchoi.github.io/fpo-control">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于机器人控制的流政策梯度</div>
<div class="mono" style="margin-top:8px">基于似然的政策梯度方法是从奖励中训练机器人控制政策的主要方法。这些方法依赖于可微分的动作似然性，限制政策输出为简单的分布，如高斯分布。在本研究中，我们展示了流匹配政策梯度——一种绕过似然计算的最新框架——如何有效地用于在具有挑战性的机器人控制环境中训练和微调更具表现力的政策。我们引入了一种改进的目标，使得在腿部运动、类人运动跟踪和操作任务中取得成功，并在两台类人机器人上实现稳健的仿真到现实转移。然后，我们展示了训练动态的消融和分析。结果表明，政策在从头开始训练时如何利用流表示进行探索，以及在微调时相较于基线的稳健性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the training of robot control policies, which traditionally rely on likelihood-based methods that limit policy outputs to simple distributions. The authors propose using flow matching policy gradients, a framework that eliminates the need for likelihood computation, to develop more expressive policies. Experimental results demonstrate the effectiveness of their improved objective in various tasks, including legged locomotion and humanoid motion tracking, achieving robust performance in sim-to-real transfer across two humanoid robots, with findings indicating better exploration and fine-tuning capabilities compared to existing baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强机器人控制策略的训练，传统方法依赖于限制策略输出为简单分布的基于似然的方法。作者提出使用流匹配策略梯度，这一框架避免了似然计算，从而开发出更具表现力的策略。实验结果表明，他们改进的目标在腿部运动、人形运动跟踪和操作等多种任务中取得了成功，并在类人机器人上实现了稳健的仿真到现实转移，突显了流表示在探索和改进微调鲁棒性方面相较于现有基线的优势。</div>
</details>
</div>
<div class="card">
<div class="title">AgentRx: Diagnosing AI Agent Failures from Execution Trajectories</div>
<div class="meta-line">Authors: Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath, Chetan Bansal</div>
<div class="meta-line">First: 2026-02-02T18:54:07+00:00 · Latest: 2026-02-02T18:54:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02475v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02475v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentRx：从执行轨迹诊断AI代理失败</div>
<div class="mono" style="margin-top:8px">AI代理经常以难以定位的方式失败，因为执行是概率性的、长时间跨度的、多代理的，并且受到噪声工具输出的影响。我们通过手动注释失败的代理运行来填补这一空白，并发布了一个包含115个失败轨迹的新基准，涵盖结构化API工作流、事件管理和开放式网络/文件任务。每个轨迹都注释了一个关键失败步骤和一个来自基于扎根理论的跨领域失败分类法的类别。为了减轻失败归因的人力成本，我们提出了AGENTRX，一个自动化的领域无关诊断框架，能够准确定位失败代理轨迹中的关键失败步骤。它综合约束，逐步评估，并生成一个可审计的约束违规验证日志及相关证据；基于LLM的评判者使用该日志来定位关键步骤和类别。我们的框架在三个领域中改善了步骤定位和失败归因，相较于现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in diagnosing failures of AI agents, which are often difficult to localize due to the probabilistic and complex nature of their executions. The authors developed a novel benchmark consisting of 115 manually annotated failed trajectories across various tasks and created AGENTRX, an automated diagnostic framework that identifies critical failure steps in these trajectories. The key experimental findings demonstrate that AGENTRX significantly enhances the accuracy of step localization and failure attribution compared to existing methods across three different domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决AI代理在复杂和概率性环境中发生故障时，难以识别根本原因的问题。作者开发了一个包含115个手动注释的失败执行轨迹的新基准，涵盖各种任务，并建立了一个故障分类法来对这些故障进行分类。他们引入了AGENTRX，一个自动化诊断框架，通过综合约束并系统性地评估这些约束，识别关键故障步骤，从而在多个领域中相比现有方法提高了故障的定位和归因能力。</div>
</details>
</div>
<div class="card">
<div class="title">MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents</div>
<div class="meta-line">Authors: Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang, Haodong Yue, Wenya Wang</div>
<div class="meta-line">First: 2026-02-02T18:53:28+00:00 · Latest: 2026-02-02T18:53:28+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/MemSkill</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02474v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02474v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/MemSkill">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemSkill：为自我进化代理学习和发展记忆技能</div>
<div class="mono" style="margin-top:8px">大多数大型语言模型（LLM）代理的记忆系统依赖于一小组静态的、手工设计的操作来提取记忆。这些固定的程序硬编码了人类关于存储内容和如何修订记忆的先验知识，使其在多样的交互模式下变得僵化，并在长历史上效率低下。为此，我们提出了\textbf{MemSkill}，将这些操作重新构架为可学习和可进化的记忆技能，形成结构化和可重用的例程，用于从交互轨迹中提取、整合和修剪信息。受到代理技能设计理念的启发，MemSkill采用一个\emph{控制器}，学习选择一小组相关技能，并配合一个基于LLM的\emph{执行器}，生成技能引导的记忆。除了学习技能选择，MemSkill还引入一个\emph{设计者}，定期审查在选定技能产生不正确或不完整记忆的困难案例，并通过提出改进和新技能来进化技能集。总之，MemSkill形成了一个闭环程序，改善了技能选择策略和技能集本身。在LoCoMo、LongMemEval、HotpotQA和ALFWorld上的实验表明，MemSkill在任务表现上优于强基线，并在不同设置中具有良好的泛化能力。进一步的分析揭示了技能如何进化，为LLM代理提供了更具适应性和自我进化的记忆管理的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing memory systems in Large Language Model (LLM) agents, which rely on static operations that hinder adaptability and efficiency in managing long histories. The authors propose MemSkill, a framework that transforms memory operations into learnable and evolvable skills, utilizing a controller to select relevant skills and an LLM-based executor to generate memory outputs. Experimental results on various benchmarks, including LoCoMo and HotpotQA, show that MemSkill significantly enhances task performance compared to strong baselines and demonstrates effective generalization across different scenarios, while also providing insights into the evolution of memory skills for improved adaptability in LLM agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有大型语言模型（LLM）代理的记忆系统的局限性，这些系统依赖于静态操作，限制了适应性和效率。作者提出了MemSkill框架，将记忆操作转变为可学习和可演化的技能，利用控制器选择相关技能，并通过基于LLM的执行器创建技能指导的记忆。在LoCoMo和HotpotQA等多个基准测试中的实验结果表明，MemSkill在任务表现上优于强基线，并在不同设置中表现出有效的泛化，同时提供了关于记忆技能演化的见解，以实现更具适应性的记忆管理。</div>
</details>
</div>
<div class="card">
<div class="title">HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos</div>
<div class="meta-line">Authors: Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui, Qifeng Chen, Jingbo Wang, Jiangmiao Pang, Ping Tan</div>
<div class="meta-line">First: 2026-02-02T18:53:01+00:00 · Latest: 2026-02-02T18:53:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanX：从人类视频中迈向灵活且可推广的人形交互技能</div>
<div class="mono" style="margin-top:8px">使人形机器人能够执行灵活和适应性的交互任务一直是机器人技术中的核心挑战。目前的方法受到现实交互数据稀缺或需要细致的任务特定奖励工程的瓶颈限制，这限制了它们的可扩展性。为缩小这一差距，我们提出了HumanX，一个完整的框架，将人类视频编译为可推广的现实世界交互技能，无需任务特定奖励。HumanX集成了两个共同设计的组件：XGen，一个数据生成管道，从视频中合成多样且物理上合理的机器人交互数据，同时支持可扩展的数据增强；以及XMimic，一个统一的模仿学习框架，学习可推广的交互技能。在篮球、足球、羽毛球、货物拾取和反应战斗五个不同领域进行评估，HumanX成功获得10种不同技能，并将其零-shot转移到物理Unitree G1人形机器人。所学能力包括复杂的动作，如无外部感知的假动作转身后仰跳投，以及持续的人机传球序列等交互任务，后者是从单个视频演示中学习的。我们的实验表明，HumanX的推广成功率比以前的方法高出8倍以上，展示了一条可扩展且与任务无关的学习多功能现实世界机器人交互技能的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations in current humanoid robot interaction capabilities, which are often hindered by a lack of realistic data and the need for specific reward engineering. The authors present HumanX, a comprehensive framework that generates generalizable interaction skills from human videos without requiring task-specific rewards. Through the integration of a data generation pipeline (XGen) and an imitation learning framework (XMimic), the system was evaluated across five domains, successfully acquiring ten skills and demonstrating over eight times higher generalization success compared to previous methods, including complex maneuvers and sustained interaction tasks learned from a single video demonstration.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前类人机器人交互能力的局限性，这些局限性受到缺乏真实交互数据和对特定奖励工程需求的制约。作者提出了HumanX，这是一个全面的框架，可以从人类视频中生成通用的交互技能，而无需依赖特定任务的奖励。通过集成数据生成管道（XGen）和模仿学习框架（XMimic），HumanX成功地学习并转移了10种不同的技能，涵盖了篮球和反应战斗等多个领域，应用于物理类人机器人。实验结果表明，HumanX的通用化成功率比以往方法高出8倍以上，突显了其在可扩展和适应性机器人交互技能方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning</div>
<div class="meta-line">Authors: Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu, Shiyi Zhan, Yiyuan Ma, Liang Xiang, Xingyan Bin, Di He</div>
<div class="meta-line">First: 2026-02-02T18:52:52+00:00 · Latest: 2026-02-02T18:52:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02472v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02472v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARKLING：平衡信号保留与对称破坏以实现宽度渐进学习</div>
<div class="mono" style="margin-top:8px">渐进学习（PL）通过逐步增加模型规模来减少预训练计算开销。尽管之前的研究广泛探讨了深度扩展，但宽度扩展仍然显著缺乏研究，现有的少数方法仅限于训练的早期阶段。然而，在中期扩展宽度对于最大化计算节省至关重要，但由于严重的训练不稳定性，这仍然是一个巨大的挑战。实证研究表明，在此阶段的简单初始化会破坏激活统计，导致损失峰值，而基于复制的初始化则引入梯度对称，阻碍特征多样性。为了解决这些问题，我们提出了SPARKLING（平衡信号保留与对称破坏以实现宽度渐进学习），这是一个用于中期宽度扩展的新框架。我们的方法通过RMS尺度一致性实现信号保留，在扩展过程中稳定激活统计。通过不对称优化器状态重置和学习率重新升温确保对称破坏。在Mixture-of-Experts（MoE）模型上的大量实验表明，在多个宽度轴和优化器家族中，SPARKLING始终优于从头训练，并在$2\times$宽度扩展下将训练成本降低了多达35%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of width expansion in Progressive Learning (PL), which has been largely overlooked compared to depth expansion, particularly during mid-stage training where instabilities can arise. The authors propose a novel framework called SPARKLING, which balances signal preservation and symmetry breaking to facilitate effective width expansion. Experimental results show that SPARKLING stabilizes activation statistics and enhances feature diversity, leading to a consistent performance improvement over training from scratch and achieving up to a 35% reduction in training costs with $2\times$ width expansion across various Mixture-of-Experts models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决渐进学习（PL）中宽度扩展的挑战，特别是在训练的中期，现有方法已被证明不足。作者提出了一种名为SPARKLING的新框架，旨在平衡信号保留和对称破坏，以稳定宽度扩展期间的训练。实验结果表明，SPARKLING有效地维持了激活统计数据并增强了特征多样性，导致相较于从头训练的一致性能提升，并在各种专家混合模型中实现了$2\times$宽度扩展下训练成本降低高达35%。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-head automated segmentation by incorporating detection head into the contextual layer neural network</div>
<div class="meta-line">Authors: Edwin Kys, Febian Febian</div>
<div class="meta-line">First: 2026-02-02T18:51:25+00:00 · Latest: 2026-02-02T18:51:25+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02471v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过将检测头纳入上下文层神经网络实现多头自动分割</div>
<div class="mono" style="margin-top:8px">基于深度学习的自动分割在放射治疗中越来越多地被使用，但传统模型在缺乏目标结构的切片中常常产生解剖上不合理的假阳性或幻觉。我们提出了一种基于Swin U-Net的门控多头Transformer架构，增强了切片间上下文集成和并行检测头，能够通过多层感知机进行切片级结构检测，并通过上下文增强流进行像素级分割。检测输出对分割预测进行门控，以抑制解剖上无效切片中的假阳性，并使用切片级Tversky损失进行训练，以解决类别不平衡问题。在癌症影像档案馆的前列腺解剖边缘案例数据集上的实验表明，门控模型显著优于非门控的仅分割基线，平均Dice损失为$0.013 \pm 0.036$，而非门控模型为$0.732 \pm 0.314$，检测概率与解剖存在性强相关，有效消除了虚假分割。相比之下，非门控模型在所有切片中表现出更高的变异性和持续的假阳性。这些结果表明，基于检测的门控增强了自动分割应用中的鲁棒性和解剖合理性，减少了幻觉预测，而不影响有效切片的分割质量，并为提高临床放射治疗自动轮廓工作流程的可靠性提供了有前景的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the reliability of automated segmentation in radiotherapy, as conventional models often generate anatomically implausible false positives in slices without target structures. The authors propose a gated multi-head Transformer architecture based on Swin U-Net, which integrates inter-slice context and incorporates a parallel detection head to perform both slice-level structure detection and pixel-level segmentation. Experimental results on the Prostate-Anatomical-Edge-Cases dataset show that the gated model significantly outperforms a non-gated baseline, achieving a mean Dice loss of 0.013 ± 0.036 compared to 0.732 ± 0.314, effectively reducing false positives and enhancing anatomical plausibility without compromising segmentation quality in valid slices.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高放射治疗中自动分割的可靠性，因为传统模型在缺乏目标结构的切片中常常产生不符合解剖学的假阳性。作者提出了一种基于Swin U-Net的门控多头Transformer架构，该架构整合了切片间的上下文，并包含一个并行检测头，以增强分割精度。在前列腺解剖边缘案例数据集上的实验结果表明，门控模型显著优于非门控基线，平均Dice损失为0.013 ± 0.036，而非门控模型为0.732 ± 0.314，有效减少了假阳性并提高了分割结果的解剖学合理性。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</div>
<div class="meta-line">Authors: Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</div>
<div class="meta-line">First: 2026-02-02T18:50:57+00:00 · Latest: 2026-02-02T18:50:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02470v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02470v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the &quot;reversal curse&quot; -- when trained on forward knowledge data of the form &quot;$A \rightarrow B$&quot; (e.g., Alice&#x27;s husband is Bob), the model is unable to deduce the reversal knowledge &quot;$B \leftarrow A$&quot; (e.g., Bob&#x27;s wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form &quot;$A \to A$&quot; (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过身份桥打破自回归语言模型中的反转诅咒</div>
<div class="mono" style="margin-top:8px">自回归大型语言模型（LLMs）在许多复杂任务中取得了显著成功，但在非常简单的逻辑推理中仍然可能失败，例如“反转诅咒”——当在形式为“$A \rightarrow B$”的前向知识数据上训练时（例如，爱丽丝的丈夫是鲍勃），模型在测试时无法推导出反向知识“$B \leftarrow A$”（例如，鲍勃的妻子是爱丽丝）。大量先前研究表明，这种失败是自回归因果LLMs的固有、基本限制，表明这些模型倾向于记忆事实级别的知识，而不是捕捉更高层次的规则。在本文中，我们通过展示这种看似基本的限制可以通过稍微调整训练数据来减轻，提出了一个简单的正则化数据配方，称为身份桥，形式为“$A \to A$”（例如，爱丽丝的名字是爱丽丝）。理论上，我们证明在这种配方下，即使是一个单层变换器也可以通过分析梯度下降的隐含偏差来打破反转诅咒。实证上，我们展示了一个经过1B预训练的语言模型，使用所提出的数据配方进行微调，在反转任务上达到了40%的成功率，而仅在前向知识数据上训练时成功率几乎为零。我们的工作为反转诅咒提供了新的理论基础，并提供了一条原则性、低成本的路径，以鼓励LLMs从数据中学习更高层次的规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of autoregressive large language models (LLMs) in logical reasoning, particularly the &#x27;reversal curse&#x27; where models struggle to infer reverse relationships from forward knowledge. The authors propose a method called the Identity Bridge, which involves modifying the training data to include pairs of the form &#x27;$A \to A$&#x27; to help models learn higher-level rules. Experimental results demonstrate that a 1B parameter pretrained language model fine-tuned with this approach achieves a 40% success rate on reversal tasks, significantly improving performance compared to a near-zero success rate when trained only on forward-knowledge data.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自回归大型语言模型（LLMs）在逻辑推理中的局限性，特别是模型在从前向知识推导反向关系时的“反转诅咒”现象。作者提出了一种名为身份桥的方法，通过修改训练数据以包含形式为&#x27;$A \to A$&#x27;的对。理论分析表明，这种方法使得即使是简单模型也能克服反转诅咒，实证结果显示，经过这种方法微调的1B参数预训练模型在反转任务上的成功率达到40%，而仅在前向知识数据上训练时成功率几乎为零。</div>
</details>
</div>
<div class="card">
<div class="title">Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation</div>
<div class="meta-line">Authors: Ahmed M. Elshazly, Ahmed Arafa</div>
<div class="meta-line">First: 2026-02-02T18:50:51+00:00 · Latest: 2026-02-02T18:50:51+00:00</div>
<div class="meta-line">Comments: To appear in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02469v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02469v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于空中聚合的年龄感知边缘盲联邦学习</div>
<div class="mono" style="margin-top:8px">我们研究了在无线衰落信道上进行的联邦学习（FL），多个设备同时发送其模型更新。我们提出了一种高效的年龄感知边缘盲空中FL方法，该方法不需要设备的信道状态信息（CSI）。相反，参数服务器（PS）使用多个天线，并基于其估计的信道增益总和应用最大比合并（MRC）来检测参数更新。一个关键挑战是正交子载波的数量有限；因此，传输多个参数需要多个正交频分复用（OFDM）符号，这会增加延迟。为了解决这个问题，PS每轮仅选择一小部分模型坐标，使用AgeTop-\(k\)，该方法首先选择幅度最大的条目，然后选择自上次被选中以来等待时间最长的\(k\)个坐标。这确保所有选定的参数适合一个OFDM符号，从而减少延迟。我们提供了一个收敛界限，突出了使用更多天线阵列元素的优势，并展示了一个关键权衡：增加\(k\)会降低压缩误差，但会增加信道噪声的影响。实验结果表明：（i）更多的PS天线显著提高了准确性和收敛速度；（ii）在相对良好的信道条件下，AgeTop-\(k\)优于随机选择；（iii）最佳的\(k\)取决于信道，在噪声环境中较小的\(k\)更好。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance federated learning (FL) over wireless fading channels without requiring channel state information (CSI) at the devices. The authors propose an age-aware edge-blind over-the-air FL method that utilizes multiple antennas at the parameter server (PS) and employs maximum-ratio combining (MRC) for detecting parameter updates. Key experimental findings indicate that increasing the number of PS antennas significantly improves accuracy and convergence speed, AgeTop-k selection outperforms random selection in favorable channel conditions, and the optimal value of k varies with channel conditions, with smaller k being preferable in noisy environments.</div>
<div class="mono" style="margin-top:8px">本研究解决了在无线衰落信道上进行联邦学习的挑战，其中多个设备同时发送模型更新而无需通道状态信息。提出的方法是年龄感知的边缘盲联邦学习，利用参数服务器的多个天线并采用最大比率合并来检测更新，同时使用AgeTop-k选择小部分模型坐标以减少延迟。主要发现表明，增加天线数量可以提高准确性和收敛速度，AgeTop-k在良好的信道条件下优于随机选择，且k的最佳值随信道条件变化，在噪声环境中较小的k更为理想。</div>
</details>
</div>
<div class="card">
<div class="title">Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</div>
<div class="meta-line">Authors: Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, Louis Berridge</div>
<div class="meta-line">First: 2025-03-10T20:12:06+00:00 · Latest: 2026-02-02T18:50:47+00:00</div>
<div class="meta-line">Comments: 24 pages, 14 figures. Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, and Oliver Powell contributed equally to this paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.07825v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.07825v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Helios 2.0：一种针对基于事件传感器的可穿戴设备优化的强大超低功耗手势识别系统</div>
<div class="mono" style="margin-top:8px">我们提出了一项可穿戴技术的进展：一种移动优化的实时超低功耗事件摄像头系统，使智能眼镜能够自然地进行手势控制，显著改善用户体验。尽管计算机视觉中的手势识别已取得显著进展，但在创建直观、适应不同用户和环境的系统以及足够节能以用于实际可穿戴应用方面仍面临重大挑战。我们的方法通过精心选择的微手势来应对这些挑战：拇指在食指上横向滑动（双向）和拇指与食指指尖之间的双重捏合。这些以人为本的交互利用自然的手部动作，确保直观的可用性，无需用户学习复杂的命令序列。为了克服用户和环境的变异性，我们开发了一种新颖的仿真方法，能够在不进行广泛的现实世界数据收集的情况下实现全面的领域采样。我们的功率优化架构保持了卓越的性能，在具有多样化用户和环境的基准数据集上实现了超过80%的F1分数。所得到的模型在利用高通Snapdragon Hexagon DSP时仅以6-8毫瓦的功耗运行，我们的2通道实现超过70%的F1准确率，而我们的6通道模型在用户研究中超过80%的F1准确率，涵盖所有手势类别。这些结果仅使用合成训练数据实现。与使用DSP时相比，这在F1准确率上提高了20%，功耗降低了25倍。这一进展使得在可穿戴设备中部署超低功耗视觉系统更近一步，并为无缝的人机交互开辟了新的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance user experience in wearable technology through intuitive hand gesture control for smart glasses while addressing challenges related to adaptability and energy efficiency. The authors developed a mobile-optimized, ultra-low-power event camera system that recognizes specific microgestures, such as lateral thumb swipes and double pinches, using a novel simulation methodology for comprehensive domain sampling. The experimental results demonstrate that their power-optimized architecture achieves F1 scores above 80% on benchmark datasets, operates at only 6-8 mW with the Qualcomm Snapdragon Hexagon DSP, and improves F1 accuracy by 20% while reducing power consumption by 25 times compared to existing systems, thereby advancing the feasibility of ultra-low-power vision systems in wearables.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过开发一个强大、超低功耗的手势识别系统来增强可穿戴技术中的用户体验，该系统允许智能眼镜进行自然的手势控制。作者采用了一个移动优化的实时事件摄像头系统，识别特定的微手势，如侧向拇指滑动和双指捏合，同时通过一种新颖的仿真方法解决用户变异性和能效的挑战。实验结果表明，该系统在基准数据集上的F1得分超过80%，在使用高通Snapdragon Hexagon DSP时功耗仅为6-8毫瓦，并且与现有系统相比，F1准确率提高了20%，功耗降低了25倍，从而促进了可穿戴设备中无缝的人机交互。</div>
</details>
</div>
<div class="card">
<div class="title">Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</div>
<div class="meta-line">Authors: Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2026-02-02T18:50:07+00:00 · Latest: 2026-02-02T18:50:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Avenir-Web：模仿人类体验的多模态网络代理与基础专家混合</div>
<div class="mono" style="margin-top:8px">尽管多模态大型语言模型取得了进展，但自主网络代理在复杂和动态的网络界面上仍然难以可靠地执行长期任务。现有代理通常面临元素定位不准确、缺乏特定网站的程序知识以及不稳定的长期任务跟踪和记忆，尤其是在复杂的文档对象模型结构上操作时。为了解决这些局限性，我们推出了Avenir-Web，这是一种在实际部署中在Online-Mind2Web基准上实现新开源最先进技术的网络代理。Avenir-Web利用基础专家混合、经验模仿规划以纳入程序先验，以及结合自适应记忆的任务跟踪清单，以实现跨多种用户界面范式的稳健和无缝交互。我们在Online-Mind2Web上评估Avenir-Web，这是一个严格的实时和以用户为中心的网络任务基准。我们的结果表明，Avenir-Web显著超越了之前的开源代理，并与顶级专有模型达成性能平衡，从而为可靠的网络代理在实时网站上建立了新的开源最先进技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of autonomous web agents in executing long-horizon tasks on complex web interfaces, addressing issues such as inaccurate element grounding and unstable task tracking. The authors introduce Avenir-Web, which employs a Mixture of Grounding Experts and Experience-Imitation Planning to incorporate procedural knowledge, alongside a task-tracking checklist and adaptive memory for enhanced interaction. Experimental results show that Avenir-Web achieves a new state of the art on the Online-Mind2Web benchmark, outperforming previous open-source agents and matching the performance of leading proprietary models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高自主网页代理在复杂网页界面上执行长时间任务的可靠性，解决了元素定位不准确和任务跟踪不稳定等问题。作者提出了Avenir-Web，采用了混合定位专家、经验模仿规划和结合自适应记忆的任务跟踪清单，以增强在各种用户界面上的交互。实验结果表明，Avenir-Web在Online-Mind2Web基准测试中设立了新的开源最先进水平，超越了之前的开源代理，并达到了与领先的专有模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</div>
<div class="meta-line">Authors: Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel</div>
<div class="meta-line">First: 2026-02-02T18:49:06+00:00 · Latest: 2026-02-02T18:49:06+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02465v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02465v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentisOculi：揭示心理意象推理的局限性</div>
<div class="mono" style="margin-top:8px">前沿模型正从仅仅摄取视觉信息的多模态大型语言模型（MLLMs）转向能够本地交错生成的统一多模态模型（UMMs）。这一转变引发了将中间可视化作为推理辅助工具的兴趣，类似于人类的心理意象。该理念的核心在于以目标导向的方式形成、维持和操控视觉表征。为了评估和探讨这一能力，我们开发了MentisOculi，这是一个程序化的、分层的多步骤推理问题套件，适合视觉解决方案，旨在挑战前沿模型。评估从潜在标记到显式生成图像的视觉策略，我们发现它们通常未能提高性能。对UMMs的分析特别揭示了一个关键限制：尽管它们具备解决任务的文本推理能力，并且有时能够生成正确的视觉图像，但它们遭受累积生成错误，未能利用真实的可视化。我们的发现表明，尽管视觉思维具有内在吸引力，但尚未对模型推理产生益处。MentisOculi建立了分析和弥补这一差距的必要基础，适用于多种模型家族。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the limitations of reasoning with mental imagery in unified multimodal models (UMMs), motivated by the transition from multimodal large language models (MLLMs) to UMMs that can generate interleaved visual and textual outputs. The authors developed MentisOculi, a suite of multi-step reasoning problems designed to evaluate the effectiveness of visual strategies in enhancing model performance. The experimental results indicate that these visual strategies, including both latent tokens and generated imagery, generally do not improve performance, revealing that UMMs, despite their ability to generate correct visuals, struggle with compounding generation errors and do not effectively utilize even accurate visualizations, highlighting a significant gap in model reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究探讨了统一多模态模型（UMMs）在利用视觉表征时推理的局限性，研究动机是中间视觉化在推理中可能的帮助作用，类似于人类的心理意象。作者开发了MentisOculi，这是一个结构化的多步骤推理问题套件，旨在评估各种视觉策略的有效性，包括潜在标记和生成的图像。主要发现表明，这些视觉策略通常不会提高性能，而UMMs尽管能够生成正确的视觉效果，却受到累积生成错误的困扰，无法有效利用准确的视觉化，表明视觉思维目前并未改善模型推理。</div>
</details>
</div>
<div class="card">
<div class="title">Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Gabriele Maraia, Marco Valentino, Fabio Massimo Zanzotto, Leonardo Ranaldi</div>
<div class="meta-line">First: 2026-02-02T18:48:44+00:00 · Latest: 2026-02-02T18:48:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model&#x27;s internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model&#x27;s activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内容不变推理中的抽象激活空间</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在三段论推理中的演绎判断常常存在困难，系统性地将语义合理性与形式有效性混淆，这一现象被称为内容效应。即使模型生成逐步解释，这种偏见仍然存在，表明中间推理可能继承影响答案的相同语义捷径。最近的方法提出通过增加推理时的结构约束来缓解这一问题，或者通过鼓励抽象的中间表示，或直接干预模型的内部计算；然而，可靠地抑制语义干扰仍然是一个未解决的挑战。为了使形式推理对语义内容的敏感性降低，我们引入了一个抽象引导推理的框架，明确将结构推理与词汇语义分开。我们构建了配对的内容丰富和抽象的三段论，并使用模型在抽象输入上的激活来定义抽象推理空间。然后，我们学习轻量级的抽象器，从内容条件的残差流状态中预测与该空间对齐的表示，并通过前向传播中的多层干预整合这些预测。以跨语言迁移作为测试平台，我们展示了与抽象对齐的引导减少了内容驱动的错误，并提高了对有效性敏感的性能。我们的结果将激活级别的抽象定位为增强LLMs在语义干扰下形式推理鲁棒性的可扩展机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the content effect in Large Language Models (LLMs), where models conflate semantic plausibility with formal validity in syllogistic reasoning. The authors propose a framework for abstraction-guided reasoning that separates structural inference from lexical semantics by constructing paired content-laden and abstract syllogisms, using model activations on abstract inputs to define an abstract reasoning space. The key findings indicate that using abstraction-aligned steering through lightweight Abstractors significantly reduces content-driven errors and enhances validity-sensitive performance, demonstrating a scalable approach to improving formal reasoning robustness in LLMs against semantic interference.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在推理判断中的挑战，特别是内容效应问题，即模型混淆语义合理性与形式有效性。为此，作者提出了一种抽象引导推理框架，明确将结构推理与词汇语义分开，利用成对的内容丰富和抽象的三段论进行实验。实验结果表明，使用与抽象对齐的引导显著减少了内容驱动的错误，并提高了对有效性敏感的性能，表明激活级别的抽象可以有效改善形式推理在语义干扰下的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Conflict-Aware Client Selection for Multi-Server Federated Learning</div>
<div class="meta-line">Authors: Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</div>
<div class="meta-line">First: 2026-02-02T18:47:16+00:00 · Latest: 2026-02-02T18:47:16+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向冲突的多服务器联邦学习客户端选择</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）作为一种有前景的分布式机器学习（ML）方法，使得客户端能够在不暴露原始数据的情况下进行协作模型训练，从而保护用户隐私并降低通信成本。尽管有这些好处，传统的单服务器FL由于需要聚合大量客户端的模型，导致高通信延迟。多服务器FL虽然可以将工作负载分配到边缘服务器，但重叠的客户端覆盖和不协调的选择常常导致资源争用，造成带宽冲突和训练失败。为了解决这些局限性，我们提出了一种去中心化的强化学习方法，结合冲突风险预测，称为RL CRP，以优化多服务器FL系统中的客户端选择。具体而言，每个服务器使用基于其稀疏历史客户端选择序列的分类隐马尔可夫模型来估计客户端选择冲突的可能性。然后，结合公平性奖励机制，以促进长期客户端参与，从而最小化训练延迟和资源争用。大量实验表明，所提出的RL-CRP框架有效减少了服务器间的冲突，并显著提高了收敛速度和通信成本方面的训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of multi-server federated learning (FL) by addressing the issues of resource contention and communication latency caused by overlapping client coverage. The authors propose a decentralized reinforcement learning approach, named RL CRP, which utilizes a categorical hidden Markov model to predict client selection conflicts based on historical data, along with a fairness-aware reward mechanism to encourage consistent client participation. Experimental results show that the RL-CRP framework significantly reduces inter-server conflicts and enhances training efficiency, as evidenced by improved convergence speed and reduced communication costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决多服务器联邦学习中由于重叠客户端覆盖导致的通信延迟和资源竞争问题，来提高效率。作者提出了一种名为RL CRP的去中心化强化学习方法，该方法利用分类隐马尔可夫模型进行冲突风险预测，以优化多个服务器之间的客户端选择。实验结果表明，RL-CRP框架有效减少了服务器间的冲突，并显著提高了训练效率，表现为更快的收敛速度和更低的通信成本。</div>
</details>
</div>
<div class="card">
<div class="title">Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</div>
<div class="meta-line">Authors: Han Bao, Zheyuan Zhang, Pengcheng Jing, Zhengqing Yuan, Kaiwen Shi, Yanfang Ye</div>
<div class="meta-line">First: 2026-02-02T18:46:16+00:00 · Latest: 2026-02-02T18:46:16+00:00</div>
<div class="meta-line">Comments: 65 pages, 40 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02455v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02455v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>漂移基准：通过多轮交互诊断LLM代理在输入故障下的合作崩溃</div>
<div class="mono" style="margin-top:8px">随着大型语言模型过渡到自主代理，用户输入经常违反合作假设（例如，隐含意图、缺失参数、错误前提或模糊表达），造成文本评估无法捕捉的执行风险。现有基准通常假设指令明确或将评估限制为仅文本的单轮澄清，因此无法测量在基础执行风险下的多轮消歧。我们引入\textbf{漂移基准}，这是第一个通过多轮澄清在状态导向和服务导向执行环境中评估输入故障下代理语用学的诊断基准。基于经典的沟通理论，\textbf{漂移基准}提供了合作崩溃的统一分类法，并采用了基于角色的用户模拟器和\textbf{Rise}评估协议。实验表明，在这些故障下性能显著下降，澄清效果因用户角色和故障类型而异。\MethodName连接了澄清研究和代理安全评估，使系统化诊断可能导致不安全执行的失败成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the execution risks faced by Large Language Models (LLMs) when transitioning to autonomous agents, particularly due to user inputs that violate cooperative assumptions. The authors introduce Drift-Bench, a diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn interactions in various execution environments. Experimental results reveal significant performance declines when LLMs encounter these input faults, with the effectiveness of clarification varying based on user personas and fault types, highlighting the need for improved safety evaluations in agentic systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLMs）在转变为自主代理时面临的执行风险，特别是由于用户输入违反合作假设所导致的风险。作者提出了Drift-Bench，这是一个诊断基准，旨在通过多轮澄清在各种执行环境中评估代理的语用学。主要实验结果表明，当LLMs遇到这些故障时，性能显著下降，澄清的有效性因用户角色和故障类型的不同而有所变化。</div>
</details>
</div>
<div class="card">
<div class="title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</div>
<div class="meta-line">Authors: Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang</div>
<div class="meta-line">First: 2026-02-02T18:44:45+00:00 · Latest: 2026-02-02T18:44:45+00:00</div>
<div class="meta-line">Comments: https://world-gymnast.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02454v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://world-gymnast.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界体操：在世界模型中使用强化学习训练机器人</div>
<div class="mono" style="margin-top:8px">机器人通过与物理世界的互动进行学习，受到物理交互成本的根本限制。两种替代方案，专家演示的监督微调（SFT）和基于软件的模拟器中的强化学习（RL），都受到可用专家数据量和操作的模拟与现实差距的限制。随着最近从真实世界视频-动作数据中学习的世界模型的出现，我们提出一个问题：在世界模型中训练策略是否比监督学习或软件模拟更有效，以实现更好的真实机器人性能。我们提出了World-Gymnast，它通过在一个动作条件的视频世界模型中展开策略，并用视觉-语言模型（VLM）对展开进行奖励，来执行视觉-语言-动作（VLA）策略的RL微调。在桥机器人设置中，World-Gymnast的表现比SFT高出多达18倍，比软件模拟器高出多达2倍。更重要的是，World-Gymnast展示了使用世界模型的RL的有趣能力，包括在来自世界模型的多样语言指令和新场景上进行训练，在新场景中的测试时训练，以及在线迭代世界模型和策略改进。我们的结果表明，学习一个世界模型并在云中训练机器人策略可能是弥合在演示中工作的机器人与能够在任何家庭中工作的机器人之间差距的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional robot learning methods, which are hindered by the costs of physical interaction and the availability of expert data. The authors propose a method called World-Gymnast, which utilizes reinforcement learning (RL) in a world model derived from real-world video-action data to train a vision-language-action (VLA) policy. Experimental results show that World-Gymnast significantly outperforms supervised finetuning by up to 18 times and software simulation by up to 2 times on the Bridge robot setup, while also demonstrating advanced capabilities such as training on diverse language instructions and adapting to novel scenes during test-time.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器人学习中由于物理交互成本高以及监督微调和基于软件的模拟所带来的挑战。作者提出了一种名为World-Gymnast的方法，该方法利用强化学习在从真实视频-动作数据中获得的世界模型中微调视觉-语言-动作策略。实验结果表明，World-Gymnast在性能上比监督微调高出多达18倍，比软件模拟高出多达2倍，同时还展示了在多样化语言指令和新场景中的训练能力，表明基于云的世界模型学习可能会增强机器人在现实世界中的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</div>
<div class="meta-line">Authors: Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao</div>
<div class="meta-line">First: 2026-02-02T18:43:57+00:00 · Latest: 2026-02-02T18:43:57+00:00</div>
<div class="meta-line">Comments: Working paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02453v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用漫画思考：通过结构化视觉叙事增强多模态推理</div>
<div class="mono" style="margin-top:8px">链式思维推理推动大型语言模型从文本思考扩展到图像和视频思考。然而，不同模态仍然存在明显的局限性：静态图像难以表示时间结构，而视频则引入了大量冗余和计算成本。在本研究中，我们提出了用漫画思考，这是一种视觉推理范式，利用漫画作为一种信息密度高的媒介，介于图像和视频之间。漫画保留了时间结构、嵌入文本和叙事连贯性，同时所需的推理成本显著较低。我们系统地研究了基于漫画的两条推理路径，并在一系列推理任务和长上下文理解任务上进行了评估。实验结果表明，用漫画思考在多步骤时间和因果推理任务上优于用图像思考，同时在效率上显著高于用视频思考。进一步分析表明，不同的漫画叙事结构和风格在各任务中一致地影响性能，表明漫画作为一种有效的中间视觉表示，有助于改善多模态推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multimodal reasoning by addressing the limitations of existing visual representations, specifically the inefficiencies of static images and the redundancy of videos. The authors propose a new paradigm called Thinking with Comics, which utilizes comics as a medium that maintains temporal structure and narrative coherence while reducing reasoning costs. Experimental results demonstrate that this approach outperforms traditional image-based reasoning in multi-step temporal and causal tasks, while also being more efficient than video-based reasoning, indicating that different comic structures and styles significantly influence performance across various reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决静态图像和视频在表示时间结构和降低计算成本方面的局限性来增强多模态推理。作者提出了一种新的视觉推理范式，称为“漫画思维”，利用漫画作为一种媒介，保持叙事连贯性和嵌入文本，同时比视频更高效。实验结果表明，这种方法在多步骤时间和因果推理任务中优于传统的基于图像的推理，同时比基于视频的方法更高效，漫画叙事结构和风格的变化对不同任务的表现产生影响。</div>
</details>
</div>
<div class="card">
<div class="title">Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization</div>
<div class="meta-line">Authors: Patrick Cooper, Alvaro Velasquez</div>
<div class="meta-line">First: 2026-02-02T18:43:52+00:00 · Latest: 2026-02-02T18:43:52+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02451v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p &lt; 0.001, Cohen&#x27;s d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动因果实验者 (ACE)：通过直接偏好优化学习干预策略</div>
<div class="mono" style="margin-top:8px">发现因果关系需要控制实验，但实验者面临一个顺序决策问题：每个干预揭示的信息应指导下一步尝试。传统方法如随机抽样、贪婪信息最大化和循环覆盖将每个决策视为孤立，无法从经验中学习自适应策略。我们提出主动因果实验者 (ACE)，将实验设计学习为顺序策略。我们的关键见解是，尽管随着知识的积累，绝对信息增益会减少（使基于价值的强化学习不稳定），但候选干预之间的相对比较在整个过程中仍然具有意义。ACE通过直接偏好优化利用这一点，从成对干预比较中学习，而不是非平稳奖励幅度。在合成基准、物理模拟和经济数据中，ACE在相同干预预算下实现了70-71%的改进（p &lt; 0.001，Cohen&#x27;s d ~ 2）。值得注意的是，学习到的策略自主发现碰撞机制需要对父变量进行集中干预，这是一种纯粹从经验中产生的理论基础策略。这表明基于偏好的学习可以恢复原则性的实验策略，补充理论与学习的领域适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of discovering causal relationships through controlled experiments, where sequential decision-making is crucial for optimizing intervention strategies. The authors introduce the Active Causal Experimentalist (ACE), a method that learns experimental design as a sequential policy using Direct Preference Optimization, focusing on pairwise comparisons of interventions rather than relying on diminishing absolute information gains. Experimental results demonstrate that ACE outperforms traditional methods by achieving a 70-71% improvement over baselines across various benchmarks, including synthetic data, physics simulations, and economic data, while also revealing effective strategies for collider mechanisms based solely on experiential learning.</div>
<div class="mono" style="margin-top:8px">本研究解决了通过控制实验发现因果关系的挑战，传统方法无法从顺序决策中自适应学习。作者提出了主动因果实验者（ACE），将实验设计形式化为顺序策略，利用直接偏好优化从干预的成对比较中学习。实验结果表明，ACE在各种合成基准、物理模拟和经济数据中比基线方法提高了70-71%，同时也揭示了基于经验学习的有效碰撞机制策略。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation</div>
<div class="meta-line">Authors: Seo Taek Kong, R. Srikant</div>
<div class="meta-line">First: 2026-02-02T18:41:06+00:00 · Latest: 2026-02-02T18:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov&#x27;s inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有限样本Wasserstein误差界和非线性随机逼近的集中不等式</div>
<div class="mono" style="margin-top:8px">本文推导了非线性随机逼近算法在Wasserstein-$p$距离下的非渐近误差界。为了获得最后一次迭代的显式有限样本保证，我们发展了一种耦合论证，将离散时间过程与极限Ornstein-Uhlenbeck过程进行比较。我们的分析适用于由一般噪声条件驱动的算法，包括鞅差分和遍历马尔可夫链的函数。作为这一结果的补充，我们通过直接分析处理Polyak-Ruppert平均的收敛速率，该分析在相同的一般设置下适用。假设驱动噪声满足非渐近中心极限定理，我们表明归一化的最后迭代在$p$-Wasserstein距离下以$γ_n^{1/6}$的速率收敛到高斯分布，其中$γ_n$是步长。同样，Polyak-Ruppert平均在Wasserstein距离下以$n^{-1/6}$的速率收敛。这些分布保证意味着高概率集中不等式，改善了从矩界和马尔可夫不等式推导出的结果。我们通过考虑两个应用展示了这种方法的实用性：（1）线性随机逼近，我们明确量化了迭代从重尾到高斯行为的过渡，从而弥合了最近有限样本分析与渐近理论之间的差距；（2）随机梯度下降，我们建立了收敛到中心极限定理的速率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to derive non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. The authors employ a coupling argument to compare the discrete-time process with a limiting Ornstein-Uhlenbeck process, applicable to algorithms influenced by various noise conditions. The key findings indicate that under a non-asymptotic central limit theorem, the normalized last iterates converge to a Gaussian distribution at a rate of order $γ_n^{1/6}$, and the Polyak-Ruppert average converges at a rate of order $n^{-1/6}$, leading to improved high-probability concentration inequalities compared to traditional moment bounds and Markov&#x27;s inequality, with practical applications in linear stochastic approximation and stochastic gradient descent.</div>
<div class="mono" style="margin-top:8px">本文针对非线性随机逼近算法中的有限样本误差界限进行了研究，重点关注Wasserstein-$p$距离。作者采用耦合论证将离散时间过程与极限的Ornstein-Uhlenbeck过程进行比较，适用于受各种噪声条件影响的算法。研究结果表明，在非渐近中心极限定理的假设下，归一化的最后迭代以$γ_n^{1/6}$的速率收敛到高斯分布，而Polyak-Ruppert平均以$n^{-1/6}$的速率收敛，从而得出比传统矩界限和马尔可夫不等式更好的高概率集中不等式，并在线性随机逼近和随机梯度下降中得到了应用。</div>
</details>
</div>
<div class="card">
<div class="title">RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval</div>
<div class="meta-line">Authors: Tyler Skow, Alexander Martin, Benjamin Van Durme, Rama Chellappa, Reno Kriz</div>
<div class="meta-line">First: 2026-02-02T18:40:37+00:00 · Latest: 2026-02-02T18:40:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02444v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02444v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANKVIDEO：用于文本到视频检索的推理重排序</div>
<div class="mono" style="margin-top:8px">重排序是现代检索系统的关键组成部分，通常将高效的第一阶段检索器与更具表现力的模型配对以优化结果。尽管大型推理模型在以文本为中心的重排序中取得了快速进展，但基于推理的视频检索重排序仍然未被充分探索。为了解决这一空白，我们引入了RANKVIDEO，一种用于视频检索的基于推理的重排序器，它明确地对查询-视频对进行推理，利用视频内容评估相关性。RANKVIDEO采用两阶段课程进行训练，包括基于感知的监督微调，随后是结合点对点、对偶和教师置信度蒸馏目标的重排序训练，并通过数据合成管道支持构建推理密集的查询-视频对。在大规模MultiVENT 2.0基准上的实验表明，RANKVIDEO在两阶段框架内始终提高检索性能，在nDCG@10上平均提高31%，并且优于仅基于文本和视觉-语言的重排序替代方案，同时更高效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video retrieval systems by addressing the underexplored area of reasoning-based reranking, particularly in the context of text-to-video retrieval. The authors introduce RANKVIDEO, a reasoning-based reranker that evaluates the relevance of query-video pairs by leveraging video content, trained through a two-stage curriculum that includes perception-grounded supervised fine-tuning and a combination of pointwise, pairwise, and teacher confidence distillation objectives. Experimental results on the MultiVENT 2.0 benchmark show that RANKVIDEO significantly improves retrieval performance, achieving an average increase of 31% on nDCG@10 compared to existing text-only and vision-language reranking methods, while also being more efficient.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过整合基于推理的重排序来增强视频检索系统，而这一方法在文本中心的方法中尚未得到充分利用。作者提出了RANKVIDEO，这是一种重排序器，通过利用视频内容评估查询-视频对的相关性，采用了包括监督微调和点对点、对偶以及教师信心蒸馏目标的两阶段课程进行训练。在MultiVENT 2.0基准上的实验结果表明，RANKVIDEO显著提高了检索性能，nDCG@10的平均提升达31%，并超越了文本仅和视觉-语言重排序方法，同时保持了效率。</div>
</details>
</div>
<div class="card">
<div class="title">Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE</div>
<div class="meta-line">Authors: Yuanteng Chen, Peisong Wang, Nanxin Zeng, Yuantian Shao, Gang Li, Jing Liu, Jian Cheng</div>
<div class="meta-line">First: 2026-02-02T18:39:33+00:00 · Latest: 2026-02-02T18:39:33+00:00</div>
<div class="meta-line">Comments: 24 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02443v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>确定的头部，不确定的尾部：测试时扩展的专家样本在细粒度MoE中的应用</div>
<div class="mono" style="margin-top:8px">测试时扩展通过生成多个候选解决方案来提高LLM性能，但令牌级采样需要温度调节，这在多样性与稳定性之间进行权衡。细粒度MoE每层具有数百个训练良好的专家，并且每个令牌激活多个专家，提供了一种通过其丰富的路由空间未被探索的替代方案。我们经验性地表征了细粒度MoE路由，并发现了一个信息模式：路由器分数表现出一组高置信度专家的确定头部，后面跟着一组低置信度候选者的不确定尾部。当激活的专家较少时，单次运行的贪婪准确性保持稳定，而多样本pass@n显著下降——这表明确定头部主导核心推理能力，而不确定尾部与推理多样性相关。基于这些发现，我们提出了Expert-Sample，这是一种无训练的方法，保留高置信度选择，同时向不确定尾部注入受控随机性，从而实现多样化生成而不破坏输出的稳定性。在数学、知识推理和代码任务中对多个细粒度MoE模型进行评估，Expert-Sample始终提高pass@n和基于验证的准确性。在对Qwen3-30B-A3B-Instruct在GPQA-Diamond上进行32个并行样本评估时，pass@32从85.4%上升到91.9%，准确性从59.1%提高到62.6%，并使用Best-of-N验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the performance of large language models (LLMs) through test-time scaling, which traditionally involves token-level sampling that requires careful temperature tuning. The authors investigate fine-grained mixture of experts (MoE) models, which utilize a rich routing space with numerous trained experts, and discover a pattern where high-confidence experts form a &#x27;certain head&#x27; while low-confidence candidates create an &#x27;uncertain tail.&#x27; To leverage this finding, they propose Expert-Sample, a method that maintains high-confidence selections while introducing controlled randomness in the uncertain tail, allowing for diverse outputs without compromising stability. Experimental results show that Expert-Sample significantly improves performance metrics such as pass@n and verification-based accuracy across various tasks, with notable improvements on the Qwen3-30B-A3B-Instruct model, where pass@32 increased from 85.4% to 91.9% and accuracy rose from 59.1% to 62.6%.</div>
<div class="mono" style="margin-top:8px">这项研究的动机在于提高大型语言模型（LLM）在测试时扩展的性能，特别是解决需要温度调优的令牌级采样的局限性。作者提出了一种名为Expert-Sample的方法，该方法利用细粒度专家混合（MoE）的路由能力，该方法具有高置信度专家的确定头和低置信度候选者的不确定尾。实验结果表明，Expert-Sample在各种任务中提高了性能指标，如pass@n和基于验证的准确性，在Qwen3-30B-A3B-Instruct模型中，pass@32从85.4%提高到91.9%，准确性从59.1%提高到62.6%，在GPQA-Diamond上进行评估时表现尤为显著。</div>
</details>
</div>
<div class="card">
<div class="title">Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization</div>
<div class="meta-line">Authors: Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz, Duygu Erisken, Rana Irem Turhan</div>
<div class="meta-line">First: 2026-02-02T18:34:48+00:00 · Latest: 2026-02-02T18:34:48+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 4 tables. Submitted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向边缘AI的能效神经形态计算：结合自适应脉冲神经网络和硬件感知优化的框架</div>
<div class="mono" style="margin-top:8px">边缘AI应用日益需要超低功耗、低延迟的推理。基于事件驱动的脉冲神经网络（SNN）的神经形态计算提供了一条有吸引力的路径，但在资源受限设备上的实际部署受到训练难度、硬件映射开销和对时间动态敏感性的限制。我们提出了NeuEdge，一个将自适应SNN模型与硬件感知优化结合的边缘部署框架。NeuEdge使用一种时间编码方案，结合速率和脉冲时序模式，以减少脉冲活动，同时保持准确性，并采用硬件感知训练程序，共同优化网络结构和芯片内布局，以提高在神经形态处理器上的利用率。自适应阈值机制根据输入统计调整神经元的兴奋性，降低能耗而不降低性能。在标准视觉和音频基准测试中，NeuEdge在边缘硬件上实现了91-96%的准确率，推理延迟高达2.3毫秒，能效估计为847 GOp/s/W。对自主无人机工作负载的案例研究显示，相较于传统深度神经网络，能耗节省高达312倍，同时保持实时操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the growing demand for ultra-low-power, low-latency inference in edge AI applications, which is challenged by the limitations of traditional deep learning methods. The authors present NeuEdge, a framework that integrates adaptive spiking neural networks (SNNs) with hardware-aware optimization techniques to enhance deployment on resource-constrained devices. Key experimental results demonstrate that NeuEdge achieves 91-96% accuracy with inference latencies as low as 2.3 ms and an energy efficiency of 847 GOp/s/W, while a case study on an autonomous-drone workload reveals energy savings of up to 312 times compared to conventional deep neural networks, all while maintaining real-time operation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于应对边缘人工智能应用对超低功耗和低延迟推理的日益需求，而传统深度学习方法面临局限。作者提出了NeuEdge框架，将自适应脉冲神经网络（SNN）与硬件感知优化技术相结合，以增强在资源受限设备上的部署。实验结果表明，NeuEdge在推理延迟低至2.3毫秒的情况下实现了91-96%的准确率，并且能效高达847 GOp/s/W，而在一个自主无人机工作负载的案例研究中，显示出与传统深度神经网络相比高达312倍的节能效果，同时确保实时性能。</div>
</details>
</div>
<div class="card">
<div class="title">UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</div>
<div class="meta-line">Authors: Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-02T18:34:35+00:00 · Latest: 2026-02-02T18:34:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02437v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02437v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniReason 1.0：一个统一的推理框架，用于与世界知识对齐的图像生成和编辑</div>
<div class="mono" style="margin-top:8px">统一的多模态模型在需要深度推理的复杂合成任务中常常表现不佳，通常将文本到图像生成和图像编辑视为孤立的能力，而不是相互关联的推理步骤。为了解决这个问题，我们提出了UniReason，一个通过双重推理范式协调这两项任务的统一框架。我们将生成形式化为增强世界知识的规划，以注入隐含约束，并利用编辑能力进行细粒度的视觉细化，通过自我反思进一步纠正视觉错误。这种方法在共享表示中统一了生成和编辑，反映了人类认知过程中的规划与细化。我们通过系统构建一个覆盖五个主要知识领域（例如，文化常识、物理等）的以推理为中心的大规模数据集（约30万样本）来支持该框架，同时提供一个用于视觉自我纠正的代理生成语料库。大量实验表明，UniReason在推理密集型基准测试（如WISE、KrisBench和UniREditBench）上实现了先进的性能，同时保持了卓越的综合合成能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of unified multimodal models in complex synthesis tasks that require deep reasoning, as these models often treat text-to-image generation and image editing as separate processes. The authors propose UniReason, a unified framework that integrates these tasks through a dual reasoning paradigm, where generation is framed as world knowledge-enhanced planning and editing is used for visual refinement. Experimental results show that UniReason significantly outperforms existing models on reasoning-intensive benchmarks like WISE, KrisBench, and UniREditBench, while also demonstrating strong general synthesis capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型在需要深度推理的复杂合成任务中的表现，因为它们通常将文本到图像生成和图像编辑视为独立的过程。作者提出了UniReason，一个通过双重推理范式整合这两项任务的统一框架，通过世界知识增强生成，并通过自我修正来细化图像。实验结果表明，UniReason在WISE、KrisBench和UniREditBench等推理密集型基准上显著优于现有模型，同时也展示了强大的通用合成能力。</div>
</details>
</div>
<div class="card">
<div class="title">Maximizing Reliability with Bayesian Optimization</div>
<div class="meta-line">Authors: Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</div>
<div class="meta-line">First: 2026-02-02T18:31:58+00:00 · Latest: 2026-02-02T18:31:58+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过贝叶斯优化最大化可靠性</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是一种流行的、样本高效的昂贵黑箱优化技术。在制造中出现的一个问题是最大化设计的可靠性，或等价地最小化故障概率，该设计受到随机扰动的影响——这是一个可能涉及极其罕见故障的问题（$P_\mathrm{fail} = 10^{-6}-10^{-8}$）。在这项工作中，我们提出了两种基于汤普森采样和知识梯度的BO方法，后者近似于最小化故障概率对数的单步贝叶斯最优策略。这两种方法都结合了重要性采样，以针对极小的故障概率。实证结果表明，所提出的方法在极端和非极端情况下均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reliability of designs in manufacturing by minimizing the probability of rare failures, which can be as low as 10^{-6} to 10^{-8}. The authors propose two Bayesian optimization methods that utilize Thompson sampling and knowledge gradient approaches, with the latter designed to approximate the optimal policy for reducing failure probabilities. Experimental results demonstrate that these methods significantly outperform existing optimization techniques in both extreme and non-extreme failure scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在制造设计中最大化可靠性的问题，尤其是在处理极其罕见的故障概率时。作者提出了两种基于贝叶斯优化的方法，分别采用汤普森采样和知识梯度方法，其中知识梯度方法近似于最小化故障概率对数的最优策略。实验结果表明，这些提出的方法在极端和非极端故障概率场景中显著优于现有的优化技术。</div>
</details>
</div>
<div class="card">
<div class="title">Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning</div>
<div class="meta-line">Authors: Filip Kovačević, Hong Chang Ji, Denny Wu, Mahdi Soltanolkotabi, Marco Mondelli</div>
<div class="meta-line">First: 2026-02-02T18:31:51+00:00 · Latest: 2026-02-02T18:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02431v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02431v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全批量梯度下降优于一次性随机梯度下降：单索引学习中的样本复杂性分离</div>
<div class="mono" style="margin-top:8px">人们普遍认为，重复使用训练数据可以提高基于梯度的学习的统计效率。然而，除了线性回归之外，全批量梯度下降（GD，始终重用所有数据）相对于一次性随机梯度下降（在线SGD，仅使用每个数据点一次）的理论优势仍不清楚。在这项工作中，我们考虑学习一个具有二次激活的$d$维单索引模型，已知一次性SGD需要$n\gtrsim d\log d$样本才能实现弱恢复。我们首先表明，这个样本复杂性中的$\log d$因子在全批量球形GD的相关损失中依然存在；然而，通过简单地截断激活，全批量GD在$n \simeq d$样本下展现出有利的优化景观，从而在统计效率上优于一次性SGD（使用相同的激活）。我们通过对全批量GD在小初始化下的平方损失的轨迹分析补充了这一结果，表明$n \gtrsim d$样本和$T \gtrsim\log d$梯度步骤足以实现强（精确）恢复。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to clarify the theoretical advantages of full-batch gradient descent over one-pass stochastic gradient descent in the context of single-index learning. The authors analyze a $d$-dimensional single-index model with a quadratic activation and demonstrate that while one-pass SGD requires approximately $n \gtrsim d\log d$ samples for weak recovery, full-batch gradient descent can achieve favorable optimization with only $n \simeq d$ samples by truncating the activation. The findings indicate that full-batch GD outperforms one-pass SGD in statistical efficiency and that $n \gtrsim d$ samples along with $T \gtrsim\log d$ gradient steps are sufficient for strong recovery in the squared loss scenario.</div>
<div class="mono" style="margin-top:8px">本研究探讨了全批量梯度下降（GD）与一次性随机梯度下降（SGD）在单索引学习中的比较效率，动机源于认为重复使用训练数据可以提高统计效率。作者分析了一个具有二次激活的$d$维单索引模型，证明虽然一次性SGD需要大约$n\gtrsim d\log d$样本才能实现弱恢复，但通过截断激活，全批量GD仅需$n \simeq d$样本即可实现有利的优化。此外，他们提供了轨迹分析，表明在小初始化下，使用全批量GD在平方损失上实现强恢复只需$n \gtrsim d$样本和$T \gtrsim\log d$梯度步骤。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
