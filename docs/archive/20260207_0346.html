<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 03:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0346</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：回收适配器以寻找资源高效适应和推理的主子空间</div>
<div class="mono" style="margin-top:8px">大型模型的快速增长引发了对其环境影响和可及性公平性的担忧，因为计算成本显著。低秩适配器（LoRA）为微调大型模型提供了一种轻量级解决方案，导致大量针对不同领域的公开适配器的出现。我们提出：这些预训练的适配器能否被利用以进一步简化对新任务的适应，同时解决这些挑战？我们介绍了EigenLoRAx，这是一种参数高效的微调方法，回收现有适配器以创建与其共享领域知识对齐的主子空间，并可以在低资源场景中进一步增强正交基向量。这使得通过仅在子空间的主成分上学习轻量级系数来快速适应新任务，消除了微调整个适配器的需要。EigenLoRAx所需的参数和内存显著减少，提高了训练和推理的效率。我们的方法在不同领域和任务中表现出色，为边缘应用、个性化和在资源受限环境中公平部署大型模型提供了可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing size of large models raises concerns about their environmental impact and accessibility due to high computational costs. To address this, the authors propose EigenLoRAx, a method that recycles existing Low-Rank Adapters (LoRA) to create a principal subspace that aligns with shared domain knowledge, allowing for efficient adaptation to new tasks. Experimental results show that EigenLoRAx significantly reduces the number of parameters and memory required, leading to improved efficiency in both training and inference while maintaining strong performance across various domains and tasks.</div>
<div class="mono" style="margin-top:8px">大型模型的快速增长引发了对其环境影响和可及性的担忧，因为计算成本高昂。本文提出了EigenLoRAx，这是一种新颖的方法，通过回收现有的低秩适配器（LoRA）来建立一个捕捉共享领域知识的主子空间，从而实现对新任务的高效适应。实验结果表明，EigenLoRAx显著减少了所需的参数和内存，提高了训练和推理的效率，同时在各种领域和任务中保持了强劲的性能，使其适合于边缘计算应用和在资源受限环境中的公平部署。</div>
</details>
</div>
<div class="card">
<div class="title">Pseudo-Invertible Neural Networks</div>
<div class="meta-line">Authors: Yamit Ehrlich, Nimrod Berman, Assaf Shocher</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06042v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06042v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or &quot;Back-Projection&quot;, $x&#x27; = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x&#x27;$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, &quot;degradation&quot; is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伪可逆神经网络</div>
<div class="mono" style="margin-top:8px">摩尔-彭罗斯伪逆（PInv）是线性系统的基本解。本文提出了PInv在一般非线性领域及神经网络中的自然推广。我们引入了映射伪可逆神经网络（SPNN），这是一类专门设计用于接受可处理的非线性PInv的架构。所提出的非线性PInv及其在SPNN中的实现满足基本几何属性。其中一个属性是零空间投影或“反投影”，$x&#x27; = x + A^\dagger(y-Ax)$，它将样本$x$移动到其最接近的一致状态$x&#x27;$，使得$Ax=y$。我们形式化了非线性反投影（NLBP），这是一种通过我们定义的PInv保证非线性映射$f(x)=y$的一致性约束的方法。我们利用SPNN扩展了零样本逆问题的范围。基于扩散的零空间投影通过利用封闭形式的反投影彻底改变了线性逆问题的零样本求解。我们将此方法扩展到非线性退化。这里，“退化”被广泛概括为包括任何非线性信息损失，从光学失真到分类等语义抽象。这种方法使得复杂退化的零样本反演成为可能，并允许在不重新训练扩散先验的情况下对生成输出进行精确的语义控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective solutions to nonlinear inverse problems, building on the established Moore-Penrose Pseudo-inverse for linear systems. The authors introduce Surjective Pseudo-invertible Neural Networks (SPNN), a novel architecture designed to facilitate a tractable non-linear pseudo-inverse. Key experimental findings demonstrate that the proposed Non-Linear Back-Projection method maintains consistency for non-linear mappings and successfully extends zero-shot inversion capabilities to complex degradations, allowing for precise semantic control over generative outputs without the need for retraining the diffusion prior.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要有效解决非线性逆问题，基于已建立的线性系统的摩尔-彭罗斯伪逆。作者提出了可逆伪神经网络（SPNN），旨在实现可处理的非线性伪逆，并满足关键几何属性，包括非线性回投影（NLBP）。实验结果表明，SPNN能够成功应对各种非线性退化的零-shot 逆问题，允许在不需要重新训练扩散模型的情况下对生成输出进行精确的语义控制。</div>
</details>
</div>
<div class="card">
<div class="title">DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</div>
<div class="meta-line">Authors: Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao</div>
<div class="meta-line">First: 2026-02-05T18:59:51+00:00 · Latest: 2026-02-05T18:59:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06039v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06039v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager&#x27;s round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyTopo：通过语义匹配进行多智能体推理的动态拓扑路由</div>
<div class="mono" style="margin-top:8px">基于提示的大型语言模型构建的多智能体系统可以改善多轮推理，但大多数现有管道依赖于固定的、全程的通信模式，这与迭代问题解决的阶段性需求不匹配。我们介绍了DyTopo，一个由管理者引导的多智能体框架，在每一轮重构稀疏的有向通信图。根据管理者的轮次目标，每个智能体输出轻量级自然语言查询（需求）和\key（提供）描述符；DyTopo嵌入这些描述符并执行语义匹配，仅沿着诱导边路由私密消息。在代码生成和数学推理基准测试以及四个LLM骨干网络中，DyTopo始终优于最强基线（平均+6.2）。除了准确性，DyTopo通过不断演变的图形产生可解释的协调轨迹，使得能够定性检查通信路径如何在各轮之间重新配置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multi-agent systems that utilize large language models for improved multi-round reasoning, addressing the limitations of fixed communication patterns in iterative problem solving. The authors introduce DyTopo, a framework that dynamically reconstructs a sparse directed communication graph at each round based on the manager&#x27;s goals, where agents generate lightweight natural-language queries and descriptors for semantic matching. Experimental results demonstrate that DyTopo consistently outperforms the strongest baseline by an average of 6.2% across various benchmarks in code generation and mathematical reasoning, while also providing interpretable coordination traces that reveal how communication pathways evolve during the process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使用大型语言模型增强多智能体系统，以改善多轮推理，因为现有方法通常依赖于静态通信模式，这些模式无法适应迭代问题解决的需求。作者提出了DyTopo，这是一种由管理者引导的框架，能够在每一轮动态重构稀疏的有向通信图，智能体生成轻量级自然语言描述符以表达其需求和提供的内容，然后进行语义匹配以路由消息。实验结果表明，DyTopo在代码生成和数学推理任务中始终优于最强基线，平均提高了6.2%，同时还提供了可解释的协调轨迹，揭示了通信路径在各轮之间的演变。</div>
</details>
</div>
<div class="card">
<div class="title">SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</div>
<div class="meta-line">Authors: Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou</div>
<div class="meta-line">First: 2026-02-05T18:59:51+00:00 · Latest: 2026-02-05T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://accio-lab.github.io/SwimBird</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06040v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06040v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://accio-lab.github.io/SwimBird">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as &quot;visual thoughts&quot; into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SwimBird：在混合自回归MLLM中引发可切换推理模式</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLM）通过桥接视觉和语言，在多模态感知和推理方面取得了显著进展。然而，大多数现有的MLLM主要通过文本链式推理（CoT）进行推理，这限制了它们在视觉密集任务上的有效性。最近的方法将固定数量的连续隐藏状态作为“视觉思维”注入推理过程，改善了视觉表现，但往往以牺牲基于文本的逻辑推理为代价。我们认为，核心限制在于一种僵化的、预定义的推理模式，无法自适应地选择最适合不同用户查询的思维模式。我们引入了SwimBird，一种可切换推理的MLLM，根据输入动态切换三种推理模式：（1）仅文本推理，（2）仅视觉推理（连续隐藏状态作为视觉思维），以及（3）交错的视觉-文本推理。为了实现这一能力，我们采用了一种混合自回归的公式，将文本思维的下一个标记预测与视觉思维的下一个嵌入预测统一，并设计了一种系统的推理模式策划策略，以构建SwimBird-SFT-92K，这是一个涵盖所有三种推理模式的多样化监督微调数据集。通过实现灵活的、查询自适应的模式选择，SwimBird在显著提高视觉密集任务的表现的同时，保持了强大的文本逻辑。针对涵盖文本推理和挑战性视觉理解的多样基准的实验表明，SwimBird在状态最先进的结果上取得了显著提升，并在先前的固定模式多模态推理方法上实现了稳健的增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs), which often struggle with vision-intensive tasks due to their reliance on fixed textual reasoning patterns. The authors introduce SwimBird, a reasoning-switchable MLLM that can dynamically adapt its reasoning mode based on the input, utilizing a hybrid autoregressive approach that integrates both text and visual reasoning. Experimental results show that SwimBird significantly improves performance on vision-dense tasks while maintaining strong textual logic, achieving state-of-the-art results across various benchmarks compared to traditional fixed-pattern multimodal reasoning methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高多模态大型语言模型（MLLMs）在视觉密集任务中的推理能力，因为现有模型通常依赖固定的推理模式，限制了其有效性。作者提出了SwimBird，这是一种可切换推理模式的MLLM，能够根据输入动态调整推理模式，采用混合自回归方法，整合文本推理、视觉推理和交错推理。实验结果表明，SwimBird不仅保持了强大的文本逻辑，还显著提高了视觉任务的性能，在各种基准测试中取得了比以往固定模式多模态推理方法更好的结果。</div>
</details>
</div>
<div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CommCP：基于LLM的符合预测的高效多智能体协调</div>
<div class="mono" style="margin-top:8px">为了完成由人类以自然语言提供的任务，机器人必须解释指令，生成并回答与场景理解相关的问题，并操控目标物体。现实世界的部署通常需要多个具有不同操控能力的异构机器人协同处理不同的任务。除了对专业操控技能的需求外，有效的信息收集在完成这些任务中也至关重要。为了解决这一问题，我们将信息收集过程在完全合作的环境中形式化为一个未被充分探索的多智能体多任务具身问答（MM-EQA）问题，这是经典具身问答（EQA）的新扩展，其中有效的沟通对于协调努力而不产生冗余至关重要。为了解决这个问题，我们提出了CommCP，一个为MM-EQA设计的新型基于LLM的去中心化通信框架。我们的框架采用符合预测来校准生成的信息，从而最小化接收者的干扰并增强通信的可靠性。为了评估我们的框架，我们引入了一个MM-EQA基准，包含多样化的、照片真实的家庭场景和具身问题。实验结果表明，CommCP显著提高了任务成功率和探索效率。实验视频、代码和数据集可在我们的项目网站上获取：https://comm-cp.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective multi-agent coordination among heterogeneous robots to complete natural language assignments, which requires not only manipulation skills but also efficient information gathering. The authors propose CommCP, a decentralized communication framework based on large language models (LLMs) that utilizes conformal prediction to improve message calibration and reduce distractions during communication. Experimental results indicate that CommCP significantly improves task success rates and exploration efficiency compared to baseline methods in a newly introduced MM-EQA benchmark featuring realistic household scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要有效的多智能体协调，以便异构机器人在现实环境中完成复杂任务。作者提出了CommCP，这是一种基于大型语言模型（LLM）的去中心化通信框架，利用保形预测来提高信息传递的可靠性并减少信息收集过程中的干扰。在多智能体多任务的具身问答（MM-EQA）场景中，实验结果表明，CommCP显著提高了任务成功率和探索效率，相较于基线方法表现更佳。</div>
</details>
</div>
<div class="card">
<div class="title">InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</div>
<div class="meta-line">Authors: Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, Yu-Xiong Wang, Liangyan Gui</div>
<div class="meta-line">First: 2026-02-05T18:59:27+00:00 · Latest: 2026-02-05T18:59:27+00:00</div>
<div class="meta-line">Comments: Webpage: https://sirui-xu.github.io/InterPrior/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06035v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sirui-xu.github.io/InterPrior/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InterPrior：为基于物理的人机交互扩展生成控制</div>
<div class="mono" style="margin-top:8px">人类很少在明确的全身运动层面上规划与物体的全身交互。高层次的意图，如可供性，定义了目标，而协调的平衡、接触和操控可以自然地从潜在的物理和运动先验中涌现。扩展这些先验是使类人机器人能够在多样化的环境中组合和概括运动操控技能的关键，同时保持物理一致的全身协调。为此，我们引入了InterPrior，一个可扩展的框架，通过大规模模仿预训练和强化学习后训练学习统一的生成控制器。InterPrior首先将全参考模仿专家提炼为一个多功能的、目标条件的变分策略，该策略从多模态观察和高层次意图中重建运动。尽管提炼的策略重建了训练行为，但由于大规模人机交互的广泛配置空间，它并不能可靠地概括。为了解决这个问题，我们应用了带有物理扰动的数据增强，然后进行强化学习微调，以提高在未见目标和初始化上的能力。这些步骤共同将重建的潜在技能巩固为一个有效的流形，产生一个超越训练数据的运动先验，例如，它可以结合与未见物体的交互等新行为。我们进一步展示了其在用户交互控制中的有效性及其在真实机器人部署中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for humanoid robots to effectively plan and execute whole-body interactions with objects, leveraging high-level intentions and physical priors. The authors propose InterPrior, a scalable framework that utilizes large-scale imitation pretraining followed by reinforcement learning to create a generative controller capable of adapting to diverse human-object interaction scenarios. Experimental results show that InterPrior successfully consolidates reconstructed latent skills into a valid motion prior, enabling the system to generalize beyond the training data and perform user-interactive control, including interactions with previously unseen objects.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于使类人机器人能够有效地规划和执行与物体的复杂交互，依靠高层次意图而非明确的动作。作者提出了InterPrior，一个可扩展的框架，通过大规模模仿预训练和强化学习，开发出能够适应多样化人-物交互的生成控制器。实验结果表明，InterPrior成功地将运动技能推广到未见的目标和物体，通过数据增强和强化学习微调，提升了控制器管理复杂运动操控任务的能力，展示了其在现实世界场景中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</div>
<div class="meta-line">Authors: Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Ka</div>
<div class="meta-line">First: 2026-02-05T18:59:21+00:00 · Latest: 2026-02-05T18:59:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06034v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06034v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-Retrver：基于证据的自主推理用于通用多模态检索</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）最近被应用于通用多模态检索，其中思维链（CoT）推理改善了候选项的重新排序。然而，现有方法仍然主要依赖语言驱动，依赖静态视觉编码，缺乏主动验证细粒度视觉证据的能力，这常常导致在视觉模糊情况下的推测性推理。我们提出了V-Retrver，一个基于证据的检索框架，将多模态检索重新构建为一个基于视觉检查的自主推理过程。V-Retrver使得MLLM能够在推理过程中通过外部视觉工具选择性地获取视觉证据，执行一种多模态交替推理过程，在假设生成和针对性视觉验证之间交替进行。为了训练这样的证据收集检索代理，我们采用了一种基于课程的学习策略，结合监督推理激活、基于拒绝的精炼和与证据对齐的强化学习目标。多个多模态检索基准的实验表明，检索准确性（平均提高23.0%）、感知驱动的推理可靠性和泛化能力均有一致改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance universal multimodal retrieval by addressing the limitations of existing language-driven approaches that rely on static visual encodings and often lead to speculative reasoning. The authors propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process, allowing a Multimodal Large Language Model (MLLM) to actively gather visual evidence through external tools. Experimental results show that V-Retrver significantly improves retrieval accuracy by an average of 23.0%, alongside enhanced reliability in perception-driven reasoning and better generalization across multiple multimodal retrieval benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升多模态检索系统的性能，这些系统目前过于依赖语言和静态视觉编码，导致在模糊视觉情况下的推理存在不确定性。作者提出了V-Retrver，这是一种以证据驱动的检索框架，将检索过程重新构建为一种主动推理任务，使多模态大语言模型（MLLM）能够通过外部工具主动收集视觉证据。实验结果表明，V-Retrver在多个多模态检索基准上显著提高了检索准确率，平均提升23.0%，同时增强了感知驱动推理的可靠性和更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能通过交互学习直观物理吗？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界的直觉较差。最近的研究表明，监督微调可以提高模型在简单物理任务上的表现。然而，微调后的模型似乎并未学习到能够推广到新情境的稳健物理规则。基于认知科学的研究，我们假设模型需要与环境互动才能正确学习其物理动态。我们训练通过与环境互动学习的模型，使用强化学习。尽管通过互动学习使模型在任务内表现有所提升，但未能产生具有可推广物理直觉的模型。我们发现，在一个任务上训练的模型并不能可靠地推广到相关任务，即使这些任务共享视觉统计和物理原理，无论模型是否通过互动进行训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of pre-trained vision language models in understanding intuitive physics, as they struggle to generalize physical rules across different contexts. The authors employ reinforcement learning to train models that interact with their environment, aiming to enhance their learning of physical dynamics. However, the key findings reveal that while interaction improves performance on specific tasks, it does not lead to the development of generalizable physical intuitions, as models trained on one task fail to transfer their knowledge to related tasks, despite sharing visual statistics and physical principles.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决预训练视觉语言模型在理解直观物理方面的局限性，因为它们在不同上下文中推广物理规则时存在困难。作者采用强化学习训练模型，通过与环境的互动进行学习。关键发现表明，尽管互动提高了特定任务的表现，但并未导致可推广的物理直觉的发展，因为在一个任务上训练的模型并不能可靠地将其知识转移到相关任务上，尽管这些任务共享视觉统计和物理原理。</div>
</details>
</div>
<div class="card">
<div class="title">PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</div>
<div class="meta-line">Authors: Kavana Venkatesh, Yinhan He, Jundong Li, Jiaming Cui</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06030v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysicsAgentABM：物理引导的生成代理基础建模</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLM）的多代理系统能够实现表达丰富的代理推理，但在扩展性上成本高昂，并且在时间步对齐的状态转移模拟中校准效果较差，而经典的代理基础模型（ABM）提供了解释性，但在整合丰富的个体级信号和非平稳行为方面存在困难。我们提出了PhysicsAgentABM，它将推理转移到行为一致的代理集群：状态专用的符号代理编码机制转移先验，多模态神经转移模型捕捉时间和交互动态，具有不确定性感知的知识融合产生校准的集群级转移分布。个体代理在局部约束下随机实现转移，将群体推理与实体级变异解耦。我们进一步引入ANCHOR，一种基于跨上下文行为响应和新颖对比损失的LLM代理驱动聚类策略，将LLM调用减少了6-8倍。公共卫生、金融和社会科学的实验显示，在事件时间准确性和校准方面，相较于机制、神经和LLM基线，均取得了一致的提升。通过围绕具有不确定性感知的神经符号融合重构生成ABM，PhysicsAgentABM为可扩展和校准的LLM模拟建立了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of large language model (LLM)-based multi-agent systems in terms of scalability and calibration for state-transition simulations, as well as the challenges faced by classical agent-based models (ABMs) in integrating individual-level signals. The authors propose PhysicsAgentABM, which utilizes behaviorally coherent agent clusters, state-specialized symbolic agents for mechanistic transition priors, and a multimodal neural transition model to capture dynamics, while employing uncertainty-aware epistemic fusion for calibrated transition distributions. Experimental results demonstrate significant improvements in event-time accuracy and calibration across various domains, including public health, finance, and social sciences, outperforming traditional mechanistic, neural, and LLM baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于大型语言模型的多智能体系统在状态转移模拟中的可扩展性和校准性限制，以及经典代理模型在整合个体级信号方面面临的挑战。作者提出了PhysicsAgentABM，利用行为一致的代理集群、专门化的符号代理用于机制转移先验，以及多模态神经转移模型来捕捉动态，同时实施不确定性感知的认知融合以获得校准的转移分布。实验结果表明，PhysicsAgentABM在公共卫生、金融和社会科学等多个领域的事件时间准确性和校准性方面显著优于现有的机制、神经和LLM基线。</div>
</details>
</div>
<div class="card">
<div class="title">AP-OOD: Attention Pooling for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Claus Hofmann, Christian Huber, Bernhard Lehner, Daniel Klotz, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AP-OOD：用于分布外检测的注意力池化</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测将高维数据映射为标量OOD分数，对于机器学习模型的可靠部署至关重要。近期研究中的一个关键挑战是如何有效利用和聚合语言模型中的标记嵌入以获得OOD分数。在本研究中，我们提出了AP-OOD，这是一种超越简单平均聚合的自然语言OOD检测新方法，通过利用标记级信息。AP-OOD是一种半监督方法，灵活地在无监督和监督设置之间插值，能够使用有限的辅助异常数据。实证结果表明，AP-OOD在文本的OOD检测中设定了新的最先进水平：在无监督设置中，它将XSUM摘要的FPR95（95%真实正例下的假阳性率）从27.84%降低到4.67%，将WMT15英法翻译的FPR95从77.08%降低到70.37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve out-of-distribution (OOD) detection, which is essential for the reliable deployment of machine learning models, particularly in natural language processing. The authors propose a novel method called AP-OOD that enhances the aggregation of token embeddings from language models by utilizing token-level information rather than relying solely on average-based methods. Experimental results demonstrate that AP-OOD achieves state-of-the-art performance in OOD detection for text, significantly reducing the false positive rate at 95% true positives from 27.84% to 4.67% on the XSUM summarization task and from 77.08% to 70.37% on the WMT15 English-French translation task in the unsupervised setting.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善分布外（OOD）检测，这对机器学习模型的可靠应用至关重要。作者提出了AP-OOD，这是一种新颖的方法，通过利用标记级信息而不仅仅依赖于基于平均的聚合，来增强自然语言中的OOD检测。实验结果表明，AP-OOD在文本的OOD检测中达到了新的最先进水平，在无监督设置下，XSUM摘要任务的95%真实正例下的假阳性率从27.84%降低到4.67%，WMT15英法翻译任务的假阳性率从77.08%降低到70.37%。</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：自洽学习与无悔优化的主动推理</div>
<div class="mono" style="margin-top:8px">主动推理（AIF）通过最小化期望自由能（EFE）统一了探索与利用，利用好奇心系数平衡认知价值（信息增益）和务实价值（任务表现）。然而，何时这种平衡能够同时实现一致学习和高效决策尚不明确：不足的好奇心可能导致短视的利用并阻碍不确定性解决，而过度的好奇心则可能引发不必要的探索和遗憾。我们为最小化EFE的智能体建立了首个理论保证，表明一个单一要求——足够的好奇心——同时确保自洽学习（贝叶斯后验一致性）和无悔优化（有界累积遗憾）。我们的分析表征了这一机制如何依赖于初始不确定性、可识别性和目标对齐，从而将AIF与经典贝叶斯实验设计和贝叶斯优化连接在一个理论框架内。我们进一步将这些理论转化为实际设计指南，以调节混合学习-优化问题中的认知-务实权衡，并通过现实世界实验进行了验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the balance between exploration and exploitation in active inference (AIF), particularly how curiosity influences learning and decision-making. The authors establish a theoretical framework that guarantees self-consistent learning and no-regret optimization by demonstrating that sufficient curiosity is essential for minimizing Expected Free Energy (EFE). Experimental results validate the proposed framework, showing that the balance of epistemic and pragmatic values can be effectively tuned in hybrid learning-optimization problems, leading to coherent learning and efficient decision-making in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解主动推理（AIF）中探索与利用之间的平衡，以实现一致的学习和高效的决策。作者建立了一个理论框架，表明足够的好奇心对于确保EFE最小化代理的自洽学习和无悔优化至关重要。研究结果表明，这种平衡受到初始不确定性和目标一致性等因素的影响，并提供了调节认知-务实权衡的实用指南，这些指南通过实际实验得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Language Models and Logic Programs for Trustworthy Tax Reasoning</div>
<div class="meta-line">Authors: William Jurayj, Nils Holzenberger, Benjamin Van Durme</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-28T17:55:07+00:00 · Latest: 2026-02-05T18:58:31+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21051v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.21051v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">According to the United States Internal Revenue Service, ``the average American spends $\$270$ and 13 hours filing their taxes&#x27;&#x27;. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可信税务推理的语言模型与逻辑程序</div>
<div class="mono" style="margin-top:8px">根据美国国税局的说法，“平均美国人花费270美元和13小时来报税”。即使在美国以外，报税也需要复杂的推理，结合重叠规则的应用和数值计算。由于错误可能导致高额罚款，任何自动化系统必须提供高准确性和可审计性，这使得现代大型语言模型（LLMs）不适合此任务。我们提出了一种将LLMs与符号求解器结合以计算税务义务的方法。我们在具有挑战性的法定推理评估（SARA）数据集上评估了该系统的变体，并包括了一种基于税务错误的现实罚款估算此类系统部署成本的新方法。我们进一步展示了如何将普通文本规则的前期翻译成形式逻辑程序，并结合智能检索的示例进行形式案例表示，可以显著提高该任务的性能，并将成本降低到远低于现实世界的平均水平。我们的结果证明了将语义解析方法应用于法定推理的有效性，并显示了神经符号架构在提高可靠税务援助可及性方面的良好经济可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the complexity and potential penalties associated with tax filing, which necessitates high accuracy and auditability in automated systems. The authors propose a method that integrates large language models (LLMs) with a symbolic solver to enhance the calculation of tax obligations. Experimental results on the StAtutory Reasoning Assessment (SARA) dataset indicate that this approach, which includes translating plain-text rules into formal logic programs and utilizing intelligently retrieved exemplars, significantly improves performance and reduces costs compared to real-world averages, demonstrating the viability of neuro-symbolic architectures for reliable tax assistance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于税务申报的复杂性和潜在的罚款，这通常需要复杂的推理和准确的计算。作者提出了一种新方法，将大型语言模型与符号求解器结合，以提高税务义务计算的准确性。在对法定推理评估（SARA）数据集的实验结果表明，这种结合的方法显著提高了性能并降低了部署成本，使神经符号架构成为可靠税务援助的有希望的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Context Forcing: Consistent Autoregressive Video Generation with Long Context</div>
<div class="meta-line">Authors: Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen</div>
<div class="meta-line">First: 2026-02-05T18:58:01+00:00 · Latest: 2026-02-05T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06028v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06028v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher&#x27;s inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student&#x27;s context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文强制：具有长上下文的一致自回归视频生成</div>
<div class="mono" style="margin-top:8px">最近的实时长视频生成方法通常采用流式调优策略，试图使用短上下文（无记忆）教师训练长上下文学生。在这些框架中，学生执行长时间展开，但仅从限制在短5秒窗口的教师那里获得监督。这种结构差异造成了关键的\textbf{学生-教师不匹配}：教师无法访问长期历史，无法指导学生处理全局时间依赖性，有效限制了学生的上下文长度。为了解决这个问题，我们提出了\textbf{上下文强制}，这是一个通过长上下文教师训练长上下文学生的新框架。通过确保教师了解完整的生成历史，我们消除了监督不匹配，使得能够稳健地训练具有长期一致性的模型。为了使这一过程在极长时间（例如2分钟）内可行，我们引入了一种上下文管理系统，将线性增长的上下文转化为\textbf{慢-快记忆}架构，显著减少视觉冗余。大量结果表明，我们的方法能够实现超过20秒的有效上下文长度——比最先进的方法如LongLive和Infinite-RoPE长2到10倍。通过利用这一扩展的上下文，上下文强制在长时间内保持了卓越的一致性，超越了各种长视频评估指标上的最先进基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing real-time long video generation methods, which suffer from a student-teacher mismatch due to the short-context supervision provided by the teacher. The authors propose a novel framework called Context Forcing, which trains a long-context student using a long-context teacher to ensure that the teacher can access the full generation history. Experimental results show that this approach allows for effective context lengths exceeding 20 seconds, significantly improving long-term consistency and outperforming state-of-the-art methods like LongLive and Infinite-RoPE across various evaluation metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有实时长视频生成方法的局限性，这些方法由于短时间上下文的监督而面临学生与教师之间的不匹配。作者提出了一种名为上下文强制的创新框架，通过使用长上下文教师来训练长上下文学生，从而使监督与学生的能力相一致。实验结果表明，该方法允许有效的上下文长度超过20秒，在视频生成的长期一致性方面显著优于现有的最先进方法，如LongLive和Infinite-RoPE。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习查询感知预算层级路由以优化运行时代理内存</div>
<div class="mono" style="margin-top:8px">内存在超越单一上下文窗口的语言模型（LLM）代理中变得越来越重要，但大多数现有系统依赖于离线、与查询无关的内存构建，这可能效率低下并可能丢失与查询相关的重要信息。尽管运行时内存利用是一个自然的替代方案，但先前的工作通常会产生大量开销，并且对性能与成本的权衡提供有限的显式控制。在本研究中，我们提出了\textbf{BudgetMem}，一个用于显式、查询感知性能成本控制的运行时代理内存框架。BudgetMem将内存处理结构化为一组内存模块，每个模块提供三种预算层级（即\textsc{Low}/\textsc{Mid}/\textsc{High}）。一个轻量级路由器在模块之间执行预算层级路由，以平衡任务性能和内存构建成本，这通过强化学习训练的紧凑神经策略实现。使用BudgetMem作为统一的测试平台，我们研究了实现预算层级的三种互补策略：实现（方法复杂性）、推理（推理行为）和容量（模块模型大小）。在LoCoMo、LongMemEval和HotpotQA中，当优先考虑性能时（即高预算设置），BudgetMem超越了强基线，并在更紧的预算下提供了更好的准确性-成本边界。此外，我们的分析解开了不同层级策略的优缺点，阐明了在不同预算条件下每个维度何时提供最有利的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve memory utilization in Large Language Model (LLM) agents, which often rely on inefficient, query-agnostic memory systems that may overlook critical information. The authors introduce BudgetMem, a runtime memory framework that enables explicit, query-aware control over performance and cost by structuring memory processing into three budget tiers (Low, Mid, High) and employing a lightweight router for efficient budget-tier routing. Experimental results demonstrate that BudgetMem outperforms strong baselines in high-budget scenarios while achieving better accuracy-cost trade-offs under constrained budgets, with an analysis revealing the strengths and weaknesses of different tiering strategies across various tasks such as LoCoMo, LongMemEval, and HotpotQA.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善大型语言模型（LLM）代理的内存利用率，这些代理通常依赖于低效的、与查询无关的内存构建。作者提出了BudgetMem，这是一种运行时内存框架，允许通过将内存处理结构化为三个预算层级（低、中、高）并采用轻量级路由器进行预算层级路由，从而实现明确的、与查询相关的性能成本控制。实验结果表明，BudgetMem在高预算设置中优于强基线，同时在更紧的预算下实现了更好的准确性-成本权衡，分析揭示了不同层级策略在LoCoMo、LongMemEval和HotpotQA等任务中的优缺点。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Event-Based Shooter Models from Virtual Reality Experiments</div>
<div class="meta-line">Authors: Christopher A. McClurg, Alan R. Wagner</div>
<div class="meta-line">First: 2026-02-05T18:56:49+00:00 · Latest: 2026-02-05T18:56:49+00:00</div>
<div class="meta-line">Comments: Preprint under review for conference publication. 9 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从虚拟现实实验中学习基于事件的射手模型</div>
<div class="mono" style="margin-top:8px">虚拟现实（VR）已成为评估高风险场景（如校园枪击事件）中学校安全措施的强大工具，提供实验控制和高行为保真度。然而，在VR中评估新干预措施需要为每个条件招募新的参与者群体，这使得大规模或迭代评估变得困难。这些限制在尝试学习有效干预策略时尤其严格，因为这通常需要许多训练回合。为了解决这一挑战，我们开发了一种数据驱动的离散事件模拟器（DES），将射手运动和区域内行为建模为从VR研究中参与者行为学习的随机过程。我们使用该模拟器来检验基于机器人射手的干预策略的影响。一旦证明能够重现关键的实证模式，DES便能实现可扩展的评估和学习干预策略，这些策略直接用人类受试者训练是不可行的。总体而言，这项工作展示了一种高到中保真度的模拟工作流程，为开发和评估自主学校安全干预措施提供了可扩展的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of school security measures in high-risk scenarios, such as school shootings, using virtual reality (VR) while overcoming the challenges of recruiting new participant cohorts for each condition. The authors developed a data-driven discrete-event simulator (DES) that models shooter movement and actions based on participant behavior observed in VR studies. The key experimental finding is that the DES successfully reproduces essential empirical patterns and allows for scalable evaluation and learning of intervention strategies, which are difficult to assess directly with human subjects, thereby facilitating the development of effective autonomous school-security interventions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用虚拟现实（VR）技术改善在高风险场景（如校园枪击事件）中评估学校安全措施的能力，同时解决每个条件下需招募新参与者的挑战。作者开发了一种数据驱动的离散事件模拟器（DES），该模拟器基于在VR研究中观察到的参与者行为来模拟枪手的移动和行为。主要发现表明，该模拟器成功再现了关键的经验模式，并促进了干预策略的可扩展评估和学习，这在直接与人类受试者进行实验时是不可行的，从而为开发自主学校安全干预措施提供了有价值的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</div>
<div class="meta-line">Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</div>
<div class="meta-line">First: 2026-02-05T18:55:56+00:00 · Latest: 2026-02-05T18:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确性优化残差激活透镜 (CORAL)：可转移和校准感知的推理时间引导</div>
<div class="mono" style="margin-top:8px">大型语言模型 (LLMs) 显示出持续的误校准，尤其是在指令调优和偏好对齐之后。修改训练目标可以改善校准，但重新训练成本高昂。推理时间引导提供了一种轻量级替代方案，但大多数现有方法优化的是正确性的代理而非正确性本身。我们引入了 CORAL（正确性优化残差激活透镜），这是一种正则化的推理时间引导方法，通过权重衰减 MLP 探针捕捉模型内部激活的分布式正确性信号。我们在三个 7B 参数模型上评估了 CORAL，发现它在准确性上平均提高了 10\%，在期望校准误差 (ECE) 上平均提高了 50\%。我们还证明了这些增益在不重新训练的情况下可以转移到四个保留基准的完整发布测试集（ARC-Challenge、HellaSwag、Math-MC、OpenBookQA），平均提高了 14\% 的准确性和 49\% 的 ECE。我们的结果支持了这样一个假设：当单个神经元不足时，可以使用正则化探针提取模型内部的分布式信息。因此，CORAL 提供了一种计算高效、可转移且感知校准的方法，以提高推理期间的 MCQA 性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the persistent miscalibration observed in large language models (LLMs) after instruction tuning and preference alignment, where traditional retraining methods are costly. The authors propose CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that utilizes weight-decay MLP probes to capture distributed correctness signals from model internal activations. Experimental results show that CORAL improves accuracy by 10% and reduces expected calibration error (ECE) by 50% on average across three 7B-parameter models, with additional gains of 14% accuracy and 49% ECE improvements when applied to four held-out benchmarks without retraining.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）在指令调优和偏好对齐后持续存在的误校准问题，因为传统的再训练方法成本高昂。作者提出了CORAL（正确性优化残差激活透镜），这是一种正则化的推理时间引导方法，利用权重衰减的多层感知机探针捕捉模型内部激活中的正确性信号。实验结果表明，CORAL在三个7B参数模型上平均提高了10%的准确率，并减少了50%的预期校准误差（ECE），在四个保留基准测试上没有再训练的情况下，额外获得了14%的准确率提升和49%的ECE改善。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model&#x27;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</div>
<div class="meta-line">Authors: Ye He, Yitong Qiu, Molei Tao</div>
<div class="meta-line">First: 2026-02-05T18:55:03+00:00 · Latest: 2026-02-05T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model&#x27;s performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model&#x27;s inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的泛化可以通过对数据依赖的岭流形的归纳偏差来表征</div>
<div class="mono" style="margin-top:8px">当扩散模型不记忆训练数据集时，它是如何准确泛化的？对其生成的分布的定量理解将有助于评估模型在下游应用中的表现。因此，我们通过提出对数密度岭流形并量化生成数据与该流形的关系，明确表征扩散模型生成的内容。更准确地说，推理经历一个围绕岭流形的到达-对齐-滑动过程：轨迹首先到达流形的邻域，然后在法向方向上被推向或远离流形而对齐，最后沿切向方向在流形上滑动。在这种一般行为的范围内，不同的训练误差将导致不同的法向和切向运动，这些运动可以量化，并且这些详细的运动表征了何时出现模式间生成。对训练动态的更详细理解将导致对生成归纳偏差的更准确量化，并考虑一个随机特征模型，我们可以明确说明扩散模型的归纳偏差如何作为架构偏差和训练准确度的组合而产生，以及它们如何随着推理动态而演变。对合成多模态分布和MNIST潜在扩散的实验支持了预测的方向性效应，无论是在低维还是高维中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates how diffusion models generalize beyond memorizing training data, aiming to enhance understanding of their performance in practical applications. The authors propose a log-density ridge manifold to characterize the generated data and describe the inference dynamics as a reach-align-slide process around this manifold. Key findings indicate that different training errors influence the normal and tangent motions during inference, which can be quantified, and experiments on synthetic multimodal distributions and MNIST latent diffusion validate the predicted effects in both low- and high-dimensional settings.</div>
<div class="mono" style="margin-top:8px">本研究探讨扩散模型如何超越对训练数据的记忆进行泛化，旨在增强对其生成分布的理解，以提高下游应用的性能。作者提出了一个对数密度脊流形来表征生成的数据，并描述了围绕该流形发生的接触-对齐-滑动推理过程。对合成多模态分布和MNIST潜在扩散的实验结果表明，不同的训练误差会影响模型的法向和切向运动，支持在低维和高维设置中预测的方向效应。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of AI Protein Folding in ESMFold</div>
<div class="meta-line">Authors: Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler</div>
<div class="meta-line">First: 2026-02-05T18:54:54+00:00 · Latest: 2026-02-05T18:54:54+00:00</div>
<div class="meta-line">Comments: Our code, data, and results are available at https://folding.baulab.info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06020v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESMFold中的AI蛋白质折叠机制</div>
<div class="mono" style="margin-top:8px">蛋白质结构预测模型是如何折叠蛋白质的？我们通过追踪ESMFold如何折叠β发夹这一常见结构模式来研究这个问题。通过对模型潜变量的反事实干预，我们识别出折叠过程中的两个计算阶段。在第一阶段，早期模块初始化成对的生化信号：残基身份和相关的生化特征，如从序列表示到成对表示的电荷流。在第二阶段，后期模块发展成对的空间特征：距离和接触信息在成对表示中累积。我们证明了ESMFold结构决策背后的机制可以被局部化、通过可解释的表示追踪，并以强因果效应进行操控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to understand how AI protein structure prediction models, specifically ESMFold, fold proteins, focusing on the beta hairpin structure. The authors employ counterfactual interventions on model latents to investigate the folding process, identifying two key computational stages: the first stage initializes pairwise biochemical signals from residue identities and biochemical features, while the second stage develops pairwise spatial features through the accumulation of distance and contact information. The findings reveal that the mechanisms behind ESMFold&#x27;s structural decisions can be localized and manipulated, demonstrating strong causal effects through interpretable representations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI模型ESMFold如何预测蛋白质结构，特别关注一种常见结构动机——β发夹的折叠过程。作者通过对模型潜变量进行反事实干预，识别出折叠过程中的两个关键计算阶段。研究发现，第一阶段涉及早期模块初始化成对的生化信号，而第二阶段则由后期模块发展成对的空间特征，如距离和接触信息。该研究揭示了驱动ESMFold结构决策的机制可以被局部化、追踪和有效操控。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Token Prediction via Self-Distillation</div>
<div class="meta-line">Authors: John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein</div>
<div class="meta-line">First: 2026-02-05T18:54:48+00:00 · Latest: 2026-02-05T18:54:48+00:00</div>
<div class="meta-line">Comments: 8 pages and 5 figures in the main body</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5\%$ drop in accuracy relative to single token decoding performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自蒸馏进行多标记预测</div>
<div class="mono" style="margin-top:8px">现有的加速语言模型推理的技术，如推测解码，需要训练辅助推测模型并构建和部署复杂的推理管道。我们考虑了一种新方法，通过简单的在线蒸馏目标，将预训练的自回归语言模型从慢速的单个下一个标记预测模型转换为快速的独立多标记预测模型。最终模型保留与预训练初始检查点完全相同的实现，并且可以在不添加任何辅助验证器或其他专用推理代码的情况下进行部署。在GSM8K上，我们的方法产生的模型在准确率相对单标记解码性能下降不到5%的情况下，平均解码速度提高超过3倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiencies in language model inference caused by existing techniques that require complex auxiliary models and pipelines. The authors propose a novel method that transforms a pretrained autoregressive language model into a fast multi-token prediction model through a straightforward online distillation objective, without the need for additional verification systems. Experimental results demonstrate that their approach achieves over three times faster decoding on the GSM8K dataset with less than a 5% drop in accuracy compared to traditional single token decoding methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高语言模型推理的效率，而无需复杂的辅助模型或管道。作者提出了一种新方法，通过在线蒸馏将预训练的自回归语言模型转换为更快的多标记预测模型，同时保持原始模型的实现。实验结果表明，该方法在GSM8K数据集上实现了超过三倍的解码速度提升，且与传统的单标记解码相比，准确率下降不到5%。</div>
</details>
</div>
<div class="card">
<div class="title">MambaVF: State Space Model for Efficient Video Fusion</div>
<div class="meta-line">Authors: Zixiang Zhao, Yukun Cui, Lilun Deng, Haowen Bai, Haotong Qin, Tao Feng, Konrad Schindler</div>
<div class="meta-line">First: 2026-02-05T18:53:47+00:00 · Latest: 2026-02-05T18:53:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mambavf.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MambaVF：高效视频融合的状态空间模型</div>
<div class="mono" style="margin-top:8px">视频融合是各种视频处理任务中的基本技术。然而，现有的视频融合方法严重依赖光流估计和特征扭曲，导致计算开销大且可扩展性有限。本文提出了MambaVF，一种基于状态空间模型（SSM）的高效视频融合框架，能够在没有显式运动估计的情况下进行时间建模。首先，通过将视频融合重新表述为顺序状态更新过程，MambaVF以线性复杂度捕捉长距离时间依赖，同时显著降低计算和内存成本。其次，MambaVF提出了一种轻量级的基于SSM的融合模块，通过时空双向扫描机制替代传统的流引导对齐。该模块实现了跨帧的信息高效聚合。大量在多个基准上的实验表明，我们的MambaVF在多曝光、多聚焦、红外-可见和医学视频融合任务中达到了最先进的性能。我们强调MambaVF具有高效率，与现有方法相比，参数减少高达92.25%，计算FLOPs减少88.79%，速度提升2.1倍。项目页面：https://mambavf.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational inefficiencies and scalability limitations of existing video fusion methods that rely on optical flow estimation and feature warping. The authors introduce MambaVF, a state space model-based framework that reformulates video fusion as a sequential state update process, allowing for the capture of long-range temporal dependencies with linear complexity. Experimental results show that MambaVF achieves state-of-the-art performance across various video fusion tasks while significantly reducing parameters by up to 92.25% and computational FLOPs by 88.79%, resulting in a 2.1x speedup compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视频融合方法在光流估计和特征扭曲方面的计算开销和可扩展性限制。作者提出了MambaVF，这是一种基于状态空间模型的框架，将视频融合重新表述为一个顺序状态更新过程，从而在不进行显式运动估计的情况下实现高效的时间建模。实验结果表明，MambaVF在各种视频融合任务中实现了最先进的性能，同时将参数减少了多达92.25%，计算FLOPs减少了88.79%，与传统方法相比实现了2.1倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">Optimism Stabilizes Thompson Sampling for Adaptive Inference</div>
<div class="meta-line">Authors: Shunxing Yan, Han Zhong</div>
<div class="meta-line">First: 2026-02-05T18:52:54+00:00 · Latest: 2026-02-05T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm&#x27;s pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观主义稳定化自适应推断的汤普森采样</div>
<div class="mono" style="margin-top:8px">汤普森采样（TS）广泛用于随机多臂赌博机，但其在自适应数据收集下的推断特性较为微妙。经典的样本均值渐近理论可能失效，因为臂特定的样本大小是随机的，并通过行动选择规则与奖励耦合。我们在$K$臂高斯赌博机中研究这一现象，并将\emph{乐观主义}确定为恢复\emph{稳定性}的关键机制，稳定性是有效渐近推断的充分条件，要求每个臂的拉动次数集中在一个确定的尺度上。首先，我们证明了方差膨胀的TS \citep{halder2025stable}对于任何$K \ge 2$都是稳定的，包括多个臂最优的挑战性区域。这通过将他们的结果从双臂设置扩展到一般的$K$臂设置，解决了\citet{halder2025stable}提出的未解问题。其次，我们分析了一种替代的乐观修改，该修改保持后验方差不变，但向后验均值添加了显式的均值奖励，并建立了相同的稳定性结论。总之，适当实施的乐观主义稳定化汤普森采样，并在多臂赌博机中实现渐近有效的推断，同时仅产生轻微的额外遗憾成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the inferential properties of Thompson sampling (TS) in stochastic multi-armed bandits, motivated by the challenges posed by adaptive data collection where arm-specific sample sizes are random. The authors demonstrate that incorporating optimism into TS can restore stability, which is essential for valid asymptotic inference. They prove that variance-inflated TS is stable for any number of arms, including cases with multiple optimal arms, and also analyze an alternative method that maintains posterior variance while adding a mean bonus, both leading to the conclusion that optimism effectively stabilizes TS with only a minor increase in regret cost.</div>
<div class="mono" style="margin-top:8px">本研究解决了在随机多臂赌博机中应用汤普森采样（TS）时面临的挑战，特别是在自适应数据收集下其推断特性的问题，传统的渐近理论可能因随机的臂特定样本大小而失效。作者研究了乐观主义在稳定TS中的作用，证明了方差膨胀的TS在任意数量的臂下都是稳定的，包括多个最优臂的情况，从而将之前的结果从双臂扩展到K臂设置。此外，他们分析了一种替代的乐观方法，该方法在保持后验方差不变的同时修改后验均值，确认乐观主义有效地稳定了TS，并允许在仅略微增加遗憾成本的情况下进行有效的渐近推断。</div>
</details>
</div>
<div class="card">
<div class="title">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</div>
<div class="meta-line">Authors: Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-05T18:52:48+00:00 · Latest: 2026-02-05T18:52:48+00:00</div>
<div class="meta-line">Comments: Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06013v1">PDF</a> · <a href="https://github.com/ruihanglix/genarena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://genarena.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenArena：我们如何实现与人类对齐的视觉生成任务评估？</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展超越了传统评估方法，迫切需要采用视觉-语言模型作为替代评审。在本研究中，我们系统地调查了当前绝对逐点评分标准在广泛视觉生成任务中的可靠性。我们的分析揭示了这一范式由于随机不一致性和与人类感知的差距而受到限制。为了解决这些局限性，我们引入了GenArena，一个统一的评估框架，利用成对比较范式确保稳定和与人类对齐的评估。关键是，我们的实验发现，简单采用这一成对协议使得现成的开源模型超越顶级专有模型。值得注意的是，我们的方法将评估准确性提高了超过20%，并在权威的LMArena排行榜上实现了0.86的斯皮尔曼相关性，远远超过逐点方法的0.36相关性。基于GenArena，我们对各种任务中的最先进视觉生成模型进行了基准测试，为社区提供了严格和自动化的视觉生成评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid development of visual generation models has highlighted the inadequacies of traditional evaluation methods, prompting the need for more reliable assessment techniques. This study introduces GenArena, a new evaluation framework that employs a pairwise comparison approach to address the limitations of existing absolute pointwise scoring systems, which often suffer from stochastic inconsistencies and misalignment with human judgment. Experimental results demonstrate that using the pairwise protocol significantly enhances evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the LMArena leaderboard, in stark contrast to the 0.36 correlation observed with pointwise methods, thereby establishing a more effective standard for evaluating visual generation models across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于传统的视觉生成模型评估方法无法跟上该领域的快速发展。作者系统分析了现有绝对点对评分标准的局限性，发现其存在随机不一致性和与人类感知不对齐的问题。为了解决这些挑战，他们提出了GenArena，一个采用成对比较方法的统一评估框架，从而使评估准确性提高超过20%，并与LMArena排行榜的斯皮尔曼相关系数达到0.86，而点对方法的相关系数仅为0.36。该框架使得现成的开源模型能够超越顶级专有模型，为视觉生成任务提供了一个稳健的自动化评估标准。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</div>
<div class="meta-line">Authors: Xianyang Liu, Shangding Gu, Dawn Song</div>
<div class="meta-line">First: 2026-02-05T18:50:36+00:00 · Latest: 2026-02-05T18:50:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06008v1">PDF</a> · <a href="https://github.com/SafeRL-Lab/AgenticPay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticPay：一个多智能体LLM谈判系统用于买卖交易</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLM）的智能体越来越被期望能够自主进行谈判、协调和交易，但现有基准缺乏评估多智能体之间语言介导经济互动的原则性设置。我们介绍了AgenticPay，这是一个基准和模拟框架，用于通过自然语言驱动的多智能体买卖谈判。AgenticPay模拟了买家和卖家拥有私人约束和产品依赖估值的市场，必须通过多轮语言谈判而非仅仅数字竞标来达成协议。该框架支持超过110个任务的多样化套件，从双边谈判到多对多市场，具有结构化的行动提取和可行性、效率和福利的度量。对最先进的专有和开放权重LLM进行基准测试揭示了谈判表现的显著差距，并突出了长期战略推理中的挑战，确立了AgenticPay作为研究智能商业和基于语言的市场互动的基础。代码和数据集可在以下链接获取：https://github.com/SafeRL-Lab/AgenticPay。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of principled benchmarks for evaluating language-mediated economic interactions among multiple agents in negotiation scenarios. The authors introduce AgenticPay, a simulation framework that facilitates multi-agent buyer-seller negotiations using natural language, incorporating private constraints and product-dependent valuations. Key experimental findings indicate significant performance gaps in negotiation capabilities among state-of-the-art large language models, particularly in long-horizon strategic reasoning, thereby establishing AgenticPay as a foundational tool for exploring agentic commerce and language-based market interactions.</div>
<div class="mono" style="margin-top:8px">本研究旨在有效评估多代理之间的语言介导经济互动，因为现有基准不足。作者提出了AgenticPay，这是一个为多代理买卖谈判设计的基准和模拟框架，使用自然语言，模型中包含私人约束和产品依赖的估值。实验结果显示，专有和开放权重的大型语言模型在谈判能力上存在显著差距，特别是在长期战略推理方面，从而确立了AgenticPay作为探索代理商业和基于语言的市场互动的有价值工具。</div>
</details>
</div>
<div class="card">
<div class="title">Speech Emotion Recognition Leveraging OpenAI&#x27;s Whisper Representations and Attentive Pooling Methods</div>
<div class="meta-line">Authors: Ali Shendabadi, Parnia Izadirad, Mostafa Salehi, Mahmoud Bijankhan</div>
<div class="meta-line">First: 2026-02-05T18:46:28+00:00 · Latest: 2026-02-05T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用OpenAI的Whisper表示和注意力池化方法的语音情感识别</div>
<div class="mono" style="margin-top:8px">语音情感识别（SER）研究因缺乏标准和足够大的数据集而面临限制。最近的研究利用预训练模型提取特征用于下游任务，如SER。本研究探讨了预训练的ASR系统Whisper在语音情感识别中的能力，提出了两种基于注意力的池化方法：多头注意力平均池化和QKV池化，旨在有效降低Whisper表示的维度，同时保留情感特征。我们在英语和波斯语上进行实验，分别使用IEMOCAP和ShEMO数据集，采用Whisper Tiny和Small。我们的多头QKV架构在ShEMO数据集上取得了最先进的结果，未加权准确率提高了2.47%。我们进一步比较了不同Whisper编码器层的性能，发现中间层在波斯语数据集上的SER表现通常更好，为比HuBERT X-Large等更大模型提供了一种轻量高效的替代方案。我们的研究结果突显了Whisper作为SER表示提取器的潜力，并展示了基于注意力的池化在降维方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations in Speech Emotion Recognition (SER) caused by the scarcity of standard and large datasets. The authors propose two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, to effectively reduce the dimensionality of representations from Whisper, a pre-trained ASR system, while retaining emotional features. Experimental results show that the multi-head QKV architecture achieves state-of-the-art performance on the ShEMO dataset with a 2.47% increase in unweighted accuracy, and that intermediate layers of the Whisper encoder are more effective for SER on the Persian dataset, offering a more efficient alternative to larger models like HuBERT X-Large.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于标准数据集不足而导致的语音情感识别（SER）限制。作者提出了两种基于注意力的池化方法，即多头注意力平均池化和QKV池化，以有效减少来自预训练ASR系统Whisper的表示的维度，同时保留情感特征。实验结果表明，多头QKV架构在ShEMO数据集上实现了最先进的性能，未加权准确率提高了2.47%，并且Whisper编码器的中间层在波斯语数据集上的SER表现更佳，提供了比HuBERT X-Large等更大模型更高效的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">On Computation and Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-05T18:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>计算与强化学习</div>
<div class="mono" style="margin-top:8px">可用的计算量如何影响强化学习（RL）策略的学习？使用固定参数量的策略是否仍能从额外的计算中受益？标准的RL框架没有提供正式回答这些问题的语言。从经验上看，深度RL策略通常被参数化为具有静态架构的神经网络，将计算量与参数数量混为一谈。本文我们形式化了计算受限的策略，并证明使用更多计算的策略可以解决问题并推广到超出计算较少的策略范围的长期任务。在算法学习和无模型规划的先前工作基础上，我们提出了一种可以使用可变计算量的最小架构。我们的实验补充了我们的理论。在涵盖31个不同任务的在线和离线RL集合中，我们展示了$(1)$ 该架构通过使用更多计算实现了更强的性能，以及$(2)$ 与使用多达5倍参数的标准前馈网络或深度残差网络相比，在更长期测试任务上具有更强的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how the amount of compute available to reinforcement learning (RL) policies influences their learning capabilities, particularly whether policies with a fixed number of parameters can still benefit from increased compute. The authors formalize compute-bounded policies and demonstrate that those utilizing more compute can tackle problems and generalize to longer-horizon tasks better than those with less compute. Through experiments on 31 different tasks in both online and offline RL, they find that their proposed minimal architecture, which allows for variable compute usage, outperforms standard feedforward networks and deep residual networks, achieving stronger performance and better generalization on longer-horizon tasks even with fewer parameters.</div>
<div class="mono" style="margin-top:8px">本研究探讨可用计算资源对强化学习（RL）策略学习能力的影响，质疑固定参数策略是否仍能从增加的计算中受益。作者形式化了计算受限策略的概念，并证明使用更多计算的策略能够更有效地解决问题并推广到更长时间范围的任务。通过在31个在线和离线RL任务上的实验，研究表明，提出的利用可变计算的最小架构在性能和泛化能力上均优于标准前馈网络和深度残差网络，即使参数更少。</div>
</details>
</div>
<div class="card">
<div class="title">VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation</div>
<div class="meta-line">Authors: Jie Deng, Kaichun Yao, Libo Zhang</div>
<div class="meta-line">First: 2026-02-05T18:45:53+00:00 · Latest: 2026-02-05T18:45:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05998v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisRefiner：从视觉差异中学习以实现截图到代码生成</div>
<div class="mono" style="margin-top:8px">截图到代码生成旨在将用户界面截图转换为可执行的前端代码，忠实再现目标布局和风格。现有的多模态大型语言模型直接从截图进行此映射，但在训练时未观察到其生成代码的视觉结果。相比之下，人类开发者迭代渲染其实现，与设计进行比较，并学习视觉差异与代码更改之间的关系。受到这一过程的启发，我们提出了VisRefiner，一个训练框架，使模型能够从渲染预测与参考设计之间的视觉差异中学习。我们构建了差异对齐的监督，将视觉差异与相应的代码编辑关联起来，使模型理解外观变化如何源于实现更改。在此基础上，我们引入了自我精炼的强化学习阶段，模型通过观察渲染输出和目标设计，识别其视觉差异，并相应更新代码，从而改善生成的代码。实验表明，VisRefiner显著提高了单步生成质量和布局保真度，同时赋予模型强大的自我精炼能力。这些结果证明了从视觉差异中学习在推进截图到代码生成方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance screenshot-to-code generation by enabling models to learn from visual discrepancies between rendered outputs and reference designs, as human developers do. The authors propose VisRefiner, a training framework that incorporates difference-aligned supervision to connect visual differences with corresponding code edits, followed by a reinforcement learning stage for self-refinement. Experimental results indicate that VisRefiner significantly improves the quality of single-step code generation and layout fidelity, while also equipping models with a robust self-refinement capability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过使模型从渲染输出与目标设计之间的视觉差异中学习，来增强截图到代码的生成，这一过程类似于人类开发者的工作。作者提出了VisRefiner，一个训练框架，结合了差异对齐监督，将视觉差异与相应的代码更改联系起来，随后进行自我优化的强化学习阶段。实验结果表明，VisRefiner显著提高了单步代码生成的质量和布局保真度，同时还赋予模型强大的自我优化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Transmuting prompts into weights</div>
<div class="meta-line">Authors: Hanna Mazzawi, Benoit Dherin, Michael Munn, Michael Wunder, Javier Gonzalvo</div>
<div class="meta-line">First: 2025-10-09T18:40:39+00:00 · Latest: 2026-02-05T18:44:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08734v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt&#x27;s influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将提示转化为权重</div>
<div class="mono" style="margin-top:8px">越来越多的研究表明，通过直接修改大型语言模型的内部状态，可以有效地控制其推理时的行为，方法是对其激活进行向量加法或更新其权重矩阵。这些技术虽然强大，但通常受到经验启发式的指导，例如从对比提示的平均激活中推导引导向量。本研究为这些干预提供了理论基础，解释了它们如何从变换器架构的基本计算中产生。基于最近的发现，即提示的影响可以数学映射为与令牌相关的隐式权重更新（Dherin等，2025），我们推导出一种原则性的方法，将这些信息浓缩为与令牌无关的思维向量和思维矩阵。这些构造为现有的基于向量和矩阵的模型编辑技术提供了理论解释，并提供了一种直接的、基于计算的方法，将文本输入转化为可重用的权重更新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to better understand and control the behavior of large language models during inference by modifying their internal states. The authors propose a theoretical framework that builds on recent findings regarding the mapping of a prompt&#x27;s influence to implicit weight updates, leading to a method for creating token-independent thought vectors and matrices. Experimental results demonstrate that this approach provides a solid theoretical basis for existing model editing techniques and offers a computationally grounded method for converting textual inputs into reusable weight updates.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于为在推理时修改大型语言模型的内部状态提供理论基础，这一过程主要依赖于经验启发式方法。作者开发了一种原则性的方法，将提示的影响浓缩为与令牌无关的思维向量和矩阵，基于隐式权重更新的概念。主要实验结果表明，这些构造为现有的模型编辑技术提供了理论解释，并使得将文本输入转换为可重用权重更新的直接方法成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Inference on Stopped Random Walks in Online Advertising</div>
<div class="meta-line">Authors: Jia Yuan Yu</div>
<div class="meta-line">First: 2026-02-05T18:43:29+00:00 · Latest: 2026-02-05T18:43:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05997v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05997v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user&#x27;s interaction-trajectory, and each advertiser&#x27;s bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线广告中停止随机游走的因果推断</div>
<div class="mono" style="margin-top:8px">我们考虑一个在在线广告系统中经常遇到的因果推断问题，其中出版商（例如，Instagram、TikTok）通过偶尔向每个用户展示通过拍卖选择的广告，与人类用户和广告商反复互动。每个处理对应于广告机制的一个参数值（例如，拍卖保留价），我们希望通过实验估计相应的长期处理效应（例如，年度广告收入）。在我们的设置中，处理不仅影响展示广告的瞬时收入，还改变每个用户的互动轨迹，以及每个广告商的出价策略——后者受到有限预算的约束。特别是，每个处理甚至可能影响人群的规模，因为用户与可接受的广告机制互动的时间更长。我们放弃经典的独立同分布假设，将实验测量（例如，广告收入）建模为停止随机游走，并使用预算分割实验设计、安斯科姆定理、类似于瓦尔德的方程和中心极限定理来构建长期处理效应的置信区间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of estimating long-term treatment effects in online advertising systems, where the interaction between publishers and users is influenced by various advertising parameters. The authors propose a novel approach that models the experimental measurements as a stopped random walk, moving away from traditional independent and identically distributed (i.i.d.) assumptions. Key findings indicate that their method, which incorporates budget-splitting experimental design and statistical theorems, effectively constructs confidence intervals for the long-term treatment effects, revealing how advertising mechanisms can impact both revenue and user engagement over time.</div>
<div class="mono" style="margin-top:8px">本研究解决了在线广告系统中估计长期处理效应的挑战，在这种系统中，用户与广告商之间的互动受到各种处理参数的影响。作者提出了一种新方法，将实验测量建模为停止随机游走，摆脱了经典的独立同分布（i.i.d.）假设。主要发现表明，所提出的方法结合了预算拆分实验设计和统计定理，有效构建了长期处理效应的置信区间，揭示了广告机制如何影响用户参与和广告商行为的洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonal Self-Attention</div>
<div class="meta-line">Authors: Leo Zhang, James Martens</div>
<div class="meta-line">First: 2026-02-05T18:42:57+00:00 · Latest: 2026-02-05T18:42:57+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05996v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正交自注意力</div>
<div class="mono" style="margin-top:8px">软最大自注意力（SSA）是变换器架构的关键组成部分。然而，当在无跳跃架构中使用时，这些架构旨在改善表示学习，最近的研究突出了SSA固有的不稳定性，原因在于引发秩崩溃和条件不良的雅可比矩阵。在本研究中，我们设计了一种新颖的注意力机制：正交自注意力（OSA），旨在绕过SSA的问题，以便使（非因果）变换器在没有跳跃连接和归一化层的情况下更容易训练。特别地，OSA通过将由查询-键值形成的斜对称矩阵映射到矩阵指数，参数化注意力矩阵为正交。我们展示了这一点可以通过利用查询-键值的低秩结构来实际实现，从而使OSA的计算复杂度和内存成本与序列长度线性缩放。此外，我们推导了一种初始化方案，证明确保OSA的雅可比矩阵是良好条件的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the instability issues associated with Softmax Self-Attention (SSA) in skipless Transformer architectures, which can lead to rank collapse and poorly-conditioned Jacobians. The authors propose a new attention mechanism called Orthogonal Self-Attention (OSA), which parametrises the attention matrix to be orthogonal by mapping a skew-symmetric matrix derived from query-key values through the matrix exponential. Experimental results demonstrate that OSA can be implemented efficiently, with its computational complexity and memory requirements scaling linearly with sequence length, and an initialisation scheme is provided that ensures a well-conditioned Jacobian for OSA.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在无跳跃的Transformer架构中，Softmax自注意力（SSA）所带来的不稳定性问题，这可能导致秩崩溃和条件不良的雅可比矩阵。作者提出了一种新的注意力机制，称为正交自注意力（OSA），通过使用从查询-键值派生的反对称矩阵并应用矩阵指数，来将注意力矩阵参数化为正交。实验结果表明，OSA可以高效实现，其计算复杂度和内存成本与序列长度线性相关，并提供了一种初始化方案，确保OSA的雅可比矩阵条件良好。</div>
</details>
</div>
<div class="card">
<div class="title">Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</div>
<div class="meta-line">Authors: Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</div>
<div class="meta-line">First: 2026-02-05T18:42:00+00:00 · Latest: 2026-02-05T18:42:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05993v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05993v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose &quot;Diamond Maps&quot;, stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>钻石地图：通过随机流图实现高效的奖励对齐</div>
<div class="mono" style="margin-top:8px">流动和扩散模型生成高质量样本，但在训练后将其适应用户偏好或约束仍然成本高昂且脆弱，这一挑战通常被称为奖励对齐。我们认为，高效的奖励对齐应该是生成模型本身的一个属性，而不是事后考虑，并重新设计模型以提高适应性。我们提出了“钻石地图”，这是一种随机流图模型，能够在推理时高效且准确地对齐任意奖励。钻石地图将许多模拟步骤摊销为单步采样器，类似于流图，同时保留了实现最佳奖励对齐所需的随机性。这种设计通过实现价值函数的高效和一致估计，使搜索、序列蒙特卡洛和引导可扩展。我们的实验表明，钻石地图可以通过从GLASS流中蒸馏高效学习，获得更强的奖励对齐性能，并且比现有方法更具可扩展性。我们的结果指向了一条实用的路线，使生成模型能够在推理时快速适应任意偏好和约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of reward alignment in generative models, which is often costly and unreliable when adapting to user preferences post-training. The authors introduce &#x27;Diamond Maps&#x27;, a novel approach using stochastic flow map models designed for efficient and accurate reward alignment during inference. Experimental results demonstrate that Diamond Maps can be effectively learned through distillation from GLASS Flows, outperform existing methods in reward alignment performance, and offer improved scalability for adapting to diverse preferences and constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高生成模型对用户偏好和约束的适应性，解决训练后奖励对齐所面临的挑战。作者提出了&quot;Diamond Maps&quot;，一种使用随机流图模型的新方法，旨在实现推理过程中高效和准确的奖励对齐。实验结果表明，Diamond Maps可以通过从GLASS Flows中蒸馏高效学习，在奖励对齐性能和可扩展性方面优于现有方法，从而为生成模型在推理时快速适应多样化用户需求提供了实用解决方案。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
