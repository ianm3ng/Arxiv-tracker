<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-05 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260105_0325</div>
    <div class="row"><div class="card">
<div class="title">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</div>
<div class="meta-line">Authors: Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam</div>
<div class="meta-line">First: 2025-12-31T17:32:31+00:00 · Latest: 2025-12-31T17:32:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24986v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24986v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a &quot;render and wait&quot; paradigm toward an interactive dialogue with a modern, physics-informed pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysTalk：基于语言的实时3D高斯场物理模拟</div>
<div class="mono" style="margin-top:8px">现实的视觉模拟无处不在，但其创建需要计算时间、渲染和专业的动画知识。从文本输入生成开放词汇的视觉效果成为一种有前景的解决方案，可以释放巨大的创造潜力。然而，当前的流程缺乏物理真实感和有效的语言接口，需要缓慢的离线优化。相比之下，PhysTalk以3D高斯点云（3DGS）场景作为输入，将任意用户提示实时转换为基于物理的交互式4D动画。大型语言模型（LLM）生成可执行代码，直接通过轻量级代理和粒子动力学修改3DGS参数。值得注意的是，PhysTalk是第一个将3DGS与物理模拟器直接耦合的框架，而无需依赖耗时的网格提取。尽管保持开放词汇，这一设计通过碰撞感知的基于物理的任意多材料对象操控，实现了交互式3D高斯动画。最后，PhysTalk是无训练且计算轻量的：这使得4D动画广泛可及，并将这些工作流程从“渲染和等待”范式转向与现代物理信息管道的交互对话。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to enhance the creation of realistic visual simulations by addressing the limitations of current methods that require extensive computing time and expert knowledge. The authors developed PhysTalk, a framework that utilizes 3D Gaussian Splatting (3DGS) to convert user prompts into real-time, physics-based interactive 4D animations, employing a large language model to generate executable code that modifies 3DGS parameters. Key findings indicate that PhysTalk successfully integrates 3DGS with a physics simulator without the need for time-consuming mesh extraction, allowing for interactive manipulation of multi-material objects while being computationally lightweight and accessible for users.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决当前需要大量计算时间和专业知识的管道限制，来增强现实视觉模拟的创建。作者提出了PhysTalk，一种利用3D高斯点云（3DGS）将用户提示转化为实时、基于物理的交互式4D动画的方法，利用大型语言模型生成可执行代码以修改3DGS参数。主要实验结果表明，PhysTalk成功地将3DGS与物理模拟器结合在一起，无需耗时的网格提取，从而实现对多材料对象的交互式操控，同时保持计算轻量级和用户可访问性。</div>
</details>
</div>
<div class="card">
<div class="title">CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection</div>
<div class="meta-line">Authors: Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</div>
<div class="meta-line">First: 2025-10-16T15:27:10+00:00 · Latest: 2025-12-31T05:45:29+00:00</div>
<div class="meta-line">Comments: 28 pages, 13 Figures, 12 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14792v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14792v2">PDF</a> · <a href="https://github.com/hchoi256/cotpl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. Code and models are available at https://github.com/hchoi256/cotpl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoT-PL：视觉链式思维推理与伪标签相结合的开放词汇目标检测</div>
<div class="mono" style="margin-top:8px">开放词汇目标检测（OVD）旨在识别和定位训练期间未见的物体类别。最近的方法通常利用视觉-语言模型（VLM）通过图像-文本对齐生成伪标签，使检测器能够在没有明确监督的情况下推广到未见类别。然而，这些方法过于依赖直接的图像-文本匹配，忽视了解释语义复杂场景所必需的中间推理步骤。这导致在拥挤或遮挡的视觉环境中鲁棒性有限。本文介绍了CoT-PL，一个将结构化视觉链式思维（CoT）推理应用于伪标签过程的新框架。CoT-PL将物体理解分解为三个可解释的步骤：（1）即使对于未见物体的区域感知，（2）通过零-shot推理进行类别识别，以及（3）背景定位以分离语义复杂的物体。关键是，第三步自然激励我们的对比背景学习（CBL），利用预计算的背景线索作为负样本，促进物体与背景之间的特征解耦。通过这种方式，CoT推理和CBL形成了一个集成管道，专门针对拥挤或遮挡场景中的鲁棒伪标签。在这两种设置中，我们的新类别伪标签质量分别比最佳先前方法提高了103.4%和168.4%。我们的广泛实验表明，CoT-PL在开放词汇COCO上实现了+7.7 AP50，在LVIS上实现了+2.9 mask AP，设定了新的最先进水平。代码和模型可在https://github.com/hchoi256/cotpl获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance open-vocabulary object detection (OVD) by addressing the limitations of existing methods that rely heavily on direct image-text matching, which struggle in complex visual contexts. The authors propose CoT-PL, a framework that incorporates structured visual chain-of-thought reasoning into the pseudo-labeling process, breaking down object understanding into three steps: region perception, category recognition through zero-shot reasoning, and background grounding. Experimental results show that CoT-PL significantly improves the quality of novel-class pseudo-labels, achieving relative improvements of 103.4% and 168.4% in crowded and occluded scenes, respectively, and setting new state-of-the-art performance with +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在复杂视觉环境中依赖直接图像-文本匹配的局限性，来增强开放词汇物体检测（OVD）。作者提出了CoT-PL框架，将结构化视觉链式思维推理整合到伪标签生成过程中，将物体理解分解为三个步骤：区域感知、通过零样本推理进行类别识别以及背景定位。实验结果表明，CoT-PL在拥挤和遮挡场景中显著提高了伪标签质量，相较于之前的方法取得了103.4%和168.4%的相对提升，并在开放词汇COCO上实现了+7.7 AP50，在LVIS的新类别上实现了+2.9 mask AP，设定了新的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</div>
<div class="meta-line">Authors: Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen</div>
<div class="meta-line">First: 2025-11-14T17:34:20+00:00 · Latest: 2025-12-31T05:45:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11512v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.11512v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉、语言和视觉模态对齐的协作表示学习</div>
<div class="mono" style="margin-top:8px">触觉感知为视觉和语言提供了丰富且互补的信息，使机器人能够感知细致的物体属性。然而，现有的触觉传感器缺乏标准化，导致冗余特征，阻碍了跨传感器的泛化。此外，现有方法未能充分整合触觉、语言和视觉模态之间的中间通信。为此，我们提出了TLV-CoRe，一种基于CLIP的触觉-语言-视觉协作表示学习方法。TLV-CoRe引入了一种传感器感知调制器，以统一不同传感器的触觉特征，并采用与触觉无关的解耦学习来剥离无关的触觉特征。此外，引入了统一桥接适配器，以增强共享表示空间内的三模态交互。为了公平评估触觉模型的有效性，我们进一步提出了RSS评估框架，关注不同方法的鲁棒性、协同性和稳定性。实验结果表明，TLV-CoRe显著提高了传感器无关的表示学习和跨模态对齐，为多模态触觉表示提供了新的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the integration of tactile sensing with vision and language modalities in robotics, addressing the challenges posed by non-standardized tactile sensors and the redundancy in features that impede cross-sensor generalization. The authors propose TLV-CoRe, a collaborative representation learning method that utilizes a Sensor-Aware Modulator to standardize tactile features and implements tactile-irrelevant decoupled learning to separate irrelevant features. Key experimental findings indicate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, validated through the newly introduced RSS evaluation framework, which emphasizes Robustness, Synergy, and Stability across various methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强触觉感知与视觉和语言模态的整合，因为现有的触觉传感器缺乏标准化，阻碍了跨传感器的泛化。作者提出了TLV-CoRe，这是一种基于CLIP的协作表示学习方法，引入了传感器感知调制器以统一触觉特征，并采用与触觉无关的解耦学习来分离无关特征。实验结果表明，TLV-CoRe显著改善了传感器无关的表示学习和跨模态对齐，表明这是触觉多模态表示的一个有前景的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment</div>
<div class="meta-line">Authors: Pengfei Zhao, Rongbo Luan, Wei Zhang, Peng Wu, Sifeng He</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-08T02:33:35+00:00 · Latest: 2025-12-31T02:51:07+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06970v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06970v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite Contrastive Language-Image Pretraining (CLIP)&#x27;s remarkable capability to retrieve content across modalities, a substantial modality gap persists in its feature space. Intriguingly, we discover that off-the-shelf MLLMs (Multimodal Large Language Models) demonstrate powerful inherent modality alignment properties. While recent MLLM-based retrievers with unified architectures partially mitigate this gap, their reliance on coarse modality alignment mechanisms fundamentally limits their potential. In this work, We introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel framework that leverages the fine grained alignment priors inherent in MLLM to guide cross modal representation learning. MAPLE formulates the learning process as reinforcement learning with two key components: (1) Automatic preference data construction using off-the-shelf MLLM, and (2) a new Relative Preference Alignment (RPA) loss, which adapts Direct Preference Optimization (DPO) to the embedding learning setting. Experimental results show that our preference-guided alignment achieves substantial gains in fine-grained cross-modal retrieval, underscoring its effectiveness in handling nuanced semantic distinctions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过偏好对齐引导跨模态表示与MLLM先验</div>
<div class="mono" style="margin-top:8px">尽管对比语言-图像预训练（CLIP）在跨模态内容检索方面表现出色，但其特征空间中仍存在显著的模态差距。有趣的是，我们发现现成的多模态大语言模型（MLLM）展现出强大的固有模态对齐特性。虽然最近基于MLLM的统一架构检索器在一定程度上缓解了这一差距，但它们对粗糙模态对齐机制的依赖从根本上限制了其潜力。在本研究中，我们引入了MAPLE（用于嵌入的模态对齐偏好学习），这是一个新颖的框架，利用MLLM固有的细粒度对齐先验来引导跨模态表示学习。MAPLE将学习过程形式化为强化学习，包含两个关键组件：（1）使用现成的MLLM自动构建偏好数据，以及（2）一种新的相对偏好对齐（RPA）损失，将直接偏好优化（DPO）适应于嵌入学习设置。实验结果表明，我们的偏好引导对齐在细粒度跨模态检索中取得了显著提升，强调了其在处理细微语义差异方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the significant modality gap in the feature space of Contrastive Language-Image Pretraining (CLIP), despite its strong retrieval capabilities. The authors propose a novel framework called MAPLE (Modality-Aligned Preference Learning for Embeddings), which utilizes the inherent modality alignment properties of off-the-shelf Multimodal Large Language Models (MLLMs) to enhance cross-modal representation learning. Through reinforcement learning, MAPLE incorporates automatic preference data construction and a new Relative Preference Alignment loss, leading to substantial improvements in fine-grained cross-modal retrieval performance, demonstrating its effectiveness in capturing nuanced semantic distinctions.</div>
<div class="mono" style="margin-top:8px">本研究解决了尽管对比语言-图像预训练（CLIP）具有强大的检索能力，但其特征空间中仍存在显著的模态差距的问题。作者提出了MAPLE，一个利用多模态大型语言模型（MLLM）固有模态对齐特性的框架，通过强化学习增强跨模态表示学习。关键实验结果表明，偏好引导的对齐方法在细粒度跨模态检索中取得了显著改善，证明了其在处理细微语义差异方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression</div>
<div class="meta-line">Authors: Manikanta Kotthapalli, Banafsheh Rekabdar</div>
<div class="meta-line">First: 2025-12-31T01:07:17+00:00 · Latest: 2025-12-31T01:07:17+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24547v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24547v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The exponential growth of video traffic has placed increasing demands on bandwidth and storage infrastructure, particularly for content delivery networks (CDNs) and edge devices. While traditional video codecs like H.264 and HEVC achieve high compression ratios, they are designed primarily for pixel-domain reconstruction and lack native support for machine learning-centric latent representations, limiting their integration into deep learning pipelines. In this work, we present a Multi-Scale Vector Quantized Variational Autoencoder (MS-VQ-VAE) designed to generate compact, high-fidelity latent representations of low-resolution video, suitable for efficient storage, transmission, and client-side decoding. Our architecture extends the VQ-VAE-2 framework to a spatiotemporal setting, introducing a two-level hierarchical latent structure built with 3D residual convolutions. The model is lightweight (approximately 18.5M parameters) and optimized for 64x64 resolution video clips, making it appropriate for deployment on edge devices with constrained compute and memory resources. To improve perceptual reconstruction quality, we incorporate a perceptual loss derived from a pre-trained VGG16 network. Trained on the UCF101 dataset using 2-second video clips (32 frames at 16 FPS), on the test set we achieve 25.96 dB PSNR and 0.8375 SSIM. On validation, our model improves over the single-scale baseline by 1.41 dB PSNR and 0.0248 SSIM. The proposed framework is well-suited for scalable video compression in bandwidth-sensitive scenarios, including real-time streaming, mobile video analytics, and CDN-level storage optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于感知低分辨率视频压缩的层次向量量化潜变量</div>
<div class="mono" style="margin-top:8px">视频流量的指数增长对带宽和存储基础设施提出了越来越高的要求，特别是对于内容分发网络（CDN）和边缘设备。虽然传统视频编码器如H.264和HEVC实现了高压缩比，但它们主要设计用于像素域重建，缺乏对以机器学习为中心的潜在表示的原生支持，限制了它们在深度学习管道中的集成。在本研究中，我们提出了一种多尺度向量量化变分自编码器（MS-VQ-VAE），旨在生成紧凑、高保真的低分辨率视频潜在表示，适合高效存储、传输和客户端解码。我们的架构将VQ-VAE-2框架扩展到时空设置，引入了一个由3D残差卷积构建的两级层次潜在结构。该模型轻量级（约18.5M参数），并针对64x64分辨率的视频片段进行了优化，适合在计算和内存资源受限的边缘设备上部署。为了提高感知重建质量，我们结合了来自预训练VGG16网络的感知损失。在UCF101数据集上使用2秒的视频片段（32帧，16 FPS）进行训练，在测试集上我们达到了25.96 dB PSNR和0.8375 SSIM。在验证中，我们的模型在单尺度基线基础上提高了1.41 dB PSNR和0.0248 SSIM。所提出的框架非常适合在带宽敏感场景中进行可扩展的视频压缩，包括实时流媒体、移动视频分析和CDN级存储优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for bandwidth and storage due to the exponential growth of video traffic motivates the development of more efficient video compression methods. This study introduces a Multi-Scale Vector Quantized Variational Autoencoder (MS-VQ-VAE) that generates compact latent representations for low-resolution video, enhancing integration with machine learning applications. Experimental results show that the model achieves a PSNR of 25.96 dB and an SSIM of 0.8375 on the UCF101 dataset, outperforming the single-scale baseline by 1.41 dB PSNR and 0.0248 SSIM, demonstrating its effectiveness for scalable video compression in bandwidth-sensitive environments.</div>
<div class="mono" style="margin-top:8px">视频流量的不断增长突显了传统视频编码器在支持机器学习应用方面的局限性。本研究提出了一种多尺度向量量化变分自编码器（MS-VQ-VAE），用于生成低分辨率视频的紧凑潜在表示，从而提高存储和传输效率。该模型利用两级层次潜在结构，并针对64x64分辨率的视频片段进行了优化，在UCF101数据集上实现了25.96 dB的PSNR和0.8375的SSIM，超越了单尺度基线1.41 dB PSNR和0.0248 SSIM，适用于实时流媒体和移动视频分析等带宽敏感的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Text-to-Image Models and Their Representation of People from Different Nationalities Engaging in Activities</div>
<div class="meta-line">Authors: Abdulkareem Alsudais</div>
<div class="meta-line">First: 2025-04-08T05:37:06+00:00 · Latest: 2025-12-30T13:55:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.06313v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.06313v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates how popular text-to-image (T2I) models, DALL-E 3 and Gemini 3 Pro Preview, depict people from 206 nationalities when prompted to generate images of individuals engaging in common everyday activities. Five scenarios were developed, and 2,060 images were generated using input prompts that specified nationalities across five activities. When aggregating across activities and models, results showed that 28.4% of the images depicted individuals wearing traditional attire, including attire that is impractical for the specified activities in several cases. This pattern was statistically significantly associated with regions, with the Middle East &amp; North Africa and Sub-Saharan Africa disproportionately affected, and was also associated with World Bank income groups. Similar region- and income-linked patterns were observed for images labeled as depicting impractical attire in two athletics-related activities. To assess image-text alignment, CLIP, ALIGN, and GPT-4.1 mini were used to score 9,270 image-prompt pairs. Images labeled as featuring traditional attire received statistically significantly higher alignment scores when prompts included country names, and this pattern weakened or reversed when country names were removed. Revised prompt analysis showed that one model frequently inserted the word &quot;traditional&quot; (50.3% for traditional-labeled images vs. 16.6% otherwise). These results indicate that these representational patterns can be shaped by several components of the pipeline, including image generator, evaluation models, and prompt revision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像模型及其对不同国籍人群参与活动的表现</div>
<div class="mono" style="margin-top:8px">本文研究了流行的文本到图像（T2I）模型DALL-E 3和Gemini 3 Pro Preview在生成参与日常活动的个体图像时，如何描绘来自206个国籍的人。开发了五种场景，并使用指定国籍的输入提示生成了2060幅图像。在跨活动和模型汇总时，结果显示28.4%的图像描绘了穿着传统服饰的个体，包括在多种情况下不适合指定活动的服饰。这种模式与地区有显著统计关联，中东和北非以及撒哈拉以南非洲受到不成比例的影响，并且与世界银行收入组相关联。在两个与体育相关的活动中，观察到类似的地区和收入相关模式，标记为描绘不实用服饰的图像。为了评估图像与文本的对齐，使用CLIP、ALIGN和GPT-4.1 mini对9270对图像-提示进行评分。当提示中包含国家名称时，标记为传统服饰的图像获得了显著更高的对齐分数，而当去掉国家名称时，这种模式减弱或反转。修订的提示分析显示，一个模型经常插入“传统”一词（传统标记图像为50.3% vs. 其他为16.6%）。这些结果表明，这些表现模式可以受到多个管道组件的影响，包括图像生成器、评估模型和提示修订。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study examines how popular text-to-image models, specifically DALL-E 3 and Gemini 3 Pro Preview, represent individuals from 206 nationalities engaged in everyday activities, motivated by concerns over cultural representation in AI-generated imagery. The researchers generated 2,060 images based on prompts specifying nationalities across five scenarios, revealing that 28.4% of the images depicted individuals in traditional attire, often impractical for the activities, with significant regional disparities particularly affecting the Middle East &amp; North Africa and Sub-Saharan Africa. Additionally, the study utilized CLIP, ALIGN, and GPT-4.1 mini to evaluate image-text alignment, finding that images labeled as traditional received higher alignment scores when prompts included country names, indicating that the representation patterns are influenced by various components of the generation and evaluation process.</div>
<div class="mono" style="margin-top:8px">本研究考察了流行的文本到图像模型，特别是DALL-E 3和Gemini 3 Pro Preview，如何表现来自206个国籍的个体参与日常活动。研究人员基于指定国籍的提示生成了2060幅图像，涵盖五种场景。结果显示，28.4%的图像描绘了穿着传统服饰的个体，这些服饰在许多情况下对活动来说并不实用，且存在显著的地区差异，尤其影响中东和北非以及撒哈拉以南非洲，并与世界银行收入组相关。此外，图像与文本的对齐评估显示，当提示中包含国家名称时，标记为传统的图像获得了更高的对齐分数，这表明表现模式受到图像生成过程各个组件的影响。</div>
</details>
</div>
<div class="card">
<div class="title">ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation</div>
<div class="meta-line">Authors: Ziquan Liu, Zhewei Zhu, Xuyang Shi</div>
<div class="meta-line">First: 2025-12-30T13:38:30+00:00 · Latest: 2025-12-30T13:38:30+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24224v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP&#x27;s internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP&#x27;s internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere&quot; paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARM：一种可学习的即插即用模块，用于基于CLIP的开放词汇语义分割</div>
<div class="mono" style="margin-top:8px">开放词汇语义分割（OVSS）在根本上受到CLIP粗糙的图像级表示的限制，缺乏精确的像素级细节。现有的无训练方法试图通过从昂贵的外部基础模型（例如，SAM，DINO）导入先验知识或通过将静态的手工设计启发式应用于CLIP的内部特征来解决这个问题。这些方法要么计算开销大，要么效果不佳。我们提出了注意力精炼模块（ARM），这是一种轻量级的可学习模块，能够有效地解锁和精炼CLIP的内部潜力。与静态融合方法不同，ARM学习自适应地融合层次特征。它采用语义引导的交叉注意力块，使用强大的深度特征（K，V）选择和精炼细节丰富的浅层特征（Q），然后是自注意力块。关键创新在于“训练一次，随处使用”的范式。在通用数据集（例如，COCO-Stuff）上训练一次后，ARM作为多种无训练框架的通用即插即用后处理器。大量实验表明，ARM在多个基准上始终提升基线性能，推理开销微乎其微，建立了一种高效且有效的无训练OVSS范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of open-vocabulary semantic segmentation (OVSS) caused by the coarse, image-level representations of CLIP, which do not provide precise pixel-level details. The authors propose the Attention Refinement Module (ARM), a lightweight and learnable module that refines CLIP&#x27;s internal features through a semantically-guided cross-attention block and a self-attention block, allowing for adaptive fusion of hierarchical features. Experimental results demonstrate that ARM significantly enhances baseline performance across multiple benchmarks while maintaining minimal inference overhead, thereby offering an efficient solution for training-free OVSS.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决开放词汇语义分割（OVSS）中CLIP的粗糙图像级表示所带来的局限性，这种表示缺乏像素级细节。作者提出了注意力细化模块（ARM），这是一个轻量且可学习的模块，通过语义引导的交叉注意力块自适应地融合层次特征，从而细化CLIP的内部特征。实验结果表明，ARM显著提升了多个基准测试中的基线性能，同时保持了最小的推理开销，从而为无训练的OVSS提供了一种高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</div>
<div class="meta-line">Authors: Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</div>
<div class="meta-line">First: 2025-12-30T13:25:22+00:00 · Latest: 2025-12-30T13:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24212v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANGER：通过上下文适应的单目零-shot语义导航框架</div>
<div class="mono" style="margin-top:8px">在复杂环境中高效寻找目标是现实世界具身应用的基础。尽管最近多模态基础模型的进展使得零-shot目标导航成为可能，允许机器人在不进行微调的情况下搜索任意物体，但现有方法面临两个主要限制：（1）对模拟器提供的精确深度和姿态信息的高度依赖，限制了其在现实场景中的适用性；（2）缺乏上下文学习（ICL）能力，使得快速适应新环境变得困难，例如利用短视频。为了解决这些挑战，我们提出了RANGER，一个新颖的零-shot、开放词汇的语义导航框架，仅使用单目相机操作。RANGER利用强大的3D基础模型，消除了对深度和姿态的依赖，同时展现出强大的ICL能力。通过简单观察新环境的短视频，该系统还可以显著提高任务效率，而无需架构修改或微调。该框架集成了几个关键组件：基于关键帧的3D重建、语义点云生成、视觉-语言模型（VLM）驱动的探索价值估计、高级自适应航点选择和低级动作执行。在HM3D基准和现实环境中的实验表明，RANGER在导航成功率和探索效率方面表现出竞争力，同时展现出优越的ICL适应性，无需先前的环境3D映射。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance target finding in complex environments for real-world applications, addressing limitations in existing zero-shot object goal navigation methods that rely heavily on precise depth and pose information and lack in-context learning capabilities. The authors propose RANGER, a monocular zero-shot semantic navigation framework that utilizes 3D foundation models to operate without depth and pose dependencies while incorporating strong in-context learning through short video observations. Experimental results on the HM3D benchmark and real-world environments show that RANGER achieves competitive navigation success rates and exploration efficiency, demonstrating superior adaptability to new environments without prior 3D mapping requirements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高机器人在复杂环境中寻找目标的能力，解决现有零-shot目标导航方法对深度和姿态信息的高度依赖。作者提出了RANGER，一个单目零-shot语义导航框架，利用3D基础模型消除了对深度和姿态数据的需求，同时具备上下文学习能力。实验结果表明，RANGER在HM3D基准和真实环境中实现了竞争性的导航成功率和探索效率，展现出在没有先前3D映射要求的情况下对新环境的优越适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</div>
<div class="meta-line">Authors: TsaiChing Ni, ZhenQi Chen, YuanFu Yang</div>
<div class="meta-line">First: 2025-12-30T11:45:22+00:00 · Latest: 2025-12-30T11:45:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24160v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向开放词汇的工业缺陷理解：大规模多模态数据集</div>
<div class="mono" style="margin-top:8px">我们提出了IMDD-1M，这是第一个大规模工业多模态缺陷数据集，包含1,000,000个对齐的图像-文本对，旨在推动制造和质量检查的多模态学习。IMDD-1M包含高分辨率的真实缺陷，涵盖60多个材料类别和400多种缺陷类型，每种缺陷都有专家验证的注释和详细描述缺陷位置、严重性和上下文属性的细粒度文本。该数据集支持广泛的应用，包括分类、分割、检索、字幕生成和生成建模。在IMDD-1M的基础上，我们从头开始训练了一个基于扩散的视觉-语言基础模型，专门针对工业场景。该模型作为一个可泛化的基础，可以通过轻量级微调高效适应专业领域。它所需的任务特定数据不到专用专家模型的5%，却实现了可比的性能，突显了数据高效基础模型适应在工业检查和生成中的潜力，为可扩展、领域自适应和知识驱动的制造智能铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multimodal learning for manufacturing and quality inspection through the introduction of a comprehensive dataset. The authors present IMDD-1M, a large-scale Industrial Multimodal Defect Dataset consisting of 1,000,000 aligned image-text pairs that cover over 60 material categories and 400 defect types, complete with expert-verified annotations. They train a diffusion-based vision-language foundation model from this dataset, which demonstrates that it can achieve comparable performance to specialized models while requiring less than 5% of the task-specific data, indicating the effectiveness of data-efficient model adaptation in industrial applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过引入一个全面的数据集来增强制造和质量检查的多模态学习。作者提出了IMDD-1M，这是一个大型工业多模态缺陷数据集，包含1,000,000对对齐的图像-文本对，其中包括各种材料类别和缺陷类型的高分辨率真实缺陷图像，并附有专家验证的注释。利用该数据集，研究人员从头开始训练了一个基于扩散的视觉-语言基础模型，证明其在需要不到5%的任务特定数据的情况下能够达到与专业模型相当的性能，从而展示了数据高效模型适应在工业应用中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications</div>
<div class="meta-line">Authors: Zijian Liu</div>
<div class="meta-line">First: 2025-08-10T20:17:38+00:00 · Latest: 2025-12-30T05:49:16+00:00</div>
<div class="meta-line">Comments: A short, self-contained version has been accepted at ALT 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07473v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.07473v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Online Convex Optimization (OCO), when the stochastic gradient has a finite variance, many algorithms provably work and guarantee a sublinear regret. However, limited results are known if the gradient estimate has a heavy tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this work examines different old algorithms for OCO (e.g., Online Gradient Descent) in the more challenging heavy-tailed setting. Under the standard bounded domain assumption, we establish new regrets for these classical methods without any algorithmic modification. Remarkably, these regret bounds are fully optimal in all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting that OCO with heavy tails can be solved effectively without any extra operation (e.g., gradient clipping). Our new results have several applications. A particularly interesting one is the first provable and optimal convergence result for nonsmooth nonconvex optimization under heavy-tailed noise without gradient clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and extend our ideas to optimistic algorithms to handle different cases simultaneously.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有重尾的在线凸优化：旧算法、新遗憾与应用</div>
<div class="mono" style="margin-top:8px">在在线凸优化（OCO）中，当随机梯度具有有限方差时，许多算法可以证明有效并保证次线性遗憾。然而，如果梯度估计具有重尾，即随机梯度仅在某个 $\mathsf{p}\in\left(1,2\right]$ 下承认有限的 $\mathsf{p}$-阶中心矩，则已知的结果有限。基于此，本文考察了在更具挑战性的重尾环境中，OCO 的不同旧算法（例如在线梯度下降）。在标准有界域假设下，我们为这些经典方法建立了新的遗憾，而无需任何算法修改。值得注意的是，这些遗憾界在所有参数中都是完全最优的（即使在不知道 $\mathsf{p}$ 的情况下也能实现），这表明具有重尾的 OCO 可以有效解决，而无需任何额外操作（例如梯度裁剪）。我们的新结果有多个应用。其中一个特别有趣的应用是在重尾噪声下，无需梯度裁剪的非光滑非凸优化的第一个可证明和最优收敛结果。此外，我们探索了更广泛的设置（例如光滑 OCO），并将我们的想法扩展到乐观算法，以同时处理不同情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limited understanding of Online Convex Optimization (OCO) when the stochastic gradient exhibits heavy tails, which poses challenges for existing algorithms that typically assume finite variance. The authors investigate classical OCO algorithms, such as Online Gradient Descent, under the heavy-tailed condition and derive new regret bounds without modifying the algorithms. The key findings reveal that these regret bounds are optimal across all parameters, indicating that OCO can be effectively addressed in heavy-tailed scenarios without additional operations like gradient clipping, and the results have significant implications for nonsmooth nonconvex optimization and other broader settings in OCO.</div>
<div class="mono" style="margin-top:8px">本研究解决了在线凸优化（OCO）在处理重尾随机梯度时的挑战，与有限方差的情况相比，重尾情况的现有结果有限。作者在有界域假设下研究了经典算法，如在线梯度下降，并推导出新的遗憾界限，这些界限在所有参数上都是最优的，无需对算法进行修改。研究结果表明，在重尾场景中，OCO可以有效管理，应用包括在没有梯度裁剪的情况下，首次获得重尾噪声下非光滑非凸优化的最优收敛结果。</div>
</details>
</div>
<div class="card">
<div class="title">ISOPO: Proximal policy gradients without pi-old</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2025-12-29T10:30:29+00:00 · Latest: 2025-12-30T03:46:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23353v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23353v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ISOPO：无 π-old 的近端策略梯度</div>
<div class="mono" style="margin-top:8px">本文介绍了等距策略优化（ISOPO），这是一种高效的方法，可以在单个梯度步骤中近似自然策略梯度。相比之下，现有的近端策略方法如 GRPO 或 CISPO 使用多个梯度步骤和重要性比率裁剪的变体，以近似相对于参考策略的自然梯度步骤。在其最简单的形式中，ISOPO 在与优势收缩之前，先在费舍尔度量下对每个序列的对数概率梯度进行归一化。ISOPO 的另一个变体基于每层的神经切线核转换微批优势。ISOPO 在单个反向传播中逐层应用此转换，与普通的 REINFORCE 相比，计算开销可以忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of proximal policy gradient methods in reinforcement learning by introducing Isometric Policy Optimization (ISOPO), which approximates the natural policy gradient in a single gradient step. The method involves normalizing the log-probability gradient of each sequence in the Fisher metric and contracting it with the advantages, with an alternative variant that adjusts microbatch advantages using the neural tangent kernel layer-wise. Experimental results demonstrate that ISOPO can achieve comparable performance to existing methods while incurring negligible computational overhead compared to traditional REINFORCE.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于寻找一种更高效的方法来近似强化学习中的自然策略梯度。作者提出了等距策略优化（ISOPO），通过在Fisher度量中对每个序列的对数概率梯度进行归一化，并在单个梯度步骤中与优势进行收缩，从而简化了过程，这与现有需要多个步骤的方法形成对比。实验结果表明，ISOPO在性能上与传统的近端策略方法相当，同时计算开销极小，使其成为策略优化任务的可行替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</div>
<div class="meta-line">Authors: Zhenguo Zhang, Haohan Zheng, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang</div>
<div class="meta-line">First: 2025-12-16T03:19:28+00:00 · Latest: 2025-12-29T12:27:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14044v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14044v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning. While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels. Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and &quot;zoom in&quot; on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model&#x27;s significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniDrive-R1：基于强化学习的交错多模态思维链用于可信的视觉-语言自主驾驶</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLM）在自主驾驶（AD）等安全关键领域的部署受到可靠性失败的严重阻碍，尤其是物体幻觉。这种失败源于它们对无基础、基于文本的思维链（CoT）推理的依赖。虽然现有的多模态CoT方法试图进行缓解，但它们存在两个根本缺陷：（1）感知和推理阶段的解耦，阻碍了端到端的联合优化；（2）依赖昂贵的密集定位标签。因此，我们引入了OmniDrive-R1，一个为自主驾驶设计的端到端VLM框架，通过交错的多模态思维链（iMCoT）机制统一感知和推理。我们的核心创新是基于强化学习的视觉定位能力，使模型能够自主引导注意力并“放大”关键区域进行细致分析。该能力得益于我们纯粹的两阶段强化学习训练流程和Clip-GRPO算法。至关重要的是，Clip-GRPO引入了一种无注释的基于过程的定位奖励。该奖励不仅消除了对密集标签的需求，还通过强制视觉焦点与文本推理之间的实时跨模态一致性，避免了外部工具调用的不稳定性。在DriveLMM-o1上的大量实验表明，我们的模型显著改善。与基线Qwen2.5VL-7B相比，OmniDrive-R1将整体推理得分从51.77%提高到80.35%，最终答案准确率从37.81%提高到73.62%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the reliability issues of Vision-Language Models (VLMs) in autonomous driving, particularly focusing on the problem of object hallucination due to ungrounded reasoning. The authors propose OmniDrive-R1, an end-to-end VLM framework that integrates perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism, utilizing a reinforcement-driven visual grounding capability. Experimental results on the DriveLMM-o1 dataset show that OmniDrive-R1 significantly enhances reasoning scores from 51.77% to 80.35% and improves final answer accuracy from 37.81% to 73.62% compared to the baseline model Qwen2.5VL-7B.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉语言模型（VLM）在自动驾驶中可靠性失败的问题，特别是由于无基础推理导致的物体幻觉。作者提出了OmniDrive-R1，这是一种端到端的VLM框架，通过交错的多模态思维链（iMCoT）机制整合感知和推理，利用强化驱动的视觉定位能力。DriveLMM-o1上的实验结果表明，OmniDrive-R1显著提高了性能，与基线模型Qwen2.5VL-7B相比，整体推理得分从51.77%提高到80.35%，最终答案准确率从37.81%提高到73.62%。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuned Vision Transformers Capture Complex Wheat Spike Morphology for Volume Estimation from RGB Images</div>
<div class="meta-line">Authors: Olivia Zumsteg, Nico Graf, Aaron Haeusler, Norbert Kirchgessner, Nicola Storni, Lukas Roth, Andreas Hund</div>
<div class="meta-line">First: 2025-06-22T15:02:18+00:00 · Latest: 2025-12-29T10:33:48+00:00</div>
<div class="meta-line">Comments: 18 pages, 22 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.18060v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.18060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating three-dimensional morphological traits such as volume from two-dimensional RGB images presents inherent challenges due to the loss of depth information, projection distortions, and occlusions under field conditions. In this work, we explore multiple approaches for non-destructive volume estimation of wheat spikes using RGB images and structured-light 3D scans as ground truth references. Wheat spike volume is promising for phenotyping as it shows high correlation with spike dry weight, a key component of fruiting efficiency. Accounting for the complex geometry of the spikes, we compare different neural network approaches for volume estimation from 2D images and benchmark them against two conventional baselines: a 2D area-based projection and a geometric reconstruction using axis-aligned cross-sections. Fine-tuned Vision Transformers (DINOv2 and DINOv3) with MLPs achieve the lowest MAPE of 5.08\% and 4.67\% and the highest correlation of 0.96 and 0.97 on six-view indoor images, outperforming fine-tuned CNNs (ResNet18 and ResNet50), wheat-specific backbones, and both baselines. When using frozen DINO backbones, deep-supervised LSTMs outperform MLPs, whereas after fine-tuning, improved high-level representations allow simple MLPs to outperform LSTMs. We demonstrate that object shape significantly impacts volume estimation accuracy, with irregular geometries such as wheat spikes posing greater challenges for geometric methods than for deep learning approaches. Fine-tuning DINOv3 on field-based single side-view images yields a MAPE of 8.39\% and a correlation of 0.90, providing a novel pipeline and a fast, accurate, and non-destructive approach for wheat spike volume phenotyping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微调视觉变换器捕捉复杂小麦穗形态以从RGB图像估算体积</div>
<div class="mono" style="margin-top:8px">从二维RGB图像估算三维形态特征如体积面临固有挑战，因为深度信息丢失、投影失真和田间条件下的遮挡。在这项工作中，我们探索了多种方法，通过使用RGB图像和结构光3D扫描作为真实参考，进行小麦穗的非破坏性体积估算。小麦穗体积在表型分析中具有前景，因为它与穗干重高度相关，后者是结果效率的关键组成部分。考虑到穗的复杂几何形状，我们比较了不同神经网络在从2D图像中进行体积估算的表现，并将其与两种传统基准进行对比：基于2D面积的投影和使用轴对齐截面的几何重建。微调的视觉变换器（DINOv2和DINOv3）与MLP结合在六视图室内图像上实现了最低的MAPE为5.08\%和4.67\%，以及最高的相关性0.96和0.97，超越了微调的CNN（ResNet18和ResNet50）、小麦特定骨干网络和两个基准。当使用冻结的DINO骨干时，深度监督的LSTM优于MLP，而在微调后，改进的高层表示使简单的MLP超越LSTM。我们证明了物体形状显著影响体积估算的准确性，像小麦穗这样的不规则几何形状对几何方法提出了更大的挑战，而对深度学习方法则相对较小。在基于田间的单侧视图图像上微调DINOv3，获得了8.39\%的MAPE和0.90的相关性，提供了一种新颖的流程以及快速、准确和非破坏性的小麦穗体积表型分析方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of estimating three-dimensional morphological traits, specifically the volume of wheat spikes, from two-dimensional RGB images, which often suffer from depth information loss and occlusions. The authors employ various neural network approaches, particularly fine-tuned Vision Transformers, and compare their performance against traditional methods such as 2D area-based projections and geometric reconstructions. The key findings indicate that fine-tuned Vision Transformers (DINOv2 and DINOv3) achieve the lowest mean absolute percentage error (MAPE) of 5.08% and 4.67%, respectively, along with high correlation coefficients of 0.96 and 0.97, outperforming other models and conventional baselines, thus providing a promising method for non-destructive wheat spike volume estimation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从二维RGB图像中估计三维形态特征（特别是小麦穗体积）所面临的挑战，这些图像常常存在深度信息丢失和投影失真。研究采用多种神经网络方法，特别是微调的视觉变换器，来估计小麦穗的体积，并使用结构光3D扫描作为验证的真实参考。主要发现表明，微调的视觉变换器（DINOv2和DINOv3）分别实现了最低的平均绝对百分比误差（MAPE）为5.08%和4.67%，相关系数高达0.96和0.97，超越了传统方法和其他神经网络架构在体积估计准确性上的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</div>
<div class="meta-line">Authors: KunHo Heo, GiHyun Kim, SuYeon Kim, MyeongAh Cho</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-06T11:33:09+00:00 · Latest: 2025-12-29T07:15:13+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025. Code: https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04714v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04714v2">PDF</a> · <a href="https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D Semantic Scene Graph Prediction aims to detect objects and their semantic relationships in 3D scenes, and has emerged as a crucial technology for robotics and AR/VR applications. While previous research has addressed dataset limitations and explored various approaches including Open-Vocabulary settings, they frequently fail to optimize the representational capacity of object and relationship features, showing excessive reliance on Graph Neural Networks despite insufficient discriminative capability. In this work, we demonstrate through extensive analysis that the quality of object features plays a critical role in determining overall scene graph accuracy. To address this challenge, we design a highly discriminative object feature encoder and employ a contrastive pretraining strategy that decouples object representation learning from the scene graph prediction. This design not only enhances object classification accuracy but also yields direct improvements in relationship prediction. Notably, when plugging in our pretrained encoder into existing frameworks, we observe substantial performance improvements across all evaluation metrics. Additionally, whereas existing approaches have not fully exploited the integration of relationship information, we effectively combine both geometric and semantic features to achieve superior relationship prediction. Comprehensive experiments on the 3DSSG dataset demonstrate that our approach significantly outperforms previous state-of-the-art methods. Our code is publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向增强3D场景图预测的对象中心表示学习</div>
<div class="mono" style="margin-top:8px">3D语义场景图预测旨在检测3D场景中的对象及其语义关系，已成为机器人技术和AR/VR应用的重要技术。尽管之前的研究解决了数据集的局限性，并探索了包括开放词汇设置在内的各种方法，但它们往往未能优化对象和关系特征的表示能力，过度依赖图神经网络，尽管其判别能力不足。在本研究中，我们通过广泛分析表明，对象特征的质量在决定整体场景图准确性方面起着关键作用。为了解决这一挑战，我们设计了一种高度判别的对象特征编码器，并采用了一种对比预训练策略，将对象表示学习与场景图预测解耦。这一设计不仅提高了对象分类的准确性，还直接改善了关系预测。值得注意的是，当将我们的预训练编码器插入现有框架时，我们观察到所有评估指标的性能显著提升。此外，尽管现有方法未能充分利用关系信息的整合，我们有效地结合了几何特征和语义特征，以实现更优的关系预测。在3DSSG数据集上的全面实验表明，我们的方法显著优于之前的最先进方法。我们的代码已公开，地址为 https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve 3D Semantic Scene Graph Prediction, which is essential for robotics and AR/VR applications, by addressing the limitations of previous methods that rely heavily on Graph Neural Networks and do not optimize object and relationship feature representation. The authors propose a novel object feature encoder and a contrastive pretraining strategy that separates object representation learning from scene graph prediction, enhancing both object classification and relationship prediction accuracy. Experimental results on the 3DSSG dataset show that this approach significantly outperforms existing state-of-the-art methods across all evaluation metrics, particularly by effectively integrating geometric and semantic features for better relationship prediction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决以往方法过于依赖图神经网络的局限性，来改善3D语义场景图预测，这对机器人技术和增强现实/虚拟现实应用至关重要。作者提出了一种新方法，包括一个高度区分的物体特征编码器和对比预训练策略，将物体表示学习与场景图预测分离。对3DSSG数据集的实验结果表明，该方法显著提高了物体分类准确性和关系预测能力，相较于现有的最先进方法，在所有评估指标上都取得了显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis</div>
<div class="meta-line">Authors: Zijian Liu</div>
<div class="meta-line">First: 2025-12-29T03:35:53+00:00 · Latest: 2025-12-29T03:35:53+00:00</div>
<div class="meta-line">Comments: Part of this work is in submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23178v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23178v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimization under heavy-tailed noise has become popular recently, since it better fits many modern machine learning tasks, as captured by empirical observations. Concretely, instead of a finite second moment on gradient noise, a bounded ${\frak p}$-th moment where ${\frak p}\in(1,2]$ has been recognized to be more realistic (say being upper bounded by $σ_{\frak l}^{\frak p}$ for some $σ_{\frak l}\ge0$). A simple yet effective operation, gradient clipping, is known to handle this new challenge successfully. Specifically, Clipped Stochastic Gradient Descent (Clipped SGD) guarantees a high-probability rate ${\cal O}(σ_{\frak l}\ln(1/δ)T^{1/{\frak p}-1})$ (resp. ${\cal O}(σ_{\frak l}^2\ln^2(1/δ)T^{2/{\frak p}-2})$) for nonsmooth convex (resp. strongly convex) problems, where $δ\in(0,1]$ is the failure probability and $T\in\mathbb{N}$ is the time horizon. In this work, we provide a refined analysis for Clipped SGD and offer two faster rates, ${\cal O}(σ_{\frak l}d_{\rm eff}^{-1/2{\frak p}}\ln^{1-1/{\frak p}}(1/δ)T^{1/{\frak p}-1})$ and ${\cal O}(σ_{\frak l}^2d_{\rm eff}^{-1/{\frak p}}\ln^{2-2/{\frak p}}(1/δ)T^{2/{\frak p}-2})$, than the aforementioned best results, where $d_{\rm eff}\ge1$ is a quantity we call the $\textit{generalized effective dimension}$. Our analysis improves upon the existing approach on two sides: better utilization of Freedman&#x27;s inequality and finer bounds for clipping error under heavy-tailed noise. In addition, we extend the refined analysis to convergence in expectation and obtain new rates that break the known lower bounds. Lastly, to complement the study, we establish new lower bounds for both high-probability and in-expectation convergence. Notably, the in-expectation lower bounds match our new upper bounds, indicating the optimality of our refined analysis for convergence in expectation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the increasing relevance of optimization under heavy-tailed noise in modern machine learning tasks, which necessitates a more realistic approach to gradient noise modeling. The authors employ a refined analysis of Clipped Stochastic Gradient Descent (Clipped SGD) to derive faster convergence rates for nonsmooth and strongly convex problems, improving upon existing methods by better utilizing Freedman&#x27;s inequality and providing tighter bounds for clipping errors. Key experimental findings reveal new convergence rates that surpass previous results and establish optimality in convergence in expectation, with new lower bounds that align with the upper bounds achieved.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在现代机器学习任务中，重尾噪声下的优化变得越来越重要，这需要对梯度噪声建模采取更现实的方法。作者采用了对剪切随机梯度下降（Clipped SGD）的精细分析，该方法通过梯度剪切来应对重尾噪声带来的挑战。主要发现表明，所提出的方法在收敛速度上优于先前的结果，具体为对于非光滑凸问题达到 ${\cal O}(σ_{\frak l}d_{\rm eff}^{-1/2{\frak p}}\ln^{1-1/{\frak p}}(1/δ)T^{1/{\frak p}-1})$，对于强凸问题达到 ${\cal O}(σ_{\frak l}^2d_{\rm eff}^{-1/{\frak p}}\ln^{2-2/{\frak p}}(1/δ)T^{2/{\frak p}-2})$，同时建立了新的下界，确认了该分析在期望收敛性方面的最优性。</div>
</details>
</div>
<div class="card">
<div class="title">Trust Region Masking for Long-Horizon LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T20:41:59+00:00 · Latest: 2025-12-28T20:41:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23075v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时间跨度 LLM 强化学习的信任区域屏蔽</div>
<div class="mono" style="margin-top:8px">大型语言模型的策略梯度方法优化从滚动策略 $π_{\text{roll}}$ 的样本计算的替代目标。当 $π_{\text{roll}} \ne π_θ$ 时，替代目标与真实目标之间存在近似误差。先前的研究表明，由于实现偏差、专家混合路由的不连续性和分布式训练的滞后，这种离策略不匹配在现代 LLM-RL 中是不可避免的。经典的信任区域界限使得结果误差的规模为 $O(T^2)$，其中序列长度为 $T$，这使得它们在长时间跨度任务中变得无效。我们推导出两个更紧的界限：一个是 Pinsker-Marginal 界限，规模为 $O(T^{3/2})$，另一个是 Mixed 界限，规模为 $O(T)$。关键是，这两个界限都依赖于 $D_{kl}^{tok,max}$——序列中所有位置的最大 token 级 KL 散度。这本质上是一个序列级别的量：它需要检查整个轨迹才能计算，因此无法通过像 PPO 剪切这样的与 token 无关的方法来控制。我们提出了信任区域屏蔽 (TRM)，如果任何 token 违反信任区域，则从梯度计算中排除整个序列，为长时间跨度 LLM-RL 提供了首个非空的单调改进保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the approximation error in policy gradient methods for large language models (LLMs) caused by off-policy mismatches during reinforcement learning (RL). The authors derive two tighter bounds on the error, specifically a Pinsker-Marginal bound scaling as O(T^{3/2}) and a Mixed bound scaling as O(T), which depend on the maximum token-level KL divergence across a sequence. They introduce Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates the trust region, achieving non-vacuous monotonic improvement guarantees for long-horizon LLM-RL tasks.</div>
<div class="mono" style="margin-top:8px">该研究解决了大语言模型（LLM）中策略梯度方法因离线策略不匹配而导致的近似误差问题，这一问题因实现差异和分布式训练滞后等因素而加剧。作者推导了对该误差的更紧界限，特别是一个以O(T^{3/2})为尺度的Pinsker-Marginal界限和一个以O(T)为尺度的混合界限，这两者均依赖于序列中最大令牌级KL散度。他们提出了信任区域掩蔽（TRM）方法，该方法在任何令牌违反信任区域时排除整个序列的梯度计算，为长时间跨度的LLM强化学习任务实现了非空的单调改进保证。</div>
</details>
</div>
<div class="card">
<div class="title">Video-BrowseComp: Benchmarking Agentic Video Research on Open Web</div>
<div class="meta-line">Authors: Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao</div>
<div class="meta-line">First: 2025-12-28T19:08:27+00:00 · Latest: 2025-12-28T19:08:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23044v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web&#x27;s most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Video-BrowseComp：开放网络上自主视频研究的基准测试</div>
<div class="mono" style="margin-top:8px">自主智能体的演变正在重新定义信息获取，从被动检索转向主动、开放式的网络研究。然而，尽管文本和静态多模态智能体取得了快速进展，但在处理网络上最动态的模态：视频方面，仍然存在显著的模态差距。现有的视频基准主要集中在被动感知上，将策划的剪辑提供给模型，而不需要外部检索。它们未能评估自主视频研究，这需要主动询问视频时间线、交叉引用分散的证据，并验证与开放网络的声明。为了解决这一差距，我们提出了\textbf{Video-BrowseComp}，这是一个包含210个问题的挑战性基准，专为开放网络自主视频推理量身定制。与之前的基准不同，Video-BrowseComp 强制依赖于时间视觉证据，确保答案不能仅通过文本搜索得出，而需要导航视频时间线以验证外部声明。我们对最先进模型的评估揭示了一个关键瓶颈：即使是像 GPT-5.1（带搜索）这样的先进搜索增强模型也仅达到 15.24\% 的准确率。我们的分析表明，这些模型在很大程度上依赖于文本代理，在元数据丰富的领域（例如，带有情节摘要的电视节目）表现出色，但在元数据稀缺、动态环境（例如，体育、游戏）中崩溃，在这些环境中视觉基础至关重要。作为第一个开放网络视频研究基准，Video-BrowseComp 将该领域从被动感知推进到主动视频推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap in evaluating agentic video research, as existing benchmarks focus primarily on passive video perception. The authors introduce Video-BrowseComp, a benchmark consisting of 210 questions designed to assess open-web agentic video reasoning, which requires active engagement with video timelines and verification of claims. Experimental results indicate that even advanced models like GPT-5.1 (with Search) achieve only 15.24% accuracy, highlighting their reliance on textual information and their struggles in dynamic, metadata-sparse environments where visual grounding is crucial.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决评估主动视频研究中的显著差距，因为现有基准主要集中在被动视频感知上。作者提出了Video-BrowseComp，这是一个包含210个问题的基准，旨在评估开放网络的主动视频推理，这需要积极参与视频内容，而不仅仅依赖文本搜索。实验结果表明，即使是像GPT-5.1（带搜索）这样的先进模型也仅达到15.24%的准确率，表明这些模型在需要视觉证据的动态环境中表现不佳，突显了视频推理方法改进的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</div>
<div class="meta-line">Authors: Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi</div>
<div class="meta-line">First: 2025-12-28T18:24:19+00:00 · Latest: 2025-12-28T18:24:19+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 figures, 10 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23035v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xavierjiezou.github.io/Co2S/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过共同引导和共同融合实现稳定的半监督遥感分割</div>
<div class="mono" style="margin-top:8px">半监督遥感（RS）图像语义分割为减轻全面标注的负担提供了有前景的解决方案，但它在根本上面临伪标签漂移的问题，即确认偏差导致训练过程中错误的积累。在这项工作中，我们提出了Co2S，一个稳定的半监督RS分割框架，协同融合来自视觉-语言模型和自监督模型的先验信息。具体而言，我们构建了一个异构双学生架构，包括两个不同的基于ViT的视觉基础模型，分别初始化为预训练的CLIP和DINOv3，以减轻错误积累和伪标签漂移。为了有效地结合这些不同的先验，提出了一种显式-隐式语义共同引导机制，利用文本嵌入和可学习查询分别提供显式和隐式的类别级引导，从而共同增强语义一致性。此外，开发了一种全局-局部特征协同融合策略，有效地将CLIP捕获的全局上下文信息与DINOv3生成的局部细节融合，使模型能够生成高度精确的分割结果。在六个流行数据集上的广泛实验表明，所提方法的优越性，在各种划分协议和不同场景中始终实现领先的性能。项目页面可访问 https://xavierjiezou.github.io/Co2S/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of pseudo-label drift in semi-supervised remote sensing image semantic segmentation, which can lead to errors during training due to confirmation bias. The authors propose a framework called Co2S that combines insights from vision-language models and self-supervised models through a heterogeneous dual-student architecture using ViT-based models initialized with CLIP and DINOv3. Experimental results on six datasets indicate that Co2S effectively mitigates error accumulation and achieves superior segmentation performance across various scenarios and partition protocols.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决半监督遥感图像语义分割中的伪标签漂移问题，该问题可能由于确认偏差导致训练过程中的错误。作者提出了一种名为Co2S的框架，通过异构双学生架构整合来自视觉-语言模型和自监督模型的先验知识，使用基于ViT的模型初始化CLIP和DINOv3。对六个流行数据集的实验结果表明，Co2S有效减轻了错误积累，并在各种协议和场景中实现了优越的分割性能。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision</div>
<div class="meta-line">Authors: Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki</div>
<div class="meta-line">First: 2025-12-28T15:21:20+00:00 · Latest: 2025-12-28T15:21:20+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures. Preprint under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22969v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP-Joint-Detect：基于对比视觉-语言监督的端到端目标检测器联合训练</div>
<div class="mono" style="margin-top:8px">传统目标检测器依赖于交叉熵分类，这可能对类别不平衡和标签噪声敏感。我们提出了CLIP-Joint-Detect，这是一个简单且与检测器无关的框架，通过端到端联合训练集成了CLIP风格的对比视觉-语言监督。一个轻量级的并行头将区域或网格特征投影到CLIP嵌入空间，并通过InfoNCE对比损失和辅助交叉熵项将其与可学习的类别特定文本嵌入对齐，同时优化所有标准检测损失。该方法无缝适用于两阶段和单阶段架构。我们在使用Faster R-CNN的Pascal VOC 2007+2012和使用现代YOLO检测器（YOLOv11）的规模较大的MS COCO 2017基准上进行了验证，取得了一致且显著的改进，同时保持实时推理速度。大量实验和消融研究表明，使用可学习文本嵌入的联合优化显著提升了不同架构和数据集上的闭集检测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of conventional object detectors, which often struggle with class imbalance and label noise due to their reliance on cross-entropy classification. The authors propose CLIP-Joint-Detect, a framework that employs end-to-end joint training with CLIP-style contrastive vision-language supervision, utilizing a lightweight parallel head to align region features with text embeddings through InfoNCE contrastive loss alongside standard detection losses. Experimental results on Pascal VOC 2007+2012 with Faster R-CNN and MS COCO 2017 with YOLOv11 show significant improvements in detection performance while maintaining real-time inference speed, demonstrating the effectiveness of joint optimization with learnable text embeddings across various architectures and datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统目标检测器的局限性，这些检测器由于依赖交叉熵分类，常常在类别不平衡和标签噪声方面表现不佳。作者提出了CLIP-Joint-Detect框架，该框架通过端到端的联合训练，利用CLIP风格的对比视觉-语言监督，使用轻量级并行头将特征投影到CLIP嵌入空间，并与类别特定的文本嵌入对齐。对Pascal VOC 2007+2012（使用Faster R-CNN）和MS COCO 2017（使用YOLOv11）的实验结果表明，在保持实时推理速度的同时，检测性能显著提高，表明可学习文本嵌入的联合优化增强了不同架构和数据集上的闭集检测能力。</div>
</details>
</div>
<div class="card">
<div class="title">$\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain</div>
<div class="meta-line">Authors: Nate Rothschild, Moshe Kimhi, Avi Mendelson, Chaim Baskin</div>
<div class="meta-line">First: 2025-09-28T18:31:24+00:00 · Latest: 2025-12-28T06:34:58+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24022v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image reconstruction from corrupted images is crucial across many domains. Most reconstruction networks are trained on post-ISP sRGB images, even though the image-signal-processing pipeline irreversibly mixes colors, clips dynamic range, and blurs fine detail. This paper uses the rain degradation problem as a use case to show that these losses are avoidable, and demonstrates that learning directly on raw Bayer mosaics yields superior reconstructions. To substantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction pipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy scenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii) introduce Information Conservation Score (ICS), a color-invariant metric that aligns more closely with human opinion than PSNR or SSIM. On the test split, our raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2% ICS, while running faster with half of the GFLOPs. The results advocate an ISP-last paradigm for low-level vision and open the door to end-to-end learnable camera pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$\mathbf{R}^3$: 重建、原始和降雨：在拜耳域直接去雨</div>
<div class="mono" style="margin-top:8px">从受损图像中进行图像重建在许多领域至关重要。大多数重建网络是在后ISP的sRGB图像上训练的，尽管图像信号处理管道不可逆地混合了颜色、剪切了动态范围并模糊了细节。本文以降雨退化问题为案例，表明这些损失是可以避免的，并展示了直接在原始拜耳马赛克上学习可以获得更优的重建。为了证实这一主张，我们（i）评估后ISP和拜耳重建管道，（ii）策划Raw-Rain，这是第一个公开的真实雨天场景基准，捕获了12位拜耳和位深匹配的sRGB，（iii）引入信息保留分数（ICS），这是一种颜色不变的度量，与人类意见的对齐程度比PSNR或SSIM更高。在测试集上，我们的原始域模型使sRGB结果提高了最高+0.99 dB PSNR和+1.2% ICS，同时运行速度更快，GFLOPs减少了一半。结果支持低级视觉的ISP最后范式，并为端到端可学习的相机管道打开了大门。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of image reconstruction from corrupted images, emphasizing the limitations of traditional post-ISP sRGB training methods that compromise color fidelity and detail. The authors propose a novel approach that directly learns from raw Bayer mosaics to improve reconstruction quality, specifically targeting the rain degradation problem. Their findings, supported by the introduction of the Raw-Rain benchmark and the Information Conservation Score (ICS) metric, demonstrate that the raw-domain model outperforms sRGB results by up to +0.99 dB PSNR and +1.2% ICS, while also being more computationally efficient, thus promoting an ISP-last paradigm for low-level vision tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了从损坏图像中重建图像的挑战，特别关注雨水退化问题。作者提出了一种直接从原始Bayer马赛克学习的方法，而不是使用后ISP的sRGB图像，旨在避免与传统图像信号处理相关的损失。实验结果表明，原始域模型显著优于sRGB结果，在PSNR上提高了最高+0.99 dB，在新引入的信息保留分数上提高了+1.2%，同时在计算资源方面也更高效。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image</div>
<div class="meta-line">Authors: Po-Chih Wu</div>
<div class="meta-line">First: 2025-12-28T06:18:22+00:00 · Latest: 2025-12-28T06:18:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22801v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低质量图像中开放词汇目标检测性能评估</div>
<div class="mono" style="margin-top:8px">开放词汇目标检测使模型能够定位和识别超出预定义类别集的对象，并期望实现与人类表现相当的识别能力。在本研究中，我们旨在评估现有模型在低质量图像条件下的开放词汇目标检测任务的性能。为此，我们引入了一个新的数据集，模拟现实世界中的低质量图像。在我们的评估实验中，我们发现尽管开放词汇目标检测模型在低级图像退化下的mAP分数没有显著下降，但在高级图像退化下所有模型的性能都急剧下降。OWLv2模型在不同类型的退化中表现 consistently 更好，而OWL-ViT、GroundingDINO和Detic则表现出显著的性能下降。我们将发布我们的数据集和代码，以促进未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the performance of open-vocabulary object detection models in low-quality image conditions, aiming to understand their recognition capabilities compared to human performance. The researchers introduced a new dataset that simulates low-quality images to conduct their evaluation. The key findings reveal that while the models maintained stable mean Average Precision (mAP) scores under low-level image degradation, there was a significant performance drop under high-level degradation, with OWLv2 models consistently outperforming others like OWL-ViT, GroundingDINO, and Detic, which showed notable declines in performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估开放词汇物体检测模型在低质量图像条件下的性能，目标是实现与人类相似的识别能力。研究人员引入了一个新的数据集，以模拟低质量图像来评估现有模型。主要发现表明，尽管模型在低级图像降级下保持了平均精度（mAP）分数，但在高级降级下性能显著下降，其中OWLv2模型的表现优于其他模型，如OWL-ViT、GroundingDINO和Detic，这些模型的性能显著下降。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</div>
<div class="meta-line">Authors: Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo</div>
<div class="meta-line">First: 2025-12-23T13:58:12+00:00 · Latest: 2025-12-28T06:08:50+00:00</div>
<div class="meta-line">Comments: 12 pages, 9 figures, BMVC 2025 submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20374v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20374v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的区域感知特征融合用于结肠镜图像的自动BBPS评分</div>
<div class="mono" style="margin-top:8px">准确评估肠道清洁度对有效的结肠镜检查至关重要。波士顿肠道准备评分系统（BBPS）提供了一个标准化的评分系统，但在手动执行时存在主观性和观察者间变异性。本文为支持稳健的训练和评估，构建了一个高质量的结肠镜数据集，包含517个受试者的2240张图像，并附有专家一致同意的BBPS评分。我们提出了一种新颖的自动BBPS评分框架，利用CLIP模型与基于适配器的迁移学习以及专门的粪便特征提取分支。我们的方法将全局视觉特征与粪便相关的文本先验融合，以提高肠道清洁度评估的准确性，而无需明确的分割。在我们的数据集和公共NERTHU数据集上的广泛实验表明，我们的方法优于现有基线，突显了其在计算机辅助结肠镜分析中的临床应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of bowel cleanliness assessment during colonoscopy procedures, as the current Boston Bowel Preparation Scale (BBPS) is subject to subjectivity and inter-observer variability. The authors developed a novel automated BBPS scoring framework that utilizes the CLIP model with adapter-based transfer learning and a specialized fecal-feature extraction branch, integrating global visual features with stool-related textual information. Experimental results on a newly constructed dataset of 2,240 colonoscopy images and the public NERTHU dataset show that this approach outperforms existing methods, indicating its potential for effective clinical application in computer-aided colonoscopy analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高结肠镜检查过程中肠道清洁度评估的准确性，解决使用波士顿肠道准备评分标准（BBPS）进行手动评分时的主观性和变异性。作者开发了一种新颖的自动评分框架，利用CLIP模型与基于适配器的迁移学习以及专门的粪便特征提取分支，将全局视觉特征与与粪便相关的文本信息相结合。对新构建的2240张结肠镜图像数据集和公共NERTHU数据集的实验结果表明，该方法显著优于现有方法，表明其在计算机辅助结肠镜分析中的临床应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</div>
<div class="meta-line">Authors: Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen</div>
<div class="meta-line">First: 2025-11-13T04:28:03+00:00 · Latest: 2025-12-28T03:53:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09948v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as &quot;a good photo&quot; or &quot;a bad photo.&quot; However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越余弦相似度的幅度感知CLIP无参考图像质量评估</div>
<div class="mono" style="margin-top:8px">最近的研究将对比语言-图像预训练（CLIP）模型重新用于无参考图像质量评估（NR-IQA），通过测量图像嵌入与文本提示（如“好照片”或“坏照片”）之间的余弦相似度。然而，这种语义相似性忽视了一个关键但未被充分探索的线索：CLIP图像特征的幅度，我们实证发现其与感知质量之间存在强相关性。在本研究中，我们引入了一种新颖的自适应融合框架，结合了幅度感知质量线索与余弦相似度。具体而言，我们首先提取绝对CLIP图像特征，并应用Box-Cox变换对特征分布进行统计归一化，以减轻语义敏感性。得到的标量摘要作为语义归一化的辅助线索，补充了基于余弦的提示匹配。为了有效整合这两种线索，我们进一步设计了一种基于置信度的融合方案，根据每个项的相对强度自适应加权。对多个基准IQA数据集的广泛实验表明，我们的方法在没有任何特定任务训练的情况下，始终优于标准的基于CLIP的IQA和最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance No-Reference Image Quality Assessment (NR-IQA) by addressing the limitations of existing methods that rely solely on cosine similarity in the CLIP model. The authors propose a novel adaptive fusion framework that incorporates a magnitude-aware quality cue alongside cosine similarity, utilizing absolute CLIP image features and applying a Box-Cox transformation for normalization. Experimental results across various benchmark IQA datasets show that this approach significantly improves performance over standard CLIP-based methods and state-of-the-art baselines, achieving better accuracy without requiring task-specific training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有仅依赖CLIP模型余弦相似度的方法的局限性，来增强无参考图像质量评估（NR-IQA）。作者提出了一种新颖的自适应融合框架，将基于幅度的质量线索与余弦相似度结合在一起。该方法涉及提取绝对CLIP图像特征，应用Box-Cox变换进行归一化，并实施自信引导的融合方案以加权每个线索的贡献。在多个基准IQA数据集上的实验结果表明，该方法显著优于传统的基于CLIP的方法和最先进的基线，且无需特定任务的训练。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Visual Affordance from Audio</div>
<div class="meta-line">Authors: Lidong Lu, Guo Chen, Zhu Wei, Yicheng Liu, Tong Lu</div>
<div class="meta-line">First: 2025-12-01T18:58:56+00:00 · Latest: 2025-12-28T02:41:14+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02005v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.02005v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jscslld.github.io/AVAGFormer/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从音频学习视觉可供性</div>
<div class="mono" style="margin-top:8px">我们介绍了音频-视觉可供性定位（AV-AG），这是一个新任务，旨在从动作声音中分割物体交互区域。与现有依赖文本指令或演示视频的方法不同，这些方法常常受到模糊或遮挡的限制，音频提供了实时、语义丰富且视觉独立的线索，用于可供性定位，从而使交互区域的理解更加直观。为支持此任务，我们构建了第一个AV-AG数据集，包含大量动作声音、物体图像和像素级可供性注释。该数据集还包括一个未见子集，以评估零样本泛化。此外，我们提出了AVAGFormer，一个配备语义条件交叉模态混合器和双头解码器的模型，有效融合音频和视觉信号以进行掩码预测。实验表明，AVAGFormer在AV-AG上实现了最先进的性能，超越了相关任务的基线。全面分析突出了AV-AG与AVS之间的区别、端到端建模的好处以及每个组件的贡献。代码和数据集已发布在https://jscslld.github.io/AVAGFormer/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a more intuitive understanding of object interaction regions, as existing methods often suffer from ambiguity and occlusion when relying on textual instructions or demonstration videos. The authors introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments interaction regions based on action sounds, and support it with the creation of the first AV-AG dataset, which includes action sounds, object images, and pixel-level affordance annotations. They propose the AVAGFormer model, which utilizes a semantic-conditioned cross-modal mixer and a dual-head decoder to effectively integrate audio and visual signals for mask prediction, achieving state-of-the-art performance in AV-AG and demonstrating significant advantages over baseline methods in related tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于利用音频线索更直观地理解物体交互区域，因为现有方法常因文本指令的模糊性或视频中的遮挡而受到限制。作者提出了一项新任务，称为音频-视觉可供性定位（AV-AG），并构建了第一个AV-AG数据集，该数据集包括动作声音、物体图像和像素级可供性注释，以及一个用于零样本泛化评估的未见子集。他们提出了一种名为AVAGFormer的模型，利用语义条件的跨模态混合器和双头解码器有效整合音频和视觉信号进行掩膜预测，在AV-AG上实现了最先进的性能，超越了相关任务的基线，并通过分析展示了其方法的优势。</div>
</details>
</div>
<div class="card">
<div class="title">SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering</div>
<div class="meta-line">Authors: Jan Melechovsky, Ambuj Mehrish, Abhinaba Roy, Dorien Herremans</div>
<div class="meta-line">First: 2025-08-05T13:49:04+00:00 · Latest: 2025-12-27T19:25:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03448v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.03448v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster&#x27;s enhanced outputs over other baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SonicMaster：可控的一体化音乐修复与母带制作</div>
<div class="mono" style="margin-top:8px">音乐录音常常受到音质问题的影响，如过度混响、失真、削波、音调不平衡和立体声图像狭窄，尤其是在没有专业设备或专业知识的非专业环境中制作时。这些问题通常需要使用单独的专业工具和手动调整来修正。本文介绍了SonicMaster，这是第一个统一的生成模型，用于音乐修复和母带制作，能够通过基于文本的控制处理广泛的音频伪影。SonicMaster根据自然语言指令进行条件处理，以应用针对性的增强，或可以在自动模式下进行一般修复。为了训练该模型，我们构建了SonicMaster数据集，这是一个大型配对降质和高质量音轨的数据集，通过模拟常见降质类型，使用属于五个增强组的十九种降质函数：均衡、动态、混响、幅度和立体声。我们的方法利用流匹配生成训练范式，学习一种音频转换，将降质输入映射到其清洁、母带版本，并由文本提示引导。客观音频质量指标表明，SonicMaster在所有伪影类别中显著提高了音质。此外，主观听音测试确认听众更喜欢SonicMaster的增强输出，而不是其他基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address common audio quality issues in music recordings, particularly those produced in non-professional settings. The authors introduce SonicMaster, a unified generative model for music restoration and mastering that utilizes text-based control to enhance audio quality. They constructed the SonicMaster dataset, which includes paired degraded and high-quality tracks, and employed a flow-matching generative training paradigm to train the model. Experimental results show that SonicMaster significantly improves sound quality across various audio artifacts, and subjective listening tests indicate that listeners prefer its outputs over other methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决音乐录音中常见的音频质量问题，特别是在非专业环境中制作的录音。作者提出了SonicMaster，这是一个统一的生成模型，用于音乐修复和母带处理，利用基于文本的控制来提高音频质量。通过构建SonicMaster数据集，该数据集包括配对的降质和高质量音轨，并采用流匹配生成训练范式，该模型有效地将降质输入映射到改进的输出。实验结果表明，SonicMaster在各种音频伪影上显著提高了音质，客观指标和主观听觉测试均表明听众更喜欢其输出而非现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">View Selection for 3D Captioning via Diffusion Ranking</div>
<div class="meta-line">Authors: Tiange Luo, Justin Johnson, Honglak Lee</div>
<div class="meta-line">First: 2024-04-11T17:58:11+00:00 · Latest: 2025-12-27T07:50:55+00:00</div>
<div class="meta-line">Comments: Dataset link: https://huggingface.co/datasets/tiange/Cap3D</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.07984v3">Abs</a> · <a href="https://arxiv.org/pdf/2404.07984v3">PDF</a> · <a href="https://huggingface.co/datasets/tiange/Cap3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object&#x27;s characteristics. By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过扩散排序进行3D字幕的视图选择</div>
<div class="mono" style="margin-top:8px">可扩展的注释方法对于构建广泛的3D文本数据集至关重要，促进更广泛的应用。然而，现有方法有时会导致生成幻觉字幕，影响字幕质量。本文探讨了3D物体字幕中的幻觉问题，重点关注Cap3D方法，该方法使用预训练模型将3D物体渲染为2D视图进行字幕生成。我们指出一个主要挑战：某些渲染的3D物体视图不典型，偏离标准图像字幕模型的训练数据，导致幻觉。为了解决这个问题，我们提出了DiffuRank，这是一种利用预训练的文本到3D模型评估3D物体与其2D渲染视图之间对齐的方法，其中高对齐的视图能够准确代表物体的特征。通过对所有渲染视图进行排名，并将排名最高的视图输入到GPT4-Vision中，我们提高了字幕的准确性和细节，使Cap3D数据集中20万条字幕得以修正，并将其扩展到Objaverse和Objaverse-XL数据集的100万条字幕。此外，我们展示了DiffuRank的适应性，通过将其应用于预训练的文本到图像模型进行视觉问答任务，超越了CLIP模型的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the quality of captions generated for 3D objects, addressing the issue of hallucinated captions that arise from existing methods. The authors propose a novel approach called DiffuRank, which utilizes a pre-trained text-to-3D model to evaluate the alignment between 3D objects and their 2D rendered views, thereby identifying and ranking the most representative views. The key findings demonstrate that by applying DiffuRank, the accuracy and detail of captions can be significantly enhanced, resulting in the correction of 200,000 captions in the Cap3D dataset and expanding the dataset to 1 million captions across Objaverse and Objaverse-XL, while also showing improved performance in a Visual Question Answering task compared to the CLIP model.</div>
<div class="mono" style="margin-top:8px">本研究解决了3D物体描述中虚假描述的问题，特别是在将3D物体转换为2D视图进行标注的Cap3D方法中。作者提出了一种新方法DiffuRank，利用预训练的文本到3D模型评估3D物体与其2D表示之间的对齐程度，确保仅选择最具代表性的视图。实验结果表明，该方法显著提高了描述的准确性和细节，纠正了Cap3D数据集中20万个描述，并将数据集扩展到100万个描述，同时在视觉问答任务中表现优于CLIP模型。</div>
</details>
</div>
<div class="card">
<div class="title">SAM 3D for 3D Object Reconstruction from Remote Sensing Images</div>
<div class="meta-line">Authors: Junsheng Yao, Lichao Mou, Qingyu Li</div>
<div class="meta-line">First: 2025-12-27T03:47:39+00:00 · Latest: 2025-12-27T03:47:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22452v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于遥感图像的3D物体重建的SAM 3D</div>
<div class="mono" style="margin-top:8px">单目3D建筑重建对于可扩展的城市建模至关重要，但现有方法通常需要特定任务的架构和密集的监督。本文首次系统评估了SAM 3D，这是一种通用的图像到3D基础模型，用于单目遥感建筑重建。我们在纽约城市数据集的样本上将SAM 3D与TRELLIS进行基准测试，采用Frechet Inception Distance (FID)和基于CLIP的最大均值差异 (CMMD)作为评估指标。实验结果表明，与TRELLIS相比，SAM 3D生成了更连贯的屋顶几何形状和更清晰的边界。我们进一步通过分段-重建-组合管道扩展SAM 3D到城市场景重建，展示了其在城市场景建模中的潜力。我们还分析了实际限制并讨论了未来的研究方向。这些发现为在城市3D重建中部署基础模型提供了实用指导，并激励未来整合场景级结构先验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve monocular 3D building reconstruction from remote sensing images, which is crucial for scalable urban modeling but often relies on specialized architectures and extensive supervision. The authors systematically evaluate SAM 3D, a general-purpose image-to-3D foundation model, against TRELLIS using samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. The results indicate that SAM 3D achieves superior performance by producing more coherent roof geometries and sharper boundaries than TRELLIS, and it is further extended to urban scene reconstruction through a segment-reconstruct-compose pipeline, highlighting its potential for urban modeling and identifying areas for future research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善从遥感图像中进行单目3D建筑重建的能力，这对可扩展的城市建模至关重要，但现有方法通常依赖于特定任务的架构和大量监督。作者对SAM 3D这一通用图像到3D基础模型进行了系统评估，与TRELLIS在纽约城市数据集的样本上进行比较，采用Frechet Inception Distance (FID)和基于CLIP的最大均值差异（CMMD）作为评估指标。实验结果表明，SAM 3D在屋顶几何形状和边界清晰度上优于TRELLIS，并通过分段重建组合管道进一步扩展到城市场景重建，突显了其在城市建模中的潜力，并识别了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model</div>
<div class="meta-line">Authors: Yimin Zhu, Lincoln Linlin Xu</div>
<div class="meta-line">First: 2025-02-27T02:35:49+00:00 · Latest: 2025-12-26T23:15:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.19700v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.19700v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data augmentation effectively addresses the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC). While most methodologies extend features in the latent space, few leverage text-driven generation to create realistic and diverse samples. Recently, text-guided diffusion models have gained significant attention due to their ability to generate highly diverse and high-quality images based on text prompts in natural image synthesis. Motivated by this, this paper proposes Txt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis method to address the ISSD in HSIC. The proposed approach uses a denoising diffusion model, which iteratively removes Gaussian noise to generate hyperspectral samples conditioned on textual descriptions. First, to address the high-dimensionality of hyperspectral data, a universal variational autoencoder (VAE) is designed to map the data into a low-dimensional latent space, which provides stable features and reduces the inference complexity of diffusion model. Second, a semi-supervised diffusion model is designed to fully take advantage of unlabeled data. Random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are used to simulate the varying degrees of mixing. Third, the VAE decodes HSI from latent space generated by the diffusion model with the language conditions as input. In our experiments, we fully evaluate synthetic samples&#x27; effectiveness from statistical characteristics and data distribution in 2D-PCA space. Additionally, visual-linguistic cross-attention is visualized on the pixel level to prove that our proposed model can capture the spatial layout and geometry of the generated data. Experiments demonstrate that the performance of the proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models, state-of-the-art CNNs, and semi-supervised methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语言信息的超光谱图像合成用于不平衡小样本分类的半监督条件扩散模型</div>
<div class="mono" style="margin-top:8px">数据增强有效解决了超光谱图像分类中的不平衡小样本数据（ISSD）问题。虽然大多数方法在潜在空间中扩展特征，但很少利用文本驱动生成来创建真实且多样的样本。最近，文本引导的扩散模型因其能够基于文本提示生成高度多样和高质量图像而受到广泛关注。基于此，本文提出了Txt2HSI-LDM(VAE)，一种新颖的基于语言信息的超光谱图像合成方法，以解决HSIC中的ISSD问题。所提出的方法使用去噪扩散模型，迭代去除高斯噪声，以生成基于文本描述的超光谱样本。首先，为了解决超光谱数据的高维性，设计了一种通用变分自编码器（VAE），将数据映射到低维潜在空间，从而提供稳定的特征并降低扩散模型的推理复杂性。其次，设计了一种半监督扩散模型，以充分利用未标记数据。使用随机多边形空间裁剪（RPSC）和潜在特征的不确定性估计（LF-UE）来模拟不同程度的混合。第三，VAE从扩散模型生成的潜在空间解码HSI，以语言条件作为输入。在我们的实验中，我们从统计特征和2D-PCA空间中的数据分布全面评估合成样本的有效性。此外，视觉-语言交叉注意力在像素级别上可视化，以证明我们提出的模型能够捕捉生成数据的空间布局和几何形状。实验表明，所提出的Txt2HSI-LDM(VAE)的性能超越了经典骨干模型、最先进的CNN和半监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of imbalanced-small sample data (ISSD) in hyperspectral image classification (HSIC) by proposing a novel language-informed hyperspectral image synthesis method called Txt2HSI-LDM(VAE). This method employs a denoising diffusion model to generate hyperspectral samples based on textual descriptions, utilizing a universal variational autoencoder (VAE) to reduce the high-dimensionality of hyperspectral data. Experimental results indicate that the proposed approach outperforms classical backbone models, state-of-the-art CNNs, and other semi-supervised methods, demonstrating its effectiveness in generating realistic and diverse samples for improved classification performance.</div>
<div class="mono" style="margin-top:8px">本研究通过提出一种新的语言信息超光谱图像合成方法Txt2HSI-LDM(VAE)，解决了超光谱图像分类中的不平衡小样本数据（ISSD）问题。该方法采用去噪扩散模型，根据文本描述生成超光谱样本，并利用通用变分自编码器（VAE）降低超光谱数据的高维度，提高推理效率。实验结果表明，该方法的性能超过了传统模型、最先进的卷积神经网络（CNN）和半监督技术，证明了其在生成真实且多样化样本以提高分类性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning</div>
<div class="meta-line">Authors: Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang</div>
<div class="meta-line">First: 2025-12-26T11:43:21+00:00 · Latest: 2025-12-26T11:43:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22315v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoZoomer：用于长视频推理的强化学习时间聚焦</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）在视觉-语言任务中取得了显著进展，但由于上下文窗口的限制，在长视频理解方面仍然有限。因此，现有方法往往依赖于均匀帧采样或静态预选择，这可能会忽视关键证据，并且在推理过程中无法纠正其初始选择错误。为克服这些限制，我们提出了VideoZoomer，这是一种新颖的代理框架，使MLLMs能够在推理过程中动态控制其视觉焦点。从粗略的低帧率概述开始，VideoZoomer调用时间缩放工具，在自主选择的时刻获取高帧率剪辑，从而以多轮交互的方式逐步收集细粒度证据。因此，我们采用了两阶段训练策略：在经过筛选的示例和反思轨迹数据集上进行冷启动监督微调阶段，随后进行强化学习以进一步优化代理策略。大量实验表明，我们的7B模型提供了多样化和复杂的推理模式，在广泛的长视频理解和推理基准测试中表现出色。这些新兴能力使其在挑战性任务中始终超越现有的开源模型，甚至与专有系统相抗衡，同时在减少帧预算的情况下实现更高的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of Multimodal Large Language Models (MLLMs) in understanding long videos due to their restricted context window, which often leads to ineffective frame sampling and selection errors. The authors introduce VideoZoomer, an innovative framework that allows MLLMs to dynamically adjust their visual focus during reasoning by starting with a low-frame-rate overview and using a temporal zoom tool to select high-frame-rate clips at specific moments. Experimental results show that the 7B model outperforms existing open-source models and competes with proprietary systems in long video reasoning tasks, demonstrating diverse reasoning patterns and improved efficiency with reduced frame budgets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大型语言模型（MLLMs）在理解长视频时由于上下文窗口限制和无效帧选择方法而面临的局限性。作者提出了VideoZoomer，一个允许MLLMs在推理过程中动态调整视觉焦点的框架，通过从低帧率概览开始，使用时间缩放工具在选择的时刻选择高帧率剪辑。实验结果表明，该7B模型在长视频推理任务中显著超越现有的开源模型，并与专有系统竞争，展示了多样的推理模式和在减少帧预算下的提高效率。</div>
</details>
</div>
<div class="card">
<div class="title">Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</div>
<div class="meta-line">Authors: Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue</div>
<div class="meta-line">First: 2025-12-26T04:51:23+00:00 · Latest: 2025-12-26T04:51:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21860v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21860v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM&#x27;s last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无训练条件图像嵌入框架利用大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">条件图像嵌入是关注于特定图像方面的特征表示，这些方面由给定的文本条件（例如，颜色、类型）指示，这一直是一个具有挑战性的问题。尽管最近的视觉基础模型，如CLIP，提供了丰富的图像表示，但它们并未设计为关注特定条件。在本文中，我们提出了DIOR，一种利用大型视觉语言模型（LVLM）生成条件图像嵌入的方法。DIOR是一种无训练的方法，它提示LVLM用与给定条件相关的单个词描述图像。然后提取LVLM最后一个标记的隐藏状态向量作为条件图像嵌入。DIOR提供了一种通用解决方案，可以应用于任何图像和条件，而无需额外的训练或特定任务的先验知识。在条件图像相似性任务上的全面实验结果表明，DIOR的表现优于现有的无训练基线，包括CLIP。此外，DIOR在多个设置中相比需要额外训练的方法表现更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of generating conditional image embeddings that focus on specific aspects of an image based on textual conditions. The authors propose DIOR, a training-free method that utilizes a large vision-language model (LVLM) to create these embeddings by prompting the model to describe an image with a single word related to the given condition, and then extracting the hidden state vector of the LVLM&#x27;s last token. Experimental results indicate that DIOR significantly outperforms existing training-free baselines, including CLIP, and also shows superior performance compared to methods requiring additional training across various settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成条件图像嵌入的问题，该嵌入关注图像的特定方面，这些方面由文本条件指示。作者提出了DIOR，这是一种无训练的方法，利用大型视觉语言模型（LVLM）创建这些嵌入，通过提示LVLM用相关的单个词描述图像，从中提取最后一个标记的隐藏状态向量。实验结果表明，DIOR显著优于现有的无训练基线，包括CLIP，并且在多种设置下也优于需要额外训练的方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
