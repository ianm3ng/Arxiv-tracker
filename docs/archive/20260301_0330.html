<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-03-01 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260301_0330</div>
    <div class="row"><div class="card">
<div class="title">MediX-R1: Open Ended Medical Reinforcement Learning</div>
<div class="meta-line">Authors: Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed, Mohamed Zidan, Fahad Khan, Salman Khan, Rao Anwer, Hisham Cholakkal</div>
<div class="meta-line">First: 2026-02-26T18:59:46+00:00 · Latest: 2026-02-26T18:59:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MediX-R1：开放式医学强化学习</div>
<div class="mono" style="margin-top:8px">我们介绍了MediX-R1，这是一个开放式的医学多模态大语言模型（MLLM）的强化学习（RL）框架，能够提供超越多项选择格式的临床基础自由形式答案。MediX-R1通过基于组的RL和针对医学推理定制的复合奖励来微调基线视觉-语言骨干：基于LLM的准确性奖励通过严格的YES/NO决策判断语义正确性，基于医学嵌入的语义奖励捕捉同义词和术语变体，以及轻量级格式和模态奖励强制执行可解释推理和模态识别。这种多信号设计为开放式输出提供了稳定、信息丰富的反馈，而传统的可验证或仅限MCQ的奖励则无法满足需求。为了衡量进展，我们提出了一个统一的评估框架，适用于文本-only和图像+文本任务，使用基于参考的LLM作为评判者，取代脆弱的字符串重叠指标，捕捉语义正确性、推理和上下文对齐。尽管仅使用约51K的指令示例，MediX-R1在标准医学LLM（仅文本）和VLM（图像+文本）基准测试中取得了优异的结果，超越了强大的开源基线，并在开放式临床任务上取得了特别大的提升。我们的结果表明，具有全面奖励信号和基于LLM评估的开放式RL是实现多模态模型中可靠医学推理的可行路径。我们的训练模型、策划数据集和源代码可在https://medix.cvmbzuai.com获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop an open-ended Reinforcement Learning framework, MediX-R1, that enhances medical multimodal large language models by enabling clinically relevant, free-form responses rather than limiting them to multiple-choice formats. The method involves fine-tuning a vision-language backbone using Group Based RL and a composite reward system tailored for medical reasoning, which includes an LLM-based accuracy reward, a semantic reward based on medical embeddings, and additional rewards for format and modality recognition. The experimental results indicate that MediX-R1, despite being trained on approximately 51,000 instruction examples, achieves superior performance on standard medical benchmarks, particularly excelling in open-ended clinical tasks, thus demonstrating the effectiveness of comprehensive reward signals and LLM-based evaluation in improving medical reasoning capabilities in multimodal models.</div>
<div class="mono" style="margin-top:8px">MediX-R1的研究动机在于通过使医学多模态大语言模型（MLLM）能够提供临床基础的自由形式答案，而不仅限于多项选择格式，从而增强其能力。主要方法是使用基于组的强化学习（RL）和专为医学推理设计的复合奖励系统，对视觉-语言骨干进行微调，该系统包括针对准确性、语义正确性和模态识别的各种奖励。关键实验结果表明，尽管MediX-R1仅在约51,000个指令示例上进行训练，但在标准医学LLM和视觉-语言模型基准测试中表现优异，尤其在开放式临床任务中表现突出，从而证明了使用全面奖励信号的开放式RL在多模态背景下实现可靠医学推理的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Optimization for 4D Human-Scene Reconstruction in the Wild</div>
<div class="meta-line">Authors: Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou</div>
<div class="meta-line">First: 2025-01-04T01:53:51+00:00 · Latest: 2026-02-26T18:59:39+00:00</div>
<div class="meta-line">Comments: Project Page: https://vail-ucla.github.io/JOSH/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.02158v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.02158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vail-ucla.github.io/JOSH/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外4D人类-场景重建的联合优化</div>
<div class="mono" style="margin-top:8px">重建人类运动及其周围环境对于理解人类与场景的互动以及预测人类在场景中的运动至关重要。尽管在受限环境中捕捉人类-场景互动方面取得了很大进展，但这些先前的方法很难从网络视频中重建自然多样的人类运动和场景上下文。在本研究中，我们提出了JOSH，一种基于优化的新方法，用于从单目视频中进行野外4D人类-场景重建。JOSH利用密集场景重建和人类网格恢复的技术作为初始化，然后利用人类-场景接触约束联合优化场景、相机姿态和人类运动。实验结果表明，JOSH通过场景几何和人类运动的联合优化，在全球人类运动估计和密集场景重建方面取得了更好的结果。我们进一步设计了一个更高效的模型JOSH3R，并直接使用来自网络视频的伪标签进行训练。JOSH3R通过仅使用JOSH预测的标签进行训练，超越了其他无优化方法，进一步证明了其准确性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the reconstruction of human motion and surrounding environments to better understand human-scene interactions and predict movements in diverse contexts, particularly from web videos. The authors propose JOSH, an optimization-based method for 4D human-scene reconstruction that integrates dense scene reconstruction and human mesh recovery, utilizing human-scene contact constraints to jointly optimize scene geometry, camera poses, and human motion. Experimental results indicate that JOSH significantly enhances global human motion estimation and dense scene reconstruction compared to previous methods, and the subsequent model, JOSH3R, trained with pseudo-labels from web videos, demonstrates superior performance over optimization-free methods, showcasing its accuracy and generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强对人类与场景互动的理解，并改善对多样化环境中人类运动的预测，特别是来自网络视频的预测。作者提出了JOSH，这是一种基于优化的4D人类-场景重建方法，结合了密集场景重建和人类网格恢复，利用人类-场景接触约束共同优化场景几何、相机姿态和人类运动。实验结果表明，JOSH在全球人类运动估计和密集场景重建方面显著优于以往方法，而后续模型JOSH3R通过网络视频的伪标签进行训练，表现出比无优化方法更优越的性能，突显了其准确性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Model Agreement via Anchoring</div>
<div class="meta-line">Authors: Eric Eaton, Surbhi Goel, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell</div>
<div class="meta-line">First: 2026-02-26T18:59:32+00:00 · Latest: 2026-02-26T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23360v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23360v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.
  We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过锚定实现模型一致性</div>
<div class="mono" style="margin-top:8px">许多研究旨在控制模型不一致性——即两个机器学习模型在预测中不一致的程度。我们采用一种简单且标准的模型不一致性概念，适用于实值预测问题，即在独立样本上训练的两个模型之间预测的期望平方差，而不协调训练过程。我们希望能够通过一些自然的训练过程参数将不一致性驱动到零，并使用可以应用于现有训练方法的分析。我们开发了一种简单的通用技术，通过锚定两个模型的平均值来证明独立模型不一致性的界限。然后，我们将该技术应用于证明四种常用机器学习算法的不一致性界限：（1）在任意模型类上的堆叠聚合（其中不一致性随着堆叠模型数量k的增加而驱动到0）；（2）梯度提升（其中不一致性随着迭代次数k的增加而驱动到0）；（3）具有架构搜索的神经网络训练（其中不一致性随着优化的架构大小n的增加而驱动到0）；（4）在所有固定深度的回归树上进行回归树训练（其中不一致性随着树架构深度d的增加而驱动到0）。为清晰起见，我们在一维回归和平方误差损失的背景下推导初始界限，但随后表明我们的所有结果可以推广到具有任何强凸损失的多维回归。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to control model disagreement, which refers to the extent of differing predictions between two machine learning models trained independently. The authors propose a method based on anchoring to the average of two models to establish bounds on model disagreement. The key experimental findings demonstrate that this technique effectively reduces disagreement to zero across various algorithms, including stacked aggregation, gradient boosting, neural network training with architecture search, and regression tree training, with specific parameters such as the number of models, iterations, architecture size, and tree depth leading to convergence in one-dimensional and multi-dimensional regression settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于控制模型不一致性，即两个独立训练的机器学习模型在预测上的不一致程度。作者提出了一种基于锚定平均模型的技术，以推导独立模型不一致性的界限。他们证明该方法有效地将不一致性降低到零，适用于四种机器学习算法：堆叠聚合、梯度提升、具有架构搜索的神经网络训练和回归树训练，分别通过增加模型数量、迭代次数、架构大小或树深度来实现不一致性的减少，并确认结果可以推广到具有任意强凸损失的多维回归。</div>
</details>
</div>
<div class="card">
<div class="title">A Dataset is Worth 1 MB</div>
<div class="meta-line">Authors: Elad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen</div>
<div class="meta-line">First: 2026-02-26T18:59:03+00:00 · Latest: 2026-02-26T18:59:03+00:00</div>
<div class="meta-line">Comments: 23 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23358v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据集的价值为1 MB</div>
<div class="mono" style="margin-top:8px">数据集服务器通常必须将相同的大负载分发给多个客户端，导致巨大的通信成本。由于客户端经常在不同的硬件和软件框架上操作，传输预训练模型通常不可行；相反，代理需要原始数据在本地训练自己的任务特定模型。虽然数据集蒸馏试图压缩训练信号，但当前方法在高分辨率数据上难以扩展，且很少能实现足够小的文件。在本文中，我们提出了伪标签作为数据（PLADA），一种完全消除像素传输的方法。我们假设代理预加载了一个大型、通用、未标记的参考数据集（例如，ImageNet-1K，ImageNet-21K），并通过仅传输特定图像的类别标签来传达新任务。为了解决参考数据集和目标数据集之间的分布不匹配问题，我们引入了一种修剪机制，过滤参考数据集，仅保留与目标任务最语义相关的图像标签。这个选择过程同时最大化训练效率并最小化传输负载。在10个不同的数据集上的实验表明，我们的方法可以以低于1 MB的负载传递任务知识，同时保持高分类准确率，为高效的数据集服务提供了有希望的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to reduce the massive communication costs associated with distributing large datasets to clients with diverse hardware and software frameworks. The authors propose a novel method called Pseudo-Labels as Data (PLADA), which eliminates the need for pixel transmission by allowing agents to use a preloaded generic unlabeled dataset and only transmit class labels for relevant images. Experimental results across 10 diverse datasets show that PLADA can effectively transfer task knowledge with a communication payload of less than 1 MB while maintaining high classification accuracy, demonstrating its potential for efficient dataset serving.</div>
<div class="mono" style="margin-top:8px">本研究的动机是减少向具有不同硬件和软件框架的客户端分发大型数据集所产生的巨大通信成本。作者提出了一种新方法，称为伪标签作为数据（PLADA），通过允许代理利用预加载的通用未标记参考数据集，仅传输特定图像的类别标签，从而消除了像素传输的需要。在10个不同数据集上的实验结果表明，PLADA能够有效地以低于1 MB的传输负载转移任务知识，同时保持高分类准确率，显示出其在高效数据集服务中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training</div>
<div class="meta-line">Authors: Aheli Saha, René Schuster, Didier Stricker</div>
<div class="meta-line">First: 2026-02-26T18:57:52+00:00 · Latest: 2026-02-26T18:57:52+00:00</div>
<div class="meta-line">Comments: 12 pages, International Conference on Pattern Recognition Applications and Methods</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23357v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于联合分布训练的事件驱动目标检测中的自适应感知传感器泛化</div>
<div class="mono" style="margin-top:8px">生物启发的事件相机因其异步和低延迟的特性，最近吸引了大量研究。这些特性提供了高动态范围并显著减少运动模糊。然而，由于其输出信号的性质新颖，现有数据的变异性存在差距，且对表征其信号的参数缺乏广泛分析。本文通过深入理解内在参数如何影响基于事件数据训练的模型性能，特别是在目标检测方面，来解决这些问题。我们还利用我们的发现扩展下游模型的传感器无关鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by the limited variability of data and the lack of comprehensive analysis of intrinsic parameters in bio-inspired event cameras, which are known for their asynchronous and low-latency capabilities. The authors employ a method that involves joint distribution training to enhance the understanding of how these intrinsic parameters influence the performance of models trained on event data, particularly in the context of object detection. Key experimental findings indicate that the insights gained from this analysis can be leveraged to improve the robustness of downstream models, making them more sensor-agnostic.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于对生物启发式事件相机的日益关注，这些相机提供异步和低延迟的性能，但面临数据变异性有限和输出信号分析不足的问题。作者采用联合分布训练方法，探讨内在参数如何影响基于事件数据训练的模型性能，特别是在物体检测的背景下。关键实验结果表明，理解这些参数可以增强下游模型的鲁棒性，使其在不同场景中更具传感器无关性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</div>
<div class="meta-line">Authors: Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata</div>
<div class="meta-line">First: 2026-02-26T18:55:06+00:00 · Latest: 2026-02-26T18:55:06+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23353v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SOTAlign：通过最优传输实现单模态视觉和语言模型的半监督对齐</div>
<div class="mono" style="margin-top:8px">柏拉图表示假设认为，在不同模态上训练的神经网络会收敛到一个共享的世界统计模型。最近的研究通过使用轻量级对齐层对冻结的预训练视觉和语言模型进行对齐，利用了这种收敛性，但通常依赖于对比损失和数百万对样本。在本研究中，我们探讨是否可以在显著减少监督的情况下实现有意义的对齐。我们引入了一种半监督设置，其中使用少量图像-文本对对预训练的单模态编码器进行对齐，同时结合大量未配对数据。为了解决这一挑战，我们提出了SOTAlign，一个两阶段框架，首先使用线性教师从有限的配对数据中恢复粗略的共享几何，然后通过基于最优传输的散度在未配对样本上精细化对齐，该方法在不对目标空间过度约束的情况下转移关系结构。与现有的半监督方法不同，SOTAlign有效利用未配对的图像和文本，在数据集和编码器对之间学习稳健的联合嵌入，并显著超越监督和半监督基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to achieve effective alignment between unimodal vision and language models with minimal supervision, challenging the reliance on large paired datasets. The authors introduce SOTAlign, a semi-supervised framework that aligns pretrained unimodal encoders using a limited number of image-text pairs and a vast amount of unpaired data. The key experimental findings demonstrate that SOTAlign successfully recovers a shared geometry from the paired data and refines the alignment using optimal transport, resulting in robust joint embeddings that significantly outperform existing supervised and semi-supervised methods.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要在最小监督下有效对齐单模态视觉和语言模型，挑战了对大量配对样本的依赖。作者提出了SOTAlign，这是一种两阶段的半监督框架，首先从有限的图像-文本对中建立粗略的共享几何，然后通过基于最优传输的散度利用未配对数据来细化这种对齐。实验结果表明，SOTAlign通过有效利用未配对的图像和文本，在不同数据集和编码器对之间学习稳健的联合嵌入，显著优于现有的监督和半监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">Scale Can&#x27;t Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</div>
<div class="meta-line">Authors: Amita Kamath, Jack Hessel, Khyathi Chandu, Jena D. Hwang, Kai-Wei Chang, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-26T18:54:06+00:00 · Latest: 2026-02-26T18:54:06+00:00</div>
<div class="meta-line">Comments: TACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., &quot;at the game today!&quot; is a more likely caption than &quot;a photo of 37 people standing behind a field&quot;. We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规模无法克服语用学：报告偏见对视觉-语言推理的影响</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）缺乏推理能力一直是研究讨论的前沿。我们认为这种行为源于其训练数据中的报告偏见。即，人们在交流视觉内容时默认省略了监督某些类型推理所需的隐性信息；例如，“今天在比赛上！”比“37个人站在一个场地后面的一张照片”更可能成为标题。我们通过语用学理论的视角研究了流行的VLMs OpenCLIP、LLaVA-1.5和Molmo的基础数据，发现报告偏见导致四种推理技能（空间、时间、否定和计数）的表现不足，尽管语料库是网络规模和/或合成生成的。通过一组精心策划的基准，我们证明了：（i）VLMs在训练数据中因报告偏见被抑制的上述推理类型上表现不佳；（ii）与普遍看法相反，扩大数据规模、模型规模和多语言并不会默认导致这些技能的出现；但，（iii）有效地纳入专门收集的注释以获取隐性信息是有效的。我们的发现强调了需要更有意图的训练数据策划方法，而不是依赖规模来实现推理能力的出现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the persistent lack of reasoning capabilities in Vision-Language Models (VLMs), attributing this issue to a reporting bias in training data that omits critical tacit information necessary for certain reasoning tasks. The authors analyze the data used in popular VLMs like OpenCLIP, LLaVA-1.5, and Molmo through pragmatic theories, revealing that this bias leads to inadequate representation of reasoning skills such as spatial, temporal, negation, and counting. Their experiments show that VLMs struggle with these reasoning types due to the bias, and scaling the size of data or models does not inherently improve these skills; however, they find that adding targeted annotations to capture tacit information significantly enhances performance, underscoring the importance of deliberate data curation in training.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLMs）推理能力不足的问题，认为这是由于训练数据中的报告偏差导致的，该偏差省略了某些推理任务所需的重要隐性信息。作者通过语用学的视角分析了流行的VLM数据，如OpenCLIP、LLaVA-1.5和Molmo，揭示了这种偏差导致空间、时间、否定和计数等推理技能的缺乏表现。实验结果表明，VLM在这些推理任务上表现不佳，且仅仅扩大数据或模型规模并不能改善性能；然而，他们发现，纳入专门收集的注释以捕捉隐性信息显著提升了推理能力，强调了在训练VLM时有意的数据策划的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">FlashOptim: Optimizers for Memory Efficient Training</div>
<div class="meta-line">Authors: Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock</div>
<div class="meta-line">First: 2026-02-26T18:52:22+00:00 · Latest: 2026-02-26T18:52:22+00:00</div>
<div class="meta-line">Comments: Source code is available at https://github.com/databricks/flashoptim</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23349v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23349v1">PDF</a> · <a href="https://github.com/databricks/flashoptim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashOptim：内存高效训练的优化器</div>
<div class="mono" style="margin-top:8px">标准的神经网络混合精度训练需要大量加速器内存来存储每个模型参数。这些字节不仅反映参数本身，还包括其梯度和一个或多个优化器状态变量。每个值通常需要4个字节，因此训练一个70亿参数的模型对于内存少于100GB的研究人员来说可能不切实际。
  我们介绍了FlashOptim，这是一套优化方案，通过减少每个参数的内存超过50%，同时保持模型质量和API兼容性。我们的方法引入了两个关键技术。首先，我们通过找到并利用其量化误差的紧界限来改进主权重分割。其次，我们设计了压缩函数，大大减少了8位优化器状态量化中的误差。结合16位梯度，这些技术将AdamW的内存从每个参数16字节减少到7字节，或在释放梯度时减少到5字节。它们还将模型检查点大小减少了一半以上。
  在将FlashOptim应用于SGD、AdamW和Lion的实验中，未在任何任务上观察到质量下降，任务包括一系列标准的视觉和语言基准测试，包括Llama-3.1-8B微调。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the high memory requirements for standard mixed-precision training of neural networks, which can be prohibitive for models with billions of parameters. The authors introduce FlashOptim, a suite of optimizations that significantly reduces memory usage per parameter by over 50% while maintaining model quality and API compatibility. Key experimental findings demonstrate that FlashOptim, when applied to optimizers like SGD, AdamW, and Lion, reduces memory usage from 16 bytes to 7 bytes per parameter, or even 5 bytes with gradient release, without any measurable degradation in performance across various vision and language benchmarks, including Llama-3.1-8B finetuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决标准混合精度训练大型神经网络所需的高内存要求，这对内存有限的研究人员来说可能是一个障碍。作者提出了FlashOptim，这是一套优化方案，能够在保持模型质量和API兼容性的同时，将每个参数的内存使用减少超过50%。实验结果表明，FlashOptim有效地将AdamW优化器的内存占用从每个参数16字节减少到7字节，甚至在释放梯度时减少到5字节，并且在各种视觉和语言基准测试中，包括Llama-3.1-8B微调，没有任何可测量的质量下降。</div>
</details>
</div>
<div class="card">
<div class="title">Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</div>
<div class="meta-line">Authors: Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis, Felix Zhou, Ziyu Zhu</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-26T18:47:06+00:00 · Latest: 2026-02-26T18:47:06+00:00</div>
<div class="meta-line">Comments: Abstract truncated to arXiv limits. To appear in ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23341v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23341v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low&#x27;&#x27; information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>粗数据的均值估计：特征与高效算法</div>
<div class="mono" style="margin-top:8px">粗数据是在学习者仅观察到样本的部分信息时产生的；即包含样本的集合而不是其确切值。这种情况自然发生在测量取整、传感器限制和经济系统滞后中。我们研究从粗数据中进行的高斯均值估计，其中每个真实样本 $x$ 是从具有单位协方差的 $d$ 维高斯分布中抽取的，但仅通过包含 $x$ 的划分集合被揭示。当粗样本大致上具有“低”信息时，均值无法从观察到的样本中唯一恢复（即，该问题不可识别）。Fotakis、Kalavasis、Kontonis 和 Tzamos [FKKT21] 的最新工作表明，当未知均值可识别且划分仅由凸集合组成时，样本高效的均值估计是可能的。此外，他们还表明，在没有凸性的情况下，均值估计变得 NP-hard。然而，两个基本问题仍然悬而未决：（1）在凸划分下，均值何时可识别？（2）在可识别性和凸划分下，是否可能进行计算上高效的估计？本工作解决了这两个问题。 [...]</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges in Gaussian mean estimation from coarse data, where only partial information about samples is available due to measurement limitations and other factors. The authors investigate the conditions under which the mean is identifiable when using convex partitions and develop efficient algorithms for estimation. The key findings indicate that mean estimation is indeed possible under identifiable conditions with convex partitions, resolving previously open questions in the field and demonstrating the feasibility of computationally efficient estimation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从粗糙数据中进行高斯均值估计的挑战，这种情况下由于测量限制等因素，只能获得样本的部分信息。作者研究了在数据被划分为凸集时均值可识别的条件，并提出了高效的估计算法。主要发现表明，在这些条件下均值估计是可行的，解决了之前关于可识别性和计算效率的未解问题。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?</div>
<div class="meta-line">Authors: Tilemachos Aravanis, Vladan Stojnić, Bill Psomas, Nikos Komodakis, Giorgos Tolias</div>
<div class="meta-line">First: 2026-02-26T18:45:33+00:00 · Latest: 2026-02-26T18:45:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23339v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检索与分割：少量示例足以弥补开放词汇分割中的监督差距吗？</div>
<div class="mono" style="margin-top:8px">开放词汇分割（OVS）将视觉-语言模型（VLMs）的零样本识别能力扩展到像素级预测，使得能够对文本提示指定的任意类别进行分割。尽管最近取得了进展，OVS 仍然落后于完全监督的方法，主要面临两个挑战：用于训练 VLMs 的粗略图像级监督和自然语言的语义模糊性。我们通过引入一种少样本设置来解决这些限制，该设置通过像素标注图像的支持集增强文本提示。在此基础上，我们提出了一种检索增强的测试时适配器，通过融合文本和视觉支持特征来学习轻量级的逐图像分类器。与依赖于后期手工融合的先前方法不同，我们的方法执行学习的逐查询融合，实现了模态之间更强的协同。该方法支持不断扩展的支持集，并适用于个性化分割等细粒度任务。实验表明，我们显著缩小了零样本和监督分割之间的差距，同时保持了开放词汇能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance open-vocabulary segmentation (OVS) by addressing the limitations of coarse image-level supervision and semantic ambiguity in natural language, which hinder its performance compared to fully supervised methods. The authors introduce a few-shot setting that incorporates a support set of pixel-annotated images alongside textual prompts, and they propose a retrieval-augmented test-time adapter that creates a lightweight, per-image classifier by integrating textual and visual support features through learned, per-query fusion. Experimental results demonstrate that this approach significantly reduces the performance gap between zero-shot and supervised segmentation while maintaining the open-vocabulary capability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善开放词汇分割（OVS），因为目前由于训练数据和语义模糊的限制，其表现不及完全监督的方法。作者引入了一种少样本设置，通过像素注释图像的支持集增强文本提示，并提出了一种检索增强的测试时适配器，通过整合文本和视觉特征创建轻量级的每图像分类器。实验结果表明，该方法显著缩小了零样本和监督分割之间的性能差距，同时保持了开放词汇能力。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiable Zero-One Loss via Hypersimplex Projections</div>
<div class="meta-line">Authors: Camilo Gomez, Pengyang Wang, Liansheng Tang</div>
<div class="meta-line">First: 2026-02-26T18:41:31+00:00 · Latest: 2026-02-26T18:41:31+00:00</div>
<div class="meta-line">Comments: To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23336v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23336v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过超单纯形投影的可微零一损失</div>
<div class="mono" style="margin-top:8px">最近的机器学习进展强调将结构化优化组件集成到端到端可微模型中，从而实现更丰富的归纳偏置和与任务特定目标的更紧密对齐。在这项工作中，我们引入了一种新颖的可微近似零一损失的方法——零一损失长期以来被认为是分类性能的金标准，但由于其不可微性与基于梯度的优化不兼容。我们的方法通过约束优化框架构建了一个平滑的、保持顺序的投影到n,k维超单纯形上，形成了一个我们称之为Soft-Binary-Argmax的新算子。在推导其数学性质后，我们展示了如何高效计算其雅可比矩阵并将其集成到二分类和多分类学习系统中。实证结果表明，我们的方法在大批量训练下通过对输出logits施加几何一致性约束，实现了显著的泛化改进，从而缩小了传统大批量训练中观察到的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the integration of structured optimization in machine learning models, particularly addressing the limitations of the zero-one loss in gradient-based optimization. The authors propose a differentiable approximation to the zero-one loss by constructing a smooth projection onto the n,k-dimensional hypersimplex using a constrained optimization framework, introducing a new operator called Soft-Binary-Argmax. Experimental results demonstrate that this method significantly improves generalization in large-batch training by enforcing geometric consistency on output logits, effectively reducing the performance gap typically seen in such training scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将结构化优化组件集成到可微模型中来提高机器学习中的分类性能，解决了不可微的零一损失的局限性。作者提出了一种通过平滑投影到n,k维超简单形的可微近似零一损失的方法，利用约束优化框架开发了一个称为Soft-Binary-Argmax的算子。实验结果表明，该方法通过对输出logits施加几何一致性约束，显著改善了大批量训练中的泛化能力，有效缩小了在此类训练场景中通常观察到的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</div>
<div class="meta-line">Authors: Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh, Harshit Surana, Aryeh Tiktinsky, Shriya Atmakuri, Jonathan Bragg, Mike D&#x27;Arcy, Sergey Feldman, Amal Hassan-Ali, Rubén Lozano, Bodhisattwa Prasad Majumder, Charles McGrady, Amanpreet Singh, Brooke Vlahos, Yoav Goldberg, Doug Downey</div>
<div class="meta-line">First: 2026-02-26T18:40:28+00:00 · Latest: 2026-02-26T18:40:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23335v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解AI驱动的科学研究工具中的使用和参与：Asta交互数据集</div>
<div class="mono" style="margin-top:8px">AI驱动的科学研究工具正在迅速融入研究工作流程，但该领域缺乏对研究人员在现实环境中如何使用这些系统的清晰视角。我们展示并分析了Asta交互数据集，这是一个大规模资源，包含来自两个已部署工具（文献发现界面和科学问答界面）的超过200,000个用户查询和交互日志，这些工具位于一个基于LLM的检索增强生成平台中。利用该数据集，我们描述了查询模式、参与行为以及使用如何随着经验而演变。我们发现用户提交的查询比传统搜索更长、更复杂，并将系统视为协作研究伙伴，委派任务如起草内容和识别研究空白。用户将生成的响应视为持久的文献，非线性地重新访问和浏览输出及引用证据。随着经验的积累，用户发出更有针对性的查询，并更深入地参与支持引用，尽管即使在经验丰富的用户中，关键词式查询仍然存在。我们发布了匿名数据集和分析，并提供了新的查询意图分类法，以指导未来现实世界AI研究助手的设计，并支持现实评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand how researchers interact with AI-powered scientific research tools in real-world settings, as there is a lack of clarity in this area. The authors present the Asta Interaction Dataset, which includes over 200,000 user queries and interaction logs from two deployed tools, and analyze it to characterize query patterns and engagement behaviors. The key findings reveal that users tend to submit longer and more complex queries compared to traditional search methods, treat the system as a collaborative partner, and engage with generated responses in a non-linear manner, with more experienced users issuing targeted queries and interacting more deeply with citations, although simpler keyword queries remain common among all users.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解研究人员在实际环境中如何与人工智能驱动的科学研究工具互动，因为该领域缺乏全面的见解。作者提出了Asta交互数据集，其中包含来自两个AI工具的超过200,000个用户查询和交互日志，并分析这些数据以表征用户行为和查询模式。研究结果表明，用户提交的查询比传统搜索方法更长、更复杂，视人工智能为协作伙伴，并以非线性方式与生成的响应互动，经验丰富的用户发出更有针对性的查询，但仍然使用关键词式查询。</div>
</details>
</div>
<div class="card">
<div class="title">Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators</div>
<div class="meta-line">Authors: Yuhao Liu, Salim Ullah, Akash Kumar</div>
<div class="meta-line">First: 2026-02-26T18:40:02+00:00 · Latest: 2026-02-26T18:40:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23334v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23334v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption and accuracy. Because regular designs for multiplication on hardware cannot support the precision reconfiguration for a multi-precision Quantized Neural Network (QNN) model in runtime, we propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array design for QNN accelerators. We have implemented and evaluated our work on the Ultra96 FPGA platform. Results show that our work can achieve 1.3185 to 3.5671 times speedup in inferring mixed-precision models and has less critical path delay, supporting a higher clock frequency (250MHz).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于硬件加速器的运行时可重构多精度量化乘法的位级脉动阵列架构</div>
<div class="mono" style="margin-top:8px">神经网络加速器已广泛应用于边缘设备，以执行物体跟踪、图像识别等复杂任务。之前的研究探讨了相关轻量级加速器设计中的量化技术，以减少硬件资源消耗。然而，低精度会导致推理中的高准确性损失。因此，混合精度量化通过在不同层应用不同精度，成为权衡资源消耗和准确性的替代解决方案。由于硬件上的常规乘法设计无法支持多精度量化神经网络（QNN）模型在运行时的精度重构，我们提出了一种用于QNN加速器的运行时可重构多精度多通道位级脉动阵列设计。我们在Ultra96 FPGA平台上实现并评估了我们的工作。结果表明，我们的工作在推理混合精度模型时可以实现1.3185到3.5671倍的加速，并且具有更少的关键路径延迟，支持更高的时钟频率（250MHz）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of accuracy loss in neural network inference due to low precision in quantized models while minimizing hardware resource consumption. The authors propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array architecture specifically designed for quantized neural networks (QNNs). Experimental results on the Ultra96 FPGA platform demonstrate that this architecture achieves a speedup of 1.3185 to 3.5671 times in inferring mixed-precision models, along with reduced critical path delay, enabling operation at a higher clock frequency of 250MHz.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于低精度量化导致神经网络加速器准确性损失的问题，低精度量化通常用于减少硬件资源消耗。作者提出了一种专为量化神经网络（QNN）设计的运行时可重配置的多精度多通道位级脉冲阵列架构。对Ultra96 FPGA平台的实验结果表明，该架构在推理混合精度模型时实现了1.3185到3.5671倍的加速，同时减少了关键路径延迟，并支持250MHz的更高时钟频率。</div>
</details>
</div>
<div class="card">
<div class="title">Utilizing LLMs for Industrial Process Automation</div>
<div class="meta-line">Authors: Salim Fares</div>
<div class="meta-line">First: 2026-02-26T18:38:00+00:00 · Latest: 2026-02-26T18:38:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23331v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大型语言模型进行工业过程自动化</div>
<div class="mono" style="margin-top:8px">近年来，越来越多的出版物讨论了使用大型语言模型（LLMs）进行软件工程的最佳实践。然而，这些工作大多集中在广泛使用的通用编程语言（如Python）上，因为它们的训练数据广泛使用。LLMs在工业过程自动化领域的应用仍然未被充分探索，该领域使用的高度专业化语言通常仅在专有环境中使用。本研究旨在在工业开发过程中利用和集成LLMs，解决实际编程任务（例如，为机器人手臂生成运动例程）并加速制造系统的开发周期。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to explore the application of Large Language Models (LLMs) in the industrial process automation domain, which has been largely overlooked in favor of more common programming languages like Python. The study employs LLMs to address real-life programming tasks specific to industrial automation, such as generating movement routines for robotic arms. Key findings indicate that integrating LLMs can significantly accelerate the development cycles of manufacturing systems by effectively handling specialized programming tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索大型语言模型（LLMs）在工业过程自动化领域的应用，该领域在很大程度上被忽视，而更常见的编程语言如Python则受到更多关注。该研究利用LLMs解决与工业自动化相关的特定编程任务，例如生成机器人手臂的运动例程。主要实验结果表明，LLMs能够有效地帮助解决实际编程挑战，从而加快制造系统的开发周期。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</div>
<div class="meta-line">Authors: Kunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts, Stefan Zohren</div>
<div class="meta-line">First: 2026-02-26T18:37:36+00:00 · Latest: 2026-02-26T18:37:36+00:00</div>
<div class="meta-line">Comments: 14 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23330v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向专家投资团队：一个具有细粒度交易任务的多智能体大语言模型系统</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的进步加速了自主金融交易系统的发展。虽然主流方法部署模仿分析师和经理角色的多智能体系统，但它们往往依赖于忽视现实工作流程复杂性的抽象指令，这可能导致推理性能下降和决策透明度降低。因此，我们提出了一种多智能体LLM交易框架，明确将投资分析分解为细粒度任务，而不是提供粗粒度指令。我们在控制泄漏的回测环境下，使用日本股票数据（包括价格、财务报表、新闻和宏观信息）评估所提框架。实验结果表明，与传统粗粒度设计相比，细粒度任务分解显著提高了风险调整后的收益。关键是，对中间智能体输出的进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素。此外，我们进行标准投资组合优化，利用与股票指数的低相关性和每个系统输出的方差。这种方法实现了优越的性能。这些发现有助于在实际环境中将LLM智能体应用于交易系统时的智能体结构和任务配置设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance and transparency of autonomous financial trading systems, which often suffer from reliance on abstract instructions. The authors propose a multi-agent LLM trading framework that breaks down investment analysis into fine-grained tasks, contrasting with traditional coarse-grained approaches. Experimental results using Japanese stock data demonstrate that this fine-grained task decomposition leads to significantly improved risk-adjusted returns, and further analysis indicates that aligning analytical outputs with decision preferences is crucial for system performance. Additionally, the study incorporates standard portfolio optimization, resulting in superior performance due to low correlation with the stock index and variance in outputs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有金融交易中的多智能体系统的局限性，这些系统往往依赖于抽象指令，无法捕捉现实世界的复杂性。作者提出了一种多智能体LLM交易框架，将投资分析细分为精细任务，从而增强决策过程。使用日本股票数据的实验结果表明，这种精细化的方法相比传统方法显著提高了风险调整后的收益，进一步分析表明，分析输出与决策偏好的对齐对系统性能至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</div>
<div class="meta-line">Authors: Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros, Nathaniel Li, Aiden Kim, Yury Orlovskiy, Coleman Breen, Bryce Cai, Jasper Götting, Andrew Bo Liu, Samira Nedungadi, Paula Rodriguez, Yannis Yiming He, Mohamed Shaaban, Zifan Wang, Seth Donoughe, Julian Michael</div>
<div class="meta-line">First: 2026-02-26T18:37:23+00:00 · Latest: 2026-02-26T18:37:23+00:00</div>
<div class="meta-line">Comments: 59 pages, 33 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23329v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重用途与计算生物学任务中的LLM新手提升</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生物学基准测试中的表现越来越好，但尚不清楚它们是否能提升新手用户的能力，即使人类的表现优于仅依赖互联网资源。这种不确定性对于理解科学加速和双重用途风险至关重要。我们进行了多模型、多基准的人类提升研究，比较了具有LLM访问权限的新手与仅有互联网访问权限的新手在八个与生物安全相关的任务集上的表现。参与者在复杂问题上工作，时间充裕（最复杂任务可达13小时）。我们发现，LLM访问提供了显著的提升：使用LLM的新手的准确率是对照组的4.16倍（95% CI [2.63, 6.87]）。在四个有可用专家基准（仅互联网）的基准测试中，使用LLM的新手在其中三个上超越了专家。或许令人惊讶的是，独立的LLM往往超过了LLM辅助的新手，这表明用户未能从LLM中引出最强的贡献。尽管有安全措施，大多数参与者（89.6%）报告在获取与双重用途相关的信息时几乎没有困难。总体而言，LLM在以前仅限于受过训练的从业者的生物任务上显著提升了新手的能力，强调了在传统基准测试的同时进行持续互动提升评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether large language models (LLMs) enhance the performance of novice users in biology tasks compared to traditional internet resources, addressing concerns about scientific acceleration and dual-use risks. A multi-model, multi-benchmark study was conducted where novices with access to LLMs were compared to those using only internet resources across eight biosecurity-related task sets, allowing up to 13 hours for problem-solving. The findings revealed that novices using LLMs achieved 4.16 times greater accuracy than the control group, outperforming experts in three out of four benchmarks, and indicating that LLMs can significantly aid novices in complex biological tasks, although standalone LLMs sometimes performed better than LLM-assisted novices.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）是否能提升新手用户在生物任务中的表现，相较于传统的互联网资源，关注科学加速和双重用途风险的问题。该研究采用多模型、多基准的方法，比较了使用LLMs的新手与仅依赖互联网资源的用户在八个与生物安全相关的任务集中的表现，参与者有多达13小时的时间来解决复杂问题。研究结果显示，使用LLMs的新手的准确率比对照组高出4.16倍，在四个有专家基准的任务中，新手在三个任务上超越了专家，并且独立使用LLMs的结果往往优于新手辅助使用的结果，突显了LLMs在生物任务中显著提升新手表现的潜力，同时也引发了关于用户如何与这些模型互动的问题。</div>
</details>
</div>
<div class="card">
<div class="title">DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models</div>
<div class="meta-line">Authors: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-10-13T02:45:48+00:00 · Latest: 2026-02-26T18:32:27+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 tables, 3 figures. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10932v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact. Prior work on VLA backdoors primarily studies untargeted attacks or task-level hijacking, leaving fine-grained control over individual actions largely unexplored. In this work, we present DropVLA, an action-level backdoor attack that forces a reusable action primitive (e.g., open_gripper) to execute at attacker-chosen decision points under a realistic pipeline-black-box setting with limited data-poisoning access, using a window-consistent relabeling scheme for chunked fine-tuning. On OpenVLA-7B evaluated with LIBERO, vision-only poisoning achieves 98.67%-99.83% attack success rate (ASR) with only 0.31% poisoned episodes while preserving 98.50%-99.17% clean-task retention, and successfully triggers the targeted action within 25 control steps at 500 Hz (0.05 s). Text-only triggers are unstable at low poisoning budgets, and combining text with vision provides no consistent ASR improvement over vision-only attacks. The backdoor remains robust to moderate trigger variations and transfers across evaluation suites (96.27%, 99.09%), whereas text-only largely fails (0.72%). We further validate physical-world feasibility on a 7-DoF Franka arm with pi0-fast, demonstrating non-trivial attack efficacy under camera-relative motion that induces image-plane trigger drift. These results reveal that VLA models can be covertly steered at the granularity of safety-critical actions with minimal poisoning and without observable degradation of nominal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DropVLA：一种针对视觉-语言-动作模型的动作级后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型将多模态感知和语言指令映射到可执行的机器人动作，使其特别容易受到行为后门操控：在训练过程中引入的隐藏触发器可以诱导意外的物理动作，同时名义任务性能保持不变。之前关于VLA后门的研究主要集中在无目标攻击或任务级劫持上，个别动作的细粒度控制尚未得到充分探索。在本研究中，我们提出了DropVLA，一种动作级后门攻击，迫使可重用的动作原语（例如，open_gripper）在攻击者选择的决策点执行，在有限数据污染访问的现实管道黑箱设置下，使用窗口一致的重新标记方案进行分块微调。在使用LIBERO评估的OpenVLA-7B上，仅视觉污染的攻击成功率（ASR）达到98.67%-99.83%，仅有0.31%的污染剧集，同时保持98.50%-99.17%的干净任务保留，并在500 Hz（0.05秒）内成功触发目标动作，控制步骤为25步。文本触发器在低污染预算下不稳定，文本与视觉的结合未能在视觉单独攻击上提供一致的ASR改进。后门对中等触发器变化保持稳健，并在评估套件之间转移（96.27%，99.09%），而仅文本的攻击大多失败（0.72%）。我们进一步在7自由度的Franka臂上验证了物理世界的可行性，使用pi0-fast，展示了在相机相对运动下诱导图像平面触发器漂移的非平凡攻击有效性。这些结果揭示了VLA模型可以在安全关键动作的细粒度上被隐秘操控，且污染最小且没有可观察的名义性能下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the vulnerability of Vision-Language-Action (VLA) models to behavioral backdoor manipulations, which can lead to unintended actions while maintaining nominal performance. The authors introduce DropVLA, an action-level backdoor attack that utilizes a window-consistent relabeling scheme for fine-tuning, allowing attackers to induce specific actions at chosen decision points with limited data poisoning. Experimental results demonstrate that vision-only poisoning achieves an attack success rate of 98.67%-99.83% with only 0.31% of episodes poisoned, while maintaining a clean-task retention of 98.50%-99.17%, and the backdoor remains effective across various conditions, including physical-world validation on a robotic arm.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨视觉-语言-动作（VLA）模型在后门攻击下的脆弱性，特别是针对单个动作的细粒度控制。作者提出了DropVLA，这是一种动作级别的后门攻击，利用窗口一致的重新标记方案进行分块微调，使攻击者能够在有限的数据污染访问下，在选择的决策点诱导特定动作。实验结果表明，视觉单一污染的攻击成功率达到98.67%-99.83%，仅有0.31%的剧集被污染，同时保持98.50%-99.17%的清洁任务保留率，且该后门在不同评估套件和物理世界场景中的有效性得以保持。</div>
</details>
</div>
<div class="card">
<div class="title">Deep ensemble graph neural networks for probabilistic cosmic-ray direction and energy reconstruction in autonomous radio arrays</div>
<div class="meta-line">Authors: Arsène Ferrière, Aurélien Benoit-Lévy, Olivier Martineau-Huynh, Matías Tueros</div>
<div class="meta-line">First: 2026-02-26T18:29:48+00:00 · Latest: 2026-02-26T18:29:48+00:00</div>
<div class="meta-line">Comments: Submitted to Astroparticle Physics Journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23321v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23321v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced on ground-based radio detector arrays.
  In our approach, triggered antennas are represented as a graph structure, which serves as input for a graph neural network (GNN). By incorporating physical knowledge into both the GNN architecture and the input data, we improve the precision and reduce the required size of the training set with respect to a fully data-driven approach. This method achieves an angular resolution of 0.092° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data with realistic noise conditions.
  We also employ uncertainty estimation methods to enhance the reliability of our predictions, quantifying the confidence of the GNN&#x27;s outputs and providing confidence intervals for both direction and energy reconstruction. Finally, we investigate strategies to verify the model&#x27;s consistency and robustness under real life variations, with the goal of identifying scenarios in which predictions remain reliable despite domain shifts between simulation and reality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于自主无线电阵列中概率宇宙射线方向和能量重建的深度集成图神经网络</div>
<div class="mono" style="margin-top:8px">我们使用先进的机器学习技术，开发了一种方法，精确重建超高能宇宙射线到达方向和能量，基于它们在地面无线电探测器阵列上诱发的电压轨迹。我们的方法将触发天线表示为图结构，作为图神经网络（GNN）的输入。通过将物理知识融入GNN架构和输入数据中，我们提高了精度，并减少了相对于完全数据驱动方法所需的训练集大小。该方法在具有真实噪声条件的模拟数据上实现了0.092°的角分辨率和16.4%的电磁能量重建分辨率。我们还采用不确定性估计方法来增强预测的可靠性，量化GNN输出的置信度，并为方向和能量重建提供置信区间。最后，我们研究了验证模型在现实生活变化下的一致性和鲁棒性的策略，旨在识别在模拟与现实之间存在领域转移时预测仍然可靠的场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the reconstruction of ultra-high-energy cosmic rays&#x27; arrival direction and energy using advanced machine learning techniques. The authors developed a method that utilizes a graph neural network (GNN) with a graph structure representing triggered antennas, incorporating physical knowledge to enhance precision and reduce the training set size compared to fully data-driven methods. The results demonstrate an angular resolution of 0.092° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data, while also employing uncertainty estimation methods to quantify prediction confidence and exploring model robustness under real-world variations.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用先进的机器学习技术改善超高能宇宙射线到达方向和能量的重建。作者开发了一种方法，使用图神经网络（GNN），通过图结构表示触发天线，结合物理知识以提高精度并减少与完全数据驱动方法相比的训练集规模。实验结果表明，在模拟数据上，角分辨率达到0.092°，电磁能量重建分辨率为16.4%，同时还结合了不确定性估计方法来评估预测的可靠性，并探索模型在现实世界变化下的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">ParamMem: Augmenting Language Agents with Parametric Reflective Memory</div>
<div class="meta-line">Authors: Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen, Kun Zhang</div>
<div class="meta-line">First: 2026-02-26T18:28:04+00:00 · Latest: 2026-02-26T18:28:04+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ParamMem：增强语言代理的参数反思记忆</div>
<div class="mono" style="margin-top:8px">自我反思使语言代理能够迭代地优化解决方案，但往往会产生限制推理性能的重复输出。最近的研究尝试通过各种方法解决这一限制，其中增加反思多样性显示出希望。我们的实证分析揭示了反思多样性与任务成功之间的强正相关，进一步激励了对多样化反思信号的需求。我们引入了ParamMem，一个参数记忆模块，将跨样本反思模式编码为模型参数，通过温度控制采样实现多样化反思生成。在此模块的基础上，我们提出了ParamAgent，一个基于反思的代理框架，将参数记忆与情节记忆和跨样本记忆相结合。在代码生成、数学推理和多跳问答的广泛实验中，显示出相对于最先进基线的一致性改进。进一步分析表明，ParamMem在样本效率上表现良好，支持模型规模之间的弱到强转移，并支持自我改进而不依赖于更强的外部模型，突显了ParamMem作为增强语言代理的有效组件的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning performance of language agents by addressing the issue of repetitive outputs during self-reflection. The authors introduce ParamMem, a parametric memory module that captures cross-sample reflection patterns to facilitate diverse reflection generation through temperature-controlled sampling. Experimental results demonstrate that the proposed ParamAgent framework, which integrates ParamMem with episodic and cross-sample memory, significantly improves performance in tasks such as code generation, mathematical reasoning, and multi-hop question answering compared to existing state-of-the-art methods, while also showing sample efficiency and the ability to support self-improvement across different model scales.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高语言智能体的推理性能，因为自我反思往往导致重复输出。作者提出了ParamMem，一个参数化记忆模块，能够捕捉跨样本反思模式，通过温度控制采样实现多样化的反思生成。针对代码生成、数学推理和多跳问答等任务的实验结果表明，ParamMem显著提高了性能，相较于最先进的基线，展示了其样本效率和在不依赖更强外部模型的情况下促进自我改进的能力。</div>
</details>
</div>
<div class="card">
<div class="title">LinGuinE: Longitudinal Guidance Estimation for Volumetric Tumour Segmentation</div>
<div class="meta-line">Authors: Nadine Garibli, Mayank Patwari, Bence Csiba, Yi Wei, Kostantinos Sidiropoulos</div>
<div class="meta-line">First: 2025-06-06T13:52:33+00:00 · Latest: 2026-02-26T18:27:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06092v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.06092v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Longitudinal volumetric tumour segmentation is critical for radiotherapy planning and response assessment, yet this problem is underexplored and most methods produce single-timepoint semantic masks, lack lesion correspondence, and offer limited radiologist control. We introduce LinGuinE (Longitudinal Guidance Estimation), a PyTorch framework that combines image registration and guided segmentation to deliver lesion-level tracking and volumetric masks across all scans in a longitudinal study from a single radiologist prompt. LinGuinE is temporally direction agnostic, requires no training on longitudinal data, and allows any registration and semi-automatic segmentation algorithm to be repurposed for the task. We evaluate various combinations of registration and segmentation algorithms within the framework. LinGuinE achieves state-of-the-art segmentation and tracking performance across four datasets with a total of 456 longitudinal studies. Tumour segmentation performance shows minimal degradation with increasing temporal separation. We conduct ablation studies to determine the impact of autoregression, pathology specific finetuning, and the use of real radiologist prompts. We release our code and substantial public benchmarking for longitudinal segmentation, facilitating future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinGuinE：体积肿瘤分割的纵向引导估计</div>
<div class="mono" style="margin-top:8px">纵向体积肿瘤分割对放射治疗规划和反应评估至关重要，但这一问题尚未得到充分研究，大多数方法产生单时间点的语义掩膜，缺乏病变对应，并且提供的放射科医生控制有限。我们介绍了LinGuinE（纵向引导估计），这是一个基于PyTorch的框架，结合了图像配准和引导分割，以提供病变级别的跟踪和纵向研究中所有扫描的体积掩膜，基于单个放射科医生的提示。LinGuinE在时间方向上是无关的，不需要在纵向数据上进行训练，并允许任何配准和半自动分割算法被重新用于该任务。我们在框架内评估了各种配准和分割算法的组合。LinGuinE在四个数据集上实现了最先进的分割和跟踪性能，总共有456个纵向研究。肿瘤分割性能在时间间隔增加时表现出最小的降级。我们进行消融研究，以确定自回归、特定病理微调和使用真实放射科医生提示的影响。我们发布了代码和大量公共基准测试，以促进未来的纵向分割研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve longitudinal volumetric tumour segmentation, which is essential for effective radiotherapy planning and response assessment, as existing methods are limited to single-timepoint masks and lack lesion tracking. The authors introduce LinGuinE, a PyTorch framework that integrates image registration and guided segmentation to enable lesion-level tracking and volumetric mask generation from a single radiologist prompt, without requiring training on longitudinal data. The experimental results demonstrate that LinGuinE achieves state-of-the-art performance in segmentation and tracking across four datasets comprising 456 longitudinal studies, with minimal performance degradation observed as temporal separation increases, and the study includes ablation analyses to assess various factors influencing performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决纵向体积肿瘤分割中的挑战，这对有效的放疗规划和反应评估至关重要。作者开发了LinGuinE，这是一个PyTorch框架，结合了图像配准和引导分割，使得能够从单个放射科医生的提示中生成病灶级跟踪和体积掩膜，而无需对纵向数据进行训练。实验结果表明，LinGuinE在四个包含456个纵向研究的数据集上实现了最先进的分割和跟踪性能，且随着时间间隔的增加，性能降级最小，研究还包括消融分析，以评估自回归和放射科医生提示等各种因素的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Rapid Action Value Estimation in Memory-Constrained Environments</div>
<div class="meta-line">Authors: Aloïs Rautureau, Tristan Cazenave, Éric Piette</div>
<div class="meta-line">First: 2026-02-26T18:25:59+00:00 · Latest: 2026-02-26T18:25:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23318v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23318v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内存受限环境中的广义快速动作价值估计</div>
<div class="mono" style="margin-top:8px">广义快速动作价值估计（GRAVE）已被证明是蒙特卡洛树搜索（MCTS）算法家族中针对一般游戏玩法（GGP）的强大变体。然而，它在每个节点存储额外的胜利/访问统计数据的依赖性使其在内存受限环境中的使用变得不切实际，从而限制了其实际应用。在本文中，我们介绍了GRAVE2、GRAVER和GRAVER2算法，分别通过两级搜索、节点回收和两种技术的结合扩展了GRAVE。我们展示了这些增强功能能够显著减少存储节点的数量，同时匹配GRAVE的游戏实力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the applicability of Generalized Rapid Action Value Estimation (GRAVE) in memory-constrained environments, as the original GRAVE algorithm&#x27;s reliance on extensive win/visit statistics limits its practical use. The authors introduce three new algorithms—GRAVE2, GRAVER, and GRAVER2—that enhance GRAVE by implementing two-level search, node recycling, and a combination of both techniques. Experimental results demonstrate that these enhancements significantly reduce the number of stored nodes while maintaining the competitive playing strength of the original GRAVE algorithm.</div>
<div class="mono" style="margin-top:8px">本研究解决了通用快速行动价值估计（GRAVE）算法在内存受限环境中的局限性，这限制了其在通用游戏中的实际应用。作者提出了三种新算法——GRAVE2、GRAVER和GRAVER2，利用两级搜索和节点回收技术来增强GRAVE。实验结果表明，这些修改显著减少了存储节点的数量，同时保持了原GRAVE算法的竞争性能。</div>
</details>
</div>
<div class="card">
<div class="title">Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction</div>
<div class="meta-line">Authors: Sha Hu</div>
<div class="meta-line">First: 2026-02-26T18:22:40+00:00 · Latest: 2026-02-26T18:22:40+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23315v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a &quot;resampling&quot; based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于不变变换和重采样的认知不确定性降低</div>
<div class="mono" style="margin-top:8px">人工智能（AI）模型可以视为一个将输入映射到高维空间输出的函数。一旦设计并经过良好训练，AI模型就可以用于推理。然而，即使是经过优化的AI模型也可能由于随机和认知不确定性而产生推理错误。有趣的是，我们观察到在基于输入的不变变换推理多个样本时，推理错误可能由于认知不确定性而表现出部分独立性。利用这一见解，我们提出了一种基于“重采样”的推理方法，适用于具有多个变换版本的训练AI模型，并将推理输出聚合为更准确的结果。这种方法有潜力提高推理准确性，并为平衡模型大小和性能提供了一种策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inference errors in AI models caused by aleatoric and epistemic uncertainties. The authors propose a method called &quot;resampling&quot; that utilizes invariant transformations of input data to generate multiple samples for inference, allowing for the aggregation of outputs to enhance accuracy. Experimental results indicate that this approach can effectively reduce epistemic uncertainty and improve overall inference accuracy while balancing model size and performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于减少由于随机和认知不确定性导致的人工智能模型推理错误。作者提出了一种新方法，基于输入数据的不变变换进行重采样，从多个变换版本的输入中聚合推理输出。实验结果表明，这种重采样方法可以显著提高推理准确性，同时提供了一种平衡模型大小和性能的策略。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</div>
<div class="meta-line">Authors: Rafael R. Baptista, André de Lima Salgado, Ricardo V. Godoy, Marcelo Becker, Thiago Boaventura, Gustavo J. G. Lahr</div>
<div class="meta-line">First: 2026-02-26T18:20:26+00:00 · Latest: 2026-02-26T18:20:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23312v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model&#x27;s architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估小型语言模型在领导-跟随者互动中的零-shot和一-shot适应性</div>
<div class="mono" style="margin-top:8px">领导-跟随者互动是人机交互（HRI）中的一个重要范式。然而，在实时分配角色对于资源受限的移动和辅助机器人仍然具有挑战性。尽管大型语言模型（LLMs）在自然交流中显示出潜力，但其规模和延迟限制了在设备上的部署。小型语言模型（SLMs）提供了一种潜在的替代方案，但其在HRI中角色分类的有效性尚未系统评估。本文提出了一个针对领导-跟随者通信的SLM基准，介绍了一个从已发布数据库派生的新数据集，并通过合成样本增强以捕捉特定互动动态。我们研究了两种适应策略：提示工程和微调，在零-shot和一-shot互动模式下进行研究，并与未训练的基线进行比较。使用Qwen2.5-0.5B的实验表明，零-shot微调实现了稳健的分类性能（86.66%的准确率），同时保持低延迟（每个样本22.2毫秒），显著优于基线和提示工程方法。然而，结果还表明在一-shot模式下性能下降，增加的上下文长度挑战了模型的架构能力。这些发现表明，微调的SLMs为直接角色分配提供了有效解决方案，同时突显了对话复杂性与边缘分类可靠性之间的关键权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of real-time role assignment in leader-follower interactions for resource-constrained mobile and assistive robots, where the use of large language models is limited by their size and latency. The authors benchmark small language models (SLMs) for this purpose by creating a novel dataset and exploring two adaptation strategies: prompt engineering and fine-tuning under zero-shot and one-shot interaction modes. The experiments with Qwen2.5-0.5B demonstrate that zero-shot fine-tuning achieves a robust classification performance of 86.66% accuracy with low latency, significantly surpassing the baseline and prompt-engineered methods, although one-shot modes show a decline in performance due to increased context length challenges.</div>
<div class="mono" style="margin-top:8px">本研究解决了移动和辅助机器人中领导者-跟随者交互的实时角色分配挑战，特别是考虑到大型语言模型在设备部署中的局限性。作者通过创建一个新数据集并应用两种适应策略——提示工程和微调——在零样本和单样本交互模式下评估小型语言模型（SLMs）。实验结果表明，使用Qwen2.5-0.5B模型的零样本微调在低延迟下实现了86.66%的准确率，显著优于基线和提示工程方法，但单样本模式由于上下文长度增加而导致性能下降，影响了模型的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating the Diversity and Quality of LLM Generated Content</div>
<div class="meta-line">Authors: Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, Osbert Bastani</div>
<div class="meta-line">First: 2025-04-16T23:02:23+00:00 · Latest: 2026-02-26T18:17:44+00:00</div>
<div class="meta-line">Comments: Published at COLM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12522v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.12522v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity -- diversity among outputs that meet quality thresholds -- which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models -- particularly those trained via RL -- often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型生成内容的多样性和质量</div>
<div class="mono" style="margin-top:8px">最近的研究表明，偏好调优技术——如基于人类反馈的强化学习（RLHF）方法，如PPO和GRPO，以及替代方法如DPO——降低了多样性，这在这些模型广泛应用于需要多样化输出的场景中形成了困境。我们认为，忽视质量的多样性具有有限的实际价值。为了解决这个问题，我们引入了一个有效语义多样性测量框架——在满足质量阈值的输出之间的多样性——更好地反映大型语言模型（LLMs）的实际效用。通过使用不需要人类干预的开放式任务，我们发现了一些反直觉的结果：当使用不明确考虑质量的多样性指标时，偏好调优模型——特别是通过RL训练的模型——通常生成的输出多样性较低；然而，这些相同的偏好调优模型生成的有效语义多样性却高于监督微调（SFT）或基础模型。我们的分析进一步显示了另一个趋势：虽然较大的模型可能表现出比较小的模型更大的有效语义多样性，但较小的模型在固定采样预算内生成独特内容的参数效率始终更高。这些发现对需要多样化且高质量输出的应用具有实际意义，从创意辅助到合成数据生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the dilemma posed by preference-tuning techniques, such as RLHF methods, which reduce diversity in large language models (LLMs) despite their deployment in applications requiring varied outputs. The authors introduce a framework for measuring effective semantic diversity, focusing on outputs that meet quality thresholds. Their findings reveal that preference-tuned models, particularly those trained via RL, produce greater effective semantic diversity compared to supervised fine-tuned or base models, even though they show lower diversity when assessed by traditional metrics. Additionally, while larger models tend to exhibit greater effective semantic diversity, smaller models are found to be more parameter-efficient in generating unique content within a fixed sampling budget.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决偏好调优技术（如RLHF方法）在大型语言模型（LLMs）中降低多样性的问题，尽管这些模型广泛应用于生成多样化输出。作者提出了一种有效语义多样性的测量框架，该框架评估满足质量阈值的输出之间的多样性。实验结果表明，尽管偏好调优模型，特别是通过RL训练的模型，通常在传统指标下产生较低的多样性，但它们实际上比监督微调或基础模型生成更大的有效语义多样性。此外，研究发现，较大模型可能表现出更大的有效语义多样性，但较小模型在固定采样预算内生成独特内容的参数效率更高，这对需要多样化和高质量输出的应用具有重要意义。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting</div>
<div class="meta-line">Authors: Shai Feldman, Stephen Bates, Yaniv Romano</div>
<div class="meta-line">First: 2025-05-07T18:46:02+00:00 · Latest: 2026-02-26T18:16:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04733v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.04733v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a framework for robust uncertainty quantification in situations where labeled training data are corrupted, through noisy or missing labels. We build on conformal prediction, a statistical tool for generating prediction sets that cover the test label with a pre-specified probability. The validity of conformal prediction, however, holds under the i.i.d assumption, which does not hold in our setting due to the corruptions in the data. To account for this distribution shift, the privileged conformal prediction (PCP) method proposed leveraging privileged information (PI) -- additional features available only during training -- to re-weight the data distribution, yielding valid prediction sets under the assumption that the weights are accurate. In this work, we analyze the robustness of PCP to inaccuracies in the weights. Our analysis indicates that PCP can still yield valid uncertainty estimates even when the weights are poorly estimated. Furthermore, we introduce uncertain imputation (UI), a new conformal method that does not rely on weight estimation. Instead, we impute corrupted labels in a way that preserves their uncertainty. Our approach is supported by theoretical guarantees and validated empirically on both synthetic and real benchmarks. Finally, we show that these techniques can be integrated into a triply robust framework, ensuring statistically valid predictions as long as at least one underlying method is valid.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带有损坏标签的保形预测：不确定插补和稳健重加权</div>
<div class="mono" style="margin-top:8px">我们提出了一个框架，用于在标签训练数据受到噪声或缺失标签影响的情况下进行稳健的不确定性量化。我们基于保形预测，这是一种生成预测集的统计工具，能够以预先指定的概率覆盖测试标签。然而，保形预测的有效性依赖于独立同分布（i.i.d）假设，而在我们的设置中，由于数据的损坏，这一假设并不成立。为了考虑这种分布转变，提出了特权保形预测（PCP）方法，利用特权信息（PI）——仅在训练期间可用的额外特征——对数据分布进行重加权，从而在权重准确的假设下产生有效的预测集。在这项工作中，我们分析了PCP对权重不准确性的稳健性。我们的分析表明，即使权重估计不佳，PCP仍然可以产生有效的不确定性估计。此外，我们引入了不确定插补（UI），这是一种不依赖于权重估计的新保形方法。相反，我们以保留不确定性的方式插补损坏的标签。我们的方法得到了理论保证，并在合成和真实基准上进行了实证验证。最后，我们展示了这些技术可以集成到一个三重稳健框架中，只要至少有一种基础方法是有效的，就能确保统计上有效的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of uncertainty quantification in scenarios where training data labels are corrupted, which can arise from noise or missing information. The authors propose a framework based on conformal prediction, specifically the privileged conformal prediction (PCP) method, which utilizes privileged information to re-weight the data distribution for valid prediction sets. Key findings demonstrate that PCP remains robust to inaccuracies in weight estimation and that the newly introduced uncertain imputation (UI) method can effectively handle corrupted labels while maintaining uncertainty, with empirical validation on both synthetic and real datasets, ultimately contributing to a triply robust framework for reliable predictions.</div>
<div class="mono" style="margin-top:8px">本研究解决了在训练数据因噪声或缺失标签而受到污染时，机器学习中不确定性量化的挑战。作者提出了一种基于保形预测的框架，特别是特权保形预测（PCP）方法，该方法利用特权信息对数据分布进行重加权，以在分布转移的情况下保持有效的预测集。他们的研究结果表明，PCP在权重估计不准确的情况下仍然具有鲁棒性，并且引入了一种新的方法称为不确定插补（UI），该方法在保留不确定性的同时插补被污染的标签。对合成和真实数据集的实证验证支持了其方法的理论保证，并且可以集成到一个三重鲁棒框架中，以确保统计有效的预测。</div>
</details>
</div>
<div class="card">
<div class="title">Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training</div>
<div class="meta-line">Authors: Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-02-24T18:43:08+00:00 · Latest: 2026-02-26T18:11:36+00:00</div>
<div class="meta-line">Comments: updated related work discussion</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21189v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.21189v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么 Pass@k 优化会降低 Pass@1：LLM 后训练中的提示干扰</div>
<div class="mono" style="margin-top:8px">Pass@k 是一个广泛使用的可验证大型语言模型任务的性能指标，包括数学推理、代码生成和简答推理。如果 $k$ 个独立采样的解决方案中有任何一个通过验证器，则定义为成功。这个多样本推理指标激励了直接优化 pass@$k$ 的推理感知微调方法。然而，先前的研究报告了一个反复出现的权衡：在这种方法下，pass@k 改善而 pass@1 降低。这个权衡在实践中很重要，因为 pass@1 通常由于延迟和成本预算、不完美的验证器覆盖以及对可靠单次回退的需求而成为一个严格的操作约束。我们研究了这个权衡的起源，并提供了一个理论表征，说明何时 pass@k 策略优化会通过提示干扰引起的梯度冲突来降低 pass@1。我们表明，pass@$k$ 策略梯度可能与 pass@1 梯度冲突，因为 pass@$k$ 优化隐式地将提示重新加权为低成功提示；当这些提示被称为负干扰时，它们的加权可能会使 pass@k 更新方向偏离 pass@1 方向。我们通过在可验证的数学推理任务上进行的大型语言模型实验来说明我们的理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the trade-off between Pass@k and Pass@1 performance metrics in large language models, motivated by the practical implications of optimizing for Pass@k while maintaining Pass@1 as a critical operational constraint. The authors employ theoretical analysis to characterize the conditions under which Pass@k optimization can lead to a decrease in Pass@1 due to gradient conflicts arising from prompt interference. Experimental results demonstrate that optimizing for Pass@k can inadvertently favor low-success prompts, resulting in updates that detract from Pass@1 performance, as evidenced by tests on verifiable mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型中Pass@k和Pass@1性能指标之间的权衡，动机在于优化Pass@k的同时保持Pass@1作为关键的操作约束的实际意义。作者采用理论框架来表征Pass@k优化导致Pass@1下降的条件，归因于由提示干扰引起的梯度冲突。实验结果表明，优化Pass@k可能无意中偏向低成功率的提示，从而对Pass@1性能产生负面影响，特别是在可验证的数学推理任务中。</div>
</details>
</div>
<div class="card">
<div class="title">ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding</div>
<div class="meta-line">Authors: Yiran Guan, Sifan Tu, Dingkang Liang, Linghao Zhu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Yuliang Liu, Xiang Bai</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T18:10:41+00:00 · Latest: 2026-02-26T18:10:41+00:00</div>
<div class="meta-line">Comments: Accept by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23306v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThinkOmni：通过引导解码提升文本推理到全模态场景</div>
<div class="mono" style="margin-top:8px">全模态推理对于智能系统理解和从多样数据源中推断至关重要。尽管现有的全模态大型语言模型（OLLM）在感知多样模态方面表现出色，但它们缺乏近期大型推理模型（LRM）的复杂推理能力。然而，通过额外训练增强OLLM的推理能力面临重大挑战，包括对高质量数据、特定任务适应和大量计算成本的需求。为了解决这些限制，我们提出了ThinkOmni，一个无训练和无数据的框架，将文本推理提升到全模态场景。ThinkOmni引入了两个关键组件：1）LRM作为引导，利用现成的LRM来指导OLLM解码过程；2）逐步对比缩放，自适应平衡感知和推理信号，无需手动超参数调优。在六个多模态推理基准上的实验表明，ThinkOmni始终提供性能提升，主要结果在MathVista上达到70.2，在MMAU上达到75.5。总体而言，ThinkOmni为全模态推理提供了灵活且可推广的解决方案，并为推理能力的泛化和应用提供了新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of omni-modal large language models (OLLM), which currently excel in perceiving diverse modalities but struggle with complex reasoning tasks. The authors propose a novel framework called ThinkOmni, which operates without additional training or data by utilizing two main components: LRM-as-a-Guide, which employs existing large reasoning models to assist in the decoding process of OLLMs, and Stepwise Contrastive Scaling, which dynamically balances perception and reasoning signals. Experimental results across six multi-modal reasoning benchmarks indicate that ThinkOmni significantly improves performance, achieving scores of 70.2 on MathVista and 75.5 on MMAU, thereby providing a flexible solution for omni-modal reasoning challenges.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强全模态大型语言模型（OLLM）的推理能力，这些模型在感知多样化模态方面表现出色，但在复杂推理任务上存在不足。作者提出了一种名为ThinkOmni的新框架，该框架无需额外训练或数据，利用两个主要组件：LRM作为引导，用于指导OLLM解码过程，以及逐步对比缩放，用于平衡感知和推理信号。对六个多模态推理基准的实验结果表明，ThinkOmni显著提高了性能，在MathVista上获得了70.2分，在MMAU上获得了75.5分，从而为全模态推理挑战提供了一种灵活的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">DRESS: A Continuous Framework for Structural Graph Refinement</div>
<div class="meta-line">Authors: Eduar Castrillo Velilla</div>
<div class="meta-line">First: 2026-02-24T12:18:42+00:00 · Latest: 2026-02-26T18:10:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20833v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20833v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as $\mathcal{O}(n^3)$ or $\mathcal{O}(n^4)$, making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, León, and Gómez, 2018) -- a parameter-free, continuous dynamical system on edges -- and show that it distinguishes the prism graph from $K_{3,3}$, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce $Δ$-DRESS, which runs DRESS on each node-deleted subgraph $G \setminus \{v\}$, connecting the framework to the Kelly--Ulam reconstruction conjecture. Both Motif-DRESS and $Δ$-DRESS empirically distinguish Strongly Regular Graphs (SRGs) -- such as the Rook and Shrikhande graphs -- that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive $\mathcal{O}(n^4)$ computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DRESS：一种连续的结构图精炼框架</div>
<div class="mono" style="margin-top:8px">Weisfeiler-Lehman (WL) 层次是图同构测试和结构分析的基石框架。然而，超越 1-WL 到 3-WL 及更高层次需要张量运算，其复杂度为 $\mathcal{O}(n^3)$ 或 $\mathcal{O}(n^4)$，使得在大图上计算代价高昂。本文从原始 DRESS 方程 (Castrillo, León, 和 Gómez, 2018) 开始——这是一个无参数的、在边上的连续动力系统——并展示它能够区分棱柱图和 $K_{3,3}$，而 1-WL 证明无法分离这对图。然后，我们将其推广到 Motif-DRESS，替换三角邻域为任意结构图案，并在三个充分条件下收敛到唯一的固定点，进一步推广到 Generalized-DRESS，这是一个由邻域算子、聚合函数和范数的选择参数化的抽象模板。最后，我们引入 $Δ$-DRESS，它在每个节点删除的子图 $G \setminus \{v\}$ 上运行 DRESS，将该框架与 Kelly-Ulam 重建猜想联系起来。Motif-DRESS 和 $Δ$-DRESS 在经验上区分强正则图 (SRGs)——如车马图和 Shrikhande 图——这些图使 3-WL 产生困惑。我们的结果确立了 DRESS 家族作为一个高度可扩展的框架，在著名基准图上经验上超越了 1-WL 和 3-WL，而没有高达 $\mathcal{O}(n^4)$ 的计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of the Weisfeiler-Lehman (WL) hierarchy in graph isomorphism testing, particularly the computational challenges of scaling beyond 1-WL due to tensor-based operations. The authors develop a continuous dynamical system called DRESS, starting from the Original-DRESS equation, which successfully distinguishes between the prism graph and K_{3,3}, a task that 1-WL cannot accomplish. They further extend this framework to Motif-DRESS and Generalized-DRESS, which can handle arbitrary structural motifs and various neighborhood operators, respectively, and introduce $Δ$-DRESS to analyze node-deleted subgraphs. Experimental results demonstrate that both Motif-DRESS and $Δ$-DRESS effectively differentiate Strongly Regular Graphs that challenge 3-WL, establishing the DRESS family as a scalable alternative that outperforms traditional WL methods on benchmark graphs without incurring high computational costs.</div>
<div class="mono" style="margin-top:8px">本研究解决了Weisfeiler-Lehman (WL) 层次在图同构测试中的局限性，特别是超越1-WL时的计算挑战。作者提出了一种名为DRESS的连续动力系统，从Original-DRESS方程出发，有效地区分了1-WL无法分离的图。进一步开发的Motif-DRESS和Generalized-DRESS增强了框架的灵活性和可扩展性，并引入了$Δ$-DRESS以与Kelly-Ulam重构猜想相联系。实验结果表明，Motif-DRESS和$Δ$-DRESS能够成功区分挑战3-WL的强正则图，确立了DRESS家族作为现有方法的更高效替代方案，而不需要高昂的计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">A Proper Scoring Rule for Virtual Staining</div>
<div class="meta-line">Authors: Samuel Tonks, Steve Hood, Ryan Musso, Ceridwen Hopely, Steve Titus, Minh Doan, Iain Styles, Alexander Krull</div>
<div class="meta-line">First: 2026-02-26T18:09:49+00:00 · Latest: 2026-02-26T18:09:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23305v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>虚拟染色的适当评分规则</div>
<div class="mono" style="margin-top:8px">用于高通量筛选的生成虚拟染色（VS）模型可以为每个输入和细胞提供可能生物特征值的后验分布估计。然而，在评估VS模型时，真实后验是不可用的。现有的评估协议仅检查数据集上边际分布的准确性，而不是预测的后验分布。我们引入信息增益（IG）作为细胞级评估框架，使得可以直接评估预测的后验。IG是一个严格的适当评分规则，并具有合理的理论动机，允许可解释性，并用于比较模型和特征之间的结果。我们使用IG和其他指标在一个广泛的HTS数据集上评估基于扩散和GAN的模型，并显示IG可以揭示其他指标无法发现的显著性能差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of generative virtual staining (VS) models used in high-throughput screening (HTS), as existing methods fail to assess the predicted posteriors effectively. The authors propose a novel evaluation framework based on information gain (IG), which serves as a strictly proper scoring rule for direct assessment of predicted posteriors. Experimental results demonstrate that IG can uncover significant performance differences between diffusion- and GAN-based models that are not detectable by other evaluation metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善用于高通量筛选（HTS）的生成虚拟染色（VS）模型的评估，因为现有方法未能直接评估预测后验。作者提出信息增益（IG）作为一种细胞级评估框架，作为严格的适当评分规则，提供了可解释性和跨模型及特征比较的理论基础。实验结果表明，IG有效地揭示了扩散模型和GAN模型在全面HTS数据集上的显著性能差异，而其他指标则未能发现这些差异。</div>
</details>
</div>
<div class="card">
<div class="title">Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications</div>
<div class="meta-line">Authors: Ilya Balabin, Thomas M. Kaiser</div>
<div class="meta-line">First: 2026-02-26T18:09:16+00:00 · Latest: 2026-02-26T18:09:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23303v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23303v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理力学第一部分：化学生物学中机器学习的因果机制理论及其影响</div>
<div class="mono" style="margin-top:8px">机器学习技术现在在全球的研究实验室中被广泛应用。通过机器学习和人工智能技术，在大数据集处理方面取得了显著进展。这一进展增强了实验者消化数据和对感兴趣现象进行新预测的能力。然而，从自然科学数据集中生成的机器学习预测器通常被视为黑箱，广泛使用而未详细考虑所关注数据集的因果结构。已有工作试图将因果关系引入自然现象的机器学习模型讨论中；然而，缺乏坚实统一的理论处理。这三篇论文系列探讨了化学理论、生物理论、概率理论和因果关系的结合，以纠正自然科学中机器学习的当前因果缺陷。本文是该系列的第一部分，提供了化学生物学现象的基础因果结构的正式框架，并通过新概念“焦点”扩展到机器学习，定义为机器学习算法在大数据集中缩小到隐藏基础机制的能力。还提供了这些原则在一类Akt抑制剂上的初步证明。第二篇包含第二部分的论文将正式探讨化学相似性，第三部分将提供大量实验证据，说明隐藏的因果结构如何削弱化学生物学中的所有机器学习。该系列旨在为化学生物学建立一种新的数学框架，以建模自然中的机制，而无需还原主义的工具：推理力学。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of machine learning (ML) models in the natural sciences, particularly their treatment as black boxes without consideration of causal structures. The authors propose a formal framework that integrates chemical theory, biological theory, probability theory, and causality to improve the understanding of ML in chemical biology. Key experimental findings include the introduction of the concept of &#x27;focus,&#x27; which allows ML algorithms to identify hidden mechanisms in large datasets, demonstrated through initial proofs involving a family of Akt inhibitors.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决化学生物学中机器学习（ML）模型的局限性，这些模型通常作为黑箱运行，缺乏对数据集中因果结构的清晰理解。作者提出了一个正式框架，整合了化学理论、生物理论、概率理论和因果关系，以增强该领域ML的可解释性。关键实验结果通过焦点的概念展示了该框架的应用，使ML算法能够识别大数据集中隐藏的机制，以一组Akt抑制剂为例进行了初步证明。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260228_0344.html">20260228_0344</a>
<a href="archive/20260227_0352.html">20260227_0352</a>
<a href="archive/20260226_0358.html">20260226_0358</a>
<a href="archive/20260225_0400.html">20260225_0400</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0335.html">20260223_0335</a>
<a href="archive/20260222_0334.html">20260222_0334</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0348.html">20260220_0348</a>
<a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
