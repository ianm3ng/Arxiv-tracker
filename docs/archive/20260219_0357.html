<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-19 03:57</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260219_0357</div>
    <div class="row"><div class="card">
<div class="title">Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution</div>
<div class="meta-line">Authors: Christopher David Roberts</div>
<div class="meta-line">First: 2026-02-17T18:59:55+00:00 · Latest: 2026-02-17T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15830v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度学习后处理方法的集成规模依赖性：激励示例和概念验证解决方案</div>
<div class="mono" style="margin-top:8px">公平评分奖励表现得像来自与验证观测相同分布的样本的集成预测成员。因此，当大型训练集成不可用或计算上不可行时，它们作为训练数据驱动的集成预测或后处理方法的损失函数是一个有吸引力的选择。调整后的连续排名概率评分（aCRPS）在集成规模方面是公平和无偏的，前提是预测成员是可交换的，并且可以解释为来自潜在预测分布的条件独立抽样。然而，引入成员之间结构依赖性的分布感知后处理方法可能会违反这一假设，从而使aCRPS变得不公平。我们使用两种旨在最小化有限集成的期望aCRPS的方法来演示这一效果：（1）逐个成员的线性校准，通过对样本集成均值的共同依赖耦合成员，以及（2）一种深度学习方法，通过跨集成维度的变换器自注意力耦合成员。在这两种情况下，结果对集成规模敏感，aCRPS的明显增益可能对应于由过度离散特征化的系统性不可靠性。我们引入轨迹变换器作为概念验证，证明可以实现集成规模独立性。这种方法是后处理集成与变换器（PoET）框架的适应，应用自注意力于提前时间，同时保持aCRPS所需的条件独立性。当应用于来自ECMWF季节性预测系统的每周平均$T_{2m}$预测时，该方法成功减少了系统性模型偏差，同时改善或维持了预测可靠性，无论是在训练中使用的集成规模（3与9个成员）还是实时预测（9与100个成员）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges of ensuring fairness in ensemble forecast scoring when large training ensembles are not feasible. The authors employ two methods to minimize the expected adjusted continuous ranked probability score (aCRPS) of finite ensembles: a linear member-by-member calibration and a deep-learning approach utilizing transformer self-attention. The findings reveal that both methods exhibit sensitivity to ensemble size, leading to potential over-dispersion and unreliability in aCRPS, while the proposed trajectory transformers demonstrate a proof-of-concept solution that maintains ensemble-size independence and improves forecast reliability across varying ensemble sizes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在集成预测中使用公平评分方法，特别是在大型训练集成不切实际的情况下。作者研究了两种旨在最小化有限集成的期望调整连续排名概率评分（aCRPS）的方法：线性逐成员校准和利用变换器自注意力的深度学习方法。研究结果表明，这两种方法对集成规模敏感，aCRPS的明显改善可能导致系统性不可靠。他们提出了轨迹变换器作为概念验证解决方案，保持了aCRPS所需的条件独立性，成功减少了模型偏差，并在实时应用中提高了不同集成规模的预测可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Operationalising the Superficial Alignment Hypothesis via Task Complexity</div>
<div class="meta-line">Authors: Tomás Vergara-Browne, Darshan Patil, Ivan Titov, Siva Reddy, Tiago Pimentel, Marius Mosbach</div>
<div class="meta-line">First: 2026-02-17T18:59:39+00:00 · Latest: 2026-02-17T18:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15829v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过任务复杂性实现表面对齐假设</div>
<div class="mono" style="margin-top:8px">表面对齐假设（SAH）认为，大型语言模型在预训练期间学习了大部分知识，而后训练仅仅是将这些知识显现出来。然而，SAH缺乏精确的定义，这导致了（i）支持它的不同且看似正交的论点，以及（ii）对它的重要批评。我们提出了一种新的度量标准，称为任务复杂性：实现任务目标性能的最短程序的长度。在这个框架中，SAH简单地声称，预训练模型大幅降低了在许多任务上实现高性能的复杂性。我们的定义统一了支持SAH的先前论点，将其解释为寻找这些短程序的不同策略。通过实验，我们估计了数学推理、机器翻译和指令跟随的任务复杂性；然后我们展示了在预训练模型的条件下，这些复杂性可以显著降低。此外，我们发现预训练使得在我们的任务上获得强大性能成为可能，但可能需要数千兆字节长度的程序来访问它们。另一方面，后训练则将达到相同性能的复杂性降低了几个数量级。总体而言，我们的结果强调任务适应通常需要惊人少量的信息——通常只有几千字节。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to clarify the superficial alignment hypothesis (SAH), which suggests that large language models primarily acquire knowledge during pre-training, with post-training serving to reveal this knowledge. To operationalize the SAH, the authors introduce a new metric called task complexity, defined as the length of the shortest program needed to achieve a target performance on a task. Through experiments on mathematical reasoning, machine translation, and instruction following, they demonstrate that pre-trained models significantly reduce the complexity of achieving high performance, often requiring only a few kilobytes of information, while post-training further decreases this complexity by several orders of magnitude.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于澄清表面对齐假设（SAH），该假设认为大型语言模型主要在预训练期间学习，而后训练则用于揭示这些知识。为了使SAH具备可操作性，作者引入了一种新的度量标准，称为任务复杂性，定义为实现目标性能所需的最短程序长度。他们的实验估计了数学推理、机器翻译和指令跟随的任务复杂性，结果表明，预训练模型显著降低了实现高性能所需的复杂性，通常仅需几千字节，而后训练则进一步将这种复杂性降低了几个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知型类人跑酷：通过运动匹配链式动态人类技能</div>
<div class="mono" style="margin-top:8px">尽管近期类人运动的进展已实现了在多种地形上的稳定行走，但捕捉高度动态人类动作的灵活性和适应性仍然是一个开放的挑战。特别是在复杂环境中的灵活跑酷，不仅需要低级别的鲁棒性，还需要类人运动的表现力、长时间技能组合和感知驱动的决策能力。本文提出了感知型类人跑酷（PHP），这是一个模块化框架，使类人机器人能够自主执行基于视觉的长时间跑酷，穿越具有挑战性的障碍课程。我们的方法首先利用运动匹配，将其表述为特征空间中的最近邻搜索，以将重新定向的原子人类技能组合成长时间的运动轨迹。该框架能够灵活组合和顺畅过渡复杂的技能链，同时保持动态人类动作的优雅和流畅性。接下来，我们为这些组合动作训练运动跟踪强化学习（RL）专家策略，并使用DAgger和RL的组合将其提炼为单一的基于深度的多技能学生策略。关键是，感知与技能组合的结合使得自主的、上下文感知的决策成为可能：机器人仅使用机载深度传感器和离散的2D速度指令，选择并执行跨越、攀爬、翻越或滚落不同几何形状和高度的障碍物。我们通过在Unitree G1类人机器人上进行广泛的现实世界实验来验证我们的框架，展示了高度动态的跑酷技能，如攀爬高达1.25米（占机器人高度的96%）的高障碍物，以及对实时障碍扰动的闭环适应的长时间多障碍物穿越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of replicating the agility and adaptability of dynamic human motions in humanoid robots, particularly in the context of parkour. The authors present a modular framework called Perceptive Humanoid Parkour (PHP), which utilizes motion matching to compose human skills into long-horizon kinematic trajectories and employs reinforcement learning to train a multi-skill policy. Experimental results show that the framework enables a humanoid robot to autonomously navigate complex obstacle courses, successfully performing dynamic parkour maneuvers such as climbing obstacles up to 1.25 meters high and adapting to real-time changes in the environment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使类人机器人能够在复杂环境中执行灵活的跑酷动作的挑战，这不仅需要稳健性，还需要类人运动表现和感知驱动的决策能力。作者提出了一种名为感知类人跑酷（PHP）的模块化框架，该框架利用运动匹配将人类技能组合成长时间的运动轨迹，从而实现复杂技能之间的平滑过渡。实验结果表明，该框架使类人机器人能够自主导航具有挑战性的障碍课程，成功攀爬高达1.25米的障碍物，并能够适应环境中的实时变化。</div>
</details>
</div>
<div class="card">
<div class="title">Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence</div>
<div class="meta-line">Authors: Alisa Vinogradova, Vlad Vinogradov, Luba Greenwood, Ilya Yasny, Dmitry Kobyzev, Shoman Kasbekar, Kong Nguyen, Dmitrii Radkevich, Roman Doronin, Andrey Doronichev</div>
<div class="meta-line">First: 2026-02-16T18:57:49+00:00 · Latest: 2026-02-17T18:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15019v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15019v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface &quot;under-the-radar&quot; assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today&#x27;s Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全球猎寻：用于投资、业务发展和竞争情报的药物资产侦查广泛搜索AI代理</div>
<div class="mono" style="margin-top:8px">生物制药创新已发生转变：许多新药资产现在源自美国以外，主要通过区域性非英语渠道披露。最近的数据表明，超过85%的专利申请源自美国以外，中国几乎占全球总量的一半。越来越多的学术成果也来自非美国。行业估计中国在全球药物开发中占30%，涵盖1200多个新候选药物。在这个高风险环境中，未能发现“低调”资产给投资者和业务发展团队带来了数十亿美元的风险，使资产侦查成为一个覆盖至关重要的竞争领域，速度和完整性驱动价值。然而，今天的深度研究AI代理在跨异构多语言来源实现高召回发现方面仍落后于人类专家。我们提出了一种药物资产侦查的基准方法论和一个调优的基于树的自学习Bioptic代理，旨在实现完整的、无幻觉的侦查。我们使用多语言多代理管道构建了一个具有挑战性的完整性基准：复杂的用户查询与主要不在美国雷达范围内的真实资产配对。为了反映真实交易的复杂性，我们收集了来自专家投资者、业务发展和风险投资专业人士的筛选查询，并将其用作先验条件生成基准查询。对于评分，我们使用经过专家意见校准的LLM作为评判标准。在这个基准上，我们的Bioptic代理实现了79.7%的F1分数，超过了Claude Opus 4.6（56.2%）、Gemini 3 Pro + Deep Research（50.6%）、OpenAI GPT-5.2 Pro（46.6%）、Perplexity Deep Research（44.2%）和Exa Websets（26.9%）。随着计算能力的增加，性能急剧提高，支持了更多计算带来更好结果的观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for effective drug asset scouting in a rapidly evolving bio-pharmaceutical landscape, where a significant portion of new drug assets originates outside the U.S., particularly in China. The authors propose a benchmarking methodology and develop a tree-based self-learning Bioptic Agent designed to enhance the discovery of drug assets across diverse, multilingual sources without hallucination. Experimental results demonstrate that the Bioptic Agent achieves a 79.7% F1 score on a challenging completeness benchmark, significantly outperforming other AI models, indicating its effectiveness in identifying under-the-radar assets critical for investors and business development teams.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于识别来自美国以外的药物资产的重要性日益增加，尤其是现在超过85%的专利申请来自非美国来源，中国是一个重要的贡献者。作者提出了一种基准测试方法和一种基于树的自学习Bioptic Agent，旨在提高在多语言来源中发现药物资产的能力，且不出现幻觉。实验结果表明，Bioptic Agent在一个具有挑战性的完整性基准测试中达到了79.7%的F1分数，显著优于其他AI代理，表明增加计算资源可以改善药物资产侦察的性能。</div>
</details>
</div>
<div class="card">
<div class="title">stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation</div>
<div class="meta-line">Authors: Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur, Yann LeCun, Randall Balestriero</div>
<div class="meta-line">First: 2026-02-09T18:04:22+00:00 · Latest: 2026-02-17T18:58:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08968v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定世界模型v1：可重复的世界建模研究与评估</div>
<div class="mono" style="margin-top:8px">世界模型已成为学习环境动态的紧凑预测表示的强大范式，使代理能够推理、规划并超越直接经验进行概括。尽管最近对世界模型的兴趣增加，但大多数可用实现仍然是特定于出版物的，严重限制了其可重用性，增加了错误的风险，并降低了评估标准化。为了解决这些问题，我们引入了稳定世界模型（SWM），这是一个模块化、经过测试和文档化的世界模型研究生态系统，提供高效的数据收集工具、标准化环境、规划算法和基线实现。此外，SWM中的每个环境都支持可控的变化因素，包括视觉和物理属性，以支持鲁棒性和持续学习研究。最后，我们通过使用SWM研究DINO-WM中的零-shot鲁棒性来展示其效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing World Model implementations, which are often specific to publications and hinder reusability and standardization. The authors developed stable-worldmodel (SWM), a modular and well-documented ecosystem that includes tools for data collection, standardized environments, and planning algorithms. Key experimental findings demonstrate the effectiveness of SWM in studying zero-shot robustness in DINO-WM, showcasing its potential for advancing research in world modeling and continual learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有世界模型实现的局限性，这些实现通常与特定出版物相关，缺乏可重用性和标准化。作者提出了stable-worldmodel (SWM)，这是一个模块化且文档齐全的世界建模研究生态系统，包含数据收集工具、标准化环境和规划算法。关键实验结果表明，SWM在研究DINO-WM的零-shot鲁棒性方面的有效性，展示了其通过可控的环境属性变化支持鲁棒性和持续学习研究的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing</div>
<div class="meta-line">Authors: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad</div>
<div class="meta-line">First: 2026-02-17T18:58:04+00:00 · Latest: 2026-02-17T18:58:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15823v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15823v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CrispEdit：用于可扩展非破坏性LLM编辑的低曲率投影</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）编辑中的一个核心挑战是能力保持：成功改变目标行为的方法可能会悄悄地操纵编辑代理并破坏一般能力，产生类似于代理/奖励黑客的退化行为。我们提出了CrispEdit，一种可扩展且有原则的二阶编辑算法，将能力保持视为一个明确的约束，统一并概括了几种现有的编辑方法。CrispEdit将编辑公式化为约束优化，并通过将编辑更新投影到能力损失景观的低曲率子空间来强制执行约束。CrispEdit的核心是通过Bregman散度表达能力约束，其二次形式精确地产生高斯-牛顿Hessian，即使基础模型未训练到收敛。我们使用Kronecker分解的近似曲率（K-FAC）和一种新颖的无矩阵投影器，使这一二阶过程在LLM规模上高效，利用Kronecker结构避免构建庞大的投影矩阵。在标准模型编辑基准测试中，CrispEdit在保持能力退化平均低于1%的同时，实现了高编辑成功率，显著优于之前的编辑器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of capability preservation in large language model (LLM) editing, where existing methods can inadvertently corrupt general capabilities. The authors introduce CrispEdit, a second-order editing algorithm that formulates editing as constrained optimization, ensuring capability preservation by projecting updates onto a low-curvature subspace of the capability-loss landscape. Experimental results demonstrate that CrispEdit achieves high editing success rates while maintaining capability degradation below 1% on average across various datasets, marking a significant improvement over previous editing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLM）编辑中的能力保持问题，现有方法可能会无意中降低一般能力。作者提出了CrispEdit，这是一种二阶编辑算法，将编辑形式化为约束优化，通过将更新投影到能力损失景观的低曲率子空间来确保能力保持。实验结果表明，CrispEdit在各种数据集上实现了高编辑成功率，同时保持能力降级平均低于1%，显著优于之前的编辑方法。</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics</div>
<div class="meta-line">Authors: Anna Zimmel, Paul Setinek, Gianluca Galletti, Johannes Brandstetter, Werner Zellinger</div>
<div class="meta-line">First: 2026-02-17T18:55:18+00:00 · Latest: 2026-02-17T18:55:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过D-最优统计稳定高维仿真代理的测试时间适应</div>
<div class="mono" style="margin-top:8px">机器学习代理在工程中越来越多地用于加速昂贵的仿真，但训练和部署之间的分布变化常常导致严重的性能下降（例如，未见的几何形状或配置）。测试时间适应（TTA）可以缓解这种变化，但现有方法主要针对具有结构化输出和视觉对齐输入输出关系的低维分类，导致在高维、非结构化和回归问题中不稳定。我们通过提出一个基于存储最大信息（D-最优）统计的TTA框架来解决这一挑战，该框架在测试时共同实现稳定适应和原则性参数选择。当应用于预训练的仿真代理时，我们的方法在可忽略的计算成本下实现了高达7%的分布外改进。据我们所知，这是首次系统性展示高维仿真回归和生成设计优化的有效TTA，已在SIMSHIFT和EngiBench基准上验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of performance degradation in machine learning surrogates used for engineering simulations due to distribution shifts between training and deployment. The authors propose a Test-Time Adaptation (TTA) framework that utilizes D-optimal statistics to enable stable adaptation and effective parameter selection during testing. Experimental results show that this method achieves up to 7% improvement in out-of-distribution performance with minimal computational overhead, marking a significant advancement in TTA for high-dimensional simulation regression and generative design optimization, as validated on the SIMSHIFT and EngiBench benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于训练和部署之间的分布变化导致工程模拟中机器学习代理性能下降的问题。作者提出了一种基于D-最优统计的测试时适应（TTA）框架，以实现测试期间的稳定适应和有效参数选择。实验结果表明，该方法在计算开销极小的情况下，能够实现高达7%的分布外性能提升，标志着在高维模拟回归和生成设计优化中，TTA的显著进展，已在SIMSHIFT和EngiBench基准上得到验证。</div>
</details>
</div>
<div class="card">
<div class="title">VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation</div>
<div class="meta-line">Authors: Hui Ren, Yuval Alaluf, Omer Bar Tal, Alexander Schwing, Antonio Torralba, Yael Vinker</div>
<div class="meta-line">First: 2026-02-17T18:55:03+00:00 · Latest: 2026-02-17T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频草图生成器：视频模型先验使多样化的顺序草图生成成为可能</div>
<div class="mono" style="margin-top:8px">草图绘制本质上是一个顺序过程，其中笔画以有意义的顺序绘制，以探索和完善想法。然而，大多数生成模型将草图视为静态图像，忽视了创意绘图背后的时间结构。我们提出了一种数据高效的顺序草图生成方法，该方法将预训练的文本到视频扩散模型适应于生成草图过程。我们的关键见解是，大型语言模型和视频扩散模型在此任务中提供了互补的优势：LLM提供语义规划和笔画排序，而视频扩散模型则作为强大的渲染器，生成高质量、时间一致的视觉效果。我们通过将草图表示为短视频，其中笔画在空白画布上逐步绘制，并由文本指定的排序指令引导，来利用这一点。我们引入了一种两阶段微调策略，将笔画排序的学习与草图外观的学习解耦。笔画排序使用具有受控时间结构的合成形状组合进行学习，而视觉外观则从少至七个手动创作的草图过程中提炼，这些过程捕捉了全局绘图顺序和单个笔画的连续形成。尽管人类绘制的草图数据极为有限，我们的方法仍能生成高质量的顺序草图，紧密遵循文本指定的排序，同时展现丰富的视觉细节。我们进一步通过扩展如笔刷风格条件和自回归草图生成，展示了我们方法的灵活性，增强了可控性和互动协作绘图的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the sequential nature of sketch generation, which is often overlooked by existing generative models that treat sketches as static images. The authors propose a data-efficient method that adapts pretrained text-to-video diffusion models to create sketching processes, leveraging the strengths of large language models for semantic planning and stroke ordering, alongside video diffusion models for high-quality rendering. The experimental results show that their approach, which utilizes a two-stage fine-tuning strategy and minimal human-drawn data, successfully generates high-quality sequential sketches that adhere to specified text orderings while maintaining rich visual detail, and also allows for extensions like brush style conditioning and autoregressive sketch generation for improved interactivity and control.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过认识到素描是一个需要时间结构的顺序活动，从而增强素描生成的过程，而现有的生成模型往往忽视这一点。作者提出了一种数据高效的方法，利用预训练的文本到视频扩散模型生成素描，将素描视为短视频，其中笔画根据文本指令以指定顺序绘制。关键实验结果表明，他们的方法成功地结合了大型语言模型进行语义规划和视频扩散模型进行渲染，能够在极少的人类绘图数据下生成高质量的顺序素描，并且遵循指定的顺序，同时允许额外的功能，如画笔风格调节和互动绘图能力。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning</div>
<div class="meta-line">Authors: Oswin So, Eric Yang Yu, Songyuan Zhang, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-17T18:53:31+00:00 · Latest: 2026-02-17T18:53:31+00:00</div>
<div class="meta-line">Comments: ICLR 2026. The project page can be found at https://oswinso.xyz/fge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用强化学习解决具有未知可行性的参数鲁棒避免问题</div>
<div class="mono" style="margin-top:8px">深度强化学习（RL）的最新进展在高维控制任务中取得了强劲的结果，但将RL应用于可达性问题时存在根本的不匹配：可达性旨在最大化系统可以无限安全保持的状态集合，而RL则优化用户指定分布下的期望回报。这种不匹配可能导致在仍然处于安全集合中的低概率状态上表现不佳的策略。一种自然的替代方法是将问题框架化为对指定初始状态、动态和安全集合的初始条件集进行鲁棒优化，但该问题是否有解取决于指定集合的可行性，而这一点在事先是未知的。我们提出了可行性引导探索（FGE）方法，该方法同时识别在其下存在安全策略的可行初始条件子集，并学习在该初始条件集上解决可达性问题的策略。实证结果表明，FGE学习的策略在MuJoCo模拟器和Kinetix模拟器中对具有挑战性的初始条件的覆盖率比现有最佳方法高出50%以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of applying deep reinforcement learning (RL) to reachability problems, where traditional RL methods may yield suboptimal policies for low-probability states within a safe set. To tackle this issue, the authors introduce Feasibility-Guided Exploration (FGE), a method that identifies feasible initial conditions and concurrently learns a policy for the reachability problem. Experimental results show that FGE achieves over 50% more coverage than existing methods in complex scenarios within the MuJoCo and Kinetix simulators.</div>
<div class="mono" style="margin-top:8px">本研究解决了将深度强化学习（RL）应用于可达性问题的挑战，目标是最大化确保无限安全的状态集，这与RL关注的期望回报形成对比。作者提出了一种新方法，称为可行性引导探索（FGE），该方法在识别安全策略的可行初始条件的同时，学习可达性问题的策略。实验结果表明，FGE在MuJoCo和Kinetix模拟器中的复杂场景下，覆盖率比现有方法高出50%以上。</div>
</details>
</div>
<div class="card">
<div class="title">Developing AI Agents with Simulated Data: Why, what, and how?</div>
<div class="meta-line">Authors: Xiaoran Liu, Istvan David</div>
<div class="meta-line">First: 2026-02-17T18:53:27+00:00 · Latest: 2026-02-17T18:53:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用模拟数据开发AI代理：为什么、什么以及如何？</div>
<div class="mono" style="margin-top:8px">由于数据量和质量不足仍然是现代子符号AI采用的主要障碍，合成数据生成技术需求旺盛。模拟提供了一种适合的、系统的方法来生成多样的合成数据。本章向读者介绍了基于模拟的合成数据生成的关键概念、好处和挑战，以及一个参考框架，用于描述、设计和分析基于数字双胞胎的AI模拟解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the challenge posed by insufficient data volume and quality in the adoption of modern subsymbolic AI. The authors employ simulation as a systematic method for generating diverse synthetic data to address this issue. Key findings indicate that simulation-based synthetic data generation can effectively support AI training by providing a structured framework to describe, design, and analyze digital twin-based AI simulation solutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是现代子符号人工智能在采用过程中面临的数据量和质量不足的问题。作者采用模拟作为生成多样化合成数据的系统方法，以解决这一问题。主要发现强调了基于模拟的合成数据生成的概念、优势和挑战，以及用于设计和分析数字双胞胎基础的人工智能模拟解决方案的参考框架。</div>
</details>
</div>
<div class="card">
<div class="title">Token-Based Audio Inpainting via Discrete Diffusion</div>
<div class="meta-line">Authors: Tali Dror, Iftach Shoham, Moshe Buchris, Oren Gal, Haim Permuter, Gilad Katz, Eliya Nachmani</div>
<div class="meta-line">First: 2025-07-11T06:25:49+00:00 · Latest: 2026-02-17T18:53:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08333v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.08333v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training. Visit our project page for examples and code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于令牌的音频修复通过离散扩散</div>
<div class="mono" style="margin-top:8px">音频修复旨在恢复降质录音中缺失的片段。以往的基于扩散的方法在缺失区域较大时表现不佳。我们提出了首个将离散扩散应用于预训练音频令牌器的令牌化音乐表示的方法，实现了长时间缺口的稳定和语义一致的修复。我们的方法进一步结合了两种训练方法：基于导数的正则化损失，强制平滑的时间动态，以及基于跨度的吸收过渡，在扩散过程中提供结构化的损坏。在MusicNet和MAESTRO数据集上进行的实验显示，我们的方法在150毫秒及以上的缺口长度范围内，始终优于强基线。这项工作推动了音乐音频修复，并为离散扩散模型训练引入了新的方向。请访问我们的项目页面以获取示例和代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve audio inpainting techniques for restoring missing segments in degraded recordings, particularly when the gaps are large. The authors propose a novel method that utilizes discrete diffusion on tokenized music representations from a pre-trained audio tokenizer, which allows for stable and semantically coherent restoration of long gaps. Experimental results demonstrate that their approach outperforms existing strong baselines on the MusicNet and MAESTRO datasets, effectively handling gaps of 150 ms and longer, thus contributing to advancements in musical audio restoration and offering new insights for discrete diffusion model training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善音频修复技术，以恢复降质录音中缺失的片段，特别是当缺失区域较大时。作者提出了一种新方法，利用从预训练音频标记器获得的标记音乐表示上的离散扩散，能够稳定且语义一致地修复长时间的缺口。在MusicNet和MAESTRO数据集上的实验结果表明，该方法显著优于现有方法，尤其是在150毫秒及以上的缺口情况下，从而推动了音乐音频修复的进展，并为离散扩散模型训练提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Avey-B</div>
<div class="meta-line">Authors: Devang Acharya, Mohammad Hammoud</div>
<div class="meta-line">First: 2026-02-17T18:50:40+00:00 · Latest: 2026-02-17T18:50:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15814v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention&#x27;s ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Avey-B</div>
<div class="mono" style="margin-top:8px">紧凑的预训练双向编码器在计算和内存预算紧张的情况下仍然是工业NLP的支柱。它们的有效性源于自注意力能够通过序列级并行性提供高质量的双向上下文化，这一点在BERT风格的架构中得到了普及。最近，Avey被引入作为一种自回归、无注意力的替代方案，自然适应仅编码器的改编。在本文中，我们为仅编码器范式重新构造了Avey，并提出了对其架构的几项创新，包括解耦的静态和动态参数化、以稳定性为导向的归一化和神经压缩。结果表明，这种重新构造的架构与四种广泛使用的基于Transformer的编码器相比表现良好，在标准的标记分类和信息检索基准上始终优于它们，同时在长上下文中更有效地扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of pretrained bidirectional encoders in natural language processing, particularly under constraints of compute and memory. The authors reformulate the Avey model, an autoregressive and attention-free architecture, for an encoder-only application, introducing innovations such as decoupled parameterizations and stability-oriented normalization. Experimental results demonstrate that the modified Avey architecture outperforms four established Transformer-based encoders on token-classification and information-retrieval tasks, while also exhibiting improved scalability for longer contexts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高预训练双向编码器在自然语言处理中的效率，特别是在计算和内存的限制下。作者将Avey模型（一种自回归和无注意力的架构）重新构造为仅编码器的方法，引入了去耦参数化和稳定性导向的归一化等创新。实验结果表明，新的架构在标准的标记分类和信息检索基准测试中优于四种流行的基于Transformer的编码器，同时在处理更长上下文时也具有更高的扩展效率。</div>
</details>
</div>
<div class="card">
<div class="title">Horizon Imagination: Efficient On-Policy Rollout in Diffusion World Models</div>
<div class="meta-line">Authors: Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-08T16:07:04+00:00 · Latest: 2026-02-17T18:50:34+00:00</div>
<div class="meta-line">Comments: This paper will be published in the ICLR 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08032v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08032v2">PDF</a> · <a href="https://github.com/leor-c/horizon-imagination">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地平线想象：扩散世界模型中的高效在线策略展开</div>
<div class="mono" style="margin-top:8px">我们研究了基于扩散的世界模型用于强化学习，这些模型提供了高生成保真度，但在控制方面面临关键的效率挑战。目前的方法要么在推理时需要重量级模型，要么依赖高度顺序的想象，这两者都带来了巨大的计算成本。我们提出了地平线想象（HI），这是一种针对离散随机策略的在线想象过程，可以并行去噪多个未来观测。HI结合了一种稳定机制和一种新颖的采样计划，将去噪预算与应用去噪的有效时间范围解耦，同时支持子帧预算。在Atari 100K和Craftium上的实验表明，我们的方法在去噪步骤减半的子帧预算下保持了控制性能，并在不同的时间表下实现了更优的生成质量。代码可在https://github.com/leor-c/horizon-imagination获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the efficiency challenges faced by diffusion-based world models in reinforcement learning, which typically require heavy computational resources for inference and sequential imagination. The authors introduce Horizon Imagination (HI), an on-policy imagination process that allows for parallel denoising of multiple future observations, incorporating a stabilization mechanism and a novel sampling schedule that separates the denoising budget from the effective horizon. Experimental results on Atari 100K and Craftium demonstrate that HI maintains control performance with a reduced denoising budget and achieves improved generation quality across various schedules.</div>
<div class="mono" style="margin-top:8px">本研究解决了扩散基础世界模型在强化学习中面临的效率挑战，这些模型通常需要重型模型或顺序想象，导致高计算成本。作者提出了Horizon Imagination (HI)，一种并行去噪多个未来观察的在线想象过程，具有稳定机制和新颖的采样调度，能够将去噪预算与有效时间范围分离。对Atari 100K和Craftium的实验结果表明，HI在减少去噪预算的情况下保持了控制性能，并在不同调度下实现了更好的生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Agnostic Continual Learning for Chest Radiograph Classification</div>
<div class="meta-line">Authors: Muthu Subash Kavitha, Anas Zafar, Amgad Muneer, Jia Wu</div>
<div class="meta-line">First: 2026-02-17T18:47:30+00:00 · Latest: 2026-02-17T18:47:30+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15811v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无任务依赖的胸部X光分类持续学习</div>
<div class="mono" style="margin-top:8px">胸部X光分类器的临床部署需要能够在新数据集可用时更新的模型，而无需在先前观察到的数据上重新训练或降低验证性能。我们首次研究了胸部X光分类的任务增量持续学习设置，其中异构胸部X光数据集顺序到达，推理时没有任务标识符。我们提出了一种基于持续适配器的路由学习策略（CARL-XRay），该策略保持固定的高容量主干，并逐步分配轻量级的任务特定适配器和分类头。潜在任务选择器在任务适应特征上运行，并利用通过紧凑原型和特征级经验重放保留的当前和历史上下文。该设计支持在顺序更新中稳定的任务识别和适应，同时避免原始图像存储。在大规模公共胸部X光数据集上的实验表明，在持续数据集摄取下，表现出强大的性能保留和可靠的任务感知推理。CARL-XRay在任务未知部署下优于联合训练，达到更高的路由准确率（75.0% vs. 62.5%），同时在具有真实任务身份的oracle设置中保持竞争性的诊断性能，AUROC为0.74，在任务未知推理下为0.75，使用的可训练参数显著更少。最后，所提出的框架为持续临床部署提供了联合训练和重复完全重新训练的实用替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop chest radiograph classifiers that can be updated with new datasets without retraining on previously seen data or losing validated performance. The authors introduce a continual adapter-based routing learning strategy called CARL-XRay, which employs a fixed high-capacity backbone while incrementally adding lightweight task-specific adapters and classifier heads. Experimental results on large-scale public chest radiograph datasets show that CARL-XRay achieves robust performance retention and reliable task-aware inference, outperforming joint training in task-unknown scenarios with a routing accuracy of 75.0% compared to 62.5%, while maintaining competitive diagnostic performance with AUROC scores of 0.74 and 0.75 under different settings, all with significantly fewer trainable parameters.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于开发能够在不需要对先前数据进行重新训练的情况下更新的新数据集的胸部放射图分类器，从而确保验证性能保持不变。作者提出了一种称为CARL-XRay的持续适配器基础路由学习策略，该策略利用固定的高容量主干，同时逐步添加轻量级特定任务的适配器和分类头。在大规模公共数据集上的实验结果表明，CARL-XRay的路由准确率为75.0%，相比之下，联合训练的准确率为62.5%；同时在oracle设置下AUROC得分为0.74，在任务未知条件下为0.75，且所需的可训练参数显著减少。</div>
</details>
</div>
<div class="card">
<div class="title">Decision Quality Evaluation Framework at Pinterest</div>
<div class="meta-line">Authors: Yuqi Tian, Robert Paine, Attila Dobi, Kevin O&#x27;Sullivan, Aravindh Manickavasagam, Faisal Farooq</div>
<div class="meta-line">First: 2026-02-17T18:45:55+00:00 · Latest: 2026-02-17T18:45:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework&#x27;s practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Pinterest的决策质量评估框架</div>
<div class="mono" style="margin-top:8px">在线平台需要强大的系统来大规模执行内容安全政策。这些系统的一个关键组成部分是评估人类代理和大型语言模型（LLMs）所做的审查决策的质量。然而，由于成本、规模和可信度之间的固有权衡，以及不断变化的政策的复杂性，这种评估具有挑战性。为了解决这个问题，我们提出了一个在Pinterest开发和部署的全面决策质量评估框架。该框架以由主题专家（SMEs）策划的高可信度黄金集（GDS）为中心，作为真实基准。我们引入了一个自动化智能采样管道，利用倾向评分有效扩展数据集覆盖范围。我们展示了该框架在几个关键领域的实际应用：基准测试各种LLM代理的成本-性能权衡，建立数据驱动的提示优化的严格方法，管理复杂的政策演变，以及通过持续验证确保政策内容普遍性指标的完整性。该框架使得从主观评估转变为数据驱动和定量的内容安全系统管理实践成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for effective evaluation systems for content moderation decisions on online platforms, particularly in balancing cost, scale, and trustworthiness. The authors developed a Decision Quality Evaluation Framework at Pinterest, which utilizes a high-trust Golden Set curated by subject matter experts as a benchmark for evaluating moderation quality. Key findings include the framework&#x27;s successful application in benchmarking cost-performance trade-offs of various Large Language Models, optimizing data-driven prompts, managing policy evolution, and ensuring the accuracy of policy content metrics through continuous validation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是在线平台需要有效的评估系统来评估内容安全政策下的审核决策。作者在Pinterest开发了一个决策质量评估框架，该框架利用由主题专家策划的高信任度黄金集作为评估基准。主要实验结果表明，该框架成功地促进了对各种大型语言模型的成本-性能权衡的基准测试，建立了数据驱动的提示优化方法，管理复杂的政策演变，并通过持续验证确保政策内容指标的完整性。</div>
</details>
</div>
<div class="card">
<div class="title">Should You Use Your Large Language Model to Explore or Exploit?</div>
<div class="meta-line">Authors: Keegan Harris, Aleksandrs Slivkins</div>
<div class="meta-line">First: 2025-01-31T23:42:53+00:00 · Latest: 2026-02-17T18:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00225v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.00225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. While previous work has largely study the ability of LLMs to solve combined exploration-exploitation tasks, we take a more systematic approach and use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that reasoning models show the most promise for solving exploitation tasks, although they are still too expensive or too slow to be used in many practical settings. Motivated by this, we study tool use and in-context summarization using non-reasoning models. We find that these mitigations may be used to substantially improve performance on medium-difficulty tasks, however even then, all LLMs we study perform worse than a simple linear regression, even in non-linear settings. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你应该使用大型语言模型进行探索还是开发？</div>
<div class="mono" style="margin-top:8px">我们评估当前一代大型语言模型（LLMs）在面对探索-开发权衡时帮助决策代理的能力。虽然之前的研究主要集中在LLMs解决结合探索-开发任务的能力上，但我们采取了更系统的方法，在各种（上下文）赌博任务中使用LLMs进行探索和开发。我们发现推理模型在解决开发任务方面最具潜力，尽管在许多实际环境中仍然太昂贵或太慢。因此，我们研究了使用非推理模型的工具使用和上下文摘要。我们发现这些缓解措施可以显著提高中等难度任务的表现，但即便如此，我们研究的所有LLMs在非线性环境中表现仍不如简单的线性回归。另一方面，我们发现LLMs确实有助于探索具有内在语义的大动作空间，通过建议合适的候选者进行探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the effectiveness of large language models (LLMs) in assisting decision-making agents with exploration-exploitation tradeoffs, motivated by the need for systematic evaluation beyond previous combined task approaches. The researchers employed LLMs in isolated exploration and exploitation contexts within various bandit tasks. The findings reveal that while reasoning models excel in exploitation tasks, they are often impractical due to cost and speed; however, non-reasoning models can enhance performance on medium-difficulty tasks through tool use and in-context summarization, although all LLMs underperform compared to simple linear regression. Conversely, LLMs demonstrate utility in exploring large action spaces with inherent semantics by proposing suitable candidates for exploration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在帮助决策代理处理探索与利用权衡方面的有效性，填补了以往研究主要集中于综合任务的空白。作者系统地评估了LLMs在各种上下文赌博任务中单独进行探索和利用的能力。研究发现，尽管推理模型在利用任务中表现出色，但其高成本和慢速性能限制了实际应用；然而，使用工具和上下文摘要的非推理模型可以提高中等难度任务的表现，尽管所有LLMs的表现仍低于简单线性回归。相反，LLMs在探索具有内在语义的大型动作空间方面是有益的，能够建议合适的探索候选。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety</div>
<div class="meta-line">Authors: Max Springer, Chung Peng Lee, Blossom Metevier, Jane Castleman, Bohdan Turbal, Hayoung Jung, Zeyu Shen, Aleksandra Korolova</div>
<div class="meta-line">First: 2026-02-17T18:39:15+00:00 · Latest: 2026-02-17T18:39:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15799v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐崩溃的几何学：当微调破坏安全性时</div>
<div class="mono" style="margin-top:8px">在良性任务上微调对齐的语言模型会不可预测地降低安全防护，即使训练数据不包含有害内容且开发者没有对抗意图。我们展示了当前的解释，即微调更新应与高维参数空间中的安全关键方向正交，提供了错误的安慰：我们表明这种正交性在梯度下降的动态下是结构不稳定的并会崩溃。我们通过一种新颖的几何分析解决了这个问题，证明对齐集中在具有尖锐曲率的低维子空间中，形成了一种脆弱的结构，一级方法无法检测或防御。虽然初始微调更新确实可能避免这些子空间，但微调损失的曲率产生的二阶加速系统性地将轨迹引导到对齐敏感区域。我们通过对齐不稳定性条件形式化了这一机制，这三个几何属性在共同满足时会导致安全性下降。我们的主要结果建立了一个四次扩展法则：对齐损失随着训练时间的四次方增长，受对齐几何的尖锐度和微调任务与安全关键参数之间的曲率耦合强度的支配。这些结果揭示了当前安全范式中的结构盲点。主流的安全微调方法仅解决了一个根本动态问题的初始快照。对齐脆弱性不是一个需要修补的缺陷；它是曲面上梯度下降的内在几何属性。我们的结果激励了曲率感知方法的发展，并希望进一步推动对齐安全分析从反应式红队转向开放权重模型部署的预测性诊断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the unexpected degradation of safety guardrails in fine-tuned language models, even when trained on benign data and without malicious intent. The authors employ a novel geometric analysis to demonstrate that the common belief in orthogonal updates to safety-critical directions is flawed, as this orthogonality is unstable under gradient descent dynamics. Their findings reveal that alignment loss increases with the fourth power of training time due to the sharp curvature of alignment geometry, highlighting a critical structural vulnerability in current safety practices and suggesting the need for curvature-aware methods in alignment safety analysis.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无害数据上微调语言模型时，安全防护措施意外退化的现象，即使开发者没有敌意。作者采用新颖的几何分析方法，证明了普遍认为的微调更新应与安全关键方向正交的观点是错误的，因为这种正交性在梯度下降动态下是不稳定的。他们建立了一个定律，表明对齐损失随着训练时间的四次方增长，这揭示了当前安全范式中的关键结构脆弱性，并强调了在对齐安全分析中需要采用考虑曲率的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning depth-3 circuits via quantum agnostic boosting</div>
<div class="meta-line">Authors: Srinivasan Arunachalam, Arkopal Dutt, Alexandru Gheorghiu, Michael de Oliveira</div>
<div class="meta-line">First: 2025-09-17T22:28:29+00:00 · Latest: 2026-02-17T18:38:29+00:00</div>
<div class="meta-line">Comments: 53 pages; Typos fixed for depth-3 circuits result</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14461v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.14461v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We initiate the study of quantum agnostic learning of phase states with respect to a function class $\mathsf{C}\subseteq \{c:\{0,1\}^n\rightarrow \{0,1\}\}$: given copies of an unknown $n$-qubit state $|ψ\rangle$ which has fidelity $\textsf{opt}$ with a phase state $|φ_c\rangle=\frac{1}{\sqrt{2^n}}\sum_{x\in \{0,1\}^n}(-1)^{c(x)}|x\rangle$ for some $c\in \mathsf{C}$, output $|φ\rangle$ which has fidelity $|\langle φ| ψ\rangle|^2 \geq \textsf{opt}-\varepsilon$. To this end, we give agnostic learning protocols for the following classes: (i) Size-$t$ decision trees which runs in time $\textsf{poly}(n,t,1/\varepsilon)$. This also implies $k$-juntas can be agnostically learned in time $\textsf{poly}(n,2^k,1/\varepsilon)$. (ii) $s$-term DNF formulas in time $\textsf{poly}(n,(s/\varepsilon)^{\log \log (s/\varepsilon) \cdot \log(1/\varepsilon)})$.
  Our main technical contribution is a quantum agnostic boosting protocol which converts a weak agnostic learner, which outputs a parity state $|φ\rangle$ such that $|\langle φ|ψ\rangle|^2\geq \textsf{opt}/\textsf{poly}(n)$, into a strong learner which outputs a superposition of parity states $|φ&#x27;\rangle$ such that $|\langle φ&#x27;|ψ\rangle|^2\geq \textsf{opt} - \varepsilon$.
  Using quantum agnostic boosting, we obtain a $n^{O(\log(n/\varepsilon) \cdot \log \log n)}$-time algorithm for $\varepsilon$-learning $\textsf{poly}(n)$-sized depth-$3$ circuits (consisting of $\textsf{AND}$, $\textsf{OR}$, $\textsf{NOT}$ gates) in the uniform $\textsf{PAC}$ model given quantum examples. Classically, obtaining an algorithm with a similar complexity has been an open question in the $\textsf{PAC}$ model and our work answers this given quantum examples.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the challenge of quantum agnostic learning of phase states, specifically aiming to improve the fidelity of output states in relation to given unknown n-qubit states. The authors develop a quantum agnostic boosting protocol that enhances a weak learner into a strong learner capable of producing a superposition of parity states with high fidelity. The key experimental finding is that this method enables an efficient algorithm for learning polynomial-sized depth-3 circuits in the uniform PAC model, achieving a time complexity of n^{O(log(n/ε)·log log n)}, which addresses a previously open question in classical learning theory when provided with quantum examples.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于量子无关学习相位状态的挑战，特别是旨在以高保真度学习决策树和DNF公式类的函数。作者开发了一种量子无关提升协议，将弱学习者增强为强学习者，使其能够输出具有更高保真度的奇偶态叠加。主要实验结果是建立了一种多项式时间算法，用于在均匀PAC模型中使用量子示例学习深度-3电路，这解决了关于相同背景下经典算法的一个未解问题。</div>
</details>
</div>
<div class="card">
<div class="title">GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance</div>
<div class="meta-line">Authors: Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche</div>
<div class="meta-line">First: 2026-01-16T17:02:00+00:00 · Latest: 2026-02-17T18:27:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11440v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11440v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenDA：通过无分类器扩散引导在复杂城市区域进行生成数据同化</div>
<div class="mono" style="margin-top:8px">城市风流重建对于评估空气质量、热量扩散和行人舒适度至关重要，但在仅有稀疏传感器数据的情况下仍然具有挑战性。我们提出了GenDA，一个生成数据同化框架，能够从有限观测中在非结构化网格上重建高分辨率风场。该模型采用基于多尺度图的扩散架构，经过计算流体动力学（CFD）模拟训练，并将无分类器引导解释为一种学习的后验重建机制：无条件分支学习几何感知的流动先验，而传感器条件分支在采样过程中注入观测约束。这种形式化方法使得在不重新训练的情况下实现障碍感知重建和在未见几何形状、风向和网格分辨率上的泛化。我们考虑使用相同的重建程序进行稀疏固定传感器和基于轨迹的观测。在与监督图神经网络（GNN）基线和经典降阶数据同化方法进行评估时，GenDA将相对均方根误差（RRMSE）降低了25-57%，并将结构相似性指数（SSIM）提高了23-33%。实验在英国布里斯托尔一个真实城市社区的雷诺兹平均纳维-斯托克斯（RANS）模拟上进行，特征雷诺兹数为$\mathrm{Re}\approx2\times10^{7}$，具有复杂的建筑几何和不规则地形。所提出的框架为在复杂领域进行生成的、几何感知的数据同化提供了一条可扩展的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve urban wind flow reconstruction for better assessment of air quality and pedestrian comfort, particularly when only limited sensor data is available. The authors propose GenDA, a generative data assimilation framework that utilizes a multiscale graph-based diffusion architecture trained on computational fluid dynamics simulations to reconstruct high-resolution wind fields on unstructured meshes. Experimental results show that GenDA significantly outperforms supervised graph neural network baselines and classical data assimilation methods, achieving a reduction in relative root-mean-square error by 25-57% and an increase in structural similarity index by 23-33% across various tested meshes, demonstrating its effectiveness in handling complex urban geometries and irregular terrains.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善城市风流重建，以更好地评估空气质量和行人舒适度，特别是在仅有有限传感器数据的情况下。作者提出了GenDA，这是一种生成数据同化框架，利用基于多尺度图的扩散架构，经过计算流体动力学模拟训练，从稀疏观测中重建高分辨率风场。实验结果表明，GenDA显著优于监督图神经网络基线和经典的降阶数据同化方法，在不同网格配置下，相对均方根误差降低了25-57%，结构相似性指数提高了23-33%，并通过对英国布里斯托尔复杂城市区域的模拟进行了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings</div>
<div class="meta-line">Authors: Suhyung Jang, Ghang Lee, Jaekun Lee, Hyunjun Lee</div>
<div class="meta-line">First: 2026-02-17T18:26:36+00:00 · Latest: 2026-02-17T18:26:36+00:00</div>
<div class="meta-line">Comments: 42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI&#x27;s semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI&#x27;s ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大型语言模型编码增强建筑语义在人工智能模型训练中的保留</div>
<div class="mono" style="margin-top:8px">准确表示建筑语义，包括通用对象类型和特定子类型，对于建筑、工程、施工和运营（AECO）行业中有效的人工智能模型训练至关重要。传统编码方法（如独热编码）往往无法传达密切相关子类型之间的细微关系，限制了人工智能的语义理解。为了解决这一局限，本研究提出了一种新颖的训练方法，采用大型语言模型（LLM）嵌入（如OpenAI GPT和Meta LLaMA）作为编码，以保留建筑语义中的细微差别。我们通过训练GraphSAGE模型对五个高层住宅建筑信息模型（BIM）中的42个建筑对象子类型进行分类来评估所提方法。测试了多种嵌入维度，包括原始高维LLM嵌入（1,536、3,072或4,096）和通过套娃表示模型生成的1,024维紧凑嵌入。实验结果表明，LLM编码优于传统的独热编码基线，其中llama-3（紧凑）嵌入的加权平均F1分数为0.8766，而独热编码为0.8475。结果强调了利用基于LLM的编码来增强人工智能解读复杂领域特定建筑语义的能力的前景。随着LLM和降维技术的能力不断发展，这种方法在AECO行业的语义阐述任务中具有广泛应用的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for accurate representation of building semantics in AI model training within the architecture, engineering, construction, and operation (AECO) industry, as conventional encoding methods often fail to capture nuanced relationships among subtypes. The study introduces a novel training approach that utilizes large language model (LLM) embeddings to better preserve these distinctions. Experimental results from training GraphSAGE models on 42 building object subtypes across five high-rise BIMs showed that LLM encodings significantly outperformed traditional one-hot encoding, with the llama-3 compacted embedding achieving a weighted average F1-score of 0.8766 compared to 0.8475 for one-hot encoding, indicating the effectiveness of LLM-based encodings in enhancing AI&#x27;s semantic comprehension of complex building semantics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善建筑语义在人工智能模型训练中的表示，这对建筑、工程、施工和运营（AECO）行业至关重要。研究提出了一种新方法，利用大型语言模型（LLM）嵌入来捕捉建筑对象子类型之间的细微关系，解决了传统编码方法（如独热编码）的局限性。实验结果表明，所提出的方法，特别是使用llama-3紧凑嵌入，达到了0.8766的加权平均F1分数，超越了独热编码基线的0.8475分，表明LLM编码在增强人工智能对复杂建筑语义理解方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">This human study did not involve human subjects: Validating LLM simulations as behavioral evidence</div>
<div class="meta-line">Authors: Jessica Hullman, David Broska, Huaman Sun, Aaron Shaw</div>
<div class="meta-line">First: 2026-02-17T18:18:38+00:00 · Latest: 2026-02-17T18:18:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15785v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>这项人类研究不涉及人类受试者：验证大型语言模型模拟作为行为证据</div>
<div class="mono" style="margin-top:8px">越来越多的文献使用大型语言模型（LLMs）作为合成参与者，在社会科学实验中生成具有成本效益和几乎即时的响应。然而，对于何时这些模拟支持对人类行为的有效推断，指导有限。我们对比了两种获取因果效应有效估计的策略，并澄清了每种策略适用于探索性研究与确认性研究的假设。启发式方法试图通过提示工程、模型微调和其他旨在减少LLM引起的不准确性的修复策略，建立模拟和观察到的人类行为是可互换的。尽管对许多探索性任务有用，启发式方法缺乏确认性研究通常所需的正式统计保证。相反，统计校准结合辅助人类数据与统计调整，以解释观察到的响应与模拟响应之间的差异。在明确的假设下，统计校准保持有效性，并提供比仅依赖人类参与者的实验更低成本的因果效应更精确的估计。然而，这两种方法的潜力取决于LLMs对相关人群的近似程度。当研究人员目光短浅地专注于用LLMs替代研究中的人类参与者时，我们考虑了被忽视的机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to evaluate the validity of using large language models (LLMs) as synthetic participants in social science experiments, particularly in terms of their ability to provide reliable insights into human behavior. The authors compare heuristic approaches, which involve prompt engineering and model fine-tuning to align simulated and observed behaviors, with statistical calibration methods that integrate human data to adjust for discrepancies. The findings indicate that while heuristic methods are useful for exploratory research, they lack the statistical rigor needed for confirmatory studies, whereas statistical calibration offers more precise causal effect estimates at a lower cost, provided that LLMs accurately represent the target populations. The study highlights the need for a broader perspective on the use of LLMs beyond mere substitution for human subjects in research.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）作为社会科学实验中的合成参与者的日益使用，强调了对其推断人类行为有效性指导的需求。作者比较了依赖提示工程和模型调整以对齐模拟和观察行为的启发式方法与结合人类数据以纠正差异的统计校准方法。研究结果表明，尽管启发式方法对探索性研究有用，但缺乏确认研究所需的统计严谨性，而统计校准则在较低成本下提供有效且精确的因果估计，前提是LLMs能够准确代表目标人群。</div>
</details>
</div>
<div class="card">
<div class="title">Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers</div>
<div class="meta-line">Authors: Lucas Sancéré, Noémie Moreau, Katarzyna Bozek</div>
<div class="meta-line">First: 2026-02-17T18:17:52+00:00 · Latest: 2026-02-17T18:17:52+00:00</div>
<div class="meta-line">Comments: 17 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可扩展图变换器的上下文感知皮肤癌上皮细胞分类</div>
<div class="mono" style="margin-top:8px">癌症患者的全切片图像（WSIs）包含丰富的信息，可用于医学诊断或跟踪治疗进展。为了自动化分析，开发了许多基于卷积神经网络和视觉变换器的深度学习方法，并在分割和分类任务中取得了良好的表现。然而，由于WSIs的巨大尺寸和复杂的细胞组织，这些模型依赖于基于补丁的表示，失去了重要的组织级上下文。我们提出在全WSI细胞图上使用可扩展的图变换器进行分类。我们在一个具有挑战性的任务上评估了该方法：在皮肤鳞状细胞癌（cSCC）中对健康与肿瘤上皮细胞的分类，这两种细胞类型表现出非常相似的形态，因此基于图像的方法难以区分。我们首先在单个WSI上比较了基于图像和基于图的方法。图变换器模型SGFormer和DIFFormer在3折交叉验证中分别达到了$85.2 \pm 1.5$（$\pm$标准误差）和$85.1 \pm 2.5$的平衡准确率，而最佳的基于图像的方法达到了$81.2 \pm 3.0$。通过评估几种节点特征配置，我们发现最具信息性的表示结合了形态和纹理特征以及非上皮细胞的细胞类别，突显了周围细胞上下文的重要性。然后，我们将工作扩展到对来自多位患者的多个WSI进行训练。为了解决基于图像模型的计算限制，我们从每个图像中提取了四个$2560 \times 2560$像素的补丁并将其转换为图。在这种情况下，DIFFormer达到了$83.6 \pm 1.9$的平衡准确率（3折交叉验证），而最先进的基于图像的模型CellViT256达到了$78.1 \pm 0.5$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the classification of epithelial cells in whole-slide images (WSIs) from cancer patients, which are challenging due to their large size and complex cellular organization. The authors propose a method using scalable Graph Transformers to analyze full-WSI cell graphs, focusing on distinguishing healthy from tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC). Experimental results show that the Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of 85.2% and 85.1%, respectively, outperforming the best image-based method, which reached 81.2%. Additionally, when extending the analysis to multiple WSIs, DIFFormer maintained a balanced accuracy of 83.6%, while the state-of-the-art image-based model CellViT256 achieved 78.1%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善对癌症患者全切片图像（WSIs）中上皮细胞的分类，这些图像包含复杂的细胞信息，而传统的基于补丁的深度学习方法往往会丢失这些信息。作者提出了一种新方法，使用可扩展的图变换器分析全WSI细胞图，特别关注区分皮肤鳞状细胞癌（cSCC）中的健康和肿瘤上皮细胞。实验结果表明，图变换器模型SGFormer和DIFFormer分别达到了85.2%和85.1%的平衡准确率，优于最佳的基于图像的方法（81.2%）。此外，最有效的节点特征配置结合了形态和纹理特征以及周围非上皮细胞的类别，强调了组织级上下文在分类任务中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting</div>
<div class="meta-line">Authors: Ines Montoya-Espinagosa, Antonio Agudo</div>
<div class="meta-line">First: 2026-02-17T18:14:15+00:00 · Latest: 2026-02-17T18:14:15+00:00</div>
<div class="meta-line">Comments: CAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15782v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>气象数据与天空图像结合神经模型进行光伏功率预测</div>
<div class="mono" style="margin-top:8px">随着可再生能源作为传统能源替代品的使用增加，尤其是太阳能，研究如何应对光伏能量生产的变异性以进行光伏预测的兴趣日益增长，采用不同的方法论。本研究基于两个具有相同目的的研究，开发了一种短期和长期预测的混合方法。提出了一种多模态方法，结合天空图像和光伏能量历史与气象数据。主要目标是提高斜坡事件预测的准确性，增强在多云条件下的预测稳健性，并扩展超越即时预测的能力，以支持电网的更高效运行和更好的太阳能变异管理。深度神经模型用于即时预测和预测解决方案，结合了单个和多个气象变量，以及分析太阳位置。结果表明，气象数据的纳入，特别是地表长波、向下辐射以及风与太阳位置的组合，显著提高了即时预测和预测任务中的当前预测，尤其是在多云天气下。本研究强调了整合多样化数据源以提高太阳能预测模型的可靠性和可解释性的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing reliance on renewable energy, particularly solar power, has heightened the need for effective photovoltaic forecasting to manage the inherent variability in energy production. This study introduces a hybrid forecasting approach that integrates sky images, historical photovoltaic data, and meteorological information to enhance prediction accuracy. Key findings indicate that incorporating specific meteorological variables, such as surface long-wave radiation and wind data, significantly improves forecasting performance, especially under cloudy conditions, thereby supporting more efficient power grid operations and better management of solar energy variability.</div>
<div class="mono" style="margin-top:8px">随着对可再生能源，特别是太阳能的依赖增加，迫切需要改进光伏预测方法以应对能源生产的变动性。本研究提出了一种混合预测方法，结合了天空图像、历史光伏数据和气象信息，以提高预测的准确性。研究结果表明，纳入特定的气象变量，如地表长波辐射和风数据，显著提高了短期和长期预测的可靠性，尤其是在多云条件下，从而支持更好地管理太阳能变动性和电网运营。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Scaling Laws for Boosted Jet Tagging</div>
<div class="meta-line">Authors: Matthias Vigl, Nicole Hartman, Michael Kagan, Lukas Heinrich</div>
<div class="meta-line">First: 2026-02-17T18:13:01+00:00 · Latest: 2026-02-17T18:13:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15781v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强喷流标记的神经缩放法则</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的成功表明，通过模型容量和数据集规模的联合增加，计算能力的扩展是现代机器学习性能的主要驱动因素。尽管机器学习长期以来一直是高能物理（HEP）数据分析工作流程的重要组成部分，但用于训练最先进HEP模型的计算能力仍然远低于行业基础模型。随着缩放法则在该领域的研究才刚刚开始，我们使用公共JetClass数据集研究增强喷流分类的神经缩放法则。我们推导出计算最优的缩放法则，并识别出一个可以通过增加计算能力持续接近的有效性能极限。我们研究了在HEP中常见的、模拟成本高昂的数据重复如何修改缩放，从而产生可量化的有效数据集规模增益。然后，我们研究了缩放系数和渐近性能极限如何随着输入特征和粒子多重性的选择而变化，证明了增加计算能力可靠地推动性能朝向渐近极限，并且更具表现力的低级特征可以提高性能极限并在固定数据集规模下改善结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to enhance performance in High Energy Physics (HEP) data analysis through improved machine learning models, particularly in the context of boosted jet classification. The authors investigate neural scaling laws using the JetClass dataset, deriving compute optimal scaling laws and identifying a performance limit that can be approached with increased computational resources. Key findings indicate that data repetition affects scaling, leading to a quantifiable effective dataset size gain, and that varying input features and particle multiplicity can influence scaling coefficients and performance limits, ultimately demonstrating that increased compute consistently drives performance toward an asymptotic limit while more expressive features can enhance results at fixed dataset sizes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改进计算规模来提高高能物理（HEP）数据分析中的性能，这一点在大型语言模型中得到了验证。作者使用JetClass数据集研究了增强喷流分类的神经缩放法则，推导出计算最优缩放法则，并识别出可以通过增加计算接近的性能极限。主要发现表明，数据重复影响缩放，导致可量化的有效数据集大小增益，并且不同的输入特征和粒子多重性会影响缩放系数和性能极限，增加计算始终推动性能朝向渐近极限，而更具表现力的特征在固定数据集大小下提升结果。</div>
</details>
</div>
<div class="card">
<div class="title">GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems</div>
<div class="meta-line">Authors: Yiqin Yang, Xu Yang, Yuhua Jiang, Ni Mu, Hao Hu, Runpeng Xie, Ziyou Zhang, Siyuan Li, Yuan-Hua Ni, Qianchuan Zhao, Bo Xu</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-17T18:05:48+00:00 · Latest: 2026-02-17T18:05:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GlobeDiff：多智能体系统中部分可观测性的状态扩散过程</div>
<div class="mono" style="margin-top:8px">在多智能体系统领域，\emph{部分可观测性}是有效协调和决策的关键障碍。现有方法，如信念状态估计和智能体间通信，往往效果不佳。基于信念的方法受限于对过去经验的关注，未能充分利用全局信息，而通信方法通常缺乏有效利用辅助信息的稳健模型。为了解决这个问题，我们提出了全局状态扩散算法（GlobeDiff），以根据局部观察推断全局状态。通过将状态推断过程表述为多模态扩散过程，GlobeDiff克服了状态估计中的模糊性，同时以高保真度推断全局状态。我们证明了GlobeDiff在单模态和多模态分布下的估计误差是有界的。大量实验结果表明，GlobeDiff实现了优越的性能，并能够准确推断全局状态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of partial observability in multi-agent systems, which hinders effective coordination and decision-making. The authors propose the Global State Diffusion Algorithm (GlobeDiff), which formulates the state inference process as a multi-modal diffusion process to infer the global state from local observations. Experimental results show that GlobeDiff significantly outperforms existing methods by accurately inferring the global state while maintaining a bounded estimation error under various distribution scenarios.</div>
<div class="mono" style="margin-top:8px">本研究解决了多智能体系统中部分可观测性的问题，这对有效的协调和决策造成了障碍。为了解决这一问题，作者提出了全球状态扩散算法（GlobeDiff），该算法通过将状态推断视为多模态扩散过程，从局部观察中推断全局状态。实验结果表明，GlobeDiff在准确推断全局状态方面显著优于现有方法，同时在各种分布下保持了有界的估计误差。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models</div>
<div class="meta-line">Authors: Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang, Han Hu</div>
<div class="meta-line">First: 2026-02-17T18:04:13+00:00 · Latest: 2026-02-17T18:04:13+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15772v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15772v1">PDF</a> · <a href="https://github.com/sen-ye/R3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of &quot;generate-understand-regenerate&quot;. By explicitly leveraging the model&#x27;s understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解与生成：多模态模型中的优化困境</div>
<div class="mono" style="margin-top:8px">当前多模态模型的研究面临一个关键挑战，即增强生成能力往往以理解能力为代价，反之亦然。我们分析了这种权衡，认为主要原因可能是生成与理解之间的潜在冲突，这在模型内部形成了竞争动态。为了解决这个问题，我们提出了理由-反思-精炼（R3）框架。该创新算法将单步生成任务重新构建为“生成-理解-再生成”的多步过程。通过在生成过程中明确利用模型的理解能力，我们成功缓解了优化困境，实现了更强的生成结果和与生成过程相关的理解能力提升。这为设计下一代统一多模态模型提供了宝贵的见解。代码可在 https://github.com/sen-ye/R3 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in multimodal models where improving generative capabilities often compromises understanding. To tackle this issue, the authors introduce the Reason-Reflect-Refine (R3) framework, which transforms the generation task into a multi-step process of &#x27;generate-understand-regenerate&#x27;. Experimental results demonstrate that this approach effectively resolves the optimization dilemma, leading to enhanced generation outcomes and improved understanding capabilities related to the generation process. This work provides significant insights for the development of future unified multimodal models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态模型中提高生成能力往往会损害理解能力的权衡问题。作者提出了Reason-Reflect-Refine (R3) 框架，将单步生成任务转变为“生成-理解-再生成”的多步过程。实验结果表明，这种方法有效缓解了优化困境，增强了生成结果并改善了与生成过程相关的理解能力，为未来多模态模型的设计提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Advanced Assistance for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction</div>
<div class="meta-line">Authors: Gerui Xu, Boyou Chen, Huizhong Guo, Dave LeBlanc, Arpan Kusari, Efe Yarbasi, Ananna Ahmed, Zhaonan Sun, Shan Bao</div>
<div class="meta-line">First: 2025-11-13T23:32:25+00:00 · Latest: 2026-02-17T18:03:17+00:00</div>
<div class="meta-line">Comments: 36 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10853v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traffic collision reconstruction traditionally relies on human expertise and can be accurate, but pre-crash reconstruction is more challenging. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We propose a two-phase collaborative framework with reconstruction and reasoning stages. The system processes 277 rear-end lead vehicle deceleration (LVD) crashes from the Crash Investigation Sampling System (CISS, 2017 to 2022), integrating narrative reports, structured tabular variables, and scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II combines these reconstructions with Event Data Recorder (EDR) signals to (1) identify striking and struck vehicles and (2) isolate the EDR records most relevant to the collision moment, enabling inference of key pre-crash behaviors. For validation, we evaluated all LVD cases and emphasized 39 complex crashes where multiple EDR records per crash created ambiguity due to missing or conflicting data. Ground truth was set by consensus of two independent manual annotators, with a separate language model used only to flag potential conflicts for re-checking. The framework achieved 100% accuracy across 4,155 trials; three reasoning models produced identical outputs, indicating that performance is driven by the structured prompts rather than model choice. Research analysts without reconstruction training achieved 92.31% accuracy on the same 39 complex cases. Ablation tests showed that removing structured reasoning anchors reduced case-level accuracy from 99.7% to 96.5% and increased errors across multiple output dimensions. The system remained robust under incomplete inputs. This zero-shot evaluation, without domain-specific training or fine-tuning, suggests a scalable approach for AI-assisted pre-crash analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>交通事故分析的高级辅助：基于AI的多智能体预碰撞重建方法</div>
<div class="mono" style="margin-top:8px">交通碰撞重建传统上依赖于人类专业知识，虽然可以准确，但预碰撞重建更具挑战性。本研究开发了一个多智能体AI框架，重建预碰撞场景并从碎片化的碰撞数据中推断车辆行为。我们提出了一个包含重建和推理阶段的两阶段协作框架。该系统处理了277起来自碰撞调查抽样系统（CISS，2017至2022年）的追尾主车减速（LVD）碰撞，整合了叙述报告、结构化表格变量和现场图。第一阶段从多模态输入生成自然语言的碰撞重建。第二阶段将这些重建与事件数据记录器（EDR）信号结合，以（1）识别撞击和被撞车辆，以及（2）隔离与碰撞时刻最相关的EDR记录，从而推断关键的预碰撞行为。为了验证，我们评估了所有LVD案例，并强调了39起复杂碰撞，其中每起碰撞的多个EDR记录因缺失或冲突数据而产生歧义。真实情况由两名独立人工标注者的共识确定，另一个语言模型仅用于标记潜在冲突以供复查。该框架在4155次试验中实现了100%的准确率；三个推理模型产生了相同的输出，表明性能是由结构化提示驱动，而非模型选择。没有重建训练的研究分析师在同样的39个复杂案例中达到了92.31%的准确率。消融测试表明，去除结构化推理锚点使案例级准确率从99.7%降至96.5%，并增加了多个输出维度的错误。该系统在不完整输入下仍保持稳健。这种零样本评估，无需领域特定的训练或微调，表明了一种可扩展的AI辅助预碰撞分析方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of pre-crash reconstruction in traffic collision analysis, which has traditionally depended on human expertise. The study introduces a multi-agent AI framework that operates in two phases: the first phase generates natural-language reconstructions from various data inputs, while the second phase integrates these reconstructions with Event Data Recorder signals to identify vehicles involved and isolate relevant data for inferring pre-crash behaviors. The framework demonstrated 100% accuracy across 4,155 trials, with non-trained analysts achieving 92.31% accuracy on complex cases, indicating the effectiveness of structured prompts in enhancing performance and suggesting a scalable solution for AI-assisted pre-crash analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高交通碰撞分析中预碰撞重建的准确性，传统上依赖于人类专业知识。该研究提出了一种多智能体AI框架，分为两个阶段：第一阶段从各种数据输入生成自然语言的碰撞重建，第二阶段将这些重建与事件数据记录器信号结合，以识别相关车辆并隔离与推断预碰撞行为相关的数据。实验结果表明，该框架在4155次试验中实现了100%的准确率，非专家分析师在复杂案例中达到了92.31%的准确率，表明结构化提示在驱动性能方面的有效性，以及该系统在不完整数据条件下的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-Assisted Social Dining as a White Glove Service</div>
<div class="meta-line">Authors: Atharva S Kashyap, Ugne Aleksandra Morkute, Patricia Alves-Oliveira</div>
<div class="meta-line">First: 2026-02-17T17:58:25+00:00 · Latest: 2026-02-17T17:58:25+00:00</div>
<div class="meta-line">Comments: 20 pages, 9 figures. Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15767v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15767v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人辅助社交用餐作为白手套服务</div>
<div class="mono" style="margin-top:8px">机器人辅助喂食使需要帮助进食的残疾人能够独立且有尊严地享用餐食。然而，现有系统仅在实验室或家庭中进行测试，野外社交用餐环境（例如餐厅）尚未得到充分探索。为此类环境设计机器人面临独特挑战，例如动态和无监督的用餐环境，机器人需要考虑并作出响应。通过与残疾人进行投机性参与设计，并辅以半结构化访谈和定制的基于AI的视觉故事板工具，我们发现了理想的野外社交用餐场景。我们的关键见解表明，这类系统应：体现白手套服务的原则，其中机器人（1）支持多模态输入和不干扰的输出；（2）具有上下文敏感的社交行为并优先考虑用户；（3）在喂食之外扩展角色；（4）适应餐桌上的其他关系。我们的工作对机器人辅助喂食的野外和团体环境具有重要意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the dining experience for people with disabilities who require assistance while eating, particularly in social settings like restaurants where existing systems have not been tested. The study employed speculative participatory design, involving individuals with disabilities through semi-structured interviews and a custom AI-based visual storyboarding tool to identify optimal scenarios for robot-assisted social dining. The findings indicate that effective robot systems should emulate a white glove service by supporting multimodal inputs, exhibiting contextually sensitive social behaviors, expanding their roles beyond mere feeding, and adapting to various social dynamics at the dining table.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善需要进食辅助的残障人士在社交场合（如餐厅）用餐的体验，而现有的机器人辅助进食系统在这方面尚未得到充分探索。该研究采用了投机性参与设计，结合半结构化访谈和定制的基于人工智能的视觉故事板工具，以识别理想的机器人辅助社交用餐场景。主要发现表明，有效的系统应体现白手套服务的理念，具有多模态输入、上下文敏感的社交行为、超越进食的扩展角色，以及适应餐桌上各种社交动态的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Random Forests as Statistical Procedures: Design, Variance, and Dependence</div>
<div class="meta-line">Authors: Nathaniel S. O&#x27;Connell</div>
<div class="meta-line">First: 2026-02-13T17:08:43+00:00 · Latest: 2026-02-17T17:50:58+00:00</div>
<div class="meta-line">Comments: 27 pages, 2 figures. Supplementary material included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13104v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13104v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed set of covariates. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机森林作为统计程序：设计、方差和依赖性</div>
<div class="mono" style="margin-top:8px">随机森林是广泛使用的预测程序，但通常以算法方式描述，而不是作为作用于固定协变量集的统计设计。我们开发了一种有限样本、基于设计的随机森林公式，其中每棵树都是一个明确的随机条件回归函数。这种视角产生了一个森林预测器的精确方差恒等式，将有限聚合变异性与在无限聚合下仍然存在的结构依赖项分开。我们进一步使用总方差和协方差法则分解单棵树的离散性和树间协方差，孤立出两个基本设计机制——训练观察的重用和数据自适应分区的对齐。这些机制引入了严格的协方差下限，表明仅通过增加树的数量无法消除预测变异性。由此产生的框架阐明了重抽样、特征级随机化和分裂选择如何影响分辨率、树的变异性和依赖性，并确立了随机森林作为明确的有限样本统计设计，其行为由其基础的随机构造决定。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to provide a clearer statistical framework for understanding random forests, which are commonly used for prediction but often described algorithmically. The authors develop a finite-sample, design-based formulation of random forests, treating each tree as a randomized conditional regression function. Key findings include an exact variance identity that distinguishes between finite-aggregation variability and a structural dependence term, as well as a decomposition of single-tree dispersion and inter-tree covariance, revealing that predictive variability cannot be reduced merely by increasing the number of trees, thus highlighting the importance of resampling and feature-level randomization in the design of random forests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提供对随机森林的更清晰的统计理解，因为随机森林通常是以算法方式描述，而不是作为统计设计。作者开发了一种有限样本的设计基础公式，其中每棵树作为随机条件回归函数。主要发现揭示了森林预测器的确切方差恒等式，区分了有限聚合变异性和结构依赖性，表明增加树的数量并不能消除预测变异性，因为固有设计机制如训练观察的重用和数据自适应分区的对齐会影响结果。</div>
</details>
</div>
<div class="card">
<div class="title">GLM-5: from Vibe Coding to Agentic Engineering</div>
<div class="meta-line">Authors: GLM-5 Team, :, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chengxing Xie, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chen Li, Chenghua Huang, Chengwei Hu, Chenhui Zhang, Chenzheng Zhu, Congfeng Yin, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huan Liu, Huanpeng Chu, Jia&#x27;ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li, Jingwei Yuan, Jinhua Du, Jinxin Liu, Junkai Zhi, Junwen Duan, Kaiyue Zhou, Kangjian Wei, Ke Wang, Keyun Luo, Laiqiang Zhang, Leigang Sha, Liang Xu, Lindong Wu, Lintao Ding, Lu Chen, Minghao Li, Nianyi Lin, Pan Ta, Qiang Zou, Rongjun Song, Ruiqi Yang, Shangqing Tu, Shangtong Yang, Shaoxiang Wu, Shengyan Zhang, Shijie Li, Shuang Li, Shuyi Fan, Wei Qin, Wei Tian, Weining Zhang, Wenbo Yu, Wenjie Liang, Xiang Kuang, Xiangmeng Cheng, Xiangyang Li, Xiaoquan Yan, Xiaowei Hu, Xiaoying Ling, Xing Fan, Xingye Xia, Xinyuan Zhang, Xinze Zhang, Xirui Pan, Xunkai Zhang, Yandong Wu, Yanfu Li, Yidong Wang, Yifan Zhu, Yijun Tan, Yilin Zhou, Yiming Pan, Ying Zhang, Yinpei Su, Yipeng Geng, Yipeng Geng, Yong Yan, Yonglin Tan, Yuean Bi, Yuhan Shen, Yuhao Yang, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yurong Wu, Yutao Zhang, Yuxi Duan, Yuxuan Zhang, Zezhen Liu, Zhengtao Jiang, Zhenhe Yan, Zheyu Zhang, Zhixiang Wei, Zhuo Chen, Zhuoer Feng, Zijun Yao, Ziwei Chai, Ziyuan Wang, Zuzhou Zhang, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</div>
<div class="meta-line">First: 2026-02-17T17:50:56+00:00 · Latest: 2026-02-17T17:50:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15763v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15763v1">PDF</a> · <a href="https://github.com/zai-org/GLM-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLM-5：从氛围编码到自主工程</div>
<div class="mono" style="margin-top:8px">我们介绍GLM-5，这是一种下一代基础模型，旨在将氛围编码的范式转变为自主工程。在其前身的自主、推理和编码（ARC）能力基础上，GLM-5采用DSA显著降低训练和推理成本，同时保持长上下文的保真度。为了推进模型的对齐和自主性，我们实施了一种新的异步强化学习基础设施，通过将生成与训练解耦，极大提高了后训练效率。此外，我们提出了新颖的异步代理强化学习算法，进一步提高了强化学习质量，使模型能够更有效地从复杂的长时间交互中学习。通过这些创新，GLM-5在主要开放基准上实现了最先进的性能。最重要的是，GLM-5在现实世界编码任务中展现了前所未有的能力，超越了以往在端到端软件工程挑战中的基准。代码、模型和更多信息可在https://github.com/zai-org/GLM-5获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GLM-5 is to shift from vibe coding to agentic engineering by enhancing the capabilities of its predecessor in reasoning and coding. The main method involves the adoption of DSA to lower training and inference costs while ensuring long-context fidelity, alongside a new asynchronous reinforcement learning infrastructure that separates generation from training to improve post-training efficiency. Key experimental findings indicate that GLM-5 achieves state-of-the-art performance on major benchmarks and demonstrates exceptional proficiency in real-world coding tasks, outperforming previous models in end-to-end software engineering challenges.</div>
<div class="mono" style="margin-top:8px">GLM-5的研究动机在于通过提高模型的对齐性和自主性，增强从氛围编码到自主工程的转变。主要方法是采用DSA降低训练和推理成本，同时保持长上下文的保真度，并实施一种新的异步强化学习基础设施，将生成与训练解耦。主要实验结果表明，GLM-5在主要基准测试中实现了最先进的性能，并在实际编码任务中展现出卓越的能力，在端到端软件工程挑战中超越了之前的模型。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
