<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0325</div>
    <div class="row"><div class="card">
<div class="title">An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses</div>
<div class="meta-line">Authors: Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-25T02:05:41+00:00 · Latest: 2026-01-15T17:33:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures, accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17772v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.17772v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model&#x27;s utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进的有界域和平滑损失下差分隐私SGD的隐私和效用分析</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DPSGD）广泛用于在机器学习模型训练过程中保护敏感数据，但其隐私保证往往以模型性能的大幅下降为代价，因为缺乏量化隐私损失的紧密理论界限。尽管最近的努力实现了更准确的隐私保证，但仍然施加了一些禁止实际应用的假设，如凸性和复杂的参数要求，并且很少深入研究隐私机制对模型效用的影响。本文为具有一般L-平滑和非凸损失函数的DPSGD提供了严格的隐私特征，揭示了在有界域情况下迭代的隐私损失收敛性。具体而言，我们跟踪多个迭代中的隐私损失，利用噪声平滑减少特性，并进一步在不同场景中建立全面的收敛分析。特别地，我们展示了对于具有有界域的DPSGD，(i) 隐私损失在没有凸性假设的情况下仍然可以收敛，(ii) 在某些条件下，较小的有界直径可以同时改善隐私和效用，以及 (iii) 对于具有梯度裁剪的DPSGD（DPSGD-GC）和具有有界域的DPSGD-GC（DPSGD-DC）及μ-强凸人口风险函数，隐私效用权衡的可达大O阶。通过在实际环境中进行的成员推断攻击（MIA）实验验证了从理论结果中获得的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the balance between privacy and utility in Differentially Private Stochastic Gradient Descent (DPSGD), which often suffers from performance degradation due to inadequate theoretical bounds on privacy loss. The authors employ a rigorous privacy characterization approach for DPSGD with general L-smooth and non-convex loss functions, analyzing the privacy loss over multiple iterations in bounded-domain scenarios. Key findings indicate that privacy loss can converge without convexity assumptions, a smaller bounded diameter can enhance both privacy and utility under certain conditions, and they establish the big-O order of the privacy-utility trade-off for DPSGD variants, with experimental validation through membership inference attacks supporting their theoretical insights.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善差分隐私随机梯度下降（DPSGD）的性能，该方法在保护机器学习中的敏感数据时至关重要，但由于隐私损失界限不足，往往会妨碍模型的实用性。作者为具有一般L-光滑和非凸损失函数的DPSGD开发了严格的隐私特征分析，研究了在有界域场景中多个迭代的隐私损失。主要发现表明，在没有凸性假设的情况下，隐私损失仍然可以收敛，较小的有界直径在特定条件下可以同时提高隐私和实用性，并且可以量化带有梯度裁剪和有界域的DPSGD之间的隐私与实用性权衡，实验通过成员推断攻击验证了理论见解。</div>
</details>
</div>
<div class="card">
<div class="title">Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2026-01-15T15:16:15+00:00 · Latest: 2026-01-15T15:16:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>投影微批量累积为强化学习提供无参考的近端策略更新</div>
<div class="mono" style="margin-top:8px">本文介绍了投影微批量累积（PROMA），一种用于大型语言模型微调的近端策略更新方法。PROMA通过在微批量聚合之前投影序列级梯度分量，累积微批量的策略梯度。该投影在反向传播过程中逐层应用，实现了高效的实现，无需额外的前向或反向传播。实证结果表明，PROMA对局部KL散度的控制比GRPO更严格，从而实现了更稳定的策略学习。与PPO和GRPO不同，PROMA在不引起熵崩溃的情况下实现近端更新，并且不依赖于参考策略或似然比裁剪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more stable policy updates in reinforcement learning, particularly in the context of fine-tuning large language models. The authors introduce Projected Microbatch Accumulation (PROMA), a method that accumulates policy gradients across microbatches by projecting out sequence-wise gradient components during the backward pass, allowing for efficient implementation without extra passes. Experimental results demonstrate that PROMA provides tighter control of local KL divergence compared to existing methods like GRPO, leading to more stable policy learning without the issues of entropy collapse or reliance on a reference policy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为了在强化学习中实现更稳定的策略学习，特别是在大语言模型的微调过程中。作者提出了投影微批量累积（PROMA）方法，该方法通过在反向传播过程中投影序列级梯度分量来累积微批量的策略梯度，从而实现高效的实施，无需额外的前向或反向传播。实验结果表明，PROMA在局部KL散度的控制上比现有方法GRPO更为严格，从而实现了更稳定的策略更新，且没有熵崩溃或对参考策略的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">mergetune: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-15T15:15:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v1">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mergetune：视觉-语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉-语言模型（VLMs），如CLIP，往往会导致预训练知识的灾难性遗忘。之前的工作主要旨在减轻适应过程中的遗忘；然而，在此过程中，遗忘往往是不可避免的。我们引入了一种新范式，\emph{持续微调（CFT）}，旨在在零-shot模型已经适应后恢复预训练知识。我们提出了一种简单的、与模型无关的CFT策略（称为MERGETUNE），由线性模式连接（LMC）指导，可以在现有微调模型上事后应用，而无需架构更改。给定一个微调模型，我们继续微调其可训练参数（例如，软提示或线性头），以寻找一个具有两个低损失路径的持续模型，分别指向零-shot（例如，CLIP）和微调（例如，CoOp）解决方案。通过利用损失景观的几何特性，持续模型隐式地合并了这两种解决方案，恢复了在微调对应模型中丢失的预训练知识。一个挑战是，普通的LMC约束需要从预训练任务中重放数据。我们通过二阶代理近似这一约束，消除了对大规模数据重放的需求。实验表明，MERGETUNE在不增加参数的情况下，提高了CoOp在基础-新颖泛化上的调和平均值+5.6\%。我们首次在跨数据集迁移中显示出在DTD和EuroSAT上优于CLIP的表现。在稳健微调评估中，来自MERGETUNE的LMC合并模型以更低的推理成本超越了集成基线，与零-shot模型集成时实现了进一步的增益和最先进的结果。我们的代码可在\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of catastrophic forgetting in vision-language models (VLMs) like CLIP during fine-tuning. The authors introduce a novel method called continued fine-tuning (CFT) using a model-agnostic strategy named MERGETUNE, which allows for the recovery of pretrained knowledge after adaptation without requiring architectural changes. Experimental results demonstrate that MERGETUNE improves the harmonic mean of CoOp by 5.6% on base-novel generalization and achieves superior performance compared to CLIP on cross-dataset transfer tasks, while also surpassing ensemble baselines with lower inference costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉语言模型（VLMs）在微调过程中出现的灾难性遗忘问题，这通常导致预训练知识的丧失。作者提出了一种称为持续微调（CFT）的方法，使用一种名为MERGETUNE的模型无关策略，利用线性模式连通性（LMC）在模型适应后恢复丢失的知识。实验结果表明，MERGETUNE在基础-新颖泛化上提高了CoOp的调和平均值5.6%，在跨数据集迁移任务中超越了CLIP，并且在稳健微调评估中优于集成基线，同时保持较低的推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset</div>
<div class="meta-line">Authors: Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang</div>
<div class="meta-line">First: 2026-01-15T11:28:58+00:00 · Latest: 2026-01-15T11:28:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 11 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10305v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DanQing：最新的大规模中文视觉-语言预训练数据集</div>
<div class="mono" style="margin-top:8px">视觉-语言预训练（VLP）模型通过对大规模图像-文本对进行对比预训练，在各种下游任务中表现出强大的性能。大量英语图像-文本数据集（如COYO-700M和LAION-400M）的发布，使得CLIP和SigLIP等模型在跨模态检索和图像描述等任务中得到了广泛应用。然而，由于高质量中文图像-文本数据的稀缺，中文视觉-语言预训练的进展显著滞后。为了解决这一问题，我们开发了一套全面的流程来构建高质量的中文跨模态数据集。因此，我们提出了DanQing，该数据集包含从Common Crawl收集的1亿个图像-文本对。与现有数据集不同，DanQing通过更严格的选择过程进行策划，数据质量更高。此外，DanQing主要基于2024-2025年的网络数据构建，使模型能够更好地捕捉不断变化的语义趋势，从而提供更大的实际效用。我们通过对SigLIP2模型的持续预训练，将DanQing与现有数据集进行了比较。实验结果表明，DanQing在一系列中文下游任务中始终实现了优越的性能，包括零样本分类、跨模态检索和基于LMM的评估。为了促进中文视觉-语言预训练的进一步研究，我们将根据创意共享CC-BY 4.0许可证开源DanQing数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lag in Chinese vision-language pretraining due to a lack of high-quality image-text datasets. The authors developed DanQing, a large-scale dataset consisting of 100 million image-text pairs sourced from Common Crawl, using a rigorous selection process to ensure data quality. Experimental results indicate that models pre-trained on DanQing, specifically the SigLIP2 model, outperform existing datasets in various Chinese downstream tasks, including zero-shot classification and cross-modal retrieval.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由于缺乏高质量图像-文本数据集而导致的中文视觉语言预训练滞后问题，这阻碍了模型在各种任务中的表现。作者开发了一条全面的管道来构建DanQing，一个包含1亿对图像-文本对的大规模数据集，数据来源于Common Crawl，重点在于严格的选择以提高数据质量。实验结果表明，基于DanQing进行预训练的模型，特别是SigLIP2模型，在多个中文下游任务中表现优于现有数据集，包括零样本分类和跨模态检索，突显了其在推动该领域研究方面的实际效用。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍然是盲点。目前的VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器。训练过程通常将图像展平为一维补丁序列，忽略了进行空间推理所需的二维结构。我们认为，这种缺乏空间意识是VLM设计中的一个缺失维度，也是需要空间基础的应用（如机器人技术和具身人工智能）的瓶颈。为了解决这个问题，我们研究了（i）使用替代目标训练的图像编码器和（ii）二维位置编码。我们的实验表明，这些架构选择可以在多个基准上改善空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of vision-language models (VLMs) in capturing spatial relationships, which is crucial for applications like robotics and embodied AI. The authors explore alternative training objectives for image encoders and the incorporation of 2D positional encodings to enhance spatial awareness. Their experiments demonstrate that these modifications significantly improve spatial reasoning performance across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究关注视觉语言模型（VLMs）在捕捉空间关系方面的局限性，这对机器人技术和具身人工智能等应用至关重要。作者提出研究图像编码器的替代训练目标以及引入二维位置编码，以增强空间意识。实验结果表明，这些修改显著提高了多个基准测试中的空间推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP</div>
<div class="meta-line">Authors: Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-14T20:38:36+00:00 · Latest: 2026-01-14T20:38:36+00:00</div>
<div class="meta-line">Comments: Submitted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09859v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09859v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破开放权重CLIP的限制：自监督微调CLIP的优化框架</div>
<div class="mono" style="margin-top:8px">CLIP已成为多模态表示学习的基石，但提高其性能通常需要在数十亿样本上从头开始训练，这一过程成本高昂。我们提出一个不同的问题：是否可以仅使用现有的自监督数据集来提高开放权重CLIP模型在各种下游任务中的性能？与将预训练模型适应单一下游任务的监督微调不同，我们的设置旨在提高在各种任务中的整体性能。然而，正如我们的实验和先前研究所揭示的，从开放权重CLIP模型开始简单应用标准训练协议往往会失败，导致性能下降。本文介绍了TuneCLIP，一个克服性能下降的自监督微调框架。TuneCLIP有两个关键组成部分：（1）一个恢复优化统计的热身阶段，以减少冷启动偏差，灵感来自理论分析；（2）一个优化新的对比损失的微调阶段，以减轻对假负对的惩罚。我们的广泛实验表明，TuneCLIP在模型架构和规模上始终提高性能。值得注意的是，它提升了领先的开放权重模型，如SigLIP（ViT-B/16），在ImageNet及相关的分布外基准上实现了高达+2.5%的增益，在竞争激烈的DataComp基准上实现了+1.2%的增益，为高效的后预训练适应设定了新的强基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of open-weight CLIP models across various downstream tasks without the need for extensive retraining on large datasets. The authors propose TuneCLIP, a self-supervised fine-tuning framework that includes a warm-up stage to recover optimization statistics and a fine-tuning stage that optimizes a new contrastive loss to address performance degradation issues. Experimental results demonstrate that TuneCLIP significantly improves performance across different model architectures, achieving notable gains of up to +2.5% on ImageNet and +1.2% on the DataComp benchmark, thereby establishing a new baseline for efficient adaptation of open-weight models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升开放权重CLIP模型在各种下游任务中的性能，而无需在大型数据集上进行广泛的再训练。作者提出了TuneCLIP，一个自监督微调框架，包括一个恢复优化统计的热身阶段和一个优化新对比损失的微调阶段，以解决性能下降问题。实验结果表明，TuneCLIP显著提高了领先开放权重模型的性能，在ImageNet上获得了高达+2.5%的提升，在DataComp基准上获得了+1.2%的提升，从而为预训练后的高效适应建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Normalize Filters! Classical Wisdom for Deep Vision</div>
<div class="meta-line">Authors: Gustavo Perez, Stella X. Yu</div>
<div class="meta-line">First: 2025-06-04T19:32:42+00:00 · Latest: 2026-01-14T19:43:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04401v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.04401v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>归一化滤波器！深度视觉的经典智慧</div>
<div class="mono" style="margin-top:8px">经典图像滤波器，如平均或差分滤波器，经过精心归一化以确保一致性、可解释性，并避免强度偏移、光晕或振铃等伪影。相比之下，在深度网络中端到端学习的卷积滤波器缺乏这种约束。尽管它们可能类似于小波和斑点/边缘检测器，但并未以相同或任何方式进行归一化。因此，当图像经历大气传输时，其响应会失真，导致错误结果。我们通过提出滤波器归一化，随后进行可学习的缩放和偏移，类似于批量归一化，来解决这一限制。这一简单而有效的修改确保滤波器具有大气等变性，实现共域对称性。通过将经典滤波原理融入深度学习（适用于卷积神经网络和依赖卷积的视觉变换器），我们的方法在人工和自然强度变化基准上取得了显著改进。我们的ResNet34甚至可以大幅超越CLIP。我们的分析表明，未归一化的滤波器会降低性能，而滤波器归一化则规范学习，促进多样性，提高鲁棒性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of convolutional filters in deep networks, which lack normalization and can lead to distorted responses during atmospheric transfer. The authors propose a method that incorporates filter normalization followed by learnable scaling and shifting, similar to batch normalization, to ensure that filters are atmosphere-equivariant. Experimental results demonstrate that this approach significantly enhances performance on both artificial and natural intensity variation benchmarks, with a ResNet34 model outperforming CLIP by a considerable margin, indicating that normalized filters improve robustness, generalization, and learning diversity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于深度学习中未归一化卷积滤波器的局限性，这可能导致在大气传输过程中响应失真和错误结果。作者提出了一种滤波器归一化的方法，随后进行可学习的缩放和偏移，类似于批量归一化，以确保滤波器具有大气等变性。实验结果表明，该方法在人工和自然强度变化基准测试中显著提高了性能，其中ResNet34模型的表现甚至大幅超过CLIP，突显出滤波器归一化相比未归一化滤波器在提高鲁棒性、泛化能力和学习多样性方面的优势。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适应于稀有类别</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型如CLIP在零样本识别中表现出色，但在预训练期间很少见的类别（包括新出现的实体和文化特定类别）上表现不佳。我们介绍了LiteEmbed，这是一个轻量级框架，用于CLIP的少量样本个性化，使新类别能够在不重新训练编码器的情况下添加。LiteEmbed在CLIP词汇内对文本嵌入进行子空间引导优化，利用基于PCA的分解，将粗略语义方向与细粒度变化分离。两个互补目标，粗对齐和细分离，共同保持全局语义一致性，同时增强视觉相似类别之间的可区分性。一旦优化，嵌入可以即插即用，顺利替代CLIP的原始文本特征，适用于分类、检索、分割和检测任务。大量实验表明，相较于先前的方法，LiteEmbed在适应于代表性不足、稀有或未见类别方面取得了显著提升，确立了其作为有效方法的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of large-scale vision-language models like CLIP on rare classes that are not well-represented during pretraining. The authors propose LiteEmbed, a lightweight framework that allows for few-shot personalization of CLIP by optimizing text embeddings without the need for retraining the model&#x27;s encoders. Experimental results show that LiteEmbed significantly enhances the model&#x27;s ability to recognize underrepresented classes, achieving better performance in classification, retrieval, segmentation, and detection tasks compared to previous methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型视觉语言模型（如CLIP）在预训练期间未充分代表的稀有类别上的表现。作者提出了LiteEmbed，这是一种轻量级框架，允许通过优化文本嵌入来对CLIP进行少量样本个性化，而无需重新训练模型的编码器。实验结果表明，LiteEmbed显著增强了模型识别未充分代表类别的能力，在分类、检索、分割和检测任务中相比于先前的方法取得了更好的表现。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：用于科学复合图的视觉条件面板检测与标题生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个标记面板组合成单一图像，但在实际流程中，标题往往缺失或仅提供图形级摘要，使得面板级理解变得困难。本文提出了FigEx2，一种视觉条件框架，能够从复合图中定位面板并直接生成面板级标题。为了减轻开放式标题生成中多样化措辞的影响，我们引入了一种噪声感知门控融合模块，能够自适应过滤标记级特征，以稳定检测查询空间。此外，我们采用了一种分阶段优化策略，将监督学习与强化学习（RL）相结合，利用基于CLIP的对齐和基于BERTScore的语义奖励来强制执行严格的多模态一致性。为了支持高质量的监督，我们策划了BioSci-Fig-Cap，这是一个针对面板级定位的精细基准，同时还包括物理和化学领域的跨学科测试套件。实验结果表明，FigEx2在检测方面达到了优越的0.726 mAP@0.5:0.95，并在METEOR上比Qwen3-VL-8B高出0.51，在BERTScore上高出0.24。值得注意的是，FigEx2在没有任何微调的情况下，展现出显著的零样本迁移能力，能够适应分布外的科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the understanding of scientific compound figures, which often lack detailed panel-level captions. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates specific captions for each panel using a noise-aware gated fusion module to stabilize the detection query space. Experimental results indicate that FigEx2 achieves a mean Average Precision (mAP) of 0.726 for detection and outperforms the baseline model Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore, demonstrating strong zero-shot transferability to different scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高对科学复合图的理解，而这些图通常缺乏详细的面板级标题。作者提出了FigEx2，这是一种视觉条件框架，能够定位面板并为每个面板生成特定的标题，使用噪声感知门控融合模块来稳定检测查询空间。实验结果表明，FigEx2在检测中达到了0.726的平均精度（mAP），并在METEOR上比基线模型Qwen3-VL-8B高出0.51，在BERTScore上高出0.24，显示出在不同科学领域的强大零样本迁移能力，无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">Differentially private federated learning for localized control of infectious disease dynamics</div>
<div class="meta-line">Authors: Raouf Kerkouche, Henrik Zunker, Mario Fritz, Martin J. Kühn</div>
<div class="meta-line">First: 2025-09-17T14:28:04+00:00 · Latest: 2026-01-14T15:45:09+00:00</div>
<div class="meta-line">Comments: 26 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14024v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: R2 around 0.94 (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; R2 around 0.88 (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于传染病动态局部控制的差分隐私联邦学习</div>
<div class="mono" style="margin-top:8px">在流行病时期，迅速反应是减缓疫情传播的必要措施。局部方法在此反应中具有多种优势，限制了所需资源并减少了干预对更大范围的影响。然而，由于可用数据有限，在地方范围内训练单独的机器学习（ML）模型通常不可行。集中数据也面临挑战，因为其高度敏感性和隐私限制。在本研究中，我们考虑了一种基于德国县和社区的局部策略，由相关地方卫生机构（LHA）管理。为了在不妨碍详细情境数据可用性的情况下保护隐私，我们提出了一种隐私保护的预测方法，可以协助公共卫生专家和决策者。使用联邦学习（FL）的ML方法在不集中原始数据的情况下训练共享模型。将县、社区或LHA视为客户端，并在效用和隐私之间找到平衡，我们研究了具有客户端差分隐私（DP）的FL框架。我们在最近病例数的滑动窗口上训练共享的多层感知器，以预测病例数量，同时客户端仅交换规范裁剪的更新，服务器则聚合带有DP噪声的更新。我们在两个阶段对县级COVID-19数据评估该方法。如预期，严格的隐私会导致不稳定且不可用的预测。在适度强的隐私水平下，DP模型与非DP模型非常接近：2020年11月R2约为0.94（对比0.95），平均绝对百分比误差（MAPE）为26%；2022年3月R2约为0.88（对比0.93），MAPE为21%。总体而言，客户端级DP-FL可以提供有用的县级预测，并具有强隐私保障，且可行的隐私预算取决于流行病阶段，允许卫生机构之间进行隐私合规的合作以进行地方预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for rapid responses to mitigate epidemic spreading while maintaining privacy in sensitive health data. The authors propose a privacy-preserving forecasting method utilizing federated learning (FL) to train a shared multilayer perceptron model on localized COVID-19 data from German counties, allowing health authorities to collaborate without centralizing raw data. The experimental results indicate that while very strict privacy levels lead to unstable forecasts, a moderately strong differential privacy level yields predictions that closely match those of non-private models, achieving an R2 of approximately 0.94 and a mean absolute percentage error of 26% in November 2020, and an R2 of around 0.88 with a 21% MAPE in March 2022, demonstrating the effectiveness of client-level DP-FL in providing reliable forecasts with strong privacy guarantees.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用机器学习来增强对流行病的地方响应，同时解决与敏感健康数据相关的隐私问题。作者提出了一种结合客户端差分隐私的联邦学习框架，以训练一个共享的多层感知器模型，基于德国各县的COVID-19病例数进行预测。实验结果表明，尽管非常严格的隐私设置会导致不稳定的预测，但适度强的差分隐私水平可以实现准确的预测，在2020年11月达到约0.94的R2和26%的平均绝对百分比误差，在2022年3月达到约0.88的R2和21%的MAPE，表明该方法能够有效平衡地方流行病预测中的隐私和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity</div>
<div class="meta-line">Authors: Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal</div>
<div class="meta-line">First: 2026-01-14T14:03:11+00:00 · Latest: 2026-01-14T14:03:11+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09497v1">PDF</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr">Code1</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git">Code2</a> · <a href="https://github.com/Ritabrata04/cdod-icpr">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在领域特异性下实现稳健的跨数据集目标检测泛化</div>
<div class="mono" style="margin-top:8px">目标检测器在分布内通常表现良好，但在不同基准上会急剧下降。我们通过设置特异性的视角研究跨数据集目标检测（CD-OD）。我们将基准分组为具有多样日常场景的设置无关数据集和与狭窄环境相关的设置特定数据集，并在所有训练-测试对上评估标准检测器系列。这揭示了CD-OD中的明确结构：在相同设置类型内的迁移相对稳定，而跨设置类型的迁移显著下降且通常是不对称的。最严重的崩溃发生在从特定源迁移到无关目标时，并在开放标签对齐后仍然存在，表明领域转移在最困难的情况下占主导地位。为了将领域转移与标签不匹配分开，我们比较了闭合标签迁移与开放标签协议，后者使用CLIP相似性将预测类别映射到最近的目标标签。开放标签评估产生了一致但有限的增益，许多纠正的案例对应于图像证据支持的语义近失。总体而言，我们提供了在设置特异性下对CD-OD的原则性表征以及在分布转移下评估检测器的实用指导。代码将发布在\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of cross-dataset object detection (CD-OD), which often shows a significant performance drop when evaluated on different benchmarks. The authors categorize datasets into setting-agnostic and setting-specific groups and assess a standard family of detectors across various train-test pairs. Their findings indicate that while transfer performance is stable within the same setting type, it deteriorates sharply when transferring across different types, particularly from specific to agnostic settings, highlighting the dominance of domain shift. They also compare closed-label and open-label transfer methods, finding that open-label evaluation provides consistent improvements, particularly in cases where predicted classes are semantically close to target labels. This work offers a structured understanding of CD-OD under setting specificity and practical recommendations for evaluating detectors amidst distribution shifts.</div>
<div class="mono" style="margin-top:8px">本研究探讨了物体检测器在应用于不同基准时性能下降的问题，动机在于提高跨数据集物体检测（CD-OD）在不同领域特定性下的鲁棒性。作者将数据集分为设置无关和设置特定两类，并在所有训练-测试对上评估了一系列标准检测器，结果显示同一设置类型内的迁移性能相对稳定，而跨类型迁移则显著下降。主要发现表明，从特定源到无关目标的迁移性能下降最为严重，尽管开放标签评估提供了一定的改进，但仍然突显了领域转移带来的持续挑战，为CD-OD提供了结构化理解和在分布转移下评估检测器的实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models</div>
<div class="meta-line">Authors: Yizhi Chen, Ahmed Hemani</div>
<div class="meta-line">First: 2026-01-14T12:52:08+00:00 · Latest: 2026-01-14T12:52:08+00:00</div>
<div class="meta-line">Comments: Accepted to DATE Late Breaking Results 2026, Verona, Italy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09451v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破性结果：Quamba-SE：状态空间模型中激活的软边量化器</div>
<div class="mono" style="margin-top:8px">我们提出了Quamba-SE，一种用于状态空间模型（SSM）激活量化的软边量化器。与现有方法使用标准INT8操作不同，Quamba-SE采用三种自适应尺度：小值的高精度、正常值的标准尺度和异常值的低精度。这保留了异常值信息，而不是硬剪切，同时保持其他值的精度。我们在Mamba-130M上评估了6个零样本基准。结果表明，Quamba-SE在各个基准上始终优于Quamba，在单个基准上最高提高了+2.68%，在6个数据集的平均准确率上提高了+0.83%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the quantization of activations in State Space Models (SSMs) to better preserve information, particularly for outlier values. The authors introduce Quamba-SE, a soft-edge quantizer that utilizes three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers, contrasting with traditional INT8 methods that rely on hard clipping. Experimental results demonstrate that Quamba-SE outperforms the previous Quamba method, achieving improvements of up to +2.68% on individual benchmarks and an average accuracy increase of +0.83% across six zero-shot datasets evaluated on Mamba-130M.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高状态空间模型（SSM）中激活量化的性能，同时保留重要数据特征。作者提出了Quamba-SE，这是一种软边量化器，采用三种自适应尺度进行量化：小值使用高精度，正常值使用标准尺度，异常值使用低精度，这种方法相比传统的硬裁剪方法更好地保留了异常值信息。对Mamba-130M模型在六个零样本基准上的实验评估表明，Quamba-SE在各个基准上均优于之前的Quamba方法，个别基准的提升幅度可达2.68%，在所有数据集上的平均准确率提高了0.83%。</div>
</details>
</div>
<div class="card">
<div class="title">Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</div>
<div class="meta-line">Authors: Jiachen Li, Xiaojin Gong</div>
<div class="meta-line">First: 2023-10-26T08:12:53+00:00 · Latest: 2026-01-14T09:17:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.17218v3">Abs</a> · <a href="https://arxiv.org/pdf/2310.17218v3">PDF</a> · <a href="https://github.com/RikoLi/PCL-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原型对比学习的CLIP微调用于物体重识别</div>
<div class="mono" style="margin-top:8px">本研究旨在将大规模预训练的视觉-语言模型（如对比语言-图像预训练（CLIP））适应于提高物体重识别（Re-ID）在各种监督设置下的性能。尽管提示学习使得名为CLIP-ReID的近期工作取得了良好的性能，但由于ReID任务中缺乏语义标签，其基本机制和提示学习的必要性仍不清楚。在本研究中，我们首先分析了提示学习在CLIP-ReID中的作用，并识别其局限性。基于我们的调查，我们提出了一种简单而有效的方法来适应CLIP用于监督物体Re-ID。我们的方法直接使用原型对比学习（PCL）损失微调CLIP的图像编码器，消除了对提示学习的需求。在人和车的Re-ID数据集上的实验结果表明，我们的方法与CLIP-ReID相比具有竞争力。此外，我们将基于PCL的CLIP微调方法扩展到无监督场景，在那里我们实现了最先进的性能。代码可在https://github.com/RikoLi/PCL-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enhancing object re-identification (Re-ID) performance using large-scale pre-trained vision-language models like CLIP, particularly in the context of unclear mechanisms behind prompt learning. The authors analyze the limitations of prompt learning in CLIP-ReID and propose a novel method that fine-tunes the image encoder of CLIP with a prototypical contrastive learning (PCL) loss, thereby eliminating the need for prompt learning. Experimental results on person and vehicle Re-ID datasets show that this approach is competitive with CLIP-ReID and achieves state-of-the-art performance in unsupervised scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在不明确提示学习机制的情况下，利用大规模预训练的视觉-语言模型（如CLIP）提高物体重识别（Re-ID）性能的挑战。作者分析了CLIP-ReID中提示学习的局限性，并提出了一种新方法，通过原型对比学习（PCL）损失直接微调CLIP的图像编码器，从而消除对提示学习的需求。在人员和车辆Re-ID数据集上的实验结果表明，该方法与CLIP-ReID具有竞争力，并在无监督场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion</div>
<div class="meta-line">Authors: Jialu Li, Taiyan Zhou</div>
<div class="meta-line">First: 2026-01-14T06:38:12+00:00 · Latest: 2026-01-14T06:38:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpikeVAEDiff：基于神经脉冲的自然视觉场景重建通过VD-VAE和多功能扩散</div>
<div class="mono" style="margin-top:8px">从神经活动重建自然视觉场景是神经科学和计算机视觉中的一个关键挑战。我们提出了SpikeVAEDiff，一个新颖的两阶段框架，结合了非常深的变分自编码器（VDVAE）和多功能扩散模型，从神经脉冲数据生成高分辨率和语义丰富的图像重建。在第一阶段，VDVAE通过将神经脉冲信号映射到潜在表示，生成低分辨率的初步重建。在第二阶段，回归模型将神经脉冲信号映射到CLIP-Vision和CLIP-Text特征，使多功能扩散能够通过图像到图像生成来细化图像。我们在艾伦视觉编码-神经像素数据集上评估了我们的方法，并分析了不同的脑区。我们的结果表明，VISI区域表现出最显著的激活，并在重建质量中发挥关键作用。我们展示了成功和不成功的重建示例，反映了解码神经活动的挑战。与基于fMRI的方法相比，脉冲数据提供了更优越的时间和空间分辨率。我们进一步验证了VDVAE模型的有效性，并进行消融研究，表明来自特定脑区的数据显著提高了重建性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing natural visual scenes from neural activity, which is significant in both neuroscience and computer vision. The authors introduce SpikeVAEDiff, a two-stage framework that integrates a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to achieve high-resolution image reconstructions from neural spike data. Experimental results on the Allen Visual Coding-Neuropixels dataset reveal that the VISI region shows the highest activation and is crucial for reconstruction quality, with the study highlighting both successful and unsuccessful reconstruction cases, and demonstrating that spike data outperforms fMRI in terms of temporal and spatial resolution while validating the VDVAE model&#x27;s effectiveness through ablation studies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从神经活动重建自然视觉场景的挑战，这在神经科学和计算机视觉中都具有重要意义。作者提出了SpikeVAEDiff，这是一种将非常深的变分自编码器（VDVAE）与多功能扩散模型结合的两阶段框架，用于从神经脉冲数据生成高分辨率的图像重建。对Allen视觉编码-神经像素数据集的实验结果表明，VISI区域表现出最高的激活，并且对重建质量至关重要，研究强调了成功和不成功的重建案例，同时证明脉冲数据在时间和空间分辨率上优于fMRI，并且特定脑区的数据显著提高了重建性能。</div>
</details>
</div>
<div class="card">
<div class="title">SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</div>
<div class="meta-line">Authors: Youngmin Kim, Giyeong Oh, Kwangsoo Youm, Youngjae Yu</div>
<div class="meta-line">First: 2025-07-14T11:33:47+00:00 · Latest: 2026-01-14T06:25:00+00:00</div>
<div class="meta-line">Comments: Accepted to Automation in Construction. Our project page: https://winston1214.github.io/SlumpGuard/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10171v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.10171v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://winston1214.github.io/SlumpGuard/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SlumpGuard：基于AI的实时自动混凝土坍落度预测系统</div>
<div class="mono" style="margin-top:8px">混凝土的可加工性对施工质量至关重要，坍落度测试是最广泛使用的现场评估方法。然而，传统的坍落度测试是手动的，耗时且高度依赖操作员，不适合在浇筑过程中进行连续或实时监测。为了解决这些局限性，我们提出了SlumpGuard，一个基于AI的视觉系统，通过单个固定摄像头分析搅拌车漏斗的自然排放流。该系统执行自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，实现了无需传感器、硬件安装或人工干预的质量监测。我们介绍了系统设计，构建了一个包含6000多个视频片段的现场复制数据集，并报告了广泛的评估，证明了在多种现场条件下可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测。专家研究进一步揭示了人类视觉估计的显著不一致，突显了自动评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the assessment of concrete workability, which is crucial for construction quality, by addressing the limitations of traditional manual slump testing. The authors developed SlumpGuard, an AI-powered vision system that utilizes a single fixed camera to analyze the discharge flow from a mixer-truck chute, enabling automatic chute detection, pouring-event identification, and video-based slump classification without the need for additional sensors or manual intervention. Experimental results demonstrate that SlumpGuard achieves reliable chute localization, accurate pouring detection, and robust slump prediction across various field conditions, while an expert study indicates significant discrepancies in human visual estimates, underscoring the necessity for automated assessment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决传统人工坍落度测试的局限性，来改善混凝土可加工性的评估，这对建筑质量至关重要。作者开发了SlumpGuard，这是一种利用单个固定摄像头分析搅拌车漏斗排放流的AI视觉系统，能够自动检测漏斗、识别浇筑事件，并进行基于视频的坍落度分类，而无需额外的传感器或人工干预。实验结果表明，SlumpGuard在各种现场条件下实现了可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测，同时专家研究表明人类视觉估计存在显著差异，强调了自动化评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-14T06:01:44+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v2">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应视图生成与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工制作和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两个协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样但语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能，提升幅度为+2.5%。在视觉-语言学习中，它在十个数据集上将平均零-shot分类准确率提高了+12.31%（相较于CLIP）和+5.31%（相较于SLIP），并进一步将Flickr30k文本检索的R@5提高了+3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of contrastive learning by addressing the limitations in the construction of high-quality positive pairs and the lack of a quality assessment mechanism in current methods. The authors propose GenView++, a unified framework that incorporates a multi-source adaptive view generation mechanism to create diverse and semantically coherent views, alongside a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ improves MoCov2 by 2.5% on ImageNet linear classification and increases zero-shot classification accuracy by 12.31% over CLIP and 5.31% over SLIP across ten datasets, while also enhancing Flickr30k text retrieval R@5 by 3.2%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在高质量正对构建和缺乏质量评估机制方面的局限性来增强对比学习。作者提出了GenView++，一个统一框架，结合了多源自适应视图生成机制，以创建多样且语义一致的视图，以及一个质量驱动的对比学习机制，根据语义对齐和多样性动态调整训练贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的性能2.5%，在十个数据集上使得零样本分类准确率比CLIP提高了12.31%，比SLIP提高了5.31%，同时在Flickr30k文本检索R@5上提升了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-14T04:42:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0\%的图像-AUROC和92.2\%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that do not effectively balance semantic generalization and structural discriminability. The authors propose a method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through a Hierarchical Semantic-Visual Synergy (HSVS) mechanism and employs a Vision-Conditioned Prompt Generator (VCPG) for dynamic prompt generation. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升工业环境中的零样本异常检测（ZSAD），目前的技术受到单一视觉骨干网络的限制，无法有效平衡语义泛化和结构可区分性。作者提出了一种名为协同语义-视觉提示（SSVP）的方法，通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，将DINOv3的多尺度结构先验与CLIP语义空间结合。实验结果表明，SSVP在七个工业基准测试中实现了最先进的性能，在MVTec-AD上达到了93.0%的图像AUROC和92.2%的像素AUROC，显著超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives</div>
<div class="meta-line">Authors: Wisdom O. Ikezogwo, Kevin Zhang, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Linda Shapiro, Ranjay Krishna</div>
<div class="meta-line">First: 2025-01-07T23:32:05+00:00 · Latest: 2026-01-14T03:02:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.04184v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.04184v3">PDF</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data">Code1</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to $\textit{think-aloud}$ studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial and temporal association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. $\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data]}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedicalNarratives：将医学视觉与语言连接到本地化叙事</div>
<div class="mono" style="margin-top:8px">多模态模型对数据需求量大。虽然自然图像的数据集丰富，但医学图像数据集无法享受同样的奢侈。为了大规模实现医学图像的表示学习，我们转向YouTube，一个拥有大量开源医学教学视频的平台。我们整理了MedicalNarratives，一个包含470万对医学图像-文本的数据库，其中100万样本包含以空间轨迹（和边界框）形式的密集注释，118K个视频集中于轨迹事件（带对齐文本），使得超越单帧的时空基础成为可能。类似于$\textit{think-aloud}$研究，讲师在相关图像区域上悬停鼠标光标时进行讲解，MedicalNarratives中的100万张图像包含图像像素中的本地化鼠标轨迹，创建了文本与像素之间的空间和时间关联。为了评估MedicalNarratives的实用性，我们使用跨越12个医学领域的数据集训练了GenMedClip，采用类似CLIP的目标。GenMedClip在新构建的医学影像基准上，在所有12个领域中超越了之前的最先进模型。$\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[数据]}$</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the scarcity of large-scale medical image datasets necessary for training multi-modal models. The authors developed MedicalNarratives, a dataset comprising 4.7 million medical image-text pairs sourced from YouTube, which includes 1 million samples with detailed spatial annotations and 118,000 videos that provide spatiotemporal grounding. The key experimental finding is that the model GenMedClip, trained on this dataset using a CLIP-like objective, surpasses previous state-of-the-art models across all 12 medical domains evaluated in a newly constructed medical imaging benchmark.</div>
<div class="mono" style="margin-top:8px">本研究通过利用YouTube上大量开放源代码的医学教学视频，解决了医学图像数据集有限的问题，创建了MedicalNarratives数据集，该数据集包含470万对医学图像-文本配对。该方法涉及策划100万个样本，提供详细的注释，包括空间轨迹和边界框，以建立文本与图像之间的时空基础。实验结果表明，基于该数据集并采用类似CLIP的目标训练的GenMedClip，在新构建的基准测试中超越了所有12个医学领域的先前最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models</div>
<div class="meta-line">Authors: Ganxi Xu, Zhao-Rong Lai, Yuting Tang, Yonghao Song, Guoxu Zhou, Boyu wang, Jian Zhu, Jinyi Long</div>
<div class="meta-line">First: 2025-08-31T10:29:58+00:00 · Latest: 2026-01-14T01:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.00787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M/EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present a novel image-to-brain signal framework that generates M/EEG from images by leveraging the diffusion transformer architecture enhanced with cross-attention mechanisms. Specifically, we employ a diffusion transformer (DiT) architecture based on denoising diffusion implicit models (DDIM) to achieve brain signal generation. To realize the goal of image-to-brain signal conversion, we use cross-attention mechanisms to align brain signal embeddings with CLIP image embeddings. Moreover, we leverage large language models (LLMs) to generate image captions, and concatenate the resulting CLIP text embeddings with CLIP image embeddings to form unified embeddings for cross-attention alignment, enabling our model to capture core semantic information. Moreover, to capture core semantic information, we use large language models (LLMs) to generate descriptive and semantically accurate captions for images. Furthermore, we introduce a learnable spatio-temporal position encoding that combines brain region embeddings with temporal embeddings to capture both spatial and temporal characteristics of brain signals. We evaluate the framework on two multimodal benchmark datasets (THINGS-EEG2 and THINGS-MEG) and demonstrate that it generates biologically plausible brain signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP引导的多模态扩散模型的图像到脑信号生成用于视觉假体</div>
<div class="mono" style="margin-top:8px">视觉假体在恢复盲人视力方面具有巨大潜力。虽然研究人员成功利用M/EEG信号在视觉假体的脑解码阶段引发视觉感知，但在脑编码阶段将图像转换为M/EEG信号的互补过程仍然未被充分探索，阻碍了完整功能管道的形成。在本研究中，我们提出了一种新颖的图像到脑信号框架，通过利用增强了交叉注意机制的扩散变换器架构从图像生成M/EEG信号。具体而言，我们采用基于去噪扩散隐式模型（DDIM）的扩散变换器（DiT）架构来实现脑信号生成。为了实现图像到脑信号转换的目标，我们使用交叉注意机制将脑信号嵌入与CLIP图像嵌入对齐。此外，我们利用大型语言模型（LLMs）生成图像标题，并将生成的CLIP文本嵌入与CLIP图像嵌入连接，形成统一的嵌入以进行交叉注意对齐，使我们的模型能够捕捉核心语义信息。此外，为了捕捉核心语义信息，我们使用大型语言模型（LLMs）为图像生成描述性和语义准确的标题。此外，我们引入了一种可学习的时空位置编码，将脑区嵌入与时间嵌入结合，以捕捉脑信号的空间和时间特征。我们在两个多模态基准数据集（THINGS-EEG2和THINGS-MEG）上评估了该框架，并证明其生成生物学上合理的脑信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap in converting images into M/EEG signals during the brain encoding stage of visual prostheses, which is crucial for restoring vision in blind individuals. The authors propose a novel framework that utilizes a diffusion transformer architecture enhanced with cross-attention mechanisms to generate M/EEG signals from images. Key experimental findings indicate that the framework effectively produces biologically plausible brain signals when evaluated on two multimodal benchmark datasets, THINGS-EEG2 and THINGS-MEG, demonstrating its potential for advancing visual prosthesis technology.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在视觉假体的脑编码阶段将图像转换为M/EEG信号的空白，这对恢复盲人视力至关重要。作者提出了一种新颖的框架，利用增强了交叉注意机制的扩散变换器架构，从图像生成M/EEG信号。对两个多模态基准数据集THINGS-EEG2和THINGS-MEG的实验结果表明，该方法成功生成生物学上合理的脑信号，从而为视觉假体的更完整功能管道做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型快速发展，但数据在影响运动方面的作用仍不清楚。我们提出了Motive（视频生成的运动归因），这是一个以运动为中心的基于梯度的数据归因框架，能够扩展到现代大型高质量视频数据集和模型。我们利用此框架研究哪些微调片段能改善或恶化时间动态。Motive通过运动加权损失掩码将时间动态与静态外观隔离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型中，Motive识别出强烈影响运动的片段，并指导数据整理，以改善时间一致性和物理合理性。使用Motive选择的高影响数据，我们的方法在VBench上提高了运动平滑性和动态程度，与预训练基础模型相比，获得了74.1%的人工偏好胜率。据我们所知，这是第一个在视频生成模型中归因于运动而非视觉外观的框架，并利用它来整理微调数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to better understand how data influences motion in video generation models, which remains poorly understood despite advancements in the field. The authors present Motive, a motion-centric, gradient-based data attribution framework designed to analyze large, high-quality video datasets and models. Their experiments reveal that Motive can effectively isolate temporal dynamics from static appearance, allowing for the identification of fine-tuning clips that enhance motion smoothness and dynamic degree, resulting in a 74.1% human preference win rate on VBench compared to the pretrained base model. This work represents the first framework focused on motion attribution in video generation, facilitating improved data curation for enhanced temporal consistency and physical plausibility.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于更好地理解数据如何影响视频生成模型中的运动，尽管该领域已取得进展，但这一点仍然不够清晰。作者提出了Motive，这是一个以运动为中心的基于梯度的数据归因框架，旨在分析和改善大型视频数据集中的时间动态。实验结果表明，Motive有效识别出显著影响运动的剪辑，从而提高生成视频的时间一致性和物理合理性，运动平滑度和动态程度显著改善，相较于预训练基础模型，获得了74.1%的人工偏好胜率。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：用于行人重识别的视频超分辨率</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹质量通常被视为事后考虑，绝大多数研究集中于对基础模型的架构修改。这些方法忽视了一个重要的限制，在现实世界的困难场景中部署ReID系统时面临挑战。本文介绍了S3-CLIP，一种基于视频超分辨率的CLIP-ReID框架，旨在2026年WACV的VReID-XFD挑战中开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，适应视频基础的行人重识别设置。据我们所知，这项工作代表了首次系统性研究视频超分辨率作为提高行人ReID轨迹质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法在基线性能上具有竞争力，在空中到地面场景中实现了37.52%的mAP，在地面到空中场景中实现了29.16%的mAP。在地面到空中的设置中，S3-CLIP在排名准确性上取得了显著提升，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the often-overlooked quality of tracklets in person re-identification (ReID) systems, which poses challenges in real-world scenarios. The authors introduce S3-CLIP, a video super-resolution-based framework that combines advancements in super-resolution networks with task-driven pipelines specifically for video-based ReID. Experimental results indicate that S3-CLIP achieves competitive performance with a mean Average Precision (mAP) of 37.52% in aerial-to-ground and 29.16% in ground-to-aerial scenarios, along with significant improvements in ranking accuracy in the ground-to-aerial setting, with increases of 11.24%, 13.48%, and 17.98% for Rank-1, Rank-5, and Rank-10, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人脸重识别（ReID）系统中常被忽视的轨迹质量问题，这在实际应用中带来了挑战。作者提出了S3-CLIP，这是一种将视频超分辨率技术与CLIP-ReID相结合的新框架，旨在提高轨迹质量，特别是在困难的跨视角场景中。实验结果表明，S3-CLIP在空中到地面条件下的mAP达到37.52%，在地面到空中条件下的mAP为29.16%，并且在排名准确性上有显著提升，Rank-1、Rank-5和Rank-10的表现分别提高了11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Visual World with Artificial Intelligence: A Roadmap</div>
<div class="meta-line">Authors: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu</div>
<div class="meta-line">First: 2025-11-11T18:59:50+00:00 · Latest: 2026-01-13T15:42:01+00:00</div>
<div class="meta-line">Comments: Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08585v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08585v2">PDF</a> · <a href="https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://world-model-roadmap.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a &quot;window&quot; into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用人工智能模拟视觉世界：路线图</div>
<div class="mono" style="margin-top:8px">视频生成的格局正在发生变化，从专注于生成视觉吸引人的片段转向构建支持交互并保持物理合理性的虚拟环境。这些发展指向视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐式世界模型，模拟支配真实或想象世界的物理动态、代理-环境交互和任务规划。本调查提供了这一演变的系统概述，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型和视频渲染器。世界模型编码了关于世界的结构化知识，包括物理法则、交互动态和代理行为。它作为潜在的模拟引擎，使得连贯的视觉推理、长期时间一致性和目标驱动的规划成为可能。视频渲染器将这种潜在模拟转化为现实的视觉观察，有效地将视频作为“窗口”展示模拟世界。我们通过四个世代追踪视频生成的进展，其中核心能力逐步提升，最终形成一个建立在视频生成模型之上的世界模型，体现内在的物理合理性、实时多模态交互和跨多个时空尺度的规划能力。对于每一代，我们定义其核心特征，突出代表性作品，并考察其应用领域，如机器人技术、自动驾驶和互动游戏。最后，我们讨论下一代世界模型的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作的最新列表可在此链接查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the transition in video generation from creating visually appealing clips to developing interactive virtual environments that maintain physical plausibility. The authors employ a systematic survey method to conceptualize modern video foundation models, which consist of an implicit world model that encodes structured knowledge about physical laws and agent interactions, and a video renderer that produces realistic visual outputs. Key findings indicate that the evolution of video generation can be categorized into four generations, each enhancing core capabilities and culminating in a world model that supports real-time interaction and planning across various spatiotemporal scales, with applications in fields such as robotics and gaming.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于将视频生成从单纯创建视觉吸引力的剪辑提升到开发保持物理合理性的互动虚拟环境。作者系统地调查了视频基础模型的发展，这些模型结合了编码物理法则和代理行为的隐式世界模型与生成真实视觉输出的视频渲染器。主要发现揭示了视频生成能力的四个世代的进展，最终形成一个支持实时多模态互动和跨多个时空尺度规划的模型，应用于机器人技术和自动驾驶等领域，同时识别出未来发展的挑战和设计原则。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts</div>
<div class="meta-line">Authors: Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Loppi, Antti Honkela</div>
<div class="meta-line">First: 2024-06-25T06:04:58+00:00 · Latest: 2026-01-13T15:13:42+00:00</div>
<div class="meta-line">Comments: 19 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.17298v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.17298v3">PDF</a> · <a href="https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无捷径的差分隐私深度学习的高效可扩展实现</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DP-SGD）是基于差分隐私（DP）训练机器学习模型的标准算法。最常见的DP-SGD隐私会计依赖于泊松子采样以确保理论DP保证。使用泊松子采样实现计算高效的DP-SGD并非易事，这导致许多实现通过使用计算更快的子采样走捷径。我们通过实现和基准测试正确的泊松子采样的高效方法来量化在DP下训练深度学习模型的计算成本。我们发现，使用PyTorch中Opacus的DP-SGD的天真实现，其吞吐量比SGD低2.6到8倍。然而，像Ghost Clipping这样的高效梯度裁剪实现可以大致将此成本减半。我们提出了一种使用JAX的DP-SGD的替代计算高效实现，该实现使用泊松子采样，并与基于PyTorch的高效裁剪优化表现相当。我们研究了使用多达80个GPU的扩展行为，发现DP-SGD的扩展性优于SGD。我们在https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL分享我们的库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges of implementing differentially private stochastic gradient descent (DP-SGD) efficiently while adhering to theoretical privacy guarantees. The authors benchmark various methods for training deep learning models under differential privacy, focusing on the computational costs associated with Poisson subsampling. Their findings reveal that the naive implementation of DP-SGD using Opacus in PyTorch exhibits a throughput 2.6 to 8 times lower than standard SGD, but the use of efficient gradient clipping techniques like Ghost Clipping can significantly reduce this cost. Additionally, they propose a new implementation of DP-SGD using JAX that maintains the benefits of Poisson subsampling and demonstrates comparable performance to optimized PyTorch methods, with improved scaling behavior when utilizing up to 80 GPUs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高在差分隐私下训练深度学习模型的效率，使用标准的DP-SGD算法，该算法通常依赖于泊松子抽样来确保隐私保证。作者实现并基准测试了各种方法，以量化DP-SGD的计算成本，发现简单实现的速度可能显著低于标准SGD。他们发现，像Ghost Clipping这样的高效梯度裁剪技术可以将这一成本减少约50%，并提出了一种使用JAX的新实现DP-SGD，该实现保持了泊松子抽样的优势，同时在性能上与优化的PyTorch方法相当。此外，他们的实验表明，在使用多达80个GPU时，DP-SGD的扩展性优于SGD。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</div>
<div class="meta-line">Authors: Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</div>
<div class="meta-line">First: 2026-01-13T13:42:05+00:00 · Latest: 2026-01-13T13:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08557v1">PDF</a> · <a href="https://github.com/Simula/HEDGE#videohedge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHEDGE：基于熵的视觉语言模型视频幻觉检测框架，通过语义聚类和时空扰动</div>
<div class="mono" style="margin-top:8px">在视频能力的视觉语言模型（Video-VLMs）中，幻觉现象仍然频繁且置信度高，而现有的不确定性度量往往无法与正确性对齐。我们提出了VideoHEDGE，这是一个用于视频问答中幻觉检测的模块化框架，将基于熵的可靠性估计从图像扩展到时间结构化输入。给定一个视频-问题对，VideoHEDGE从干净片段和光度及时空扰动变体中提取基线答案和多个高温生成，然后使用基于自然语言推理（NLI）或嵌入的方法将结果文本输出聚类为语义假设。聚类级别的概率质量产生三个可靠性评分：语义熵（SE）、RadFlag和视觉增强语义熵（VASE）。我们在SoccerChat基准上评估VideoHEDGE，使用LLM作为评判者获得二元幻觉标签。在三个7B Video-VLM（Qwen2-VL、Qwen2.5-VL和一个SoccerChat微调模型）中，VASE在较大的失真预算下始终实现最高的ROC-AUC，而SE和RadFlag的表现往往接近随机。我们进一步表明，基于嵌入的聚类在检测性能上与基于NLI的聚类相匹配，但计算成本显著较低，并且领域微调减少了幻觉频率，但在校准方面仅带来了适度的改善。hedge-bench PyPI库支持可重复和可扩展的基准测试，完整代码和实验资源可在https://github.com/Simula/HEDGE#videohedge获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the frequent and high-confidence hallucinations in video-capable vision-language models (Video-VLMs), which existing uncertainty metrics fail to accurately assess. The authors introduce VideoHEDGE, a modular framework for detecting hallucinations in video question answering that utilizes entropy-based reliability estimation adapted for temporally structured inputs. Experimental results on the SoccerChat benchmark demonstrate that the Vision-Amplified Semantic Entropy (VASE) score consistently outperforms other reliability metrics, achieving the highest ROC-AUC, particularly under larger distortion budgets, while also showing that embedding-based clustering can match the performance of Natural Language Inference-based clustering at a lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频能力视觉语言模型（Video-VLMs）中频繁且高置信度的幻觉问题，而现有的不确定性度量往往无法准确评估。作者提出了VideoHEDGE，这是一个模块化框架，将基于熵的可靠性估计扩展到视频问答中，通过生成来自干净和扰动视频片段的基线答案和高温输出，然后对文本输出进行语义聚类。对SoccerChat基准的实验结果表明，增强语义熵（VASE）得分在较大失真预算下表现优于其他可靠性度量，获得了最高的ROC-AUC，同时还显示嵌入式聚类可以以较低的计算成本匹配基于自然语言推理的聚类性能。</div>
</details>
</div>
<div class="card">
<div class="title">Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning</div>
<div class="meta-line">Authors: Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano</div>
<div class="meta-line">First: 2025-07-18T17:59:55+00:00 · Latest: 2026-01-13T13:22:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14137v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14137v3">PDF</a> · <a href="https://github.com/valeoai/Franca">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Franca：用于可扩展视觉表示学习的嵌套俄罗斯套娃聚类</div>
<div class="mono" style="margin-top:8px">我们介绍Franca（发音为Fran-ka）：免费的第一个完全开源（数据、代码、权重）视觉基础模型，其性能与许多情况下超过最先进的专有模型，如DINOv2、CLIP、SigLIPv2等。我们的方法基于受Web-SSL启发的透明训练流程，并使用公开可用的数据：ImageNet-21K和ReLAION-2B的一个子集。除了模型发布外，我们还解决了SSL聚类方法中的关键限制。现代模型依赖于通过像Sinkhorn-Knopp这样的聚类算法将图像特征分配给大型代码本，但未能考虑聚类语义中的固有模糊性。为了解决这个问题，我们引入了一种基于嵌套俄罗斯套娃表示的参数高效多头聚类投影器。该设计逐步将特征细化为越来越细粒度的聚类，而不增加模型大小，从而实现性能和内存效率。此外，我们提出了一种新颖的位置解耦策略，明确消除密集表示中的位置偏差，从而改善语义内容的编码。这在多个下游基准测试中带来了持续的提升，证明了更清晰特征空间的实用性。我们的贡献为透明、高性能视觉模型建立了新的标准，并为更可重复和可推广的基础模型开辟了通向更广泛AI社区的道路。代码和模型检查点可在https://github.com/valeoai/Franca获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a fully open-source vision foundation model that not only matches but often exceeds the performance of leading proprietary models. The authors developed Franca, which employs a transparent training pipeline inspired by Web-SSL and utilizes publicly available datasets such as ImageNet-21K and a subset of ReLAION-2B. Key experimental findings indicate that their novel multi-head clustering projector, based on nested Matryoshka representations, effectively refines image features into finer clusters without increasing model size, while a new positional disentanglement strategy enhances semantic content encoding, resulting in improved performance on various downstream benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个完全开源的视觉基础模型，该模型能够匹配或超越领先的专有模型的性能，同时解决自监督学习（SSL）聚类方法的局限性。作者提出了Franca，采用基于嵌套马特里奥什卡表示的参数高效多头聚类投影器，将图像特征细化为细粒度聚类，而不增加模型大小。关键实验结果表明，这种方法以及一种新颖的位置信息解耦策略在下游基准测试中显著提高了性能，突显了更清晰特征空间的有效性，并为高性能视觉模型建立了新的标准。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Autoregressive Generation</div>
<div class="meta-line">Authors: Stepan Maschan, Haoxuan Qu, Jun Liu</div>
<div class="meta-line">First: 2026-01-06T17:07:27+00:00 · Latest: 2026-01-13T11:19:48+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03184v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去中心化自回归生成</div>
<div class="mono" style="margin-top:8px">我们对自回归生成的去中心化进行了理论分析。我们通过将概率生成速度表示为专家流的线性组合，定义了去中心化离散流匹配目标。我们还进行了实验，展示了在多模态语言模型的不同基准测试中，去中心化和中心化训练设置之间的等价性。具体而言，我们比较了两种不同的范式：LLaVA和InternVL 2.5-1B，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调（ViT+MLP+LLM）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the decentralization of autoregressive generation to enhance multimodal language models. The authors introduce the Decentralized Discrete Flow Matching objective, which models probability generating velocity as a combination of expert flows. Experimental results indicate that decentralized and centralized training settings yield equivalent performance across various benchmarks when comparing the LLaVA and InternVL 2.5-1B models, which utilize a fixed CLIP vision encoder and undergo full-parameter fine-tuning during instruction tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是分析多模态语言模型中自回归生成的去中心化。作者提出了去中心化离散流匹配目标，该目标将概率生成速度表示为专家流的线性组合。实验结果表明，在比较使用固定CLIP视觉编码器并在指令调优阶段进行全参数微调的LLaVA和InternVL 2.5-1B模型时，去中心化和中心化训练设置在各种基准测试中表现出等效的性能。</div>
</details>
</div>
<div class="card">
<div class="title">MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</div>
<div class="meta-line">Authors: Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra, Biplab Banerjee</div>
<div class="meta-line">First: 2026-01-13T10:44:37+00:00 · Latest: 2026-01-13T10:44:37+00:00</div>
<div class="meta-line">Comments: Accepted at InGARSS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08420v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08420v1">PDF</a> · <a href="https://github.com/AdityaChaudhary2913/CLIP_HSI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMLGNet：使用CLIP进行遥感数据的跨模态对齐</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的多模态框架——多模态语言引导网络（MMLGNet），旨在使用视觉-语言模型（如CLIP）对异构遥感模态（如高光谱成像（HSI）和激光雷达（LiDAR））进行对齐。随着多模态地球观测数据的日益增加，迫切需要有效融合光谱、空间和几何信息的方法，同时实现语义级理解。MMLGNet采用特定模态的编码器，通过双向对比学习在共享潜在空间中将视觉特征与手工制作的文本嵌入对齐。受到CLIP训练范式的启发，我们的方法弥合了高维遥感数据与语言引导解释之间的差距。值得注意的是，MMLGNet在简单的基于CNN的编码器上表现出色，在两个基准数据集上超越了几种已建立的仅视觉多模态方法，展示了语言监督的显著优势。代码可在https://github.com/AdityaChaudhary2913/CLIP_HSI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of aligning heterogeneous remote sensing modalities, such as Hyperspectral Imaging and LiDAR, with natural language semantics due to the increasing availability of multimodal Earth observation data. The proposed Multimodal Language-Guided Network (MMLGNet) utilizes modality-specific encoders and bi-directional contrastive learning to align visual features with textual embeddings in a shared latent space. The experimental results indicate that MMLGNet, using simple CNN-based encoders, significantly outperforms established multimodal visual-only methods on two benchmark datasets, highlighting the advantages of language supervision in remote sensing applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于日益需要有效融合异构遥感模态，如高光谱成像和激光雷达，与自然语言语义，以增强语义层面的理解。作者提出了一种名为多模态语言引导网络（MMLGNet）的新框架，该框架利用特定模态的编码器和双向对比学习，将视觉特征与共享潜在空间中的文本嵌入对齐。实验结果表明，MMLGNet使用简单的CNN编码器在两个基准数据集上显著优于已建立的多模态视觉方法，突显了在遥感数据解释中纳入语言监督的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care</div>
<div class="meta-line">Authors: Jorge García-Torres, Øyvind Meinich-Bache, Sara Brunner, Siren Rettedal, Vilde Kolstad, Kjersti Engan</div>
<div class="meta-line">First: 2025-03-05T07:52:52+00:00 · Latest: 2026-01-13T10:18:52+00:00</div>
<div class="meta-line">Comments: This work has been accepted at IEEE 25th International Conference on Digital Signal Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03244v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03244v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Around 10% of newborns require some help to initiate breathing, and 5\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双流热成像融合技术用于增强新生儿护理中的出生时间检测</div>
<div class="mono" style="margin-top:8px">约10%的新生儿需要帮助以启动呼吸，5%需要通气支持。准确的出生时间（ToB）记录对于优化新生儿护理至关重要，因为及时干预对适当复苏至关重要。然而，目前记录ToB的临床方法往往依赖手动过程，容易出现不准确。在本研究中，我们提出了一种新颖的双流融合系统，结合图像和视频分析的优势，从分娩室和手术室的热成像记录中准确检测ToB。通过整合静态和动态流，我们的方法捕捉到更丰富的与出生相关的时空特征，从而实现更强大和精确的ToB估计。我们证明了数据模态之间的协同作用在性能上优于单流方法。我们的系统在短视频片段中检测出生的精确度达到95.7%，召回率为84.8%。此外，借助评分聚合模块，它成功识别100%的测试案例中的ToB，较手动标注的中位绝对误差为2秒，绝对平均偏差为4.5秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of Time of Birth (ToB) documentation in neonatal care, which is crucial for timely interventions in newborns requiring assistance. The authors developed a two-stream fusion system that integrates image and video analysis to detect ToB from thermal recordings in delivery rooms and operating theaters. The experimental results show that this method achieves 95.7% precision and 84.8% recall in detecting birth events within short video clips, and it successfully identifies ToB in 100% of test cases with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于新生儿护理中准确记录出生时间（ToB）的迫切需求，因为及时干预对新生儿健康至关重要。作者开发了一种双流融合系统，结合图像和视频分析，从分娩室和手术室的热成像记录中检测ToB。实验结果表明，该方法在短视频片段中检测出生事件的精确度达到95.7%，召回率为84.8%，并且在所有测试案例中成功识别ToB，较手动标注的中位绝对误差仅为2秒。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment</div>
<div class="meta-line">Authors: Hua Ye, Hang Ding, Siyuan Chen, Yiyang Jiang, Changyuan Zhang, Xuan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-11T16:15:15+00:00 · Latest: 2026-01-13T06:56:09+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08399v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过不对齐进行对齐：边界感知的多模态对齐课程学习</div>
<div class="mono" style="margin-top:8px">大多数多模态模型将每个负样本视为相同，忽略了与正样本仅在细节上有所不同的模糊负样本。我们提出了边界感知课程与局部注意力（BACL），这是一个轻量级的附加模块，将这些边界案例转化为课程信号。边界感知负样本采样器逐渐提高难度，而对比局部注意力损失则突出不匹配发生的地方。这两个模块都是完全可微分的，并且可以与任何现成的双编码器配合使用。理论预测错误率快速下降至O(1/n)；实践中显示在CLIP上提高了高达32%的R@1，并在四个大规模基准上达到了新的SOTA，且无需额外标签。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of multimodal models treating all negative pairs uniformly, which overlooks the subtle differences in ambiguous negatives. The authors introduce the Boundary-Aware Curriculum with Local Attention (BACL), which incorporates a Boundary-aware Negative Sampler to progressively increase difficulty and a Contrastive Local Attention loss to identify mismatch areas. Experimental results demonstrate that BACL achieves up to a 32% improvement in R@1 over CLIP and sets new state-of-the-art performance on four large-scale benchmarks, all without requiring additional labels.</div>
<div class="mono" style="margin-top:8px">本研究解决了多模态模型中模糊负样本的处理问题，这些负样本常常被统一对待，尽管它们与正样本之间的差异微小。作者提出了一种名为边界感知课程学习与局部注意力（BACL）的方法，该方法结合了边界感知负样本采样器，逐步提高这些案例的难度，并采用对比局部注意力损失来定位不匹配之处。实验结果表明，BACL在R@1上比CLIP提高了多达32%，并在四个大规模基准测试中创造了新的最先进性能，且无需额外标签。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向预测的深度强化学习在奶牛场高效电力负荷调度中的应用</div>
<div class="mono" style="margin-top:8px">奶牛养殖是一个能源密集型行业，严重依赖电网电力。随着可再生能源的不断整合，可持续能源管理已成为减少对电网依赖和支持联合国可持续发展目标7（可负担和清洁能源）的关键。然而，可再生能源的间歇性特征在实时平衡供需方面带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面显示出潜力。然而，大多数基于RL的调度方法假设对未来价格或发电有完全的了解，这在动态环境中是不现实的。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，通常导致在可变电价下训练不稳定。为了解决这些挑战，本研究提出了一种深度强化学习框架，用于在奶牛场高效调度负荷，重点关注电池储存和水加热，并考虑现实的操作约束。所提出的面向预测的PPO结合了基于日时和月份的残差校准的短期需求和可再生发电预测，而PID KL PPO变体则采用比例-积分-微分控制器自适应调节KL散度，以实现稳定的策略更新。该方法在真实奶牛场数据上训练，电力成本比PPO低1%，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO将电网进口减少了13.1%，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance energy management in dairy farming, which is heavily reliant on grid electricity and faces challenges due to the intermittent nature of renewable energy sources. The study introduces a Deep Reinforcement Learning framework, specifically the Forecast Aware PPO, which utilizes short-term forecasts of demand and renewable generation to improve load scheduling while addressing the limitations of traditional RL methods that assume complete knowledge of future conditions. Experimental results indicate that the proposed method achieves up to 1% lower electricity costs compared to standard PPO, 4.8% compared to DQN, and 1.5% compared to SAC, while also reducing grid imports by 13.1% for battery scheduling, demonstrating its effectiveness in sustainable energy management for dairy farms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善奶牛养殖中的能源管理，该行业严重依赖电网电力，并面临可再生能源间歇性带来的挑战。研究提出了一种深度强化学习框架，特别是预测感知PPO，利用需求和可再生发电的短期预测，同时解决了传统强化学习方法假设完全了解未来条件的局限性。实验结果表明，该方法在电力成本上比标准PPO低1%，比DQN低4.8%，比SAC低1.5%，并且在电池调度方面减少了13.1%的电网进口，表明其在奶牛养殖可持续能源管理中的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
