<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0342</div>
    <div class="row"><div class="card">
<div class="title">MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding</div>
<div class="meta-line">Authors: Zhiyi Zhu, Xiaoyu Wu, Zihao Liu, Linlin Yang</div>
<div class="meta-line">First: 2025-06-10T07:20:12+00:00 · Latest: 2026-01-27T18:07:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08512v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08512v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MLVTG：基于Mamba的特征对齐和LLM驱动的多模态视频时间定位净化</div>
<div class="mono" style="margin-top:8px">视频时间定位（VTG）旨在定位与自然语言查询对应的视频片段，是视频理解中的一项基本但具有挑战性的任务。现有的基于Transformer的方法常常面临冗余注意力和次优的多模态对齐问题。为了解决这些局限性，我们提出了MLVTG，一个新颖的框架，集成了两个关键模块：MambaAligner和LLMRefiner。MambaAligner使用堆叠的视觉Mamba块作为骨干，而不是Transformer，以建模时间依赖性并提取稳健的视频表示以实现多模态对齐。LLMRefiner利用预训练的大型语言模型（LLM）的特定冻结层，隐式传递语义先验，增强多模态对齐而无需微调。这种双重对齐策略，通过结构化状态空间动态进行时间建模，通过文本先验进行语义净化，实现了更精确的定位。在QVHighlights、Charades-STA和TVSum上的大量实验表明，MLVTG达到了最先进的性能，并显著超越了现有基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Video Temporal Grounding (VTG), a challenging task that involves localizing video clips based on natural language queries, which is hindered by issues in existing Transformer-based methods. The authors propose a novel framework called MLVTG, which incorporates two main components: MambaAligner, utilizing stacked Vision Mamba blocks for better temporal modeling and video representation, and LLMRefiner, which employs a frozen layer of a pre-trained Large Language Model to enhance multi-modal alignment without requiring fine-tuning. Experimental results on datasets such as QVHighlights, Charades-STA, and TVSum show that MLVTG achieves state-of-the-art performance, significantly surpassing existing methods.</div>
<div class="mono" style="margin-top:8px">本研究针对视频时间定位（VTG）中的挑战，该任务涉及根据自然语言查询定位视频片段，提出了一种新的框架MLVTG。该框架包含两个主要组件：MambaAligner，利用堆叠的视觉Mamba模块进行改进的时间建模和视频表示；LLMRefiner，使用预训练大型语言模型的冻结层来增强多模态对齐，而无需微调。对QVHighlights、Charades-STA和TVSum等数据集的实验结果表明，MLVTG实现了最先进的性能，显著超越了该领域现有的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living</div>
<div class="meta-line">Authors: Huy Trinh</div>
<div class="meta-line">First: 2026-01-27T18:06:51+00:00 · Latest: 2026-01-27T18:06:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19853v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to &quot;empty room&quot; and &quot;person present&quot;, and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the &quot;person present&quot; class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas &quot;empty room&quot; samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于可解释的基于雷达的占用检测的生成潜在对齐</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们研究如何使毫米波雷达的存在检测在环境辅助生活（AAL）设置中更具可解释性，因为基于摄像头的感知引发了隐私问题。我们提出了一种生成潜在对齐（GLA）框架，该框架结合了轻量级卷积变分自编码器和冻结的CLIP文本编码器，以学习雷达范围-角度（RA）热图的低维潜在表示。潜在空间与对应于“空房间”和“有人在场”的两个语义锚点软对齐，并在该对齐的潜在空间中应用Grad-CAM，以可视化哪些空间区域支持每个存在决策。在我们的毫米波雷达数据集中，我们定性观察到“有人在场”类别产生的紧凑Grad-CAM斑块与强RA返回重合，而“空房间”样本则产生模糊或没有证据。我们还使用无关的文本提示进行了消融研究，这降低了重建和定位的效果，表明雷达特定的锚点在此设置中对于有意义的解释是重要的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for interpretable presence detection using mmWave radar in Ambient Assisted Living environments, where privacy concerns limit the use of cameras. The authors introduce a Generative Latent Alignment (GLA) framework that integrates a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to create a low-dimensional latent representation of radar Range-Angle heatmaps. Experimental results demonstrate that the &#x27;person present&#x27; class generates distinct Grad-CAM visualizations that align with strong radar returns, while &#x27;empty room&#x27; instances show less defined evidence, highlighting the effectiveness of radar-specific semantic anchors for interpretation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在环境辅助生活中使用毫米波雷达进行可解释性存在检测的需求，因为隐私问题限制了摄像头的使用。作者提出了一种生成潜在对齐（GLA）框架，该框架将轻量级卷积变分自编码器与冻结的CLIP文本编码器结合，以创建雷达范围-角度热图的低维潜在表示。实验结果表明，&#x27;有人在场&#x27;类别生成的Grad-CAM斑块紧凑且与强雷达回波对齐，而&#x27;空房间&#x27;样本则表现出模糊或缺乏证据，这表明雷达特定语义锚点在增强占用检测的可解释性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-27T18:04:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>它会零-shot吗？：预测任意查询的零-shot分类性能</div>
<div class="mono" style="margin-top:8px">视觉-语言模型如CLIP为文本和图像创建了对齐的嵌入空间，使任何人都可以通过简单命名他们想要区分的类别来构建视觉分类器。然而，在一个领域表现良好的模型在另一个领域可能会失败，非专家用户没有简单的方法来评估他们选择的VLM是否适用于他们的问题。我们基于先前的工作，使用仅文本的比较来评估模型在给定自然语言任务中的表现，并探索生成与该任务相关的合成图像的方法，以评估和改进零-shot准确性的预测。我们展示了生成的图像显著提高了基于文本的基线分数的预测质量。此外，它为用户提供了关于用于评估的图像类型的反馈。在标准CLIP基准数据集上的实验表明，基于图像的方法帮助用户在没有任何标记示例的情况下预测VLM是否对他们的应用有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge faced by non-expert users in predicting the effectiveness of Vision-Language Models (VLMs) for zero-shot classification tasks across different domains. The authors build upon previous methods that utilize text-only comparisons and introduce an approach that incorporates synthetic image generation relevant to the classification task to enhance the prediction of zero-shot accuracy. Experimental results on standard CLIP benchmark datasets reveal that this image-based method significantly improves the accuracy of predictions compared to text-only assessments, providing users with valuable feedback on the types of images influencing their evaluations.</div>
<div class="mono" style="margin-top:8px">本研究解决了非专业用户在预测视觉语言模型（VLM）在不同领域的零样本分类任务有效性时所面临的挑战。作者在之前使用仅文本比较的方法的基础上，提出了一种生成与分类任务相关的合成图像的方法，以增强零样本准确性的预测。对标准CLIP基准数据集的实验结果表明，结合生成的图像显著提高了预测的准确性，相较于仅文本评估，给用户提供了关于评估过程中使用的图像类型的有价值反馈。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts</div>
<div class="meta-line">Authors: TrungKhang Tran, TrungTin Nguyen, Gersende Fort, Tung Doan, Hien Duy Nguyen, Binh T. Nguyen, Florence Forbes, Christopher Drovandi</div>
<div class="meta-line">First: 2026-01-27T17:12:15+00:00 · Latest: 2026-01-27T17:12:15+00:00</div>
<div class="meta-line">Comments: TrungKhang Tran and TrungTin Nguyen are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19811v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视增量随机主化-最小化算法及其在专家混合中的应用</div>
<div class="mono" style="margin-top:8px">在现代统计和机器学习中，处理高容量流数据越来越普遍，而批处理模式算法通常不切实际，因为它们需要对完整数据集进行多次遍历。这促使了增量随机估计方法的发展，包括通过随机逼近公式化的增量随机期望最大化（EM）算法。在本研究中，我们重新审视并分析了增量随机主化-最小化（MM）算法的变体，它将增量随机EM作为特例进行推广。我们的方法放宽了EM的关键要求，例如显式潜变量表示，从而实现更广泛的适用性和更大的算法灵活性。我们为增量随机MM算法建立了理论保证，证明了在迭代过程中收敛到由目标函数的梯度消失特征的驻点的一致性。我们在一个软最大门控的专家混合（MoE）回归问题上展示了这些优势，该问题没有可用的随机EM算法。从经验上看，我们的方法始终优于广泛使用的随机优化器，包括随机梯度下降、均方根传播、自适应动量估计和二阶剪切随机优化。这些结果支持新增量随机算法的发展，因为软最大门控MoE架构在当代深度神经网络中对异构数据建模起着核心作用。除了合成实验外，我们还在两个真实世界数据集上验证了实际有效性，包括一个关于干旱胁迫下的玉米基因型的生物信息学研究，该研究将高维蛋白质组学与生态生理特征相结合，其中增量随机MM在预测性能上带来了稳定的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenges of processing high-volume, streaming data in modern statistics and machine learning, where traditional batch-mode algorithms are often impractical. The authors revisit an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes the incremental stochastic Expectation-Maximization (EM) algorithm, allowing for greater flexibility by relaxing key EM requirements. The study establishes theoretical guarantees for the incremental stochastic MM algorithm, demonstrating that its iterates converge to a stationary point with a vanishing gradient, and empirically shows that it outperforms several widely used stochastic optimizers on a softmax-gated mixture of experts regression problem and two real-world datasets, including a bioinformatics study, indicating its effectiveness in enhancing predictive performance in complex data scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在现代统计和机器学习中处理高容量流数据的挑战，传统的批处理算法往往不切实际。作者重新审视了一种增量随机的主化-最小化（MM）算法变体，该算法推广了增量随机期望最大化（EM）算法，通过放宽关键的EM要求来实现更大的灵活性。研究建立了增量随机MM算法的理论保证，证明了迭代结果收敛到一个驻点，并通过实证结果表明该方法在软最大门控专家回归问题和两个真实世界数据集上优于现有的随机优化器，从而突显了其在异构数据建模中提高预测性能的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Stability and Generalization of Nonconvex Optimization with Heavy-Tailed Noise</div>
<div class="meta-line">Authors: Hongxu Chen, Ke Wei, Xiaoming Yuan, Luo Luo</div>
<div class="meta-line">First: 2026-01-27T15:50:57+00:00 · Latest: 2026-01-27T15:50:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19730v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The empirical evidence indicates that stochastic optimization with heavy-tailed gradient noise is more appropriate to characterize the training of machine learning models than that with standard bounded gradient variance noise. Most existing works on this phenomenon focus on the convergence of optimization errors, while the analysis for generalization bounds under the heavy-tailed gradient noise remains limited. In this paper, we develop a general framework for establishing generalization bounds under heavy-tailed noise. Specifically, we introduce a truncation argument to achieve the generalization error bound based on the algorithmic stability under the assumption of bounded $p$th centered moment with $p\in(1,2]$. Building on this framework, we further provide the stability and generalization analysis for several popular stochastic algorithms under heavy-tailed noise, including clipped and normalized stochastic gradient descent, as well as their mini-batch and momentum variants.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重尾噪声下非凸优化的稳定性与泛化</div>
<div class="mono" style="margin-top:8px">实证证据表明，具有重尾梯度噪声的随机优化更适合描述机器学习模型的训练，而不是具有标准有界梯度方差噪声的优化。现有大多数关于这一现象的研究集中在优化误差的收敛性上，而在重尾梯度噪声下的泛化界限分析仍然有限。本文提出了一个通用框架，以建立重尾噪声下的泛化界限。具体而言，我们引入了截断论证，以在假设有界的 $p$ 阶中心矩（$p\in(1,2]$）的基础上实现泛化误差界限。在此框架的基础上，我们进一步提供了几种流行的随机算法在重尾噪声下的稳定性和泛化分析，包括剪切和归一化的随机梯度下降，以及它们的迷你批次和动量变体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the limitations in understanding generalization bounds in stochastic optimization with heavy-tailed gradient noise, which is more representative of machine learning model training than standard noise models. The authors develop a general framework that utilizes a truncation argument to establish generalization error bounds based on algorithmic stability, assuming bounded p-th centered moments with p in (1,2]. Key experimental findings include a comprehensive stability and generalization analysis of various popular stochastic algorithms, such as clipped and normalized stochastic gradient descent and their mini-batch and momentum variants, under the influence of heavy-tailed noise.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于更好地理解在重尾梯度噪声影响下，随机优化在机器学习模型中的泛化界限，因为现有研究主要集中在收敛误差上。作者开发了一个通用框架，利用截断论证在假设有界的p次中心矩（p在1到2之间）下建立泛化误差界限。主要发现表明，该框架允许对多种流行的随机算法进行全面的稳定性和泛化分析，包括在重尾噪声影响下的剪切和归一化随机梯度下降及其小批量和动量变体。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Joint Learning for Domain and Class Generalization</div>
<div class="meta-line">Authors: Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-18T04:24:11+00:00 · Latest: 2026-01-27T07:46:51+00:00</div>
<div class="meta-line">Comments: ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12253v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12253v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>领域和类别泛化的联邦联合学习</div>
<div class="mono" style="margin-top:8px">由于视觉语言模型如CLIP的参数规模大和预训练需求广泛，效率高的微调变得至关重要。现有方法通常单独解决未见类别或未见领域的问题，而未考虑两者的联合框架。本文提出了\textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization，称为\textbf{FedDCG}，这是一种新颖的方法，旨在在联邦学习环境中同时解决类别和领域泛化。我们的方法引入了一种领域分组策略，在每个组内训练类别泛化网络，以防止决策边界混淆。在推理过程中，我们根据领域相似性聚合类别泛化结果，有效整合类别和领域泛化的知识。具体而言，采用可学习网络来增强类别泛化能力，并通过解耦机制分离一般知识和领域特定知识，提高对未见领域的泛化能力。大量跨数据集的实验表明，\textbf{FedDCG}在准确性和鲁棒性方面优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for efficient fine-tuning of large-scale visual-language models like CLIP, particularly in addressing the challenges of unseen classes and domains simultaneously. The authors propose a novel method called Federated Joint Learning for Domain and Class Generalization (FedDCG), which employs a domain grouping strategy to train class-generalized networks within each group, thereby reducing decision boundary confusion. Experimental results demonstrate that FedDCG significantly outperforms existing state-of-the-art methods in accuracy and robustness across various datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于高效微调大规模视觉语言模型（如CLIP）的需求，特别是在处理未见类别和领域的挑战方面。作者提出了一种新方法，称为联合学习的联邦域和类别泛化（FedDCG），该方法采用领域分组策略训练类别泛化网络，并在推理过程中根据领域相似性聚合结果。实验结果表明，FedDCG在多个数据集上显著优于现有的最先进方法，具有更高的准确性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Astra: General Interactive World Model with Autoregressive Denoising</div>
<div class="meta-line">Authors: Yixuan Zhu, Jiaqi Feng, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-09T18:59:57+00:00 · Latest: 2026-01-27T07:42:06+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026. Code is available at: https://github.com/EternalEvan/Astra</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08931v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.08931v3">PDF</a> · <a href="https://github.com/EternalEvan/Astra">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Astra：具有自回归去噪的通用交互世界模型</div>
<div class="mono" style="margin-top:8px">最近在扩散变换器方面的进展使视频生成模型能够从文本或图像生成高质量的视频片段。然而，能够从过去的观察和行动中预测长期未来的世界模型仍然未被充分探索，特别是在通用场景和各种形式的行动中。为了解决这一问题，我们引入了Astra，一个交互式通用世界模型，能够为多样化场景（例如，自动驾驶、机器人抓取）生成真实世界的未来，并实现精确的行动交互（例如，相机运动、机器人动作）。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察并支持流式输出。我们使用噪声增强的历史记忆，以避免过度依赖过去的帧，从而平衡响应性与时间一致性。为了实现精确的行动控制，我们引入了一种行动感知适配器，直接将行动信号注入去噪过程中。我们进一步开发了一种行动专家混合体，动态路由异构行动模式，增强在探索、操作和相机控制等多样化现实任务中的通用性。Astra实现了交互式、一致性和通用的长期视频预测，并支持各种形式的交互。多个数据集的实验表明，Astra在保真度、长期预测和行动对齐方面优于现有的最先进世界模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance world models capable of predicting long-term futures from past observations and actions, which are currently underexplored in general-purpose scenarios. The authors introduce Astra, an interactive general world model that employs an autoregressive denoising architecture and temporal causal attention to generate realistic future scenarios for tasks like autonomous driving and robot grasping. Experimental results show that Astra significantly improves fidelity, long-range prediction, and action alignment compared to existing state-of-the-art models, demonstrating its effectiveness in handling diverse real-world tasks and interactions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于推进能够从过去的观察和动作中预测长期未来的世界模型，而这在通用场景中尚未得到充分探索。作者提出了Astra，这是一种交互式通用世界模型，采用自回归去噪架构和时间因果注意力，能够为包括自动驾驶和机器人抓取在内的多种场景生成真实世界的未来。关键实验结果表明，Astra在保真度、长期预测和动作对齐方面显著优于现有的最先进世界模型，证明其在处理各种交互形式和增强现实任务的多样性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment</div>
<div class="meta-line">Authors: Yuxun Tang, Lan Liu, Wenhao Feng, Yiwen Zhao, Jionghao Han, Yifeng Yu, Jiatong Shi, Qin Jin</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-02T08:53:49+00:00 · Latest: 2026-01-27T06:41:44+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01812v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.01812v4">PDF</a> · <a href="https://huggingface.co/datasets/TangRain/SingMOS-Pro">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro extends the annotations of the additional data to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent state-of-the-art approaches. Each clip is rated by at least five experienced annotators to ensure reliability and consistency. Furthermore, we investigate strategies for effectively utilizing MOS data annotated under heterogeneous standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset is publicly available at https://huggingface.co/datasets/TangRain/SingMOS-Pro.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SingMOS-Pro：一个全面的歌唱质量评估基准</div>
<div class="mono" style="margin-top:8px">歌唱声音生成进展迅速，但评估歌唱质量仍然是一个关键挑战。人类主观评估通常以听力测试的形式进行，成本高且耗时，而现有的客观指标仅捕捉有限的感知方面。在这项工作中，我们介绍了SingMOS-Pro，一个用于自动歌唱质量评估的数据集。基于我们之前的版本SingMOS，后者仅提供总体评分，SingMOS-Pro扩展了附加数据的注释，包括歌词、旋律和整体质量，提供更广泛的覆盖和更大的多样性。该数据集包含由41个模型在12个数据集上生成的7,981个歌唱片段，涵盖了从早期系统到最新的最先进方法。每个片段至少由五位经验丰富的注释者评分，以确保可靠性和一致性。此外，我们研究了有效利用在异构标准下注释的MOS数据的策略，并在SingMOS-Pro上基准测试了几个广泛使用的相关任务评估方法，为未来的研究建立了强有力的基线和实用参考。该数据集可在https://huggingface.co/datasets/TangRain/SingMOS-Pro公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in evaluating singing quality, as traditional human assessments are costly and time-consuming, while existing objective metrics are limited. The authors introduce SingMOS-Pro, a comprehensive dataset for automatic singing quality assessment that expands upon their previous work, SingMOS, by including detailed annotations such as lyrics, melody, and overall quality across 7,981 singing clips generated by 41 models. The key findings demonstrate that the dataset, rated by multiple experienced annotators for reliability, allows for effective benchmarking of various evaluation methods, establishing strong baselines for future research in the field of singing quality assessment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决评估歌唱质量的挑战，因为传统的人类评估成本高且耗时，而现有的客观指标在感知覆盖方面有限。作者介绍了SingMOS-Pro，这是一个全面的自动歌唱质量评估数据集，基于他们之前的版本SingMOS，增加了歌词、旋律和整体质量等详细注释，涵盖了由41个模型生成的7981个歌唱片段。研究结果表明，该数据集经过多位经验丰富的注释者评估以确保可靠性，为基准测试各种评估方法提供了坚实的基础，为未来的歌唱质量评估研究建立了强有力的基准。</div>
</details>
</div>
<div class="card">
<div class="title">VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics</div>
<div class="meta-line">Authors: Zhiyu Yin, Zhipeng Liu, Kehai Chen, Lemao Liu, Jin Liu, Hong-Dong Li, Yang Xiang, Min Zhang</div>
<div class="meta-line">First: 2026-01-27T06:15:12+00:00 · Latest: 2026-01-27T06:15:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19236v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VC-Bench：开创视频连接基准的 dataset 和评估指标</div>
<div class="mono" style="margin-top:8px">当前视频生成主要关注文本或图像条件，而实际应用如视频编辑和视频博客常常需要无缝连接不同的片段。在我们的工作中，我们引入了视频连接这一创新任务，旨在生成给定起始和结束片段之间的平滑中间视频内容。然而，缺乏标准化的评估基准阻碍了该任务的发展。为了解决这一问题，我们提出了 VC-Bench，这是一个专门为视频连接设计的新基准。它包括从公共平台收集的 1,579 个高质量视频，涵盖 15 个主要类别和 72 个子类别，以确保多样性和结构性。VC-Bench 关注三个核心方面：视频质量评分 VQS、起始-结束一致性评分 SECS 和过渡平滑度评分 TSS。它们共同构成了一个超越传统质量指标的综合框架。我们在 VC-Bench 上评估了多种最先进的视频生成模型。实验结果揭示了在保持起始-结束一致性和过渡平滑度方面的显著局限性，导致整体连贯性和流畅性降低。我们期望 VC-Bench 能作为一个开创性的基准，激励和指导未来的视频连接研究。评估指标和数据集可在以下网址公开获取：https://anonymous.4open.science/r/VC-Bench-1B67/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of standardized evaluation benchmarks for the emerging task of video connecting, which is essential for applications like video editing and vlogging that require seamless transitions between clips. The authors introduced VC-Bench, a novel benchmark consisting of 1,579 high-quality videos across various categories, and developed evaluation metrics focusing on video quality, start-end consistency, and transition smoothness. Experimental evaluations of several state-of-the-art video generation models on VC-Bench demonstrated significant shortcomings in achieving start-end consistency and transition smoothness, indicating a need for improved coherence and fluidity in generated videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频编辑和视频博客等实际应用中对无缝视频连接的需求，而当前的视频生成方法无法充分满足这一需求。作者提出了VC-Bench，这是一个专门为视频连接任务设计的新基准，包含1579个来自不同类别的高质量视频，以确保多样性。主要实验结果表明，现有的最先进视频生成模型在保持起始和结束一致性及过渡平滑性方面存在困难，导致生成视频的整体连贯性和流畅性较低。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP</div>
<div class="meta-line">Authors: Sen Nie, Jie Zhang, Zhuo Wang, Shiguang Shan, Xilin Chen</div>
<div class="meta-line">First: 2026-01-27T05:24:45+00:00 · Latest: 2026-01-27T05:24:45+00:00</div>
<div class="meta-line">Comments: 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19210v1">PDF</a> · <a href="https://github.com/Summu77/CSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model&#x27;s inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比光谱校正：面向CLIP的零-shot对抗鲁棒性的测试时防御</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）如CLIP展示了显著的零-shot泛化能力，但仍然对对抗样本（AEs）高度脆弱。尽管测试时防御具有潜力，但现有方法未能提供足够的鲁棒性以抵御强攻击，并且通常受到高推理延迟和任务特定适用性的限制。为了解决这些问题，我们首先研究了AEs的内在特性，发现AEs在渐进频率衰减下表现出严重的特征不一致性。我们进一步将其归因于模型固有的光谱偏差。利用这一见解，我们提出了一种高效的测试时防御方法，称为对比光谱校正（CSR）。CSR优化一个校正扰动，以在光谱引导的对比目标下重新对齐输入与自然流形，该方法是输入自适应的。在16个分类基准上的广泛实验表明，CSR在强AutoAttack下平均超越SOTA 18.1%，且推理开销适中。此外，CSR在多种视觉任务中表现出广泛的适用性。代码可在https://github.com/Summu77/CSR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of vision-language models like CLIP to adversarial examples, which compromise their zero-shot generalization capabilities. To tackle this issue, the authors investigate the properties of adversarial examples and identify feature inconsistency under frequency attenuation as a key factor, attributing this to the model&#x27;s spectral bias. They propose a novel test-time defense method called Contrastive Spectral Rectification (CSR), which optimizes perturbations to realign inputs with the natural data manifold using a spectral-guided contrastive objective. Experimental results across 16 classification benchmarks show that CSR improves robustness against strong adversarial attacks by an average of 18.1% compared to state-of-the-art methods, with only a modest increase in inference time and broad applicability across various visual tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉语言模型如CLIP对对抗样本的脆弱性，这削弱了其零-shot泛化能力。为了解决这个问题，作者研究了对抗样本的特性，发现它们在频率衰减下表现出显著的特征不一致性，这一现象与模型的谱偏差有关。基于这一理解，他们提出了一种高效的测试时防御方法——对比谱校正（CSR），该方法优化扰动以通过谱引导对比目标重新对齐输入与其自然流形。跨越16个分类基准的实验结果表明，CSR在对抗强攻击时的鲁棒性比最先进的方法平均提高了18.1%，同时保持了适度的推理开销，并在各种视觉任务中表现出广泛的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Gulfs in UI Generation through Semantic Guidance</div>
<div class="meta-line">Authors: Seokhyeon Park, Soohyun Lee, Eugene Choi, Hyunwoo Kim, Minkyu Kweon, Yumin Song, Jinwook Seo</div>
<div class="meta-line">First: 2026-01-27T04:01:53+00:00 · Latest: 2026-01-27T04:01:53+00:00</div>
<div class="meta-line">Comments: In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While generative AI enables high-fidelity UI generation from text prompts, users struggle to articulate design intent and evaluate or refine results-creating gulfs of execution and evaluation. To understand the information needed for UI generation, we conducted a thematic analysis of UI prompting guidelines, identifying key design semantics and discovering that they are hierarchical and interdependent. Leveraging these findings, we developed a system that enables users to specify semantics, visualize relationships, and extract how semantics are reflected in generated UIs. By making semantics serve as an intermediate representation between human intent and AI output, our system bridges both gulfs by making requirements explicit and outcomes interpretable. A comparative user study suggests that our approach enhances users&#x27; perceived control over intent expression, outcome interpretation, and facilitates more predictable, iterative refinement. Our work demonstrates how explicit semantic representation enables systematic and explainable exploration of design possibilities in AI-driven UI design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义指导弥合UI生成中的鸿沟</div>
<div class="mono" style="margin-top:8px">尽管生成性AI能够从文本提示中生成高保真UI，但用户在表达设计意图和评估或完善结果时面临困难，造成执行和评估的鸿沟。为了理解UI生成所需的信息，我们对UI提示指南进行了主题分析，识别出关键设计语义，并发现它们是层次化和相互依赖的。利用这些发现，我们开发了一个系统，使用户能够指定语义、可视化关系，并提取语义在生成的UI中如何体现。通过使语义作为人类意图与AI输出之间的中介表示，我们的系统弥合了这两个鸿沟，使需求明确，结果可解释。比较用户研究表明，我们的方法增强了用户对意图表达、结果解释的感知控制，并促进了更可预测的迭代完善。我们的工作展示了明确的语义表示如何在AI驱动的UI设计中实现系统化和可解释的设计可能性探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges users face in articulating design intent and evaluating AI-generated user interfaces (UIs), which create gaps in execution and evaluation. To tackle this issue, the authors conducted a thematic analysis of UI prompting guidelines to identify hierarchical and interdependent design semantics. They developed a system that allows users to specify these semantics, visualize their relationships, and understand how they are reflected in generated UIs. A comparative user study indicated that this approach improves users&#x27; perceived control over expressing intent and interpreting outcomes, leading to more predictable and iterative refinement in AI-driven UI design.</div>
<div class="mono" style="margin-top:8px">本研究解决了用户在表达设计意图和评估AI生成用户界面（UI）时面临的挑战，这导致了执行和评估之间的差距。作者对UI提示指南进行了主题分析，以识别层次和相互依赖的设计语义，从而开发出一个系统，允许用户指定语义、可视化其关系，并理解这些语义如何在生成的UI中体现。比较用户研究表明，这种方法提高了用户在表达意图和解释结果方面的控制感，促进了更可预测和迭代的设计改进。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Unsupervised Semantic-Aware Exposure Correction</div>
<div class="meta-line">Authors: Puzhen Wu, Han Weng, Quan Zheng, Yi Zhan, Hewei Wang, Yiming Li, Jiahui Han, Rui Xu</div>
<div class="meta-line">First: 2026-01-27T02:53:18+00:00 · Latest: 2026-01-27T02:53:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19129v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP引导的无监督语义感知曝光校正</div>
<div class="mono" style="margin-top:8px">不当曝光常导致严重的细节丢失、颜色失真和对比度降低。曝光校正仍面临两个关键挑战：（1）忽视对象区域的语义信息导致颜色偏移伪影；（2）现实世界的曝光图像通常没有真实标签，其标注需要大量手动编辑。为了解决这些挑战，我们提出了一种新的无监督语义感知曝光校正网络。它包含一个自适应语义感知融合模块，有效地将从预训练的快速分割任意模型中提取的语义信息融合到共享的图像特征空间中。然后，融合的特征被我们的多尺度残差空间mamba组用于恢复细节和调整曝光。为了避免手动编辑，我们提出了一个由CLIP引导的伪真实生成器，经过微调以自动识别曝光情况并指导定制校正。此外，我们利用来自FastSAM和CLIP的丰富先验，开发了一种语义提示一致性损失，以强制无监督训练中的语义一致性和图像提示对齐。全面的实验结果表明我们的方法在校正现实世界曝光图像方面的有效性，并在数值和视觉上超越了最先进的无监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of improper exposure in images, which can lead to detail loss, color distortion, and reduced contrast, particularly in the absence of ground-truth labels for real-world exposure images. The authors propose an unsupervised semantic-aware exposure correction network that integrates a semantic-aware fusion module to combine semantic information from a pre-trained Fast Segment Anything Model with image features. Key experimental findings demonstrate that this method effectively corrects exposure in real-world images and surpasses existing unsupervised techniques in both numerical and visual assessments.</div>
<div class="mono" style="margin-top:8px">该研究解决了图像曝光不当带来的细节丢失和颜色失真等挑战，特别是由于缺乏对象级语义信息和真实世界图像中缺乏真实标签的问题。作者提出了一种无监督的语义感知曝光校正网络，该网络集成了语义感知融合模块，将预训练的快速分割任意模型的信息与图像特征结合。实验结果表明，该方法有效地校正了真实世界图像的曝光，超越了现有的无监督技术，在数值和视觉评估上均表现优异。</div>
</details>
</div>
<div class="card">
<div class="title">The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</div>
<div class="meta-line">Authors: Chenyu Mu, Xin He, Qu Yang, Wanshun Chen, Jiadi Yao, Huang Liu, Zihao Yi, Bo Zhao, Xingyu Chen, Ruotian Ma, Fanghua Ye, Erkun Yang, Cheng Deng, Zhaopeng Tu, Xiaolong Li, Linus</div>
<div class="meta-line">First: 2026-01-25T08:10:28+00:00 · Latest: 2026-01-27T02:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17737v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17737v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap&#x27;&#x27; between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>剧本就是你所需的一切：一种用于长时段对话到电影视频生成的代理框架</div>
<div class="mono" style="margin-top:8px">最近的视频生成进展产生了能够从简单文本提示合成惊人视觉内容的模型。然而，这些模型在从对话等高层概念生成长篇连贯叙事方面存在困难，揭示了创意想法与其电影执行之间的“语义差距”。为了解决这个问题，我们引入了一种新颖的端到端代理框架，用于对话到电影视频生成。我们框架的核心是ScripterAgent，一个训练用于将粗略对话翻译为细致可执行电影剧本的模型。为此，我们构建了ScriptBench，一个新的大规模基准，具有丰富的多模态上下文，通过专家指导的流程进行注释。生成的剧本随后指导DirectorAgent，该代理使用跨场景连续生成策略协调最先进的视频模型，以确保长时段的一致性。我们的综合评估，结合了AI驱动的CriticAgent和新的视觉-剧本对齐（VSA）指标，显示我们的框架显著提高了剧本的忠实度和时间保真度，适用于所有测试的视频模型。此外，我们的分析揭示了当前SOTA模型在视觉壮观与严格剧本遵循之间的关键权衡，为未来的自动化电影制作提供了宝贵的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by existing video generation models in creating long-form, coherent narratives from high-level dialogue concepts, which often results in a semantic gap between creative ideas and their cinematic execution. The authors propose an end-to-end agentic framework for dialogue-to-cinematic video generation, featuring ScripterAgent, a model that translates coarse dialogue into detailed cinematic scripts, and DirectorAgent, which utilizes these scripts to guide video generation with a continuous cross-scene strategy. Experimental results demonstrate that this framework significantly enhances script faithfulness and temporal fidelity across various video models, while also revealing a trade-off between visual spectacle and script adherence in current state-of-the-art models, offering insights for future automated filmmaking endeavors.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视频生成模型在从高层次对话概念创建连贯的长篇叙事时面临的挑战。作者提出了一种端到端的代理框架，用于对话到电影视频的生成，其中包括将对话翻译为可执行脚本的ScripterAgent和协调视频生成的DirectorAgent。实验结果表明，该框架显著提高了各种视频模型的脚本忠实度和时间一致性，同时揭示了当前最先进模型在视觉吸引力和脚本遵循之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation</div>
<div class="meta-line">Authors: Xiang Gao, Yunpeng Jia</div>
<div class="meta-line">First: 2026-01-27T02:39:20+00:00 · Latest: 2026-01-27T02:39:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19115v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FBSDiff++：改进的频带替代扩散特征用于高效且高度可控的文本驱动图像到图像翻译</div>
<div class="mono" style="margin-top:8px">随着大规模文本到图像（T2I）扩散模型在开放域图像创作中取得显著进展，越来越多的关注集中在其自然扩展到文本驱动的图像到图像（I2I）翻译领域，其中源图像作为视觉指导，辅助文本提示提供的文本指导。我们提出FBSDiff，一个新颖的框架，从全新的频域视角将现成的T2I扩散模型适配到I2I范式。通过动态频带替代扩散特征，FBSDiff以即插即用的方式实现多功能且高度可控的文本驱动I2I（无需模型训练、微调或在线优化），允许通过逐步替代潜在扩散特征的低频带、中频带和高频带，实现外观引导、布局引导和轮廓引导的I2I翻译。此外，FBSDiff灵活地通过调节替代频带的带宽，持续控制I2I相关强度。为了进一步提升图像翻译的效率、灵活性和功能性，我们提出FBSDiff++，主要在三个方面改进FBSDiff：（1）通过优化模型架构大幅加速推理速度（推理速度提升8.9倍）；（2）改进频带替代模块，允许任意分辨率和纵横比的输入源图像；（3）扩展模型功能，仅需对核心方法进行微调，即可实现局部图像操作和特定风格内容创作。大量定性和定量实验验证了FBSDiff++在I2I翻译视觉质量、效率、多功能性和可控性方面优于相关先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of text-driven image-to-image (I2I) translation using diffusion models, which have shown significant success in text-to-image generation. The authors propose FBSDiff++, a framework that adapts existing text-to-image diffusion models to the I2I paradigm through a frequency-domain approach, allowing for dynamic frequency band substitution of diffusion features. Experimental results demonstrate that FBSDiff++ achieves an 8.9 times speedup in inference, supports input images of arbitrary resolution and aspect ratio, and improves visual quality, efficiency, versatility, and controllability in I2I translation compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于文本到图像扩散模型的进展及其在文本驱动的图像到图像翻译中的潜在应用。作者提出了FBSDiff，一个将现有文本到图像模型适应于图像到图像任务的框架，采用频域方法，允许动态频带替换扩散特征。主要发现表明，FBSDiff++显著提高了推理速度，达到了8.9倍，能够处理任意分辨率的源图像，并扩展了局部图像处理的功能，实验结果显示在图像翻译任务中，相较于现有方法，视觉质量、效率、多样性和可控性均有所改善。</div>
</details>
</div>
<div class="card">
<div class="title">SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning</div>
<div class="meta-line">Authors: Momin Ahmad Khan, Yasra Chandio, Fatima Muhammad Anwar</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-25T23:15:20+00:00 · Latest: 2026-01-26T23:29:46+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.22506v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.22506v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SABRE-FL：联邦提示学习中的选择性和准确的后门拒绝</div>
<div class="mono" style="margin-top:8px">联邦提示学习已成为一种通信高效且保护隐私的范式，用于在去中心化客户端之间适应大型视觉-语言模型，如CLIP。然而，这种设置的安全性影响仍未得到充分探讨。在本研究中，我们首次研究了联邦提示学习中的后门攻击。我们表明，当恶意客户端将视觉上不可察觉的可学习噪声触发器注入输入图像时，全球提示学习者会变得易受针对性错误分类的影响，同时在干净输入上仍保持高准确性。基于这一脆弱性，我们提出了SABRE-FL，这是一种轻量级、模块化的防御机制，使用在离线分布外数据上训练的嵌入空间异常检测器过滤被污染的提示更新。SABRE-FL不需要访问原始客户端数据或标签，并且能够在多样化的数据集上进行泛化。我们理论和实证上都表明，恶意客户端可以通过基于嵌入的检测器可靠识别和过滤。在五个不同的数据集和四个基线防御中，SABRE-FL通过显著降低后门准确性而保持干净准确性，超越了所有基线，展示了强大的实证性能，并强调了未来联邦系统中对强健提示学习的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need to address security vulnerabilities in Federated Prompt Learning, particularly concerning backdoor attacks that can compromise model integrity. The authors introduce SABRE-FL, a lightweight defense mechanism that utilizes an embedding-space anomaly detector trained on out-of-distribution data to filter out poisoned prompt updates without requiring access to raw client data or labels. Experimental results demonstrate that SABRE-FL effectively identifies and mitigates the influence of malicious clients across five diverse datasets, significantly reducing backdoor accuracy while maintaining high clean accuracy, thus highlighting the importance of robust defenses in federated learning systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决联邦提示学习中的安全漏洞，特别是来自恶意客户端的后门攻击。作者提出了SABRE-FL，这是一种防御机制，利用在分布外数据上训练的嵌入空间异常检测器来过滤被污染的提示更新，而无需访问原始客户端数据或标签。五个不同数据集的实验结果表明，SABRE-FL显著降低了后门准确率，同时保持了干净准确率，超越了四个基线防御，突显了在联邦学习系统中需要强大防御的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权在线探索学习在困难问题上的推理</div>
<div class="mono" style="margin-top:8px">强化学习（RL）提高了大型语言模型（LLM）的推理能力，但最先进的方法在许多训练问题上仍然无法学习。在困难问题上，在线RL几乎不会探索到单个正确的回合，导致零奖励和没有学习信号来推动改进。我们发现，经典RL中解决这一探索问题的自然解决方案，如熵奖励、对重要性比率的更宽松裁剪或直接优化pass@k目标，并未解决此问题，且往往会使优化不稳定而不改善可解性。一个自然的替代方案是利用来自更简单问题的迁移。然而，我们表明，在RL训练期间混合简单和困难问题是适得其反的，因为光线干扰，优化集中在已经可解的问题上，从而积极抑制对更困难问题的进展。为了解决这一挑战，我们引入了特权在线探索（POPE），这是一种利用人类或其他神谕解决方案作为特权信息来指导困难问题探索的方法，不同于将神谕解决方案用作训练目标的方法（例如，离线RL方法或从SFT热启动）。POPE通过神谕解决方案的前缀增强困难问题，使RL在引导回合中获得非零奖励。至关重要的是，所产生的行为通过遵循指令和推理之间的协同作用转移回原始的无引导问题。实证结果表明，POPE扩展了可解问题的集合，并显著提高了在具有挑战性的推理基准上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of reinforcement learning (RL) in large language models (LLMs), particularly in addressing the challenges posed by hard problems where traditional on-policy RL struggles to explore effectively. The authors propose a novel method called Privileged On-Policy Exploration (POPE), which utilizes human or oracle solutions as privileged information to guide exploration in difficult scenarios, rather than using these solutions as mere training targets. Experimental results demonstrate that POPE significantly increases the number of solvable problems and improves performance on challenging reasoning benchmarks by allowing RL to achieve non-zero rewards during guided rollouts, thereby facilitating a transfer of learned behaviors back to the original unguided problems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型语言模型（LLMs）在强化学习（RL）中的推理能力，特别是在传统方法难以提供学习信号的困难问题上。作者提出了一种新方法，称为特权在线探索（POPE），该方法利用人类或oracle解决方案来指导RL中的探索，而不是将这些解决方案仅作为训练目标。实验结果表明，POPE显著增加了可解决问题的数量，并提高了在具有挑战性的推理基准上的表现，通过在引导回合中实现非零奖励，从而促进了学习行为向未引导场景的更好转移。</div>
</details>
</div>
<div class="card">
<div class="title">Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T18:59:27+00:00 · Latest: 2026-01-26T17:06:02+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16912v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16912v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索与利用：通过剪切、熵和虚假奖励重新思考可验证奖励的强化学习</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习（RLVR）中的探索与利用权衡，这是一个旨在改善大型语言模型（LLMs）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制在LLMs中引发强大的数学推理：虚假奖励通过奖励与真实情况无关的结果来抑制利用，而熵最小化则通过推动模型朝向更自信和确定的输出抑制探索，突显出一个令人困惑的动态：同时抑制利用和探索的做法提高了推理性能，但调和这些效应的基本原理仍然不清楚。我们关注两个基本问题：（i）策略熵与性能的关系，以及（ii）虚假奖励是否带来收益，可能通过剪切偏差和模型污染的相互作用。我们的结果表明，在虚假奖励下，剪切偏差降低了策略熵，导致更自信和确定的输出，而单靠熵最小化不足以改善。我们进一步提出了一个奖励失调模型，解释了为什么虚假奖励可以在污染环境中提升性能。我们的发现阐明了虚假奖励益处背后的机制，并提供了更有效的RLVR训练原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR) to enhance the reasoning capabilities of Large Language Models (LLMs). The authors analyze the effects of spurious rewards and entropy minimization on model performance, addressing how policy entropy influences reasoning and whether spurious rewards can be beneficial. The findings reveal that spurious rewards, through clipping bias, decrease policy entropy and lead to more confident outputs, while entropy minimization alone does not suffice for performance improvement, ultimately proposing a reward-misalignment model to explain the observed benefits of spurious rewards in RLVR training.</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励的强化学习（RLVR）中的探索与开发权衡，以增强大型语言模型（LLMs）的推理能力。作者分析了策略熵如何影响性能以及虚假奖励的作用，尽管它们的性质似乎相互矛盾，但仍能改善推理。研究结果表明，与虚假奖励相关的剪切偏差降低了策略熵，导致更自信的输出，而单纯的熵最小化并未带来改善。此外，提出了一种奖励失调模型，以解释虚假奖励带来的性能提升，为更有效的RLVR训练提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</div>
<div class="meta-line">Authors: Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-09T08:07:19+00:00 · Latest: 2026-01-26T16:58:19+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07915v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07915v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \textit{retrieve-then-compress} strategy using a \textbf{Visual Memory Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame&#x27;s tokens -- reducing visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by \textbf{23.9\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARC：基于记忆增强的强化学习令牌压缩用于高效视频理解</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速进展为多模态模型奠定了基础。然而，视觉语言模型（VLMs）在从图像扩展到视频时仍面临高帧率和长时长带来的高计算成本。令牌压缩是一种有前景的解决方案，但大多数现有的无训练方法会导致信息丢失和性能下降。为此，我们提出了\textbf{基于记忆增强的强化学习令牌压缩（MARC）}，它结合了结构化检索和基于强化学习的蒸馏。MARC采用\textit{先检索后压缩}策略，使用\textbf{视觉记忆检索器（VMR）}选择关键片段，并通过\textbf{压缩组相对策略优化（C-GRPO）}框架将推理能力从教师模型蒸馏到学生模型。在六个视频基准上的实验表明，MARC仅使用一帧的令牌就能达到接近基线的准确率——将视觉令牌减少了\textbf{95\%}，GPU内存减少了\textbf{72\%}，延迟减少了\textbf{23.9\%}。这证明了其在资源受限环境下（如视频问答、监控和自动驾驶）实现高效实时视频理解的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computational costs associated with extending visual language models to video understanding, particularly due to the challenges posed by high frame rates and long durations. The authors propose a novel method called Memory-Augmented Reinforcement Learning-based Token Compression (MARC), which utilizes a retrieve-then-compress strategy that combines a Visual Memory Retriever for selecting key clips and a Compression Group Relative Policy Optimization framework for distilling knowledge from a teacher model to a student model. Experimental results on six video benchmarks demonstrate that MARC can achieve near-baseline accuracy while significantly reducing visual tokens by 95%, GPU memory usage by 72%, and latency by 23.9%, indicating its effectiveness for efficient video understanding in resource-constrained environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉语言模型在处理视频时面临的高计算成本，这在高帧率和长时长的情况下尤为严重。作者提出了一种新方法，称为基于记忆增强的强化学习令牌压缩（MARC），该方法采用检索后压缩策略，结合视觉记忆检索器选择关键片段和压缩组相对策略优化框架，从教师模型向学生模型提炼知识。六个视频基准实验结果表明，MARC在显著减少视觉令牌95%、GPU内存使用72%和延迟23.9%的同时，仍能实现接近基线的准确性，表明其在资源有限环境中高效视频理解的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP&#x27;s Visual Embedding Projector is a Few-shot Cornucopia</div>
<div class="meta-line">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-10-07T17:59:59+00:00 · Latest: 2026-01-26T14:50:34+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05270v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.05270v4">PDF</a> · <a href="https://github.com/astra-vision/ProLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation&#x27;&#x27; and ``validation-free&#x27;&#x27; settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP&#x27;s lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP的视觉嵌入投影器是一个少样本的丰饶之地</div>
<div class="mono" style="margin-top:8px">我们介绍了ProLIP，这是一种简单且与架构无关的方法，用于将对比预训练的视觉-语言模型（如CLIP）适应于少样本分类。ProLIP通过对预训练权重的偏差进行Frobenius范数正则化，微调视觉编码器的投影矩阵。它在11个少样本分类基准测试中，在“少样本验证”和“无验证”设置下均实现了最先进的性能。此外，通过ProLIP的视角重新思考非线性CLIP-Adapter，我们设计了一个正则化线性适配器（RLA），其性能更佳，无需超参数调优，对学习率值的敏感性较低，并在模型权重不可访问的黑箱场景中提供了ProLIP的替代方案。除了少样本分类，ProLIP在跨数据集迁移、领域泛化、基础到新类泛化和测试时适应方面表现出色——在训练速度上比提示调优快一个数量级。代码可在https://github.com/astra-vision/ProLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve few-shot classification using contrastively pretrained vision-language models like CLIP. The authors introduce ProLIP, a method that fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization to adapt these models for few-shot tasks. Experimental results demonstrate that ProLIP achieves state-of-the-art performance across 11 benchmarks and excels in various scenarios such as cross-dataset transfer and domain generalization, outperforming existing methods like prompt tuning while being significantly faster to train.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高对比预训练的视觉-语言模型（如CLIP）在少样本分类中的表现。作者提出了ProLIP，这是一种通过Frobenius范数正则化微调视觉编码器投影矩阵的方法，以使其与预训练权重对齐。实验结果表明，ProLIP在11个少样本分类基准上达到了最先进的性能，并且在跨数据集迁移、领域泛化和测试时适应方面也表现出优势，超越了提示调优，同时训练速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">Closing the Modality Gap Aligns Group-Wise Semantics</div>
<div class="meta-line">Authors: Eleonora Grassucci, Giordano Cicchetti, Emanuele Frasca, Aurelio Uncini, Danilo Comminiello</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-26T14:36:04+00:00 · Latest: 2026-01-26T14:36:04+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>闭合模态差距对齐组级语义</div>
<div class="mono" style="margin-top:8px">在多模态学习中，CLIP被认为是学习跨多个模态共享潜在空间的\textit{事实上的}方法，将相似的表示靠近并将其与不相似的表示远离。尽管基于CLIP的损失有效地在语义层面对齐模态，但结果潜在空间往往仅部分共享，揭示了一种称为模态差距的结构不匹配现象。尽管解决这一现象的必要性仍存在争议，特别是考虑到其对实例级任务（例如检索）的有限影响，但我们证明其在组级任务（例如聚类）中的影响却非常明显。为了支持这一论点，我们提出了一种新方法，旨在在双模态设置中持续减少这种差异，并可以简单扩展到一般的$n$模态情况。通过我们的广泛评估，我们展示了我们的新见解：虽然减少差距在传统的实例级任务中仅提供边际或不一致的改进，但它显著增强了组级任务。这些发现可能会重塑我们对模态差距的理解，突显其在提高需要语义分组的任务性能中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the modality gap in multimodal learning, which refers to the structural mismatch in latent spaces when using CLIP for aligning different modalities. The authors propose a novel method aimed at consistently reducing this gap in two-modal settings, with potential extension to n-modal cases. Experimental results indicate that while the reduction of the modality gap yields minimal improvements in instance-wise tasks, it significantly enhances performance in group-level tasks such as clustering, suggesting that the modality gap plays a crucial role in tasks requiring semantic grouping.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态学习中的模态差距，这是一种结构性不匹配，尽管对实例级任务影响有限，但会影响组级任务。作者提出了一种新方法，在双模态设置中减少这一差距，并扩展到n模态情况。实验结果表明，尽管减少模态差距对实例级任务的改善有限，但在聚类等组级任务中显著提高了性能，从而强调了模态差距在语义分组任务中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation</div>
<div class="meta-line">Authors: Luca Cazzola, Ahed Alboody</div>
<div class="meta-line">First: 2025-12-12T15:32:28+00:00 · Latest: 2026-01-26T11:40:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11654v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11654v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lucazzola.github.io/publications/kinemic">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR&#x27;s requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at https://lucazzola.github.io/publications/kinemic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文中的动量挖掘：通过文本到运动蒸馏的少样本动作合成</div>
<div class="mono" style="margin-top:8px">大型标注运动数据集的获取成本仍然是基于骨架的人类活动识别（HAR）的一个关键瓶颈。尽管文本到运动（T2M）生成模型提供了一个引人注目的、可扩展的合成数据源，但其训练目标强调一般艺术运动，且数据集结构与HAR对运动学精确、类别区分动作的要求根本不同。这种差异造成了显著的领域差距，使得通用的T2M模型不适合生成适合HAR分类器的动作。为了解决这一挑战，我们提出了KineMIC（上下文中的动量挖掘），这是一个用于少样本动作合成的迁移学习框架。KineMIC通过假设文本编码空间中的语义对应关系可以为运动学蒸馏提供软监督，将T2M扩散模型适应于HAR领域。我们通过一种动量挖掘策略实现这一点，该策略利用CLIP文本嵌入建立稀疏HAR标签与T2M源数据之间的对应关系。这个过程指导微调，将通用的T2M主干转变为专门的少样本动作到运动生成器。我们使用HumanML3D作为源T2M数据集，NTU RGB+D 120的一个子集作为目标HAR领域，随机选择每个动作类别的10个样本来验证KineMIC。我们的方法生成了显著更连贯的动作，提供了一个强大的数据增强源，带来了+23.1%的准确率提升。动画插图和补充材料可在https://lucazzola.github.io/publications/kinemic获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high acquisition cost of large annotated motion datasets for skeletal-based Human Activity Recognition (HAR), which limits the effectiveness of HAR classifiers. The authors propose KineMIC, a transfer learning framework that adapts a Text-to-Motion (T2M) diffusion model to the HAR domain by utilizing semantic correspondences in text encoding to provide soft supervision for kinematic distillation. Experimental results demonstrate that KineMIC significantly enhances the generation of coherent motions, achieving a 23.1% improvement in accuracy when validated with the HumanML3D dataset for T2M and a subset of NTU RGB+D 120 for HAR, using only 10 samples per action class.</div>
<div class="mono" style="margin-top:8px">本研究解决了骨骼基础的人类活动识别（HAR）中注释运动数据集获取成本高的问题，这限制了HAR分类器的有效性。作者提出了KineMIC，这是一种迁移学习框架，通过利用文本编码中的语义对应关系为运动蒸馏提供软监督，从而将文本到运动（T2M）扩散模型适应于少样本动作合成。实验结果表明，KineMIC显著提高了运动的一致性，并作为一种强大的数据增强来源，在使用HumanML3D数据集和NTU RGB+D 120的子集进行验证时，准确率提高了23.1%，仅使用每个动作类别的10个样本。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning</div>
<div class="meta-line">Authors: Zhixian Zhao, Wenjie Tian, Xiaohai Tian, Jun Zhang, Lei Xie</div>
<div class="meta-line">First: 2026-01-26T10:03:26+00:00 · Latest: 2026-01-26T10:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18321v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18321v1">PDF</a> · <a href="https://github.com/zxzhao0/SABER-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a &quot;perceive-then-reason&quot; separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>集成细粒度音视频证据以实现稳健的多模态情感推理</div>
<div class="mono" style="margin-top:8px">多模态情感分析正从静态分类转向生成推理。稳健的情感推理必须综合细粒度信号，如面部微表情和韵律，以解码复杂社会背景中的潜在因果关系。然而，当前的多模态大型语言模型（MLLMs）在细粒度感知方面面临重大限制，主要由于数据稀缺和跨模态融合不足。因此，这些模型通常表现出单模态主导，导致在复杂多模态交互中出现幻觉，特别是在视觉和声学线索微妙、模糊或甚至矛盾（例如，在讽刺场景中）时。为了解决这个问题，我们引入了SABER-LLM，一个旨在实现稳健多模态推理的框架。首先，我们构建了SABER，一个包含60万段视频剪辑的大规模情感推理数据集，采用一种新颖的六维模式进行注释，联合捕捉音视频线索和因果逻辑。其次，我们提出了结构化证据分解范式，强制在证据提取和推理之间进行“感知-再推理”分离，以减轻单模态主导。通过一致性感知的直接偏好优化，进一步增强了感知复杂场景的能力，明确鼓励在模糊或冲突的感知条件下各模态之间的对齐。在EMER、EmoBench-M和SABER-Test上的实验表明，SABER-LLM显著优于开源基线，并在解码复杂情感动态方面与闭源模型的稳健性相当。数据集和模型可在https://github.com/zxzhao0/SABER-LLM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multimodal emotion analysis by moving from static classification to generative reasoning, addressing the limitations of current Multimodal Large Language Models (MLLMs) in fine-grained perception due to data scarcity and unimodal dominance. The authors introduce SABER-LLM, a framework that includes the creation of a large-scale emotion reasoning dataset with 600K video clips annotated with a six-dimensional schema and a structured evidence decomposition paradigm that separates evidence extraction from reasoning. Experimental results show that SABER-LLM significantly outperforms existing open-source baselines and achieves robustness comparable to closed-source models in decoding complex emotional dynamics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要一种强大的多模态情感推理方法，超越静态分类，综合面部微表情和韵律特征等细粒度信号，以解决当前多模态大型语言模型（MLLM）在数据稀缺和单模态主导方面的局限性。作者提出了SABER-LLM框架，包括创建一个包含60万段视频剪辑的大规模情感推理数据集，并用六维结构进行注释，以及一种结构化证据分解范式，将证据提取与推理分开。实验结果表明，SABER-LLM显著优于现有的开源模型，并在解码复杂情感动态方面达到了与闭源模型相当的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment</div>
<div class="meta-line">Authors: Fu-An Chao, Bi-Cheng Yan, Berlin Chen</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-18T08:10:24+00:00 · Latest: 2026-01-26T08:58:34+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16387v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the untapped potential of Whisper, a well-established automatic speech recognition (ASR) foundation model, in the context of L2 spoken language assessment (SLA). Unlike prior studies that extrinsically analyze transcriptions produced by Whisper, our approach goes a step further to probe its latent capabilities by extracting acoustic and linguistic features from hidden representations. With only a lightweight classifier being trained on top of Whisper&#x27;s intermediate and final outputs, our method achieves strong performance on the GEPT picture-description dataset, outperforming existing cutting-edge baselines, including a multimodal approach. Furthermore, by incorporating image and text-prompt information as auxiliary relevance cues, we demonstrate additional performance gains. Finally, we conduct an in-depth analysis of Whisper&#x27;s embeddings, which reveals that, even without task-specific fine-tuning, the model intrinsically encodes both ordinal proficiency patterns and semantic aspects of speech, highlighting its potential as a powerful foundation for SLA and other spoken language understanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究ASR基础模型在L2英语口语评估中的隐藏潜力</div>
<div class="mono" style="margin-top:8px">本文探讨了Whisper这一成熟的自动语音识别（ASR）基础模型在L2口语评估（SLA）中的未开发潜力。与之前对Whisper生成的转录进行外部分析的研究不同，我们的方法进一步探测其潜在能力，通过从隐藏表示中提取声学和语言特征。仅在Whisper的中间和最终输出上训练一个轻量级分类器，我们的方法在GEPT图片描述数据集上取得了强劲的表现，超越了现有的尖端基线，包括多模态方法。此外，通过将图像和文本提示信息作为辅助相关线索，我们展示了额外的性能提升。最后，我们对Whisper的嵌入进行了深入分析，揭示了即使没有特定任务的微调，该模型也内在地编码了顺序熟练度模式和语音的语义方面，突显了其作为SLA和其他口语理解任务强大基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential of the Whisper automatic speech recognition (ASR) model for assessing L2 spoken language, motivated by the need for effective evaluation tools in language learning. The researchers extract acoustic and linguistic features from Whisper&#x27;s hidden representations and train a lightweight classifier on its outputs. The results show that this method achieves superior performance on the GEPT picture-description dataset compared to existing baselines, including a multimodal approach, and further improvements are noted when incorporating auxiliary image and text-prompt information. Additionally, an analysis of Whisper&#x27;s embeddings indicates that the model captures proficiency patterns and semantic aspects of speech without requiring task-specific fine-tuning, suggesting its utility for spoken language assessment and understanding tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Whisper自动语音识别（ASR）模型在L2口语评估中的潜力，旨在满足语言学习中对有效评估工具的需求。研究人员从Whisper的隐藏表示中提取声学和语言特征，并在其输出上训练轻量级分类器。研究结果表明，该方法在GEPT图片描述数据集上的表现优于现有基准，包括多模态方法，并在结合辅助图像和文本提示时进一步提高了性能。此外，对Whisper嵌入的分析显示，该模型能够捕捉口语的熟练度模式和语义特征，表明其在口语评估和理解任务中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach</div>
<div class="meta-line">Authors: Sahil Naik, Soham Bagayatkar, Pavankumar Singh</div>
<div class="meta-line">First: 2026-01-26T07:29:50+00:00 · Latest: 2026-01-26T07:29:50+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于EfficientNetB2的FER-2013面部情感识别方法</div>
<div class="mono" style="margin-top:8px">基于面部图像在人类情感检测中的应用在现实场景中是一项困难的任务，原因包括图像质量低、光照变化、姿态变化、背景干扰、类间小变异、嘈杂的众包标签以及严重的类别不平衡，这些在FER-2013数据集中48x48灰度图像中得到了体现。尽管最近使用大型CNN（如VGG和ResNet）的方法取得了合理的准确性，但它们计算成本高且内存占用大，限制了其在实时应用中的实用性。我们通过基于EfficientNetB2的轻量级高效面部情感识别管道来解决这些挑战，该管道采用两阶段的预热和微调策略进行训练。模型通过AdamW优化、解耦权重衰减、标签平滑（epsilon = 0.06）以减少注释噪声、剪切类别权重以缓解类别不平衡，以及使用dropout、混合精度训练和广泛的实时数据增强进行增强。模型采用分层的87.5%/12.5%训练-验证划分进行训练，同时保持官方测试集不变，测试准确率达到68.78%，参数数量几乎是基于VGG16的基线的十分之一。实验结果，包括每类指标和学习动态，表明训练稳定且具有强泛化能力，使得所提方法适合实时和边缘计算应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of facial emotion recognition in real-world scenarios, particularly the issues of low image quality, lighting variations, and class imbalance in the FER-2013 dataset. The authors propose a lightweight approach utilizing EfficientNetB2, which is trained with a two-stage warm-up and fine-tuning strategy, incorporating techniques such as AdamW optimization, label smoothing, and extensive data augmentation. The model achieves a test accuracy of 68.78% while using nearly ten times fewer parameters than VGG16-based models, demonstrating stable training and strong generalization suitable for real-time applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善在真实场景中进行面部情感识别的能力，因为低图像质量和FER-2013数据集中类不平衡等因素使得这一任务具有挑战性。作者提出了一种基于EfficientNetB2的轻量高效方法，采用两阶段的预热和微调策略，并结合AdamW优化、标签平滑和广泛的数据增强等技术。实验结果表明，该模型在测试集上达到了68.78%的准确率，参数数量显著少于基于VGG16的模型，显示出良好的泛化能力，适合实时应用。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</div>
<div class="meta-line">Authors: Yifan Li, Shiying Wang, Jianqiang Huang</div>
<div class="meta-line">First: 2026-01-26T06:16:53+00:00 · Latest: 2026-01-26T06:16:53+00:00</div>
<div class="meta-line">Comments: 7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18190v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18190v1">PDF</a> · <a href="https://github.com/Lcrucial1f/MPS-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关键词引导的多视角子图CLIP用于遥感图像-文本检索</div>
<div class="mono" style="margin-top:8px">视觉-语言预训练（VLP）模型如CLIP显著推动了遥感图像-文本检索（RSITR）的发展。然而，现有方法主要依赖粗粒度的全局对齐，常常忽视了航拍图像中固有的密集多尺度语义。此外，通过全量微调适应这些重型模型会产生高昂的计算成本，并且存在灾难性遗忘的风险。为了解决这些挑战，我们提出了MPS-CLIP，一个参数高效的框架，旨在将检索范式从全局匹配转变为关键词引导的细粒度对齐。具体而言，我们利用大型语言模型（LLM）提取核心语义关键词，引导Segment Anything Model（SamGeo）生成语义相关的子视角。为了高效适应冻结的主干网络，我们引入了门控全局注意力（G^2A）适配器，以最小的开销捕捉全局上下文和长距离依赖。此外，多视角表示（MPR）模块将这些局部线索聚合成强健的多视角嵌入。该框架通过结合多视角对比损失和加权三元组损失的混合目标进行优化，动态选择最大响应视角以抑制噪声并强制精确的语义匹配。在RSICD和RSITMD基准上的广泛实验表明，MPS-CLIP在平均召回率（mR）上分别达到了35.18%和48.40%的最新性能，显著优于全量微调基线和最近的竞争方法。代码可在https://github.com/Lcrucial1f/MPS-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Remote Sensing Image-Text Retrieval (RSITR) by addressing the limitations of existing methods that rely on coarse-grained global alignment, which often neglects the detailed semantics of overhead imagery. The authors propose MPS-CLIP, a parameter-efficient framework that shifts the retrieval approach to keyword-guided fine-grained alignment, utilizing a Large Language Model to extract semantic keywords and a Gated Global Attention adapter to adapt the frozen backbone with minimal computational costs. Experimental results on the RSICD and RSITMD benchmarks show that MPS-CLIP achieves state-of-the-art performance with mean Recall rates of 35.18% and 48.40%, significantly surpassing traditional full fine-tuning methods and other competitive approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法依赖粗粒度全局对齐和计算成本高昂的局限性，来改善遥感图像-文本检索（RSITR）。作者提出了MPS-CLIP，一个参数高效的框架，利用大型语言模型提取语义关键词，指导Segment Anything Model生成语义相关的子视角。实验结果表明，在RSICD和RSITMD基准测试中，MPS-CLIP分别实现了35.18%和48.40%的平均召回率，超越了全微调基线和其他竞争方法。</div>
</details>
</div>
<div class="card">
<div class="title">VideoPro: Adaptive Program Reasoning for Long Video Understanding</div>
<div class="meta-line">Authors: Chenglin Li, Feng Han, Yikun Wang, Ruilin Li, Shuai Dong, Haowen Hou, Haitao Li, Qianglong Chen, Feng Tao, Jingqi Tong, Yin Zhang, Jiaqi Wang</div>
<div class="meta-line">First: 2025-09-22T13:06:17+00:00 · Latest: 2026-01-26T02:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17743v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.17743v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models&#x27; ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoPro：用于长视频理解的自适应程序推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生成视觉任务的程序工作流方面显示出潜力。然而，以前的方法往往依赖于闭源模型，缺乏系统推理，并且在长视频问答（videoQA）中表现不佳。为了解决这些挑战，我们引入了FS-VisPR框架，这是一种自适应视觉程序推理方法，平衡了简单查询的快速推理与困难查询的慢速推理。首先，我们设计了高效的视觉模块（例如，关键片段检索和字幕检索）以支持长视频任务。然后，我们构建了一个多样化且高质量的快慢推理数据集，结合强大的LLM，以对齐开源语言模型生成视觉程序工作流的能力，称为FS-LLM。接下来，我们设计了一个快慢推理框架与FS-LLM：简单查询由VideoLLMs直接解决，而困难查询则调用视觉程序推理，受到类人推理过程的启发。在此过程中，低置信度的快速思考答案将触发第二阶段的慢推理过程，如果程序执行失败，则激活回退机制以进行快速推理。此外，我们通过在训练和推理期间的参数搜索来改进视觉程序。通过调整程序中视觉模块的参数，生成多个变体：在训练期间，选择产生正确答案的程序，而在推理期间，应用置信度最高的程序。实验表明，FS-VisPR提高了视觉程序工作流的效率和可靠性。在LVBench上取得了50.4%的准确率，超越了GPT-4o，匹配了Qwen2.5VL-72B在VideoMME上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance long-form video question answering (videoQA) by addressing the limitations of previous approaches that rely on closed-source models and lack systematic reasoning. The authors introduce the FS-VisPR framework, which employs an adaptive visual program reasoning method that combines fast reasoning for simple queries with slow reasoning for complex ones. Key experimental findings indicate that FS-VisPR significantly improves efficiency and reliability in visual program workflows, achieving 50.4% accuracy on the LVBench dataset, outperforming GPT-4o and matching the performance of Qwen2.5VL-72B on VideoMME.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过克服以往依赖封闭源模型和缺乏系统推理的局限性，来增强长视频问答（videoQA）的能力。作者提出了FS-VisPR框架，采用自适应视觉程序推理方法，区分简单和复杂查询，利用高效的视觉模块进行关键片段和字幕检索等任务。实验结果表明，FS-VisPR在LVBench数据集上达到了50.4%的准确率，超越了GPT-4o，并在VideoMME上与Qwen2.5VL-72B的表现相匹配，从而提高了视觉程序工作流的效率和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-domain EEG-based Emotion Recognition with Contrastive Learning</div>
<div class="meta-line">Authors: Rui Yan, Yibo Li, Han Ding, Fei Wang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-11-07T14:55:10+00:00 · Latest: 2026-01-25T07:14:05+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05293v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05293v2">PDF</a> · <a href="https://github.com/Departure2021/EmotionCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69\% and 73.50\%, and cross-time accuracies of 88.46\% and 77.54\%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition. The code is available at https://github.com/Departure2021/EmotionCLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于跨域EEG的情感识别与对比学习</div>
<div class="mono" style="margin-top:8px">基于脑电图（EEG）的情感识别对情感计算至关重要，但在特征利用和跨域泛化方面面临挑战。本研究引入EmotionCLIP，将识别重新构建为CLIP框架下的EEG-文本匹配任务。定制的主干网络SST-LegoViT通过多尺度卷积和Transformer模块捕捉空间、频谱和时间特征。在SEED和SEED-IV数据集上的实验显示，跨主体准确率分别为88.69\%和73.50\%，跨时间准确率为88.46\%和77.54\%，优于现有模型。结果证明了多模态对比学习在稳健EEG情感识别中的有效性。代码可在https://github.com/Departure2021/EmotionCLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve EEG-based emotion recognition, which is essential for affective computing but struggles with feature utilization and cross-domain generalization. The authors propose EmotionCLIP, which reformulates the recognition task as an EEG-text matching problem within the CLIP framework, utilizing a custom backbone called SST-LegoViT to effectively capture spatial, spectral, and temporal features. Experimental results on the SEED and SEED-IV datasets demonstrate that the model achieves cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, surpassing existing models and highlighting the potential of multimodal contrastive learning for enhancing EEG emotion recognition.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善基于脑电图（EEG）的情感识别，这对于情感计算至关重要，但在特征利用和跨领域泛化方面面临挑战。作者提出了EmotionCLIP，将识别任务重新构建为EEG-文本匹配问题，采用CLIP框架，并利用定制的骨干网络SST-LegoViT，通过多尺度卷积和Transformer模块有效捕捉空间、频谱和时间特征。在SEED和SEED-IV数据集上的实验结果显示，跨主体准确率分别达到88.69%和73.50%，跨时间准确率达到88.46%和77.54%，表明多模态对比学习在增强EEG情感识别性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</div>
<div class="meta-line">Authors: Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi</div>
<div class="meta-line">First: 2026-01-25T02:32:01+00:00 · Latest: 2026-01-25T02:32:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17657v1">PDF</a> · <a href="https://github.com/taewan2002/space-clip">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPACE-CLIP：通过自适应CLIP嵌入进行单目深度估计的空间感知</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）在语义理解方面取得了非凡的成功，但在感知几何结构方面固有地存在困难。现有方法试图通过文本提示查询CLIP来弥补这一差距，这一过程往往间接且低效。本文提出了一种根本不同的方法，使用双通道解码器。我们提出了SPACE-CLIP，这是一种架构，直接从冻结的CLIP视觉编码器中解锁和解释潜在的几何知识，完全绕过文本编码器及其相关的文本提示。语义通道解释高层特征，动态地基于全局上下文使用特征线性调制（FiLM）进行条件处理。此外，结构通道从早期层提取细粒度空间细节。这些互补流被分层融合，使语义上下文和精确几何的强健合成成为可能。在KITTI基准上的广泛实验表明，SPACE-CLIP显著优于以前的基于CLIP的方法。我们的消融研究验证了双通道的协同融合对这一成功至关重要。SPACE-CLIP为重新利用大规模视觉模型提供了一种新的、高效的、架构优雅的蓝图。所提出的方法不仅是一个独立的深度估计器，而是一个可直接集成的空间感知模块，适用于下一代具身AI系统，如视觉-语言-行动（VLA）模型。我们的模型可在https://github.com/taewan2002/space-clip获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the geometric perception capabilities of Contrastive Language-Image Pre-training (CLIP), which traditionally struggles with depth estimation. The authors propose SPACE-CLIP, a novel architecture that utilizes a dual-pathway decoder to directly extract geometric knowledge from a frozen CLIP vision encoder, bypassing the need for textual prompts. Experimental results on the KITTI benchmark demonstrate that SPACE-CLIP significantly outperforms existing CLIP-based methods, with ablation studies confirming the importance of the synergistic fusion of its semantic and structural pathways for achieving superior performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善语义理解任务中的几何感知，因为现有使用CLIP的方法在直接几何解释方面存在困难。作者提出了SPACE-CLIP，这是一种新颖的架构，利用双通道解码器直接从冻结的CLIP视觉编码器中提取潜在的几何知识，绕过文本编码器。对KITTI基准的实验结果表明，SPACE-CLIP显著优于以前的基于CLIP的方法，消融研究确认双通道的整合对于实现这种增强性能至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#x27; Contextual and Cultural Knowledge and Thinking</div>
<div class="meta-line">Authors: Xilin Jiang, Qiaolin Wang, Junkai Wu, Xiaomin He, Zhongweiyang Xu, Yinghao Ma, Minshuo Piao, Kaiyi Yang, Xiuwen Zheng, Riki Shimizu, Yicong Chen, Arsalan Firoozi, Gavin Mischler, Sukru Samet Dindar, Richard Antonello, Linyang He, Tsun-An Hsieh, Xulin Fan, Yulun Wu, Yuesheng Ma, Chaitanya Amballa, Weixiong Chen, Jiarui Hai, Ruisi Li, Vishal Choudhari, Cong Han, Yinghao Aaron Li, Adeen Flinker, Mounya Elhilali, Emmanouil Benetos, Mark Hasegawa-Johnson, Romit Roy Choudhury, Nima Mesgarani</div>
<div class="meta-line">First: 2026-01-25T01:40:15+00:00 · Latest: 2026-01-25T01:40:15+00:00</div>
<div class="meta-line">Comments: avmemeexam.github.io/public</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17645v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17645v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&amp;A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AVMeme考试：针对LLMs的上下文和文化知识及思维的多模态多语言多文化基准</div>
<div class="mono" style="margin-top:8px">互联网音视频片段通过时间变化的声音和运动传达意义，超越了文本所能表达的内容。为了检验AI模型是否能够理解人类文化背景中的这些信号，我们引入了AVMeme考试，这是一个由人类策划的基准，包含超过一千个标志性的互联网声音和视频，涵盖演讲、歌曲、音乐和音效。每个模因都配有独特的问答，评估从表面内容到上下文、情感、使用和世界知识的理解水平，以及原始年份、转录、摘要和敏感性等元数据。我们使用该基准系统地评估了最先进的多模态大型语言模型（MLLMs）与人类参与者的表现。我们的结果揭示了一个持续的局限性：当前模型在无文本的音乐和音效上表现不佳，并且在上下文和文化思维方面相较于表面内容存在困难。这些发现突显了人类对齐的多模态智能中的一个关键差距，并呼吁开发能够超越听到和看到的表面进行上下文和文化感知的模型。项目页面：avmemeexam.github.io/public</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to assess the ability of AI models to understand audio-visual content in human cultural contexts, which traditional text-based evaluations may overlook. The authors introduce the AVMeme Exam, a benchmark consisting of over one thousand curated Internet sounds and videos, each accompanied by a unique Q&amp;A to evaluate understanding across various dimensions, including context and emotion. The evaluation of state-of-the-art multimodal large language models alongside human participants reveals that these models exhibit significant limitations, particularly in interpreting textless music and sound effects, indicating a gap in their contextual and cultural understanding compared to surface-level content.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估人工智能模型是否能够理解人类文化背景下的音频视觉信号，这些信号通常超出了单纯文本的能力。作者引入了AVMeme考试，这是一个包含超过一千个策划的互联网声音和视频的基准，每个都配有独特的问答，以评估在上下文和情感等多个维度上的理解能力。对最先进的多模态大型语言模型与人类参与者的评估显示，这些模型在无文本的音乐和音效方面表现不佳，表明它们在上下文和文化思维方面与表面内容相比存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation</div>
<div class="meta-line">Authors: Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran</div>
<div class="meta-line">First: 2026-01-24T23:01:43+00:00 · Latest: 2026-01-24T23:01:43+00:00</div>
<div class="meta-line">Comments: 21 pages,7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17625v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BrainDistill：具有任务特定知识蒸馏的可植入运动解码</div>
<div class="mono" style="margin-top:8px">基于变换器的神经解码器具有大量参数，经过大规模数据集的预训练，最近在脑机接口（BCI）任务中超越了经典机器学习模型和小型神经网络。然而，它们的大量参数和高计算需求阻碍了在功率受限的可植入系统中的部署。为了解决这一挑战，我们提出了BrainDistill，一种新颖的可植入运动解码管道，将可植入神经解码器（IND）与任务特定知识蒸馏（TSKD）框架相结合。与标准特征蒸馏方法试图完整保留教师表示不同，TSKD通过监督投影明确优先考虑对解码至关重要的特征。在多个神经数据集上，IND在运动解码任务中始终优于先前的神经解码器，而其TSKD蒸馏变体在少量样本校准设置中进一步超越了替代蒸馏方法。最后，我们提出了一种量化感知训练方案，使得在训练期间学习的激活裁剪范围内进行仅整数推理成为可能。量化后的IND能够在可植入BCI的严格功率限制下部署，且性能损失最小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the deployment of neural decoders in power-constrained implantable brain-computer interfaces (BCIs), which are hindered by the large parameter counts and computational demands of existing models. The authors introduce BrainDistill, an innovative motor decoding pipeline that combines an implantable neural decoder with a task-specific knowledge distillation framework, which focuses on preserving only the critical features necessary for decoding. Experimental results demonstrate that the implantable neural decoder consistently outperforms previous models on motor decoding tasks, and its distilled variant achieves superior performance compared to other distillation methods in few-shot calibration scenarios, while also allowing for quantization-aware training that facilitates integer-only inference suitable for implantable systems with minimal performance loss.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要部署能够在功率限制下运行的有效脑机接口（BCI）系统，因为传统的基于变换器的神经解码器过于庞大且计算需求高。作者提出了BrainDistill，这是一种结合了可植入神经解码器和任务特定知识蒸馏框架的植入式运动解码管道，该框架优先考虑解码的关键特征。实验结果表明，植入式神经解码器在运动解码任务中始终优于先前的模型，而其蒸馏版本在少量样本校准场景中表现优于其他蒸馏方法，同时还允许量化感知训练，以便在功率有限的环境中以最小的性能损失进行部署。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
