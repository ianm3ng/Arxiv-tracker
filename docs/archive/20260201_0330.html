<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-01 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260201_0330</div>
    <div class="row"><div class="card">
<div class="title">RedSage: A Cybersecurity Generalist LLM</div>
<div class="meta-line">Authors: Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T18:59:57+00:00 · Latest: 2026-01-29T18:59:57+00:00</div>
<div class="meta-line">Comments: Accepted on ICLR 2026; Project page: https://risys-lab.github.io/RedSage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22159v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://risys-lab.github.io/RedSage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&amp;A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RedSage：网络安全通用 LLM</div>
<div class="mono" style="margin-top:8px">网络安全操作需要支持多样化工作流程而不暴露敏感数据的助手 LLM。现有解决方案要么依赖于具有隐私风险的专有 API，要么依赖于缺乏领域适应性的开放模型。为填补这一空白，我们通过大规模网络过滤和手动收集高质量资源，策划了 118 亿个以网络安全为重点的持续预训练数据，涵盖 28600 个文档，涉及框架、攻击技术和安全工具。在此基础上，我们设计了一个代理增强管道，模拟专家工作流程，生成 26.6 万个多轮网络安全样本用于监督微调。结合通用开源 LLM 数据，这些资源使得训练 RedSage 成为可能，RedSage 是一个开源的、可本地部署的网络安全助手，具有领域感知的预训练和后训练。为了严格评估模型，我们引入了 RedSage-Bench，这是一个包含 3 万个多项选择和 240 个开放式问答项目的基准，涵盖网络安全知识、技能和工具专长。RedSage 还在已建立的网络安全基准（如 CTI-Bench、CyberMetric、SECURE）和通用 LLM 基准上进行评估，以评估更广泛的泛化能力。在 80 亿规模下，RedSage 一直取得更好的结果，在网络安全基准上超越基线模型达 +5.59 分，在开放 LLM 排行榜任务上超越 +5.05 分。这些发现表明，领域感知的代理增强和预/后训练不仅可以增强网络安全特定的专业知识，还可以帮助提高一般推理和遵循指令的能力。所有模型、数据集和代码均可公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a cybersecurity assistant LLM that can support various workflows while ensuring data privacy, addressing the limitations of existing proprietary and open models. The authors curated a dataset of 11.8 billion tokens focused on cybersecurity through extensive web filtering and manual collection, and developed an agentic augmentation pipeline to generate 266,000 multi-turn samples for supervised fine-tuning. The experimental results show that RedSage, trained with this domain-specific data, outperforms baseline models by up to 5.59 points on cybersecurity benchmarks and 5.05 points on general LLM tasks, demonstrating improved cybersecurity expertise and enhanced general reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个能够支持多种工作流程并确保数据隐私的网络安全助手LLM，解决现有专有和开放模型的局限性。作者通过整理118亿个网络安全相关的数据，并采用代理增强管道生成26.6万个多轮样本进行监督微调，从而开发了RedSage。实验结果表明，RedSage在网络安全基准测试中比基线模型高出最多5.59分，在一般LLM任务中高出5.05分，显示出其在特定领域的专业知识和增强的通用推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">One-step Latent-free Image Generation with Pixel Mean Flows</div>
<div class="meta-line">Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</div>
<div class="meta-line">First: 2026-01-29T18:59:56+00:00 · Latest: 2026-01-29T18:59:56+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &quot;pixel MeanFlow&quot; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步无潜变量图像生成与像素均流</div>
<div class="mono" style="margin-top:8px">现代基于扩散/流的图像生成模型通常具有两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无潜变量的一步扩散/流铺平了道路。在这项工作中，我们朝着这一目标迈出了进一步的步伐，提出了“像素均流”（pMF）。我们的核心指导方针是将网络输出空间和损失空间分别进行公式化。网络目标被设计为位于假定的低维图像流形上（即x预测），而损失则通过速度空间中的均流定义。我们引入了图像流形与平均速度场之间的简单变换。在实验中，pMF在256x256分辨率（2.22 FID）和512x512分辨率（2.48 FID）上实现了一步无潜变量生成的强大结果，填补了这一领域的一个关键缺失部分。我们希望我们的研究能进一步推动基于扩散/流的生成模型的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance image generation techniques by eliminating the need for multi-step sampling and latent space, which are common in current diffusion and flow-based models. The authors propose a novel method called &quot;pixel MeanFlow&quot; (pMF), which separates the network output space from the loss space, targeting a low-dimensional image manifold while defining the loss through MeanFlow in the velocity space. Experimental results demonstrate that pMF achieves competitive performance in one-step latent-free image generation on ImageNet, with FID scores of 2.22 at 256x256 resolution and 2.48 at 512x512 resolution, indicating significant progress in this area of generative modeling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过消除扩散和基于流的模型中对多步采样和潜在空间操作的需求，推动图像生成技术的发展。作者提出了一种新方法，称为“像素均流”（pMF），该方法将网络输出空间的构造与损失空间分开，目标是低维图像流形，同时通过速度空间中的均流定义损失。实验结果表明，pMF在ImageNet上实现了一步无潜在图像生成的竞争性能，在256x256分辨率下的FID得分为2.22，在512x512分辨率下为2.48，显示出在生成建模领域的显著进展。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Hidden Gems in Model Repositories</div>
<div class="meta-line">Authors: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-29T18:59:55+00:00 · Latest: 2026-01-29T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of &quot;hidden gems&quot;, unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>发现模型库中的隐藏宝石</div>
<div class="mono" style="margin-top:8px">公共库中托管着数百万个微调模型，但社区使用仍然不成比例地集中在少数基础检查点上。我们调查这种集中是否反映了有效的市场选择，或者是否有优越的模型被系统性地忽视。通过对2000多个模型的广泛评估，我们展示了“隐藏宝石”的普遍存在，这些不受欢迎的微调模型显著优于其受欢迎的同行。值得注意的是，在Llama-3.1-8B系列中，我们发现一些下载量极少的检查点在不增加推理成本的情况下将数学表现从83.2%提高到96.0%。然而，通过对每个上传模型进行全面评估来发现这些模型在计算上是不可行的。因此，我们将模型发现形式化为多臂老虎机问题，并通过使用共享查询集和激进的淘汰计划加速顺序减半搜索算法。我们的方法在每个候选模型仅需50个查询即可检索到顶级模型，发现速度提高超过50倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that public repositories contain millions of fine-tuned models, yet usage is heavily skewed towards a few popular ones, raising questions about whether better models are being overlooked. To address this, the authors evaluate over 2,000 models and identify numerous &#x27;hidden gems&#x27; that outperform popular models, with some checkpoints in the Llama-3.1-8B family improving math performance from 83.2% to 96.0% without added inference costs. Given the impracticality of exhaustively evaluating all models, they reformulate the discovery process as a Multi-Armed Bandit problem and enhance the Sequential Halving search algorithm, achieving model retrieval with as few as 50 queries per candidate and accelerating the discovery process by over 50 times.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于观察到公共仓库中包含数百万个微调模型，但使用情况严重倾斜于少数流行模型，这引发了关于这种现象是否反映了真实模型质量或更好的选项被忽视的问题。作者评估了2000多个模型，以识别“隐藏的宝石”——那些不太受欢迎但性能优于广泛使用模型的微调模型，特别指出Llama-3.1-8B系列中的某些稀有检查点可以在不增加推理成本的情况下将数学性能从83.2%提升至96.0%。为了有效发现这些模型，他们将模型发现重新构建为多臂老虎机问题，并增强了顺序减半搜索算法，实现了每个候选模型仅需50个查询即可检索到模型，发现速度提高了50倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</div>
<div class="meta-line">Authors: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu</div>
<div class="meta-line">First: 2026-01-29T18:59:53+00:00 · Latest: 2026-01-29T18:59:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22156v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合线性注意力的正确实现：极长上下文的高效蒸馏和有效架构</div>
<div class="mono" style="margin-top:8px">混合Transformer架构结合了softmax注意力块和递归神经网络（RNN），在长上下文建模中展现了理想的性能-吞吐量权衡，但由于从头开始大规模预训练的高昂成本，其采用和研究受到阻碍。一些近期研究表明，预训练的softmax注意力块可以通过参数转移和知识蒸馏转换为RNN块。然而，这些转移方法需要大量的训练数据（超过100亿个标记），而且生成的混合模型在长上下文性能上也表现不佳，这是混合模型在推理速度上显著优于基于Transformer模型的场景。本文提出了HALO（通过层优化的混合注意力），一个将Transformer模型蒸馏为RNN-注意力混合模型的流程。我们还提出了HypeNet，这是一种通过新颖的位置编码方案（称为HyPE）和各种架构修改实现优越长度泛化的混合架构。我们使用HALO将Qwen3系列转换为HypeNet，取得了与原始Transformer模型相当的性能，同时享有优越的长上下文性能和效率。该转换仅需2.3亿个标记，不到其预训练数据的0.01%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of long-context modeling using hybrid Transformer architectures, which combine softmax attention and recurrent neural networks (RNNs), while addressing the challenges posed by the high costs of large-scale pre-training. The authors introduce HALO (Hybrid Attention via Layer Optimization), a method for distilling Transformer models into RNN-attention hybrids, and present HypeNet, a new hybrid architecture that enhances length generalization through a novel position encoding scheme and various architectural modifications. The experimental results demonstrate that converting the Qwen3 series into HypeNet using HALO achieves performance comparable to the original Transformer models, while significantly improving long-context performance and requiring only 2.3B tokens for conversion, which is less than 0.01% of the pre-training data needed for traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在混合Transformer架构中实现高效的长上下文建模，这种架构结合了softmax注意力和递归神经网络，但由于大规模预训练的高成本而面临挑战。作者提出了HALO，一个将Transformer模型蒸馏为RNN-注意力混合模型的流程，并引入了HypeNet，这是一种通过新颖的位置编码方案和架构修改来改善长度泛化的新架构。实验结果表明，使用HALO将Qwen3系列转换为HypeNet的过程在性能上与原始Transformer模型相当，同时显著提升了长上下文的性能和效率，仅需2.3B个标记进行转换。</div>
</details>
</div>
<div class="card">
<div class="title">UEval: A Benchmark for Unified Multimodal Generation</div>
<div class="meta-line">Authors: Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22155v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UEval：统一多模态生成的基准</div>
<div class="mono" style="margin-top:8px">我们介绍了UEval，这是一个用于评估统一模型的基准，即能够生成图像和文本的模型。UEval包含1000个专家策划的问题，这些问题要求模型输出中同时包含图像和文本，来源于8个真实世界任务。我们策划的问题涵盖了广泛的推理类型，从逐步指南到教科书解释。评估开放式多模态生成并非易事，因为简单的LLM作为评判者的方法可能会忽略细微之处。与之前依赖多模态大型语言模型（MLLM）来评估图像质量或文本准确性的工作不同，我们在UEval中设计了基于评分标准的评分系统。对于每个问题，参考图像和文本答案被提供给MLLM，以生成初步评分标准，包括多个评估标准，然后人类专家对这些评分标准进行细化和验证。总的来说，UEval包含10417个经过验证的评分标准，使得可扩展和细致的自动评分成为可能。UEval对当前的统一模型具有挑战性：GPT-5-Thinking的得分仅为66.4分（满分100），而最佳开源模型仅达到49.1分。我们观察到推理模型通常优于非推理模型，并且将推理痕迹从推理模型转移到非推理模型显著缩小了差距。这表明推理可能对需要复杂多模态理解和生成的任务至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a benchmark for evaluating unified multimodal models that can generate both images and text. The authors developed UEval, which consists of 1,000 expert-curated questions sourced from eight real-world tasks, covering various reasoning types. They implemented a rubric-based scoring system that allows for scalable and fine-grained automatic scoring, resulting in 10,417 validated rubric criteria. Experimental results show that current unified models struggle with this benchmark, with GPT-5-Thinking scoring only 66.4 out of 100 and the best open-source model achieving 49.1, indicating that reasoning capabilities are crucial for complex multimodal tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个用于评估统一多模态模型（能够生成图像和文本）的基准。作者开发了UEval，其中包含来自八个真实任务的1000个专家策划的问题，涵盖各种推理类型。与传统的多模态大型语言模型评估方法不同，他们实施了一种基于评分标准的评分系统，该系统通过生成初步评分标准并由人类专家进行细化。研究结果表明，当前的统一模型在UEval上表现不佳，GPT-5-Thinking的得分为66.4分，而最佳开源模型仅达到49.1分，这突显了推理在多模态任务中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Reasoning Reward Model for Agents</div>
<div class="meta-line">Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/kxfan2002/Reagent</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22154v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22154v1">PDF</a> · <a href="https://github.com/kxfan2002/Reagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索代理的推理奖励模型</div>
<div class="mono" style="margin-top:8px">代理强化学习（Agentic RL）在使代理执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍依赖于稀疏的基于结果的奖励进行训练。这种反馈无法区分中间推理质量，导致训练结果次优。本文介绍了代理推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，为代理轨迹生成结构化反馈，包括（1）明确的推理痕迹，（2）聚焦的批评，通过突出推理缺陷提供改进指导，以及（3）评估过程表现的整体评分。利用这些信号，我们系统地研究了三种集成策略：Reagent-C（文本增强的改进）、Reagent-R（奖励增强的指导）和Reagent-U（统一反馈集成）。在12个不同基准上的广泛评估表明，Reagent-U带来了显著的性能提升，在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了我们的推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the training of agents in Agentic Reinforcement Learning (Agentic RL) by addressing the limitations of sparse outcome-based rewards that do not adequately assess intermediate reasoning quality. The authors propose the Agent Reasoning Reward Model (Agent-RRM), which provides structured feedback through an explicit reasoning trace, focused critiques, and an overall performance score. Experimental results across 12 benchmarks reveal that the Reagent-U integration strategy significantly improves performance, achieving scores of 43.7% on GAIA and 46.2% on WebWalkerQA, thus demonstrating the effectiveness of the proposed reasoning reward model and training methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决稀疏结果奖励的局限性来改善代理在代理强化学习（Agentic RL）中的训练，这种奖励无法充分评估中间推理质量。作者提出了代理推理奖励模型（Agent-RRM），该模型通过明确的推理轨迹、对推理缺陷的集中批评和整体性能评分提供结构化反馈。跨越12个基准的实验结果表明，Reagent-U集成策略显著提高了性能，在GAIA上获得43.7%的得分，在WebWalkerQA上获得46.2%的得分，从而证明了所提出的推理奖励模型和训练方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</div>
<div class="meta-line">Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-29T18:59:51+00:00 · Latest: 2026-01-29T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22153v1">PDF</a> · <a href="https://github.com/hzxie/DynamicVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.infinitescript.com/project/dynamic-vla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynamicVLA：用于动态物体操控的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">操控动态物体仍然是视觉-语言-动作（VLA）模型面临的一个开放挑战，尽管在静态操控中具有强大的泛化能力，但在需要快速感知、时间预判和持续控制的动态场景中却表现不佳。我们提出了DynamicVLA，这是一个用于动态物体操控的框架，通过三个关键设计集成了时间推理和闭环适应：1）一个紧凑的0.4B VLA，使用卷积视觉编码器进行空间高效、结构忠实的编码，实现快速的多模态推理；2）连续推理，支持重叠推理和执行，以降低延迟并及时适应物体运动；3）潜在感知动作流，强制时间对齐的动作执行，弥合感知与执行之间的差距。为了填补动态操控数据的缺失基础，我们引入了动态物体操控（DOM）基准，从零开始构建，采用自动数据收集管道高效收集200K个合成剧集，涵盖2.8K个场景和206个物体，并实现快速收集2K个真实世界剧集，无需遥控操作。广泛的评估表明在响应速度、感知和泛化方面有显著改善，使DynamicVLA成为一个统一的框架，适用于各种动态物体操控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by Vision-Language-Action (VLA) models in manipulating dynamic objects, as they often excel in static scenarios but struggle with rapid perception and control in dynamic environments. The authors present DynamicVLA, a framework that incorporates temporal reasoning and closed-loop adaptation through a compact VLA model, continuous inference for timely adaptation to object motion, and latent-aware action streaming to enhance action execution. Experimental results show significant improvements in response speed, perception, and generalization, supported by the introduction of the Dynamic Object Manipulation benchmark, which includes 200K synthetic episodes and 2K real-world episodes, demonstrating the framework&#x27;s effectiveness in dynamic object manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉-语言-动作（VLA）模型在动态物体操控中面临的挑战，这些挑战需要快速感知和持续控制。作者提出了DynamicVLA框架，该框架通过一个紧凑的VLA模型、连续推理以降低延迟以及潜在感知动作流来增强动作执行，整合了时间推理和闭环适应。实验结果显示在响应速度、感知和泛化能力上有显著改善，并通过创建动态物体操控基准，包含20万条合成剧集和2000条真实世界剧集，支持有效评估。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</div>
<div class="meta-line">Authors: Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang</div>
<div class="meta-line">First: 2026-01-29T18:59:50+00:00 · Latest: 2026-01-29T18:59:50+00:00</div>
<div class="meta-line">Comments: accepted by DATE2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22151v1">PDF</a> · <a href="https://github.com/TUDa-HWAI/NN2Logic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
  The code is open source at https://github.com/TUDa-HWAI/NN2Logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破性结果：将神经网络转换为边缘计算的逻辑流</div>
<div class="mono" style="margin-top:8px">神经网络已成功应用于各种资源受限的边缘设备，这些设备通常由于电力供应有限而使用中央处理单元（CPU）而非图形处理单元（GPU）。最先进的研究仍然集中在高效执行大量乘加（MAC）操作上。然而，CPU本身并不擅长大规模执行此类数学操作，因为它们更适合执行控制流逻辑，即计算机算法。为了提高神经网络在CPU上的计算效率，本文提出将其转换为逻辑流进行执行。具体而言，神经网络首先被转换为等效的决策树，然后选择具有恒定叶子的决策路径并压缩为逻辑流。这些逻辑流由if和else结构以及减少的MAC操作组成。实验结果表明，在模拟的RISC-V CPU上，延迟可以减少多达14.9%，且没有任何准确性下降。
  代码开源于 https://github.com/TUDa-HWAI/NN2Logic</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the computational efficiency of neural networks on resource-constrained edge devices, particularly those using CPUs that are not optimized for large-scale mathematical operations. The authors propose a method that converts neural networks into equivalent decision trees, from which decision paths with constant leaves are selected and compressed into logic flows consisting of if and else structures, thereby reducing the reliance on multiply-accumulate operations. Experimental results indicate that this approach can reduce latency by up to 14.9% on a simulated RISC-V CPU without any degradation in accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高神经网络在资源受限的边缘设备上执行的效率，这些设备通常由于功率限制而依赖于CPU而非GPU。作者提出了一种将神经网络转换为等效决策树的方法，从中选择具有恒定叶子的决策路径，并将其压缩为逻辑流，利用if-else结构并最小化乘加操作。实验结果表明，该方法可以在模拟的RISC-V CPU上将延迟减少多达14.9%，且不影响准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</div>
<div class="meta-line">Authors: Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-29T18:59:24+00:00 · Latest: 2026-01-29T18:59:24+00:00</div>
<div class="meta-line">Comments: 26 pages, 31 figures, 13 tables. Project Page: https://sites.google.com/view/vi-probe/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vi-probe/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) often answer classic visual illusions &quot;correctly&quot; on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLMs是感知还是回忆？通过经典视觉错觉探测视觉感知与记忆</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）在原始图像上通常“正确”回答经典视觉错觉，但在错觉因素反转时仍保持相同的反应，尽管视觉变化对人类来说显而易见。这引发了一个根本性的问题：VLMs是感知视觉变化还是仅仅回忆记忆模式？虽然几项研究注意到了这一现象，但其潜在原因仍不清楚。为了从观察转向系统理解，本文引入了VI-Probe，一个可控的视觉错觉框架，具有分级扰动和匹配的视觉控制（无错觉诱导器），将视觉基础的感知与语言驱动的回忆区分开。与以往关注平均准确率的研究不同，我们使用极性翻转一致性、模板固定指数和与匹配控制归一化的错觉倍增器来测量稳定性和敏感性。不同家族的实验表明，反应持久性源于异质原因，而非单一机制。例如，GPT-5表现出记忆覆盖，Claude-Opus-4.1显示感知-记忆竞争，而Qwen变体则暗示视觉处理限制。我们的发现挑战了单一原因的观点，并激励基于探测的评估，测量知识和对受控视觉变化的敏感性。数据和代码可在https://sites.google.com/view/vi-probe/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Large Vision-Language Models (VLMs) perceive visual changes or simply recall memorized patterns, motivated by their consistent responses to classic visual illusions despite obvious alterations. The authors introduce VI-Probe, a framework designed to systematically assess visual perception versus memory recall through graded perturbations and matched controls. Experimental results indicate that the persistence of responses in VLMs stems from various underlying mechanisms, with different models exhibiting distinct behaviors: GPT-5 demonstrates memory override, Claude-Opus-4.1 shows competition between perception and memory, and Qwen variants reveal limitations in visual processing. These findings suggest a need for evaluation methods that account for both knowledge and sensitivity to visual changes.</div>
<div class="mono" style="margin-top:8px">本研究探讨大型视觉语言模型（VLMs）是否真正感知视觉变化，还是仅仅回忆已学模式，因为它们在经典视觉错觉中即使在错觉因素改变时也常常正确回答。为了解决这个问题，研究引入了VI-Probe框架，利用分级扰动和匹配视觉控制来区分视觉感知和记忆回忆。实验结果表明，VLMs中响应持续性的原因多种多样，不同模型表现出不同的行为：GPT-5表现出记忆覆盖，Claude-Opus-4.1展示了感知与记忆之间的竞争，而Qwen变体则表明视觉处理的局限性。这些发现表明需要评估方法来同时评估知识和对视觉变化的敏感性，而不是将响应归因于单一原因。</div>
</details>
</div>
<div class="card">
<div class="title">DynaWeb: Model-Based Reinforcement Learning of Web Agents</div>
<div class="meta-line">Authors: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu</div>
<div class="meta-line">First: 2026-01-29T18:59:07+00:00 · Latest: 2026-01-29T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynaWeb：基于模型的网络代理强化学习</div>
<div class="mono" style="margin-top:8px">自主网络代理的开发，依托大型语言模型（LLMs）和强化学习（RL），代表了通用人工智能助手的重要一步。然而，训练这些代理受到与实时互联网交互的挑战的严重阻碍，这种交互效率低、成本高且风险重重。基于模型的强化学习（MBRL）通过学习环境的世界模型以实现模拟交互，提供了一个有前景的解决方案。本文介绍了DynaWeb，一个新颖的MBRL框架，通过与训练有素的网络世界模型交互来训练网络代理，该模型能够根据代理的动作预测自然网页表示。该模型作为一个合成网络环境，使代理策略能够通过生成大量的回放动作轨迹进行高效的在线强化学习。除了免费的策略回放，DynaWeb还结合了来自训练数据的真实专家轨迹，这些轨迹在训练期间与在线策略回放随机交错，以提高稳定性和样本效率。在具有挑战性的WebArena和WebVoyager基准上进行的实验表明，DynaWeb始终显著提高了最先进的开源网络代理模型的性能。我们的研究结果确立了通过想象训练网络代理的可行性，提供了一种可扩展和高效的在线代理强化学习的方式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the training of autonomous web agents using Large Language Models and reinforcement learning, addressing the inefficiencies and risks associated with live internet interactions. The authors propose DynaWeb, a model-based reinforcement learning framework that utilizes a web world model to simulate interactions and generate action trajectories for training. Experimental results on the WebArena and WebVoyager benchmarks show that DynaWeb significantly improves the performance of existing web agent models, demonstrating the effectiveness of training agents through simulated environments and expert trajectory integration for better stability and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过大型语言模型和强化学习提高自主网络代理的训练，但在与实时互联网交互时面临重大挑战。作者提出了DynaWeb，这是一种新颖的基于模型的强化学习框架，利用网络世界模型模拟交互并生成行动轨迹以实现高效训练。在WebArena和WebVoyager基准测试中的实验结果表明，DynaWeb显著提高了现有网络代理模型的性能，证明了通过模拟环境和想象训练代理的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</div>
<div class="meta-line">Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch</div>
<div class="meta-line">First: 2026-01-29T18:58:47+00:00 · Latest: 2026-01-29T18:58:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22146v1">PDF</a> · <a href="https://huggingface.co/fineinstructions">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised &quot;predict the next word&quot; objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of &quot;instruction-tuning&quot; data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With &quot;supervised&quot; synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FineInstructions：将合成指令扩展到预训练规模</div>
<div class="mono" style="margin-top:8px">由于监督训练数据有限，大型语言模型（LLMs）通常通过自监督的“预测下一个词”目标在大量非结构化文本数据上进行预训练。为了使生成的模型对用户有用，它还在更少量的“指令调优”数据上进行进一步训练，这些数据由指令和响应的监督训练示例组成。为了克服监督数据的有限性，我们提出了一种程序，可以将互联网规模预训练文档中的知识转化为数十亿个合成指令和答案训练对。生成的数据集称为FineInstructions，使用了约1800万个由真实用户编写的查询和提示创建的指令模板。这些指令模板与来自非结构化预训练语料库的人类编写的源文档匹配并实例化。通过在这种规模上生成的“监督”合成训练数据，LLM可以仅通过指令调优目标从头开始进行预训练，这与LLMs的预期下游使用（响应用户提示）更为一致。我们进行控制的逐词训练实验，发现FineInstructions的预训练在测量自由形式响应质量的标准基准上优于标准预训练和其他提议的合成预训练技术。我们的资源可以在https://huggingface.co/fineinstructions找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of limited supervised training data for large language models (LLMs) by proposing a method to generate synthetic instruction and answer pairs from existing unstructured text data. This is achieved through the creation of a dataset called FineInstructions, which utilizes approximately 18 million instruction templates derived from real user queries and matches them with human-written documents. Experimental results demonstrate that LLMs pre-trained solely with the synthetic instruction-tuning data outperform those trained with traditional methods on benchmarks assessing response quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLMs）在监督训练数据有限的情况下生成合成指令和答案对的挑战。作者提出了一种方法，将大量预训练文档中的知识转化为数十亿个合成训练示例，使用约1800万个源自真实用户查询的指令模板。关键实验结果表明，仅使用这种合成指令调优数据（称为FineInstructions）进行预训练的LLMs，在标准基准测试中响应质量显著优于传统预训练方法和其他合成技术。</div>
</details>
</div>
<div class="card">
<div class="title">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</div>
<div class="meta-line">Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or</div>
<div class="meta-line">First: 2026-01-29T18:57:13+00:00 · Latest: 2026-01-29T18:57:13+00:00</div>
<div class="meta-line">Comments: Project webpage available at https://justdubit.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://justdubit.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JUST-DUB-IT：通过联合音视频扩散进行视频配音</div>
<div class="mono" style="margin-top:8px">音视频基础模型经过预训练，能够联合生成声音和视觉内容，最近展现出前所未有的多模态生成和编辑能力，为下游任务开辟了新机遇。在这些任务中，视频配音可以从这种先验中受益，但大多数现有解决方案仍依赖于复杂的特定任务管道，在现实环境中表现不佳。在本研究中，我们提出了一种单模型方法，通过轻量级LoRA调整基础音视频扩散模型，实现视频到视频的配音。LoRA使模型能够在生成翻译音频和同步面部动作的同时，基于输入音视频进行条件生成。为了训练这个LoRA，我们利用生成模型本身合成同一说话者的配对多语言视频。具体而言，我们在单个片段中生成带有语言切换的多语言视频，然后在每一半中修复面部和音频，以匹配另一半的语言。通过利用音视频模型丰富的生成先验，我们的方法在保持说话者身份和唇同步的同时，仍然对复杂运动和现实动态具有鲁棒性。我们证明了我们的方法生成的配音视频在视觉保真度、唇同步和鲁棒性方面优于现有的配音管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video dubbing by utilizing the capabilities of Audio-Visual Foundation Models, which can generate sound and visual content together. The authors propose a single-model approach that employs a lightweight LoRA to adapt a foundational audio-video diffusion model for video-to-video dubbing. Experimental results show that this method effectively generates high-quality dubbed videos with enhanced visual fidelity, accurate lip synchronization, and greater robustness against complex motion compared to traditional dubbing pipelines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用音视频基础模型的能力来改善视频配音，这些模型能够共同生成声音和视觉内容，从而解决现有复杂配音解决方案的局限性。作者提出了一种单模型方法，采用轻量级LoRA将基础音视频扩散模型适配用于视频到视频的配音，使其能够在生成翻译音频和同步面部动作时对输入音视频进行条件化。实验结果表明，该方法生成的配音视频在视觉保真度、唇同步性和对复杂运动及现实动态的鲁棒性方面优于传统配音管道。</div>
</details>
</div>
<div class="card">
<div class="title">Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</div>
<div class="meta-line">Authors: Grzegorz Stefanski, Alberto Presta, Michal Byra</div>
<div class="meta-line">First: 2026-01-29T18:56:41+00:00 · Latest: 2026-01-29T18:56:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>彩票路由：针对异构数据的自适应子网络</div>
<div class="mono" style="margin-top:8px">在剪枝中，彩票票据假设认为大型网络包含稀疏子网络或获胜票据，这些子网络可以单独训练以匹配其密集对应物的性能。然而，大多数现有方法假设所有输入共享一个通用的获胜票据，忽略了现实世界数据的固有异质性。在这项工作中，我们提出了彩票路由（RTL），一种自适应剪枝框架，发现多个专门的子网络，称为自适应票据，每个子网络针对一个类别、语义簇或环境条件。RTL在各种数据集和任务中始终优于单模型和多模型基线，在平衡准确性和召回率方面表现更佳，同时使用的参数比独立模型少多达10倍，并且表现出语义一致性。此外，我们识别了子网络崩溃，即在激进剪枝下的性能下降，并引入了子网络相似性评分，以实现无标签的过度稀疏化诊断。总体而言，我们的结果将剪枝重新定义为一种将模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing pruning methods that assume a single winning ticket for all inputs, which overlooks the diversity of real-world data. The authors introduce Routing the Lottery (RTL), an adaptive pruning framework that identifies multiple specialized subnetworks, or adaptive tickets, tailored to specific classes or conditions. Experimental results demonstrate that RTL outperforms both single- and multi-model baselines in balanced accuracy and recall across various datasets, while utilizing up to 10 times fewer parameters than independent models, and also reveals a new challenge of subnetwork collapse under aggressive pruning, along with a novel similarity score for diagnosing oversparsification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有剪枝方法的局限性，这些方法假设所有输入共享一个通用的获胜子网络，忽视了现实世界数据的异质性。作者提出了Routing the Lottery (RTL)，一种自适应剪枝框架，能够识别多个专门的子网络或自适应票证，针对特定类别或条件进行优化。实验结果表明，RTL在各种数据集上在平衡准确率和召回率方面优于单模型和多模型基线，同时使用的参数数量比独立模型少多达10倍，并引入了子网络相似度评分来诊断过度稀疏化问题。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers</div>
<div class="meta-line">Authors: Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang</div>
<div class="meta-line">First: 2026-01-29T18:56:12+00:00 · Latest: 2026-01-29T18:56:12+00:00</div>
<div class="meta-line">Comments: The manuscript is under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22139v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22139v1">PDF</a> · <a href="https://github.com/SUAT-AIRI/Proactive-Interactive-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提问中的推理：将推理大型语言模型从被动求解者转变为主动询问者</div>
<div class="mono" style="margin-top:8px">以推理为导向的大型语言模型（LLMs）在链式思维（CoT）提示下取得了显著进展，但仍然受到\emph{盲目自我思考}范式的根本限制：即使在关键信息缺失或模糊的情况下，仍进行广泛的内部推理。我们提出了主动互动推理（PIR），一种新的推理范式，将LLMs从被动求解者转变为主动询问者，交替进行推理和澄清。与现有的基于搜索或工具的框架主要通过查询外部环境来解决知识不确定性不同，PIR通过与用户的直接互动来解决前提和意图层面的不确定性。PIR通过两个核心组件实现：（1）一种关注不确定性的监督微调程序，使模型具备互动推理能力；（2）一种基于用户模拟器的策略优化框架，通过复合奖励驱动模型行为与用户意图对齐。在数学推理、代码生成和文档编辑的广泛实验中，PIR始终优于强基线，准确率提高了高达32.70\%，通过率提高了22.90\%，BLEU分数提高了41.36，同时减少了近一半的推理计算和不必要的互动轮次。对事实知识、问答和缺失前提场景的进一步可靠性评估确认了PIR的强泛化能力和鲁棒性。模型和代码可在以下网址公开获取：\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing reasoning-oriented Large Language Models (LLMs), which often engage in extensive internal reasoning despite lacking critical information. To address this issue, the authors propose a new paradigm called Proactive Interactive Reasoning (PIR), which transforms LLMs into proactive inquirers that seek clarification while reasoning. The method involves an uncertainty-aware supervised fine-tuning procedure and a user-simulator-based policy optimization framework. Experimental results show that PIR significantly outperforms strong baselines, achieving up to 32.70% higher accuracy and 22.90% higher pass rates, while also reducing reasoning computation and unnecessary interactions, demonstrating its robustness across various reasoning tasks.</div>
<div class="mono" style="margin-top:8px">该研究解决了依赖于广泛内部推理的推理导向大型语言模型（LLMs）在缺乏关键信息时的局限性。作者提出了一种新的范式，称为主动互动推理（PIR），通过将用户互动融入推理过程，将LLMs从被动求解者转变为主动询问者。实验结果表明，PIR在数学推理和代码生成等任务中显著优于现有方法，准确率提高了多达32.70%，通过率提高了22.90%，同时减少了推理计算和不必要的互动。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Not in My Backyard&quot;: LLMs Uncover Online and Offline Social Biases Against Homelessness</div>
<div class="meta-line">Authors: Jonathan A. Karr, Benjamin F. Herbst, Matthew L. Sisk, Xueyun Li, Ting Hua, Matthew Hauenstein, Georgina Curto, Nitesh V. Chawla</div>
<div class="meta-line">First: 2025-08-14T17:58:34+00:00 · Latest: 2026-01-29T18:55:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13187v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13187v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1&#x27;s 41.57. This demonstrates that \textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with &quot;not in my backyard&quot; narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;不在我家后院&quot;: 大型语言模型揭示对无家可归者的在线和离线社会偏见</div>
<div class="mono" style="margin-top:8px">无家可归是一个持续的社会挑战，影响着全球数百万人的生活。2025年，美国有超过876,000人经历了无家可归。社会偏见是缓解这一问题的重要障碍，塑造公众认知并影响政策制定。鉴于在线文本媒体和离线市议会讨论反映并影响部分公众舆论，识别和追踪对无家可归者的社会偏见提供了宝贵的见解。我们展示了一个新的手动标注的多领域数据集，来自Reddit、X（前身为Twitter）、新闻文章和十个美国城市的市议会会议记录。我们的16类多标签分类法创建了一个具有挑战性的长尾分类问题：一些类别在样本中出现的比例低于1%，而其他类别则超过70%。我们发现，小型人工标注数据集（1,702个样本）不足以训练有效的分类器，无论是用于微调编码器模型还是作为大型语言模型的少量示例。为了解决这个问题，我们使用GPT-4.1在更大的未标注语料库上生成伪标签。在这个扩展数据集上训练，即使是小型编码器模型（ModernBERT，150M参数）也能达到35.23的宏F1分数，接近GPT-4.1的41.57。这表明\textbf{数据量比模型大小更重要}，使得低成本、保护隐私的部署成为可能，而无需依赖商业API。我们的结果揭示了对无家可归者的负面偏见在离线和在线（尤其是在Reddit上）普遍存在，&quot;不在我家后院&quot;的叙述显示出最高的参与度。这些发现揭示了一种直接影响减贫政策制定的排斥类型，并为解决无家可归问题的从业者提供了可行的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the persistent social challenge of homelessness and the significant social biases that hinder alleviation efforts. The authors compiled a manually-annotated multi-domain dataset from various sources, including Reddit and city council meeting minutes, to analyze these biases. They found that small human-annotated datasets were inadequate for training effective classifiers, leading them to use GPT-4.1 for generating pseudo-labels on a larger corpus. This approach allowed even smaller models to achieve competitive performance, revealing that negative biases against the homeless are prevalent in both online and offline contexts, particularly in Reddit discussions, with &#x27;not in my backyard&#x27; narratives receiving the most engagement, thereby impacting poverty-reduction policymaking.</div>
<div class="mono" style="margin-top:8px">本研究关注无家可归问题及其对缓解措施的影响，2025年美国有超过876,000人经历无家可归。作者从多个来源（包括Reddit和市议会会议记录）编制了一个手动标注的多领域数据集，以分析对无家可归者（PEH）的社会偏见。他们发现小规模的人类标注数据集不足以训练有效的分类器，因此采用GPT-4.1生成更大语料库的伪标签。这一方法使得即使是较小的模型也能取得竞争性表现，揭示了对PEH的负面偏见在在线平台（如Reddit）上普遍存在，这对减贫政策制定具有重要影响。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</div>
<div class="meta-line">Authors: Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22137v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：无分布自适应矩阵函数计算以加速神经网络训练</div>
<div class="mono" style="margin-top:8px">矩阵函数如平方根、逆根和正交化在神经网络训练的预条件梯度方法中起着核心作用。这促使了避免显式特征分解并主要依赖矩阵乘法的迭代算法的发展，使其非常适合现代GPU加速器。我们提出了PRISM（多项式拟合和随机迭代草图用于矩阵函数计算），这是一个加速计算矩阵函数的迭代算法的通用框架。PRISM结合了自适应多项式近似和随机草图：在每次迭代中，它通过草图最小二乘问题拟合当前谱的多项式代理，以最小的开销适应当前实例。我们将PRISM应用于加速类似牛顿-舒尔茨的矩阵平方根和正交化迭代，这些是机器学习中的核心原语。与之前的方法不同，PRISM不需要显式的谱界限或奇异值估计；并且它会自动适应不断变化的谱。实证表明，当PRISM集成到Shampoo和Muon优化器中时，可以加速训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of neural network training by improving the computation of matrix functions, which are crucial in preconditioned gradient methods. The authors introduce PRISM, a framework that utilizes adaptive polynomial approximation and randomized sketching to accelerate iterative algorithms for computing matrix functions without requiring explicit eigendecompositions. Experimental results demonstrate that PRISM significantly speeds up training when integrated into existing optimizers like Shampoo and Muon, while also eliminating the need for spectral bounds or singular value estimates, adapting effectively to the spectrum changes during training.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于矩阵函数在神经网络训练中的预条件梯度方法中的重要性，这些方法通常依赖于显式特征分解。作者提出了PRISM，一个利用自适应多项式逼近和随机草图来更有效地计算矩阵函数的框架，无需光谱边界或奇异值估计。实验结果表明，将PRISM应用于Shampoo和Muon优化器时显著加速了训练，展示了其在机器学习应用中增强矩阵平方根和正交化迭代算法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时而非是否干预流氓代理</div>
<div class="mono" style="margin-top:8px">现有的代理安全基准报告二元准确性，将早期干预与事后分析混为一谈。在第8步标记违规的检测器可以实现干预；而在第48步报告的检测器仅提供法医价值。这一区别至关重要，但当前基准无法衡量。我们推出了StepShield，这是第一个评估违规何时被检测的基准，而不仅仅是是否被检测。StepShield包含9,213个代码代理轨迹，包括1,278个精心注释的训练对和一个具有现实8.1%流氓率的7,935轨迹测试集。流氓行为基于六类真实世界安全事件。我们提出了三种新颖的时间度量：早期干预率（EIR）、干预间隔和节省的令牌。令人惊讶的是，我们的评估显示，基于LLM的评判者实现了59%的EIR，而静态分析器仅实现了26%，这2.3倍的性能差距在标准准确性度量中完全不可见。我们进一步表明，早期检测具有直接的经济效益：我们的级联HybridGuard检测器将监控成本降低了75%，预计在企业规模下五年内累计节省1.08亿美元。通过将评估的重点从是否转向何时，StepShield为构建更安全和更具经济可行性的AI代理提供了新的基础。代码和数据在Apache 2.0许可证下发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of agent safety by distinguishing between the timing of intervention and mere detection of violations. The authors introduce StepShield, a benchmark that assesses when violations occur in agent trajectories, utilizing a dataset of 9,213 code agent trajectories with a rogue rate of 8.1%. Key findings reveal that an LLM-based judge achieves a 59% Early Intervention Rate (EIR), significantly outperforming a static analyzer&#x27;s 26% EIR, highlighting a 2.3x performance gap not captured by traditional accuracy metrics. Additionally, the study demonstrates that early detection can lead to substantial economic benefits, with the HybridGuard detector reducing monitoring costs by 75% and projecting $108 million in savings over five years at an enterprise level.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过区分干预的时机与仅仅检测违规行为，来改善代理安全的评估。作者提出了StepShield，一个评估代理轨迹中违规发生时间的基准，利用了9213个代码代理轨迹的数据集，其中违规行为的发生率为8.1%。关键发现表明，基于LLM的评判者实现了59%的早期干预率，显著优于静态分析器的26%，显示出标准准确性指标无法捕捉的2.3倍性能差距。此外，级联的HybridGuard检测器的实施显示出监控成本降低了75%，预计在五年内为企业节省1.08亿美元，从而强调了在代理安全中早期干预的经济优势。</div>
</details>
</div>
<div class="card">
<div class="title">PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</div>
<div class="meta-line">Authors: Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T18:55:36+00:00 · Latest: 2026-01-29T18:55:36+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22135v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22135v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PI-Light：物理启发的全图重光照扩散</div>
<div class="mono" style="margin-top:8px">全图重光照仍然是一个具有挑战性的问题，原因在于收集大规模结构化配对数据的困难、维持物理合理性的难度以及数据驱动先验带来的有限泛化性。现有的尝试在全场景重光照中弥合合成与真实之间的差距仍然不够理想。为了解决这些挑战，我们提出了物理启发的全图重光照扩散（$π$-Light或PI-Light），这是一个利用物理启发扩散模型的两阶段框架。我们的设计包括（i）批量感知注意力，改善一组图像内在预测的一致性，（ii）一个物理引导的神经渲染模块，强制执行物理合理的光传输，（iii）物理启发的损失，规范训练动态朝向物理有意义的景观，从而增强对真实世界图像编辑的泛化能力，以及（iv）一个经过精心策划的多样化对象和场景的数据集，在受控光照条件下捕获。所有这些组件共同实现了对预训练扩散模型的高效微调，同时为下游评估提供了坚实的基准。实验表明，$π$-Light能够合成各种材料的镜面高光和漫反射，与之前的方法相比，在真实场景中的泛化能力更强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of full-image relighting, particularly the difficulties in collecting large-scale structured paired data and maintaining physical plausibility. The authors propose a two-stage framework called PI-Light, which utilizes physics-inspired diffusion models and incorporates batch-aware attention, a physics-guided neural rendering module, and physics-inspired losses to enhance training dynamics. Experimental results show that PI-Light effectively synthesizes specular highlights and diffuse reflections across various materials, demonstrating superior generalization to real-world scenes compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决全图重光照的挑战，特别是在收集大规模结构化配对数据和确保物理合理性方面的困难。作者提出了一种名为物理启发的全图重光照扩散框架（PI-Light），该框架结合了批量感知注意力、物理引导的神经渲染模块和物理启发的损失，以增强其通用性。实验结果表明，PI-Light能够有效合成各种材料的高光和漫反射，与现有方法相比，在真实场景中的通用性更强。</div>
</details>
</div>
<div class="card">
<div class="title">Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography</div>
<div class="meta-line">Authors: Wenxuan Li, Pedro R. A. S. Bassi, Lizhou Wu, Xinze Zhou, Yuxuan Zhao, Qi Chen, Szymon Plotka, Tianyu Lin, Zheren Zhu, Marisa Martin, Justin Caskey, Shanshan Jiang, Xiaoxi Chen, Jaroslaw B. Ćwikla, Artur Sankowski, Yaping Wu, Sergio Decherchi, Andrea Cavalli, Chandana Lall, Cristian Tomasetti, Yaxing Guo, Xuan Yu, Yuqing Cai, Hualin Qiao, Jie Bao, Chenhan Hu, Ximing Wang, Arkadiusz Sitek, Kai Ding, Heng Li, Meiyun Wang, Dexin Yu, Guang Zhang, Yang Yang, Kang Wang, Alan L. Yuille, Zongwei Zhou</div>
<div class="meta-line">First: 2026-01-29T18:55:23+00:00 · Latest: 2026-01-29T18:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22134v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P &lt; 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>计算机断层扫描早期和预诊断胰腺癌检测</div>
<div class="mono" style="margin-top:8px">胰腺导管腺癌（PDAC）是最致命的实体恶性肿瘤之一，通常在晚期和无法手术的阶段被发现。由了解患者后来发展为PDAC的专家放射科医师进行的预诊断CT扫描的回顾性分析，常常揭示出之前被忽视的病变。为了帮助更早地检测这些病变，我们开发了一种名为ePAI（基于人工智能的早期胰腺癌检测）的自动化系统。该系统基于来自单一医疗中心的1,598名患者的数据进行训练。在涉及1,009名患者的内部测试中，ePAI在检测直径小于2厘米的小型PDAC时，接收者操作特征曲线下面积（AUC）达到了0.939-0.999，灵敏度为95.3%，特异性为98.7%，能够精确定位小至2毫米的PDAC。在涉及6个中心的7,158名患者的外部测试中，ePAI的AUC为0.918-0.945，灵敏度为91.5%，特异性为88.0%，能够精确定位小至5毫米的PDAC。重要的是，ePAI在临床诊断前3到36个月获得的预诊断CT扫描中检测到了原本被放射科医师忽视的PDAC。它成功检测并定位了159名患者中的75名PDAC，临床诊断前的中位提前时间为347天。我们的多读者研究表明，ePAI在灵敏度上比30名获得认证的放射科医师高出50.3%（P &lt; 0.05），同时在早期和预诊断检测PDAC时保持了95.4%的可比特异性。这些发现表明ePAI作为辅助工具在改善胰腺癌早期检测方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the late detection of pancreatic ductal adenocarcinoma (PDAC), which often occurs at an inoperable stage. To address this issue, the authors developed an automated detection system called ePAI, trained on data from 1,598 patients. In internal testing with 1,009 patients, ePAI demonstrated high performance with an AUC of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC lesions. In external testing with 7,158 patients, it maintained an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%. Notably, ePAI identified PDACs in prediagnostic scans taken 3 to 36 months prior to clinical diagnosis, detecting lesions in 75 of 159 patients with a median lead time of 347 days, and significantly outperformed 30 board-certified radiologists in sensitivity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于胰腺导管腺癌（PDAC）通常在无法手术的晚期被发现。为了解决这个问题，作者开发了一种名为ePAI的自动检测系统，该系统基于来自单一医疗中心的1,598名患者的数据进行训练。在对1,009名患者的内部测试中，ePAI显示出高达0.939-0.999的受试者工作特征曲线下面积（AUC），敏感性为95.3%，特异性为98.7%。在对7,158名患者的外部测试中，ePAI的AUC为0.918-0.945，敏感性为91.5%，特异性为88.0%。值得注意的是，ePAI在临床诊断前3至36个月的预诊CT扫描中识别出了被忽视的PDAC病例，成功检测到159例中的75例，平均提前347天发现，并在敏感性方面显著优于30名持证放射科医师。</div>
</details>
</div>
<div class="card">
<div class="title">Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</div>
<div class="meta-line">Authors: Ziming Dong, Hardik Sharma, Evan O&#x27;Toole, Jaya Prakash Champati, Kui Wu</div>
<div class="meta-line">First: 2026-01-29T18:52:54+00:00 · Latest: 2026-01-29T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22132v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>支付提示而非答案：成本高效推理的LLM引导</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）在复杂推理任务中表现出色，但其推理成本限制了大规模部署。小型语言模型（SLM）提供显著的成本节省，但在准确性上大幅落后。现有方法——路由和级联——将LLM视为全有或全无的资源：要么查询完全绕过LLM，要么LLM以全成本生成完整响应。我们引入了LLM引导，这是一种仅请求LLM的短前缀（提示）并将其提供给SLM的框架。这一简单机制在数学和编码任务中出奇有效：即使是占LLM完整响应10-30%的提示也能显著提高SLM的准确性。引导概括了路由和级联，并在oracle决策下实现了更低的成本。我们开发了一个两阶段预测器，联合确定是否需要提示以及请求多少个token。在广泛使用的数学推理（GSM8K，CNK12）和代码生成（HumanEval，MBPP）基准上，引导相较于仅使用LLM的推理成本降低了42-94%。与最先进的路由和级联基线相比，引导在匹配准确性的同时实现了高达2.8倍的成本降低。据我们所知，这是首个利用token级预算控制进行SLM-LLM协作的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high inference costs associated with deploying Large Language Models (LLMs) for complex reasoning tasks, while also acknowledging the limitations of Small Language Models (SLMs) in terms of accuracy. The authors propose a novel framework called LLM Shepherding, which allows for the retrieval of only a short prefix or hint from the LLM to enhance the performance of SLMs. Experimental results demonstrate that using hints that constitute 10-30% of the full LLM response significantly improves SLM accuracy, achieving cost reductions of 42-94% on benchmarks for mathematical reasoning and code generation, while also outperforming existing routing and cascading methods in terms of cost efficiency and accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLM）高推理成本的问题，这限制了其可扩展性，而小型语言模型（SLM）虽然提供了成本节约，但缺乏准确性。作者提出了一种新颖的框架，称为LLM Shepherding，该框架仅请求LLM的短前缀或提示，以提高SLM的性能。实验结果表明，使用占LLM完整响应10-30%的提示显著提高了SLM在数学推理和代码生成任务上的准确性，相较于仅使用LLM的推理，成本降低了42-94%，并且与现有的路由和级联方法相比，成本降低了多达2.8倍，同时保持了准确性。</div>
</details>
</div>
<div class="card">
<div class="title">SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Leonard Papenmeier, Petru Tighineanu</div>
<div class="meta-line">First: 2026-01-29T18:51:58+00:00 · Latest: 2026-01-29T18:51:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMOG：用于多目标贝叶斯优化的可扩展元学习</div>
<div class="mono" style="margin-top:8px">多目标优化旨在解决具有竞争目标的问题，通常仅能以黑箱方式访问问题，并且测量预算有限。在许多应用中，相关优化任务的历史数据是可用的，这为元学习加速优化创造了机会。贝叶斯优化作为一种有前景的黑箱优化技术，已独立扩展到元学习和多目标优化，但同时解决这两种设置的方法——用于多目标贝叶斯优化的元学习先验——仍然在很大程度上未被探索。我们提出了SMOG，一种基于多输出高斯过程的可扩展和模块化元学习模型，明确学习目标之间的相关性。SMOG在元任务和目标任务之间构建了一个结构化的联合高斯过程先验，并在条件化元数据后，产生一个通过灵活的残差多输出核增强的闭式目标任务先验。这种构造以原则性的方式将元数据的不确定性传播到目标代理中。SMOG支持分层并行训练：元任务高斯过程被拟合一次后缓存，实现了与元任务数量线性扩展。生成的代理与标准多目标贝叶斯优化获取函数无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multi-objective optimization, which often deals with competing objectives and limited measurement budgets, by leveraging historical data from related tasks through meta-learning. The authors propose SMOG, a scalable meta-learning model that utilizes a multi-output Gaussian process to learn correlations between objectives and constructs a structured joint Gaussian process prior for both meta- and target tasks. Experimental results demonstrate that SMOG effectively integrates with standard multi-objective Bayesian optimization acquisition functions, allowing for hierarchical and parallel training while achieving linear scaling with the number of meta-tasks.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于提升多目标优化的效率，特别是在可利用相关任务的历史数据时，这可以通过元学习来实现。作者提出了SMOG，这是一种可扩展的元学习模型，利用多输出高斯过程学习竞争目标之间的相关性，从而填补了现有方法未同时考虑元学习和多目标贝叶斯优化的空白。实验结果表明，SMOG有效地将元数据的不确定性整合到目标任务先验中，并在元任务数量上实现线性扩展，从而在复杂场景中促进高效优化。</div>
</details>
</div>
<div class="card">
<div class="title">World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</div>
<div class="meta-line">Authors: Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak</div>
<div class="meta-line">First: 2026-01-29T18:51:54+00:00 · Latest: 2026-01-29T18:51:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22130v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工作流世界：将世界模型引入企业系统的基准</div>
<div class="mono" style="margin-top:8px">前沿大型语言模型（LLMs）在许多领域表现出色，作为自主代理，但在复杂的企业系统中仍未经过测试，这些系统中的隐藏工作流在互联数据库之间产生级联效应。现有的企业基准评估类似于一般消费者基准的表面代理任务完成，忽视了企业中的真实挑战，如有限的可观察性、大型数据库状态和具有级联副作用的隐藏工作流。我们引入了工作流世界（WoW），这是一个基于ServiceNow的现实环境，包含4000多个业务规则和55个嵌入系统的活跃工作流，以及WoW-bench，一个评估受限代理任务完成和企业动态建模能力的234个任务的基准。我们揭示了两个主要结论：（1）前沿LLMs遭受动态盲目症，始终无法预测其行为的不可见级联副作用，导致无声的约束违反；（2）在不透明系统中的可靠性需要扎根的世界建模，代理必须在高保真反馈不可用时，心理模拟隐藏状态转变以弥补可观察性差距。为了实现可靠和有用的企业代理，WoW激励了一种新范式，以明确学习系统动态。我们发布了我们的GitHub以设置和评估WoW。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate large language models (LLMs) in complex enterprise systems, where hidden workflows and cascading effects present unique challenges not addressed by existing benchmarks. The authors introduce World of Workflows (WoW), a ServiceNow-based environment featuring over 4,000 business rules and 55 active workflows, along with WoW-bench, which consists of 234 tasks designed to assess constrained task completion and modeling of enterprise dynamics. The key findings indicate that frontier LLMs exhibit dynamics blindness, failing to anticipate the cascading side effects of their actions, and highlight the necessity for grounded world modeling to enable agents to simulate hidden state transitions effectively in opaque systems for improved reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估大型语言模型（LLMs）在复杂企业系统中的表现，这些系统的特点是隐藏的工作流程和级联效应，而现有基准未能解决这些问题。作者提出了工作流程世界（WoW），这是一个基于ServiceNow的环境，包含4000多个业务规则和55个活跃工作流程，以及WoW-bench，这是一个由234个任务组成的基准，旨在评估代理任务完成和企业动态建模能力。主要发现表明，前沿LLMs表现出动态盲目性，未能预测其行为的级联副作用，导致约束违规，而在不透明系统中有效操作需要基于现实的世界建模，以在反馈有限时模拟隐藏状态转变。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</div>
<div class="meta-line">Authors: Yifeng Ding, Lingming Zhang</div>
<div class="meta-line">First: 2026-01-29T18:50:29+00:00 · Latest: 2026-01-29T18:50:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22129v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Replay：软件工程代理的高效测试时间扩展</div>
<div class="mono" style="margin-top:8px">测试时间扩展已被广泛采用，以增强大型语言模型（LLM）代理在软件工程（SWE）任务中的能力。然而，标准方法是从头重复采样轨迹，这在计算上是昂贵的。尽管最近的方法试图通过专门的价值代理来降低成本，但它们可能会遭遇模型失调，并且无法推广到合成自定义bash脚本作为工具的现代代理。在本文中，我们介绍了SWE-Replay，这是首个高效且可推广的现代代理测试时间扩展技术，无需依赖可能噪声的价值估计。SWE-Replay通过回收先前试验的轨迹来优化扩展过程，动态选择从头探索或通过在关键中间步骤分支利用归档经验。中间步骤的选择是基于存储库探索的潜力和推理重要性，而不是外部基于LLM的质量估计。我们的评估表明，在SWE-Bench Verified上，SWE-Replay始终优于简单扩展，成本降低高达17.4%，同时保持或甚至提高性能高达3.8%。在SWE-Bench Pro和多语言的进一步评估验证了SWE-Replay的可推广性，确立了其作为软件工程代理高效测试时间扩展的坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of test-time scaling for Large Language Model (LLM) agents in software engineering tasks, as traditional methods are computationally expensive and often fail to generalize. The authors propose SWE-Replay, a novel technique that optimizes the scaling process by reusing trajectories from previous trials and selectively exploring or exploiting archived experiences based on the significance of intermediate steps. Experimental results demonstrate that SWE-Replay reduces costs by up to 17.4% while maintaining or enhancing performance by up to 3.8% on SWE-Bench Verified, with further validation on SWE-Bench Pro and Multilingual confirming its generalizability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型语言模型代理在软件工程任务中的测试时间扩展效率，因为传统方法计算成本高且通常无法推广。作者提出了SWE-Replay，这是一种新颖的技术，通过重用先前试验的轨迹来优化扩展过程，并根据存储库探索的重要性动态决定是探索新路径还是利用现有经验。实验结果表明，SWE-Replay在SWE-Bench Verified数据集上将成本降低了多达17.4%，同时保持或提高了性能，提升幅度可达3.8%，在SWE-Bench Pro和Multilingual上的进一步验证确认了其通用性。</div>
</details>
</div>
<div class="card">
<div class="title">The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR</div>
<div class="meta-line">Authors: Irsyad Adam, Zekai Chen, David Laprade, Shaun Porwal, David Laub, Erik Reinertsen, Arda Pekis, Kevin Brown</div>
<div class="meta-line">First: 2026-01-29T18:49:37+00:00 · Latest: 2026-01-29T18:49:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22128v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22128v1">PDF</a> · <a href="https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient&#x27;s trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>患者不是动态文档：一种用于纵向电子健康记录的世界模型训练范式</div>
<div class="mono" style="margin-top:8px">使用下一个词预测训练的大型语言模型（LLMs）已成功作为临床基础模型。这些语言骨干的表征在生物医学任务中表现出强大的线性探测性能，表明患者语义是通过大规模的下一个标记预测而产生的。然而，这一范式将患者视为待总结的文档，而非需模拟的动态系统；患者的轨迹是其状态在干预和时间下演变的结果，需要模拟动态而非预测标记的模型。为了解决这个问题，我们引入了SMB-Structure，一种用于结构化电子健康记录的世界模型，它将联合嵌入预测架构（JEPA）与下一个标记预测（SFT）结合。SFT将我们的模型固定在标记空间中重建未来患者状态，而JEPA仅从初始患者表征中在潜在空间中预测这些未来，迫使轨迹动态在观察下一个状态之前被编码。我们在两个大规模队列中进行了验证：纪念斯隆凯特琳癌症中心（23,319名肿瘤患者；323,000+患者年）和INSPECT（19,402名肺栓塞患者）。通过在疾病轨迹的多个点上评估的线性探测，我们证明了我们的训练范式学习到的嵌入能够捕捉到自回归基线无法恢复的疾病动态，使SMB-Structure在高患者异质性特征的复杂任务中实现了竞争性能。模型权重可在https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of existing large language models (LLMs) that treat patient data as static documents rather than dynamic systems. To overcome this, the authors propose SMB-Structure, a world model for structured electronic health records (EHR) that combines joint-embedding prediction architecture (JEPA) with next-token prediction (SFT) to simulate patient trajectories over time. Experimental results from two large cohorts, Memorial Sloan Kettering and INSPECT, show that the proposed model captures disease dynamics more effectively than autoregressive baselines, achieving competitive performance on complex tasks with high patient heterogeneity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于更好地模拟电子健康记录（EHR）中的患者轨迹，而不是将其视为静态文档。作者提出了SMB-Structure，这是一种世界模型，结合了联合嵌入预测架构（JEPA）和下一个标记预测（SFT），以重建未来患者状态并编码轨迹动态。来自纪念斯隆凯特琳癌症中心和INSPECT两个大型队列的实验结果表明，该训练范式比传统的自回归模型更有效地捕捉疾病动态，在具有高度患者异质性的复杂任务中实现了竞争性表现。</div>
</details>
</div>
<div class="card">
<div class="title">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</div>
<div class="meta-line">Authors: John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</div>
<div class="meta-line">First: 2026-01-29T18:49:27+00:00 · Latest: 2026-01-29T18:49:27+00:00</div>
<div class="meta-line">Comments: Project page: https://edit-yourself.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit-yourself.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EditYourself：基于扩散变换器的音频驱动的对话头视频生成与编辑</div>
<div class="mono" style="margin-top:8px">当前的生成视频模型在从文本和图像提示中生成新内容方面表现出色，但在编辑现有的预录视频时存在关键缺口，微小的脚本修改需要保持运动、时间一致性、说话者身份和准确的口型同步。我们介绍了EditYourself，这是一个基于DiT的音频驱动视频到视频（V2V）编辑框架，支持基于转录文本的对话头视频修改，包括无缝添加、删除和重新定时视觉内容。EditYourself在通用视频扩散模型的基础上，通过音频条件和区域感知的编辑专注训练扩展增强其V2V能力。这使得通过时空修补实现精确的口型同步和现有表演的时间一致性重构成为可能，包括在新添加的片段中合成逼真的人类运动，同时在长时间内保持视觉保真度和身份一致性。这项工作代表了生成视频模型作为专业视频后期制作实用工具的基础性步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current generative video models in editing existing pre-recorded videos, particularly in maintaining motion, temporal coherence, speaker identity, and lip synchronization when altering spoken scripts. The authors introduce EditYourself, a framework based on diffusion transformers that facilitates audio-driven video-to-video editing, allowing for transcript-based modifications such as the addition, removal, and retiming of spoken content. Key experimental findings demonstrate that EditYourself achieves precise lip synchronization and coherent restructuring of performances, effectively synthesizing realistic human motion while preserving visual fidelity and identity consistency over extended durations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前生成视频模型在编辑现有预录视频时的局限性，特别是在进行小幅脚本修改时保持运动、时间一致性、说话者身份和唇部同步。作者提出了EditYourself，这是一个基于扩散变换器的框架，能够实现音频驱动的视频到视频编辑，允许对谈话头视频进行基于文本的修改。关键实验结果表明，EditYourself通过时空修补实现了精确的唇部同步和表演的连贯重构，能够在保持视觉保真度和身份一致性的同时，实现对口语内容的添加、删除和重新定时。</div>
</details>
</div>
<div class="card">
<div class="title">Creative Image Generation with Diffusion Model</div>
<div class="meta-line">Authors: Kunpeng Song, Ahmed Elgammal</div>
<div class="meta-line">First: 2026-01-29T18:48:48+00:00 · Latest: 2026-01-29T18:48:48+00:00</div>
<div class="meta-line">Comments: Project page: https://creative-t2i.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22125v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22125v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://creative-t2i.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image&#x27;s existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的创意图像生成</div>
<div class="mono" style="margin-top:8px">创意图像生成已成为一个引人注目的研究领域，旨在生成新颖且高质量的图像，扩展想象的边界。在本研究中，我们提出了一种基于扩散模型的创意生成新框架，其中创意与图像在CLIP嵌入空间中存在的逆概率相关。与依赖于手动概念混合或排除子类别的先前方法不同，我们的方法计算生成图像的概率分布，并将其引导至低概率区域，以生成稀有、富有想象力且视觉上引人注目的输出。我们还引入了回拉机制，实现了高创意而不牺牲视觉保真度。在文本到图像的扩散模型上进行的广泛实验展示了我们创意生成框架的有效性和效率，展示了其生成独特、新颖和发人深省图像的能力。这项工作为生成模型中的创意提供了新的视角，提供了一种原则性的方法来促进视觉内容合成中的创新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance creative image generation by producing novel and high-quality images that push the limits of imagination. The authors propose a new framework utilizing diffusion models, where creativity is defined by the inverse probability of an image&#x27;s existence in the CLIP embedding space, and they employ a method that directs the probability distribution of generated images towards low-probability regions to yield rare and visually striking outputs. Experimental results demonstrate that their approach effectively generates unique and thought-provoking images while maintaining high visual fidelity, thus providing a fresh perspective on creativity in generative models.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决生成新颖高质量图像的挑战，动机在于扩展图像合成中的创造性边界。作者提出了一种利用扩散模型的新框架，其中创造性与图像在CLIP嵌入空间中存在的逆概率相关，向低概率区域移动以创造独特的输出。实验结果表明，该方法有效地生成富有想象力和视觉吸引力的图像，同时保持高保真度，从而为生成模型中的创造性提供了新的视角。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</div>
<div class="meta-line">Authors: Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank Noé, Klaus Robert Müller</div>
<div class="meta-line">First: 2026-01-29T18:47:46+00:00 · Latest: 2026-01-29T18:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习哈密顿流映射：大时间步分子动力学的平均流一致性</div>
<div class="mono" style="margin-top:8px">模拟哈密顿系统的长期演化受到稳定数值积分所需的小时间步的限制。为克服这一约束，我们引入了一个框架，通过预测选定时间跨度 $Δt$ 的平均相空间演化来学习哈密顿流映射，从而实现远超经典积分器稳定性极限的稳定大时间步更新。为此，我们对时间平均哈密顿动力学施加了平均流一致性条件。与之前的方法不同，这允许在没有未来状态访问的情况下对独立相空间样本进行训练，避免了昂贵的轨迹生成。我们的算法在多种哈密顿系统中得到了验证，特别是在使用机器学习力场（MLFF）的分子动力学模拟中有所改进。我们的模型保持了可比的训练和推理成本，但支持显著更大的积分时间步，同时直接在广泛可用的无轨迹MLFF数据集上进行训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of simulating the long-time evolution of Hamiltonian systems, which typically require small timesteps for stable numerical integration. The authors propose a framework for learning Hamiltonian Flow Maps that predicts mean phase-space evolution over larger time spans, thus enabling stable updates that exceed the stability limits of traditional integrators. Experimental results demonstrate that their method significantly enhances molecular dynamics simulations using machine-learned force fields, allowing for larger integration timesteps while maintaining comparable training and inference costs, and without the need for expensive trajectory generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决哈密顿系统长时间演化模拟的限制，这通常需要小时间步以实现稳定的数值积分。作者提出了一种学习哈密顿流映射的框架，该框架预测更大时间跨度内的平均相空间演化，从而实现超越传统积分器稳定性限制的稳定更新。实验结果表明，该方法在多个哈密顿系统中经过验证，显著改善了使用机器学习力场的分子动力学模拟，允许更大的积分时间步，同时保持可比的训练和推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">Alpha Discovery via Grammar-Guided Learning and Search</div>
<div class="meta-line">Authors: Han Yang, Dong Hao, Zhuohan Wang, Qi Shi, Xingtong Li</div>
<div class="meta-line">First: 2026-01-29T18:46:15+00:00 · Latest: 2026-01-29T18:46:15+00:00</div>
<div class="meta-line">Comments: 24 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22119v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语法引导学习和搜索发现阿尔法</div>
<div class="mono" style="margin-top:8px">自动发现公式化的阿尔法因子是量化金融中的一个核心问题。现有方法往往忽视语法和语义约束，依赖于对无结构和无界空间的穷举搜索。我们提出了AlphaCFG，一个基于语法的框架，用于定义和发现在语法上有效、财务上可解释且计算上高效的阿尔法因子。AlphaCFG使用面向阿尔法的上下文无关文法来定义树结构、大小受控的搜索空间，并将阿尔法发现公式化为树结构的语言马尔可夫决策过程，然后使用受语法感知的蒙特卡洛树搜索，通过语法敏感的价值和策略网络进行引导。对中国和美国股市数据集的实验表明，AlphaCFG在搜索效率和交易盈利能力上均优于最先进的基线。除了交易策略，AlphaCFG还作为一个通用框架，用于量化金融中符号因子的发现和精炼，包括资产定价和投资组合构建。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of automatically discovering formulaic alpha factors in quantitative finance, where existing methods often overlook important syntactic and semantic constraints. The authors introduce AlphaCFG, a grammar-based framework that defines a structured search space for alpha factors, utilizing a context-free grammar and formulating the discovery process as a linguistic Markov decision process, which is solved using a Monte Carlo Tree Search. Experimental results demonstrate that AlphaCFG significantly outperforms current state-of-the-art methods in terms of both search efficiency and trading profitability on datasets from the Chinese and U.S. stock markets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在量化金融中自动发现公式化阿尔法因子的挑战，因为现有方法往往忽视重要的句法和语义约束。作者提出了AlphaCFG，这是一种基于语法的框架，通过使用面向阿尔法的上下文无关文法定义受控的阿尔法因子搜索空间，并将发现过程公式化为树结构的语言马尔可夫决策过程，通过语法感知的蒙特卡洛树搜索进行求解。实验结果表明，AlphaCFG在中国和美国股市数据集上测试时，在搜索效率和交易盈利能力方面显著优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Defining Operational Conditions for Safety-Critical AI-Based Systems from Data</div>
<div class="meta-line">Authors: Johann Christensen, Elena Hoemann, Frank Köster, Sven Hallerbach</div>
<div class="meta-line">First: 2026-01-29T18:46:02+00:00 · Latest: 2026-01-29T18:46:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从数据定义安全关键的基于人工智能的系统的操作条件</div>
<div class="mono" style="margin-top:8px">人工智能（AI）在许多领域中不断崛起，包括众多安全关键应用。然而，对于现实世界中的复杂系统，或者当数据已经存在时，定义潜在的环境条件极具挑战性。这通常导致对AI系统必须操作的环境描述不完整。然而，这种描述称为操作设计域（ODD），在许多领域中是AI系统认证所必需的。传统上，ODD是在开发过程的早期阶段创建的，依赖于复杂的专家知识和相关标准。本文提出了一种新颖的安全设计方法，通过多维核表示法从先前收集的数据后验定义ODD。该方法通过蒙特卡洛方法和未来安全关键的防碰撞系统的真实航空案例进行了验证。此外，通过定义两个ODD相等的条件，本文表明数据驱动的ODD可以等于数据的原始潜在隐藏ODD。利用这种新颖的安全设计核基础ODD，可以实现未来数据驱动的安全关键AI系统的认证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the challenge of defining the Operational Design Domain (ODD) for AI-based systems in safety-critical applications, particularly when existing data is involved. The authors propose a novel Safety-by-Design method that utilizes a multi-dimensional kernel-based representation to define the ODD a posteriori from previously collected data. Experimental validation through Monte Carlo methods and a real-world aviation case study demonstrates that this data-driven ODD can accurately represent the original hidden ODD, facilitating the certification of safety-critical AI systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于为安全关键应用中的人工智能系统定义操作设计域（ODD）的挑战，特别是在涉及现有数据时。作者提出了一种新颖的安全设计方法，利用多维核基础表示法从先前收集的数据中事后定义ODD。该方法通过蒙特卡洛模拟和一个真实的航空案例研究进行了验证，证明数据驱动的ODD可以与原始隐藏ODD相匹配，从而促进安全关键人工智能系统的认证。</div>
</details>
</div>
<div class="card">
<div class="title">SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</div>
<div class="meta-line">Authors: Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi</div>
<div class="meta-line">First: 2026-01-29T18:41:52+00:00 · Latest: 2026-01-29T18:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22114v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SINA：一种基于人工智能的电路原理图图像到网表生成器</div>
<div class="mono" style="margin-top:8px">当前将电路原理图图像转换为机器可读网表的方法在组件识别和连通性推断方面存在困难。本文提出了SINA，一个开源的全自动电路原理图图像到网表生成器。SINA集成了深度学习以实现准确的组件检测，使用连通组件标记（CCL）进行精确的连通性提取，并采用光学字符识别（OCR）来检索组件参考标识符，同时使用视觉语言模型（VLM）进行可靠的参考标识符分配。在我们的实验中，SINA实现了96.47%的整体网表生成准确率，比最先进的方法高出2.72倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing methods in converting circuit schematic images into machine-readable netlists, particularly in component recognition and connectivity inference. The authors developed SINA, an open-source automated generator that utilizes deep learning for component detection, Connected-Component Labeling for connectivity extraction, Optical Character Recognition for retrieving component reference designators, and a Vision-Language Model for assigning these designators. Experimental results demonstrate that SINA achieves an overall netlist-generation accuracy of 96.47%, which is 2.72 times higher than the best existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善电路原理图图像转换为机器可读的网表，解决组件识别和连通性推断中的挑战。作者开发了SINA，一个开源的自动生成器，利用深度学习进行组件检测，使用连通组件标记法提取连通性，采用光学字符识别技术检索参考设计ators，并使用视觉-语言模型分配这些设计ators。实验结果表明，SINA的整体网表生成准确率达到96.47%，是现有最先进方法的2.72倍。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
