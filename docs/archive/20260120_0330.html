<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-20 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260120_0330</div>
    <div class="row"><div class="card">
<div class="title">CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation</div>
<div class="meta-line">Authors: Vanshali Sharma, Andrea Mia Bejar, Gorkem Durak, Ulas Bagci</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-16T18:09:19+00:00 · Latest: 2026-01-16T18:09:19+00:00</div>
<div class="meta-line">Comments: Accepted at ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11488v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11488v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 &quot;disagreement&quot; cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CTest-Metric：评估CT报告生成指标临床有效性的统一框架</div>
<div class="mono" style="margin-top:8px">在生成性人工智能时代，尽管关键医疗任务越来越自动化，放射学报告生成（RRG）仍然依赖于次优指标进行质量评估。因此，开发特定领域的指标一直是一个活跃的研究领域，但由于缺乏统一、明确定义的框架来评估其在临床环境中的稳健性和适用性，这一过程仍然具有挑战性。为此，我们提出了CTest-Metric，这是第一个统一的指标评估框架，包含三个模块来确定CT RRG指标的临床可行性。这些模块测试：（i）通过基于LLM的改写评估写作风格的可推广性（WSG）；（ii）在不同严重程度下进行合成错误注入（SEI）；以及（iii）使用临床医生对175个“分歧”案例的评分进行指标与专家相关性（MvE）分析。我们研究了八个广泛使用的指标（BLEU、ROUGE、METEOR、BERTScore-F1、F1-RadGraph、RaTEScore、GREEN Score、CRG），并在基于CT-CLIP编码器构建的七个LLM上进行了测试。使用我们的新框架，我们发现词汇NLG指标对风格变化高度敏感；GREEN Score与专家判断最为一致（Spearman~0.70），而CRG显示负相关；BERTScore-F1对事实错误注入的敏感性最低。我们将发布该框架、代码以及可允许的匿名评估数据部分（改写/错误注入的CT报告），以促进可重复的基准测试和未来指标的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the quality assessment of radiology report generation (RRG) in the context of increasing automation in medical tasks, as existing metrics are suboptimal and lack a unified framework. The authors developed CTest-Metric, a comprehensive framework consisting of three modules that evaluate the clinical feasibility of metrics for CT RRG, including Writing Style Generalizability, Synthetic Error Injection, and Metrics-vs-Expert correlation. Key findings indicate that lexical NLG metrics are sensitive to stylistic variations, the GREEN Score correlates well with expert judgments, while CRG shows a negative correlation, and BERTScore-F1 is the least affected by factual error injection.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善放射学报告生成（RRG）指标的质量评估，这些指标在医疗任务日益自动化的背景下被发现是次优的。作者提出了CTest-Metric，这是一个统一的框架，由三个模块组成，用于评估各种CT报告生成指标的临床可行性。关键发现表明，词汇自然语言生成指标对风格变化敏感，其中GREEN Score与专家判断的对齐度最高，而CRG指标则表现出负相关，BERTScore-F1在测试中对事实错误的敏感性最低。</div>
</details>
</div>
<div class="card">
<div class="title">Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</div>
<div class="meta-line">Authors: Wenhui Tan, Ruihua Song, Jiaze Li, Jianzhong Ju, Zhenbo Luo</div>
<div class="meta-line">First: 2026-01-16T15:14:04+00:00 · Latest: 2026-01-16T15:14:04+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11359v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think-Clip-Sample：视频理解的慢快帧选择</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）最近的进展显著推动了视频理解。然而，它们在长视频上的表现仍受到计算限制和次优帧选择的限制。我们提出了Think-Clip-Sample（TCS），这是一个无训练框架，通过两个关键组件增强长视频理解：（i）多查询推理，生成多个查询以捕捉问题和视频的互补方面；（ii）剪辑级慢快采样，自适应平衡密集局部细节和稀疏全局上下文。在MLVU、LongVideoBench和VideoMME上的大量实验表明，TCS在不同的MLLMs中始终提高性能，准确率提升高达6.9%，并且能够以50%的推理时间成本实现可比的准确率，突显了TCS在长视频理解中的效率和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of multi-modal large language models (MLLMs) in understanding long-form videos due to computational constraints and ineffective frame selection. The authors propose a training-free framework called Think-Clip-Sample (TCS), which incorporates Multi-Query Reasoning to generate multiple queries for capturing diverse aspects of the video and Clip-level Slow-Fast Sampling to balance detailed local information with broader context. Experimental results on datasets such as MLVU, LongVideoBench, and VideoMME show that TCS enhances performance across various MLLMs, achieving up to a 6.9% increase in accuracy while reducing inference time by 50%, demonstrating its efficiency and effectiveness in long video understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大语言模型（MLLMs）在理解长视频时由于计算限制和帧选择不当而导致的性能不足。作者提出了一种名为Think-Clip-Sample（TCS）的无训练框架，该框架结合了多查询推理，以生成多个查询以更好地捕捉视频的上下文，以及剪辑级慢快采样，以平衡局部细节与全局背景。对MLVU、LongVideoBench和VideoMME等数据集的实验结果表明，TCS在不同的MLLMs上提高了最多6.9%的准确率，同时将推理时间减少了50%，展示了其在长视频理解中的效率和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</div>
<div class="meta-line">Authors: Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen</div>
<div class="meta-line">First: 2025-11-14T17:34:20+00:00 · Latest: 2026-01-16T12:49:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11512v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.11512v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉、语言和视觉模态对齐的协作表示学习</div>
<div class="mono" style="margin-top:8px">触觉感知为视觉和语言提供了丰富且互补的信息，使机器人能够感知细致的物体属性。然而，现有的触觉传感器缺乏标准化，导致冗余特征，阻碍了跨传感器的泛化。此外，现有方法未能充分整合触觉、语言和视觉模态之间的中间通信。为此，我们提出了TLV-CoRe，一种基于CLIP的触觉-语言-视觉协作表示学习方法。TLV-CoRe引入了一种传感器感知调制器，以统一不同传感器的触觉特征，并采用与触觉无关的解耦学习来剥离无关的触觉特征。此外，引入了统一桥接适配器，以增强共享表示空间内的三模态交互。为了公平评估触觉模型的有效性，我们进一步提出了RSS评估框架，关注不同方法的鲁棒性、协同性和稳定性。实验结果表明，TLV-CoRe显著提高了传感器无关的表示学习和跨模态对齐，为多模态触觉表示提供了新的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the integration of tactile sensing with vision and language modalities in robotics, as existing tactile sensors are non-standardized and hinder cross-sensor generalization. The authors propose TLV-CoRe, a collaborative representation learning method that utilizes a CLIP-based approach, incorporating a Sensor-Aware Modulator to unify tactile features and a Unified Bridging Adapter to improve tri-modal interaction. Experimental results indicate that TLV-CoRe significantly enhances sensor-agnostic representation learning and cross-modal alignment, thus providing a promising avenue for multimodal tactile representation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强触觉感知与视觉和语言模态的整合，解决非标准化触觉传感器带来的冗余特征和阻碍泛化的问题。作者提出了TLV-CoRe，这是一种基于CLIP的协作表示学习方法，结合了传感器感知调制器来标准化触觉特征，以及统一桥接适配器来改善三模态交互。实验结果表明，TLV-CoRe显著提高了传感器无关的表示学习和跨模态对齐，通过新提出的RSS评估框架聚焦于鲁棒性、协同性和稳定性，展示了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification</div>
<div class="meta-line">Authors: Zhiqi Pang, Lingling Zhao, Yang Liu, Chunyu Wang, Gaurav Sharma</div>
<div class="meta-line">First: 2026-01-16T12:45:01+00:00 · Latest: 2026-01-16T12:45:01+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督多场景人物重识别的图像-文本知识建模</div>
<div class="mono" style="margin-top:8px">我们提出无监督多场景（UMS）人物重识别（ReID）作为一种新任务，在单一一致框架内扩展ReID到多样场景（跨分辨率、换装等）。为了解决UMS-ReID，我们引入图像-文本知识建模（ITKM）——一个三阶段框架，有效利用视觉-语言模型的表征能力。我们从一个预训练的CLIP模型开始，包含图像编码器和文本编码器。在第一阶段，我们在图像编码器中引入场景嵌入，并微调编码器以自适应地利用来自多个场景的知识。在第二阶段，我们优化一组学习到的文本嵌入，以与第一阶段的伪标签关联，并引入多场景分离损失，以增加场景间文本表征的差异性。在第三阶段，我们首先引入集群级和实例级异构匹配模块，以在每个场景内获得可靠的异构正样本对（例如，同一人的可见图像和红外图像）。接下来，我们提出动态文本表征更新策略，以保持文本和图像监督信号之间的一致性。多个场景的实验结果证明了ITKM的优越性和泛化能力；它不仅优于现有的场景特定方法，还通过整合来自多个场景的知识提升了整体性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of unsupervised multi-scenario person re-identification (ReID), which aims to improve identification across varying conditions such as resolution and clothing changes. The authors propose a novel image-text knowledge modeling (ITKM) framework that consists of three stages, utilizing a pre-trained CLIP model to enhance scenario adaptability. Key findings indicate that ITKM significantly outperforms existing methods tailored to specific scenarios, demonstrating improved performance by effectively integrating knowledge from diverse scenarios through innovative matching and representation strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决无监督多场景人物重识别（ReID）中的挑战，该任务涉及在不同条件下（如分辨率变化和服装变化）识别个体。作者提出了一种新的图像-文本知识建模（ITKM）框架，该框架分为三个阶段，利用预训练的CLIP模型增强视觉-语言模型的表征能力。关键实验结果表明，ITKM显著优于现有的特定场景方法，并通过有效整合多场景的知识提高整体性能，展示了其在多样化条件下的鲁棒性和通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Better Language Models Exhibit Higher Visual Alignment</div>
<div class="meta-line">Authors: Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano</div>
<div class="meta-line">First: 2024-10-09T17:59:33+00:00 · Latest: 2026-01-16T10:59:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.07173v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.07173v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How well do text-only large language models (LLMs) align with the visual world? We present a systematic evaluation of this question by incorporating frozen representations of various language models into a discriminative vision-language framework and measuring zero-shot generalization to novel concepts. We find that decoder-based models exhibit stronger visual alignment than encoders, even when controlling for model and dataset size. Moreover, language modeling performance correlates with visual generalization, suggesting that advances in unimodal LLMs can simultaneously improve vision models. Leveraging these insights, we propose ShareLock, a lightweight method for fusing frozen vision and language backbones. ShareLock achieves robust performance across tasks while drastically reducing the need for paired data and compute. With just 563k image-caption pairs and under one GPU-hour of training, it reaches 51% accuracy on ImageNet. In cross-lingual settings, ShareLock dramatically outperforms CLIP, achieving 38.7% top-1 accuracy on Chinese image classification versus CLIP&#x27;s 1.4%. Code is available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更好的语言模型展现更高的视觉对齐</div>
<div class="mono" style="margin-top:8px">文本-only 大型语言模型（LLMs）与视觉世界的对齐程度如何？我们通过将各种语言模型的冻结表示纳入一个判别性视觉-语言框架，并测量对新概念的零-shot 泛化，系统地评估了这个问题。我们发现，解码器模型展现出比编码器更强的视觉对齐，即使在控制模型和数据集大小的情况下。此外，语言建模性能与视觉泛化相关，表明单模态 LLM 的进展可以同时改善视觉模型。利用这些见解，我们提出了 ShareLock，这是一种轻量级的方法，用于融合冻结的视觉和语言骨干。ShareLock 在各项任务中实现了稳健的性能，同时大幅减少了对配对数据和计算的需求。仅使用 563k 图像-标题对和不到一个 GPU 小时的训练，它在 ImageNet 上达到了 51% 的准确率。在跨语言设置中，ShareLock 显著超越 CLIP，在中文图像分类中实现了 38.7% 的 top-1 准确率，而 CLIP 仅为 1.4%。代码可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the alignment of text-only large language models (LLMs) with visual concepts, motivated by the need to understand their performance in vision-language tasks. The authors employ a discriminative vision-language framework that integrates frozen representations from various language models to evaluate zero-shot generalization to new concepts. The findings reveal that decoder-based models demonstrate superior visual alignment compared to encoders, and that better language modeling performance is associated with improved visual generalization. Additionally, the proposed method ShareLock effectively combines frozen vision and language backbones, achieving 51% accuracy on ImageNet with minimal training data and outpacing CLIP in cross-lingual settings with 38.7% top-1 accuracy on Chinese image classification.</div>
<div class="mono" style="margin-top:8px">本研究探讨了文本-only大型语言模型（LLMs）与视觉信息的对齐情况，旨在理解它们在视觉-语言任务中的表现。作者采用一个区分性的视觉-语言框架，整合来自不同语言模型的冻结表示，以评估对新概念的零-shot泛化能力。研究结果表明，解码器模型的视觉对齐能力优于编码器模型，并且更好的语言建模性能与视觉泛化能力相关。此外，提出的方法ShareLock有效结合了视觉和语言骨干网络，在仅使用563k图像-标题对和不到一个GPU小时的训练下，在ImageNet上达到了51%的准确率，并在跨语言图像分类任务中显著超越CLIP，中文图像分类的top-1准确率达到38.7%。</div>
</details>
</div>
<div class="card">
<div class="title">MERGETUNE: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-16T04:31:59+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v2">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalisation without adding parameters. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at https://github.com/Surrey-UP-Lab/MERGETUNE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MERGETUNE：视觉-语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉-语言模型（VLMs），如CLIP，往往会导致预训练知识的灾难性遗忘。之前的工作主要旨在减轻适应过程中的遗忘；然而，在此过程中，遗忘往往是不可避免的。我们引入了一种新范式，持续微调（CFT），旨在在零-shot模型已经适应后恢复预训练知识。我们提出了一种简单的、与模型无关的CFT策略（称为MERGETUNE），该策略由线性模式连接（LMC）指导，可以在现有微调模型上事后应用，而无需架构更改。给定一个微调模型，我们继续微调其可训练参数（例如，软提示或线性头），以寻找一个具有两个低损失路径的持续模型，分别通向零-shot（例如，CLIP）和微调（例如，CoOp）解决方案。通过利用损失景观的几何特性，持续模型隐式地合并了这两种解决方案，恢复了在微调对应模型中丢失的预训练知识。一个挑战是，普通的LMC约束需要来自预训练任务的数据重放。我们通过二阶代理来近似这一约束，消除了对大规模数据重放的需求。实验表明，MERGETUNE在不增加参数的情况下，将CoOp的调和平均提高了5.6%。在稳健微调评估中，来自MERGETUNE的LMC合并模型超越了具有较低推理成本的集成基线，与零-shot模型集成时实现了进一步的增益和最先进的结果。我们的代码可在https://github.com/Surrey-UP-Lab/MERGETUNE获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of catastrophic forgetting in vision-language models (VLMs) like CLIP during fine-tuning, which often leads to the loss of pretrained knowledge. The authors propose a novel method called continued fine-tuning (CFT) through a strategy named MERGETUNE, which utilizes linear mode connectivity (LMC) to recover lost knowledge without requiring changes to the model architecture. Experimental results demonstrate that MERGETUNE enhances the harmonic mean of CoOp by 5.6% on base-novel generalization, and the LMC-merged model outperforms ensemble baselines with lower inference costs, achieving state-of-the-art results when combined with the zero-shot model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在微调视觉语言模型（VLMs）如CLIP时出现的灾难性遗忘问题，这通常导致预训练知识的丧失。作者提出了一种名为持续微调（CFT）的新方法，通过一种名为MERGETUNE的模型无关策略，利用线性模式连通性（LMC）在适应后恢复丢失的知识。实验结果表明，MERGETUNE在基础-新颖泛化上提高了CoOp的调和平均值5.6%，且没有增加参数，LMC合并模型在较低推理成本下超越了集成基线，与零-shot模型结合时实现了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses</div>
<div class="meta-line">Authors: Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-25T02:05:41+00:00 · Latest: 2026-01-15T17:33:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures, accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17772v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.17772v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model&#x27;s utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进的有界域和平滑损失下差分隐私SGD的隐私和效用分析</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DPSGD）广泛用于在机器学习模型训练过程中保护敏感数据，但其隐私保证往往以模型性能的大幅下降为代价，因为缺乏量化隐私损失的紧密理论界限。尽管最近的努力实现了更准确的隐私保证，但仍然施加了一些禁止实际应用的假设，如凸性和复杂的参数要求，并且很少深入研究隐私机制对模型效用的影响。本文为具有一般L-平滑和非凸损失函数的DPSGD提供了严格的隐私特征，揭示了在有界域情况下迭代的隐私损失收敛性。具体而言，我们跟踪多个迭代中的隐私损失，利用噪声平滑减少特性，并进一步在不同场景中建立全面的收敛分析。特别地，我们展示了对于具有有界域的DPSGD，(i) 隐私损失在没有凸性假设的情况下仍然可以收敛，(ii) 在某些条件下，较小的有界直径可以同时改善隐私和效用，以及 (iii) 对于具有梯度裁剪的DPSGD（DPSGD-GC）和具有有界域的DPSGD-GC（DPSGD-DC）及μ-强凸人口风险函数，隐私效用权衡的可达大O阶。通过在实际环境中进行的成员推断攻击（MIA）实验验证了从理论结果中获得的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Differentially Private Stochastic Gradient Descent (DPSGD) while maintaining privacy guarantees, as existing methods often compromise model utility due to restrictive assumptions. The authors develop a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, analyzing privacy loss over multiple iterations in bounded-domain scenarios. Key findings indicate that privacy loss can converge without convexity assumptions, that a smaller bounded diameter can simultaneously improve privacy and utility under certain conditions, and that specific big-O orders of the privacy-utility trade-off can be attained for different DPSGD configurations, with experimental validation through membership inference attacks confirming the theoretical insights.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强差分隐私随机梯度下降（DPSGD）的隐私保障，同时尽量减少对模型性能的负面影响，因为现有方法通常依赖于不切实际的假设。作者采用严格的隐私表征方法，分析具有一般L-平滑和非凸损失函数的DPSGD，在有界域场景中研究多个迭代中的隐私损失。主要发现表明，隐私损失在没有凸性假设的情况下仍然可以收敛，较小的有界直径在某些条件下可以同时改善隐私和效用，并且研究建立了不同DPSGD变体的隐私-效用权衡的big-O阶，实验通过成员推断攻击验证了理论见解。</div>
</details>
</div>
<div class="card">
<div class="title">Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2026-01-15T15:16:15+00:00 · Latest: 2026-01-15T15:16:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>投影微批量累积为强化学习提供无参考的近端策略更新</div>
<div class="mono" style="margin-top:8px">本文介绍了投影微批量累积（PROMA），一种用于大型语言模型微调的近端策略更新方法。PROMA通过在微批量聚合之前投影序列级梯度分量，累积微批量的策略梯度。该投影在反向传播过程中逐层应用，实现了高效的实现，无需额外的前向或反向传播。实证结果表明，PROMA对局部KL散度的控制比GRPO更严格，从而实现了更稳定的策略学习。与PPO和GRPO不同，PROMA在不引起熵崩溃的情况下实现近端更新，并且不依赖于参考策略或似然比裁剪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for stable policy learning in reinforcement learning, particularly in the context of fine-tuning large language models. The authors introduce Projected Microbatch Accumulation (PROMA), a method that accumulates policy gradients across microbatches by projecting out sequence-wise gradient components during the backward pass, allowing for efficient implementation without extra forward or backward passes. Experimental results demonstrate that PROMA achieves tighter control of local KL divergence compared to existing methods like GRPO, leading to more stable policy updates without the issues of entropy collapse or reliance on a reference policy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在强化学习中实现更稳定的策略更新，特别是在大语言模型微调方面。作者提出了投影微批量累积（PROMA）方法，该方法通过在反向传播过程中投影序列级梯度分量来累积微批量的策略梯度，从而实现高效实施，而无需额外的前向或反向传播。实验结果表明，PROMA在局部KL散度的控制上比现有方法如GRPO更为严格，从而实现了更稳定的策略学习，且没有熵崩溃或依赖参考策略的问题。</div>
</details>
</div>
<div class="card">
<div class="title">DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset</div>
<div class="meta-line">Authors: Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang</div>
<div class="meta-line">First: 2026-01-15T11:28:58+00:00 · Latest: 2026-01-15T11:28:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 11 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10305v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DanQing：最新的大规模中文视觉-语言预训练数据集</div>
<div class="mono" style="margin-top:8px">视觉-语言预训练（VLP）模型通过对大规模图像-文本对进行对比预训练，在各种下游任务中表现出强大的性能。大量英语图像-文本数据集（如COYO-700M和LAION-400M）的发布，使得CLIP和SigLIP等模型在跨模态检索和图像描述等任务中得到了广泛应用。然而，由于高质量中文图像-文本数据的稀缺，中文视觉-语言预训练的进展显著滞后。为了解决这一问题，我们开发了一个全面的管道，以构建高质量的中文跨模态数据集。因此，我们提出了DanQing，该数据集包含从Common Crawl收集的1亿个图像-文本对。与现有数据集不同，DanQing通过更严格的选择过程进行策划，提供了更优的数据质量。此外，DanQing主要基于2024-2025年的网络数据构建，使模型能够更好地捕捉不断演变的语义趋势，从而提供更大的实际效用。我们通过对SigLIP2模型的持续预训练，将DanQing与现有数据集进行了比较。实验结果表明，DanQing在一系列中文下游任务中始终实现了优越的性能，包括零-shot分类、跨模态检索和基于LMM的评估。为了促进中文视觉-语言预训练的进一步研究，我们将根据创意共享CC-BY 4.0许可证开源DanQing数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lag in Chinese vision-language pre-training due to a lack of high-quality image-text datasets. The authors developed DanQing, a large-scale dataset containing 100 million image-text pairs, by employing a rigorous selection process and utilizing data primarily from 2024-2025 web sources. Experimental results demonstrate that models pre-trained on DanQing, specifically the SigLIP2 model, outperform existing datasets in various Chinese downstream tasks, including zero-shot classification and cross-modal retrieval.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于缺乏高质量图像-文本数据而导致的中文视觉语言预训练滞后问题。作者开发了一个全面的流程，创建了DanQing，这是一个由1亿对图像-文本对组成的大规模数据集，数据来源于Common Crawl，重点在于严格的数据选择和2024-2025年的最新网页数据。实验结果表明，基于DanQing进行预训练的模型在各种中文下游任务中表现优于现有数据集，包括零样本分类和跨模态检索，突显了其在推动该领域研究方面的实际效用。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍然是盲点。目前的VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器。训练过程通常将图像展平为一维补丁序列，忽略了进行空间推理所需的二维结构。我们认为，这种缺乏空间意识是VLM设计中的一个缺失维度，也是需要空间基础的应用（如机器人技术和具身人工智能）的瓶颈。为了解决这个问题，我们研究了（i）使用替代目标训练的图像编码器和（ii）二维位置编码。我们的实验表明，这些架构选择可以在多个基准上改善空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of vision-language models (VLMs) in capturing spatial relationships, which is critical for applications like robotics and embodied AI. The authors propose a method that involves using alternative training objectives for image encoders and incorporating 2D positional encodings to enhance spatial awareness. Experimental results demonstrate that these modifications significantly improve spatial reasoning capabilities across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉语言模型（VLMs）在捕捉空间关系方面的局限性，这对于机器人技术和具身人工智能等应用至关重要。作者提出了一种方法，采用替代的图像编码器训练目标，并引入二维位置编码，以增强空间意识。实验结果表明，这些修改显著提高了在多个基准测试中的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP</div>
<div class="meta-line">Authors: Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-14T20:38:36+00:00 · Latest: 2026-01-14T20:38:36+00:00</div>
<div class="meta-line">Comments: Submitted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09859v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09859v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破开放权重CLIP的限制：自监督微调CLIP的优化框架</div>
<div class="mono" style="margin-top:8px">CLIP已成为多模态表示学习的基石，但提高其性能通常需要在数十亿样本上从头开始训练的高昂成本。我们提出一个不同的问题：是否可以仅使用现有的自监督数据集来提高开放权重CLIP模型在各种下游任务中的性能？与将预训练模型适应单一下游任务的监督微调不同，我们的设置旨在提高在各种任务中的整体性能。然而，正如我们的实验和先前研究所揭示的，从开放权重CLIP模型开始简单应用标准训练协议往往会失败，导致性能下降。本文介绍了TuneCLIP，一个克服性能下降的自监督微调框架。TuneCLIP有两个关键组成部分：（1）一个恢复优化统计的热身阶段，以减少冷启动偏差，灵感来自理论分析；（2）一个优化新的对比损失的微调阶段，以减轻对假负对的惩罚。我们的广泛实验表明，TuneCLIP在模型架构和规模上始终提高性能。值得注意的是，它提升了领先的开放权重模型，如SigLIP（ViT-B/16），在ImageNet和相关的分布外基准上实现了高达+2.5%的增益，在竞争激烈的DataComp基准上实现了+1.2%的增益，为高效的后预训练适应设定了新的强基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of open-weight CLIP models in multimodal representation learning without the need for extensive retraining on large datasets. The authors propose TuneCLIP, a self-supervised fine-tuning framework that includes a warm-up stage to recover optimization statistics and a fine-tuning stage that optimizes a new contrastive loss to address performance degradation. Experimental results demonstrate that TuneCLIP significantly improves performance across various model architectures, achieving notable gains of up to +2.5% on ImageNet and +1.2% on the DataComp benchmark, establishing a new baseline for efficient adaptation of open-weight models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高开放权重CLIP模型在各种下游任务中的性能，而无需在大型数据集上进行广泛训练。作者提出了TuneCLIP，这是一种自监督微调框架，包括一个恢复优化统计的热身阶段和一个优化新对比损失的微调阶段。实验结果表明，TuneCLIP显著提高了性能，在ImageNet上获得了高达+2.5%的增益，在DataComp基准上获得了+1.2%的增益，从而为开放权重模型的高效适应建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Normalize Filters! Classical Wisdom for Deep Vision</div>
<div class="meta-line">Authors: Gustavo Perez, Stella X. Yu</div>
<div class="meta-line">First: 2025-06-04T19:32:42+00:00 · Latest: 2026-01-14T19:43:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04401v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.04401v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>归一化滤波器！深度视觉的经典智慧</div>
<div class="mono" style="margin-top:8px">经典图像滤波器，如平均或差分滤波器，经过精心归一化以确保一致性、可解释性，并避免强度偏移、光晕或振铃等伪影。相比之下，在深度网络中端到端学习的卷积滤波器缺乏这种约束。尽管它们可能类似于小波和斑点/边缘检测器，但并未以相同或任何方式进行归一化。因此，当图像经历大气传输时，其响应会失真，导致错误结果。我们通过提出滤波器归一化，随后进行可学习的缩放和偏移，类似于批量归一化，来解决这一限制。这一简单而有效的修改确保滤波器具有大气等变性，实现共域对称性。通过将经典滤波原理融入深度学习（适用于卷积神经网络和依赖卷积的视觉变换器），我们的方法在人工和自然强度变化基准上取得了显著改进。我们的ResNet34甚至可以大幅超越CLIP。我们的分析表明，未归一化的滤波器会降低性能，而滤波器归一化则规范学习，促进多样性，提高鲁棒性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the limitations of convolutional filters in deep learning, which lack normalization and can produce distorted responses under atmospheric transfer. The authors propose a method that incorporates filter normalization followed by learnable scaling and shifting, similar to batch normalization, to ensure that filters are atmosphere-equivariant. Experimental results demonstrate that this approach significantly enhances performance on both artificial and natural intensity variation benchmarks, with a ResNet34 model outperforming CLIP by a considerable margin, indicating that normalized filters improve robustness, generalization, and learning diversity compared to unnormalized filters.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决深度学习中卷积滤波器的局限性，这些滤波器缺乏归一化，可能导致图像在大气传输中响应失真。作者提出了一种方法，结合滤波器归一化和可学习的缩放与偏移，类似于批量归一化，以确保滤波器具有大气等变性。实验结果表明，该方法在人工和自然强度变化基准测试中显著提高了性能，其中ResNet34模型的表现甚至大幅超越了CLIP，表明滤波器归一化改善了鲁棒性、泛化能力和学习多样性。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适应于稀有类别</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型如CLIP在零样本识别中表现出色，但在预训练期间很少见的类别（包括新出现的实体和文化特定类别）上表现不佳。我们介绍了LiteEmbed，这是一个轻量级框架，用于CLIP的少量样本个性化，使新类别能够在不重新训练编码器的情况下添加。LiteEmbed在CLIP的词汇中对文本嵌入进行子空间引导优化，利用基于PCA的分解，将粗略语义方向与细粒度变化分离。两个互补目标，粗对齐和细分离，共同保持全局语义一致性，同时增强视觉相似类别之间的可区分性。一旦优化，嵌入可以即插即用，顺利替代CLIP的原始文本特征，适用于分类、检索、分割和检测任务。大量实验表明，与之前的方法相比，LiteEmbed在适应于代表性不足、稀有或未见类别方面取得了显著提升，确立了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of large-scale vision-language models like CLIP in recognizing rare classes that are underrepresented during pretraining. The authors propose LiteEmbed, a lightweight framework that allows for few-shot personalization of CLIP by optimizing text embeddings without the need to retrain the model&#x27;s encoders. Experimental results show that LiteEmbed significantly enhances the model&#x27;s ability to adapt to rare or unseen classes, achieving substantial improvements in tasks such as classification, retrieval, segmentation, and detection compared to previous methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型视觉语言模型（如CLIP）在识别罕见类别（包括新出现的实体和文化特定类别）方面的表现。作者提出了LiteEmbed，这是一种轻量级框架，允许在不重新训练编码器的情况下对CLIP进行少量样本个性化，利用基于PCA的分解进行子空间引导的文本嵌入优化。实验结果表明，LiteEmbed在性能上显著优于先前的方法，通过在视觉相似类别之间实现更好的全局语义一致性和可区分性，使其在分类和检索等各种任务中有效地适应CLIP到代表性不足或未见过的类别。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：用于科学复合图的视觉条件面板检测与标题生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个标记面板组合成单一图像，但在实际流程中，标题往往缺失或仅提供图形级摘要，使得面板级理解变得困难。本文提出了FigEx2，一种视觉条件框架，能够从复合图中定位面板并直接生成面板级标题。为了减轻开放式标题生成中多样化措辞的影响，我们引入了一种噪声感知门控融合模块，能够自适应过滤标记级特征，以稳定检测查询空间。此外，我们采用了一种分阶段优化策略，将监督学习与强化学习（RL）相结合，利用基于CLIP的对齐和基于BERTScore的语义奖励来强制执行严格的多模态一致性。为了支持高质量的监督，我们策划了BioSci-Fig-Cap，这是一个针对面板级定位的精细基准，同时还包括物理和化学领域的跨学科测试套件。实验结果表明，FigEx2在检测方面达到了优越的0.726 mAP@0.5:0.95，并在METEOR上比Qwen3-VL-8B高出0.51，在BERTScore上高出0.24。值得注意的是，FigEx2在没有任何微调的情况下，展现出显著的零-shot迁移能力，能够适应分布外的科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the understanding of scientific compound figures, which often lack detailed panel-level captions. The authors propose FigEx2, a visual-conditioned framework that localizes individual panels and generates specific captions from the compound figure. Their method includes a noise-aware gated fusion module to enhance token-level feature stability and a staged optimization strategy that combines supervised learning with reinforcement learning, achieving a mean average precision of 0.726 for detection and outperforming the baseline model Qwen3-VL-8B in METEOR and BERTScore metrics, while also demonstrating strong zero-shot transferability to other scientific domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高对科学复合图的理解，因为这些图通常缺乏对单个面板的详细说明。作者提出了FigEx2，这是一种视觉条件框架，可以从复合图中定位面板并生成特定的说明，采用噪声感知门控融合模块来改善标记级特征的稳定性。实验结果表明，FigEx2在检测中实现了0.726的平均精度，并在说明指标上超越了Qwen3-VL-8B模型，展示了在不同科学领域的强大零样本迁移能力，无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">Differentially private federated learning for localized control of infectious disease dynamics</div>
<div class="meta-line">Authors: Raouf Kerkouche, Henrik Zunker, Mario Fritz, Martin J. Kühn</div>
<div class="meta-line">First: 2025-09-17T14:28:04+00:00 · Latest: 2026-01-14T15:45:09+00:00</div>
<div class="meta-line">Comments: 26 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14024v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: R2 around 0.94 (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; R2 around 0.88 (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于传染病动态局部控制的差分隐私联邦学习</div>
<div class="mono" style="margin-top:8px">在流行病时期，迅速反应是减缓疫情传播的必要措施。局部方法在此反应中具有多种优势，限制必要资源并减少干预对更大范围的影响。然而，由于可用数据有限，在地方范围内训练单独的机器学习（ML）模型通常不可行。集中数据也面临挑战，因为其高度敏感性和隐私限制。在本研究中，我们考虑基于德国县和社区的局部策略，由相关地方卫生机构（LHA）管理。为了在不妨碍详细情境数据可用性的情况下保护隐私，我们提出了一种隐私保护的预测方法，以协助公共卫生专家和决策者。使用联邦学习（FL）的ML方法在不集中原始数据的情况下训练共享模型。将县、社区或LHA视为客户端，并在效用和隐私之间找到平衡，我们研究了具有客户端差分隐私（DP）的FL框架。我们在最近病例数的滑动窗口上训练共享的多层感知器，以预测病例数量，同时客户端仅交换规范裁剪的更新，服务器则聚合带有DP噪声的更新。我们在两个阶段对县级COVID-19数据评估该方法。如预期，严格的隐私导致不稳定且不可用的预测。在适度强的隐私水平下，DP模型与非DP模型接近：2020年11月R2约为0.94（对比0.95），平均绝对百分比误差（MAPE）为26%；2022年3月R2约为0.88（对比0.93），MAPE为21%。总体而言，客户端级DP-FL可以提供有用的县级预测，并具有强隐私保障，且可行的隐私预算取决于流行病阶段，允许卫生机构之间进行隐私合规的合作以进行地方预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for rapid responses during epidemics by proposing a localized approach to infectious disease dynamics that balances privacy and data utility. The study employs a federated learning framework with client-level differential privacy to train a shared multilayer perceptron model on COVID-19 case counts from German counties, allowing local health authorities to forecast cases without centralizing sensitive data. Experimental results show that while very strict privacy leads to unstable forecasts, a moderately strong level of differential privacy yields predictions that closely match those of non-private models, achieving R2 values of approximately 0.94 and 0.88 in different phases, with mean absolute percentage errors of 26% and 21%, respectively, demonstrating the feasibility of privacy-preserving local forecasting.</div>
<div class="mono" style="margin-top:8px">该研究的动机是需要在保持数据隐私的同时快速应对疫情爆发。作者提出了一种局部联邦学习框架，采用客户端级差分隐私，通过机器学习预测传染病动态，而无需集中敏感数据。实验结果表明，尽管非常严格的隐私设置导致预测不稳定，但适度强的差分隐私水平使模型在评估COVID-19数据的两个阶段中实现了约0.94和0.88的R2值，以及26%和21%的平均绝对百分比误差，展示了在强隐私保护下有效预测的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity</div>
<div class="meta-line">Authors: Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal</div>
<div class="meta-line">First: 2026-01-14T14:03:11+00:00 · Latest: 2026-01-14T14:03:11+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09497v1">PDF</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr">Code1</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git">Code2</a> · <a href="https://github.com/Ritabrata04/cdod-icpr">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对领域特异性下的鲁棒跨数据集目标检测泛化</div>
<div class="mono" style="margin-top:8px">目标检测器在分布内通常表现良好，但在不同基准上会急剧下降。我们通过设置特异性的视角研究跨数据集目标检测（CD-OD）。我们将基准分组为具有多样日常场景的设置无关数据集和与狭窄环境相关的设置特定数据集，并在所有训练-测试对上评估标准检测器系列。这揭示了CD-OD中的明确结构：在相同设置类型内的迁移相对稳定，而跨设置类型的迁移显著下降且通常是不对称的。最严重的崩溃发生在从特定源迁移到无关目标时，并且在开放标签对齐后仍然存在，表明领域转移在最困难的情况下占主导地位。为了将领域转移与标签不匹配区分开，我们比较了闭合标签迁移与开放标签协议，后者使用CLIP相似性将预测类别映射到最近的目标标签。开放标签评估产生了一致但有限的增益，许多修正案例对应于图像证据支持的语义近失。总体而言，我们提供了在设置特异性下对CD-OD的原则性表征以及在分布转移下评估检测器的实用指导。代码将发布在\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the significant performance degradation of object detectors when applied to different datasets, particularly focusing on the concept of setting specificity. The authors evaluate a standard family of object detectors across various train-test pairs, categorizing benchmarks into setting-agnostic and setting-specific datasets. The findings reveal that while transfer within the same setting type is stable, transferring across different setting types results in substantial performance drops, especially when moving from specific sources to agnostic targets. Additionally, the study compares closed-label transfer with an open-label protocol, finding that open-label evaluation provides consistent but limited improvements, often correcting cases that are semantically close but misclassified, thereby offering insights into the challenges of domain shift in object detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决物体检测器在应用于不同基准时性能下降的问题，特别是在跨数据集物体检测（CD-OD）中。作者将数据集分为设置无关和设置特定两类，并在各种训练-测试对上评估标准检测器系列。研究结果表明，同一设置类型内的迁移相对稳定，而不同设置类型之间的迁移效果显著降低，尤其是在从特定源迁移到无关目标时，突显了领域转移在困难场景中的主导地位。此外，研究比较了闭标签和开标签转移方法，发现开标签评估提供了一致的改进，特别是在图像证据中明显存在语义近似错误的情况下，从而为在设置特异性下理解CD-OD和评估检测器在分布转移中的表现提供了结构化的指导。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models</div>
<div class="meta-line">Authors: Yizhi Chen, Ahmed Hemani</div>
<div class="meta-line">First: 2026-01-14T12:52:08+00:00 · Latest: 2026-01-14T12:52:08+00:00</div>
<div class="meta-line">Comments: Accepted to DATE Late Breaking Results 2026, Verona, Italy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09451v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破性结果：Quamba-SE：状态空间模型中激活的软边量化器</div>
<div class="mono" style="margin-top:8px">我们提出了Quamba-SE，一种用于状态空间模型（SSM）激活量化的软边量化器。与现有方法使用标准INT8操作不同，Quamba-SE采用三种自适应尺度：小值的高精度、正常值的标准尺度和异常值的低精度。这保留了异常值信息，而不是硬剪切，同时保持其他值的精度。我们在Mamba-130M上评估了6个零样本基准。结果表明，Quamba-SE在各个基准上始终优于Quamba，在单个基准上最高提高了+2.68%，在6个数据集的平均准确率上提高了+0.83%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for improved activation quantization in State Space Models (SSMs) to enhance performance without sacrificing precision. The authors introduce Quamba-SE, a soft-edge quantizer that utilizes three adaptive scales for quantization: high-precision for small values, standard for normal values, and low-precision for outliers, thereby preserving outlier information. Experimental evaluations on the Mamba-130M model across six zero-shot benchmarks demonstrate that Quamba-SE outperforms the previous Quamba method, achieving improvements of up to +2.68% on individual benchmarks and an average accuracy increase of +0.83% across all datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善状态空间模型（SSM）中激活值的量化，以更好地保留信息，特别是对于离群值。作者提出了Quamba-SE，这是一种软边量化器，利用三种自适应尺度：小值的高精度、正常值的标准尺度和离群值的低精度，与传统的INT8操作相比，后者通常会对离群值进行硬剪切。对Mamba-130M模型在六个零样本基准上的实验结果表明，Quamba-SE在各个基准上均优于之前的Quamba方法，个别基准的提升可达+2.68%，在所有评估数据集上的平均准确率提高了+0.83%。</div>
</details>
</div>
<div class="card">
<div class="title">Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</div>
<div class="meta-line">Authors: Jiachen Li, Xiaojin Gong</div>
<div class="meta-line">First: 2023-10-26T08:12:53+00:00 · Latest: 2026-01-14T09:17:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.17218v3">Abs</a> · <a href="https://arxiv.org/pdf/2310.17218v3">PDF</a> · <a href="https://github.com/RikoLi/PCL-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原型对比学习的CLIP微调用于物体重识别</div>
<div class="mono" style="margin-top:8px">本研究旨在将大规模预训练的视觉-语言模型（如对比语言-图像预训练（CLIP））适应于提高各种监督设置下的物体重识别（Re-ID）性能。尽管提示学习使得名为CLIP-ReID的近期工作取得了良好的性能，但由于ReID任务中缺乏语义标签，其基本机制和提示学习的必要性仍不清楚。在本研究中，我们首先分析了提示学习在CLIP-ReID中的作用，并识别其局限性。基于我们的调查，我们提出了一种简单而有效的方法来适应CLIP用于监督物体Re-ID。我们的方法直接使用原型对比学习（PCL）损失微调CLIP的图像编码器，消除了对提示学习的需求。在人和车的Re-ID数据集上的实验结果表明，我们的方法与CLIP-ReID相比具有竞争力。此外，我们将基于PCL的CLIP微调方法扩展到无监督场景，在该场景中我们实现了最先进的性能。代码可在https://github.com/RikoLi/PCL-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of improving object re-identification (Re-ID) performance using large-scale pre-trained vision-language models like CLIP, particularly in the context of unclear mechanisms of prompt learning. The authors analyze the limitations of prompt learning in CLIP-ReID and propose a novel method that fine-tunes the image encoder of CLIP with a prototypical contrastive learning (PCL) loss, thereby eliminating the need for prompt learning. Experimental results on person and vehicle Re-ID datasets show that this approach is competitive with CLIP-ReID and achieves state-of-the-art performance in unsupervised scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过使用大规模预训练的视觉-语言模型（如CLIP）来提高物体重识别（Re-ID）的性能，特别关注现有方法中提示学习的作用尚不明确。作者分析了CLIP-ReID中提示学习的局限性，并提出了一种新方法，通过原型对比学习（PCL）损失直接微调CLIP的图像编码器，从而消除了对提示学习的需求。在对行人和车辆Re-ID数据集的实验结果表明，该方法与CLIP-ReID具有竞争力，并在无监督场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion</div>
<div class="meta-line">Authors: Jialu Li, Taiyan Zhou</div>
<div class="meta-line">First: 2026-01-14T06:38:12+00:00 · Latest: 2026-01-14T06:38:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpikeVAEDiff：基于神经脉冲的自然视觉场景重建通过VD-VAE和多功能扩散</div>
<div class="mono" style="margin-top:8px">从神经活动重建自然视觉场景是神经科学和计算机视觉中的一个关键挑战。我们提出了SpikeVAEDiff，一个新颖的两阶段框架，结合了非常深的变分自编码器（VDVAE）和多功能扩散模型，从神经脉冲数据生成高分辨率和语义丰富的图像重建。在第一阶段，VDVAE通过将神经脉冲信号映射到潜在表示，生成低分辨率的初步重建。在第二阶段，回归模型将神经脉冲信号映射到CLIP-Vision和CLIP-Text特征，使多功能扩散能够通过图像到图像生成来细化图像。我们在艾伦视觉编码-神经像素数据集上评估了我们的方法，并分析了不同的脑区。我们的结果表明，VISI区域表现出最显著的激活，并在重建质量中发挥关键作用。我们展示了成功和不成功的重建示例，反映了解码神经活动的挑战。与基于fMRI的方法相比，脉冲数据提供了更优越的时间和空间分辨率。我们进一步验证了VDVAE模型的有效性，并进行消融研究，表明来自特定脑区的数据显著提高了重建性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing natural visual scenes from neural activity, which is significant in both neuroscience and computer vision. The authors propose SpikeVAEDiff, a two-stage framework that integrates a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to achieve high-resolution image reconstructions from neural spike data. Experimental results on the Allen Visual Coding-Neuropixels dataset reveal that the VISI region is crucial for reconstruction quality, with the method demonstrating improved performance over fMRI-based approaches due to the superior temporal and spatial resolution of spike data, while also highlighting the complexities involved in decoding neural activity through both successful and unsuccessful reconstruction examples.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从神经活动重建自然视觉场景的挑战，这在神经科学和计算机视觉中都具有重要意义。作者提出了SpikeVAEDiff，这是一个结合了非常深的变分自编码器（VDVAE）和多功能扩散模型的两阶段框架，以实现从神经尖峰数据中获得高分辨率图像重建。对Allen视觉编码-神经像素数据集的实验结果表明，VISI区域对重建质量至关重要，研究强调了成功和不成功的重建案例，并证明尖峰数据在时间和空间分辨率上优于fMRI，同时通过消融研究确认了VDVAE模型的有效性，显示特定脑区的数据在提高重建性能方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</div>
<div class="meta-line">Authors: Youngmin Kim, Giyeong Oh, Kwangsoo Youm, Youngjae Yu</div>
<div class="meta-line">First: 2025-07-14T11:33:47+00:00 · Latest: 2026-01-14T06:25:00+00:00</div>
<div class="meta-line">Comments: Accepted to Automation in Construction. Our project page: https://winston1214.github.io/SlumpGuard/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10171v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.10171v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://winston1214.github.io/SlumpGuard/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SlumpGuard：基于AI的实时自动混凝土坍落度预测系统</div>
<div class="mono" style="margin-top:8px">混凝土的可加工性对建筑质量至关重要，坍落度测试是最广泛使用的现场评估方法。然而，传统的坍落度测试是手动的，耗时且高度依赖操作员，不适合在浇筑过程中进行连续或实时监测。为了解决这些局限性，我们提出了SlumpGuard，一个基于AI的视觉系统，通过单个固定摄像头分析搅拌车漏斗的自然排放流。该系统执行自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，实现了无需传感器、硬件安装或人工干预的质量监测。我们介绍了系统设计，构建了一个包含6000多个视频片段的现场复制数据集，并报告了广泛的评估，证明了在多种现场条件下可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测。专家研究进一步揭示了人类视觉估计的显著不一致，突显了自动评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the assessment of concrete workability, which is crucial for construction quality, by addressing the limitations of traditional manual slump testing. The authors developed SlumpGuard, an AI-powered vision system that utilizes a single fixed camera to analyze the discharge flow from a mixer-truck chute, enabling automatic detection of the chute, identification of pouring events, and classification of slump without the need for additional sensors or manual intervention. Experimental results demonstrate that SlumpGuard achieves reliable chute localization, accurate pouring detection, and robust slump prediction across various field conditions, while also revealing significant discrepancies in human visual estimates, underscoring the necessity for automated assessment methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决传统手动坍落度测试的局限性，来改善混凝土可加工性的评估，这对建筑质量至关重要。作者开发了SlumpGuard，这是一种利用固定摄像头分析搅拌车漏斗排放流的AI视觉系统，能够实现自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，而无需额外传感器或人工干预。实验结果表明，SlumpGuard在各种现场条件下实现了可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测，同时专家研究显示人类视觉估计存在显著差异，强调了自动评估方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-14T06:01:44+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v2">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应视图生成与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两个协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样且语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能，提升幅度为+2.5%。在视觉-语言学习中，它在十个数据集上将平均零-shot分类准确率提高了+12.31%（相较于CLIP）和+5.31%（相较于SLIP），并进一步将Flickr30k文本检索的R@5提高了+3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance contrastive learning by addressing the limitations in constructing high-quality positive pairs and the lack of a quality assessment mechanism. The authors propose GenView++, a unified framework that incorporates a multi-source adaptive view generation mechanism for creating diverse and semantically coherent views, alongside a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ improves MoCov2 by +2.5% on ImageNet linear classification and increases zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, while also enhancing Flickr30k text retrieval R@5 by +3.2%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决当前方法在构建高质量正对和缺乏质量评估机制方面的局限性，来提高对比学习的有效性。作者提出了GenView++，一个统一框架，结合了多源自适应视图生成机制，以创建多样且语义一致的视图，以及一个质量驱动的对比学习机制，根据语义对齐和多样性动态重新加权训练贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的性能2.5%，在十个数据集上将CLIP的零样本分类准确率提高了12.31%，将SLIP的准确率提高了5.31%，同时在Flickr30k文本检索R@5上提高了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-14T04:42:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0\%的图像-AUROC和92.2\%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that struggle with balancing semantic generalization and structural discriminability. The authors propose a method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through a Hierarchical Semantic-Visual Synergy (HSVS) mechanism, combining multi-scale structural priors from DINOv3 with the CLIP semantic space. Experimental results demonstrate that SSVP significantly improves performance on seven industrial benchmarks, achieving state-of-the-art results with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临局限，无法有效平衡全局语义泛化与细粒度结构可区分性。作者提出了一种新方法，称为协同语义-视觉提示（SSVP），通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，并利用视觉条件提示生成器（VCPG）促进动态提示生成。实验结果表明，SSVP在七个工业基准测试中达到了最先进的性能，在MVTec-AD上实现了93.0%的图像AUROC和92.2%的像素AUROC，显著超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives</div>
<div class="meta-line">Authors: Wisdom O. Ikezogwo, Kevin Zhang, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Linda Shapiro, Ranjay Krishna</div>
<div class="meta-line">First: 2025-01-07T23:32:05+00:00 · Latest: 2026-01-14T03:02:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.04184v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.04184v3">PDF</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data">Code1</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to $\textit{think-aloud}$ studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial and temporal association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. $\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data]}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedicalNarratives：将医学视觉与语言连接到本地化叙事</div>
<div class="mono" style="margin-top:8px">多模态模型对数据需求量大。虽然自然图像的数据集丰富，但医学图像数据集无法享受同样的奢侈。为了大规模实现医学图像的表示学习，我们转向YouTube，一个拥有大量开源医学教学视频的平台。我们整理了MedicalNarratives，一个包含470万对医学图像-文本的数据库，其中100万样本包含以空间轨迹（和边界框）形式的密集注释，118K个视频集中在轨迹事件上（带有对齐文本），使得超越单帧的时空基础成为可能。类似于$\textit{think-aloud}$研究，讲师在相关图像区域上悬停鼠标光标时进行讲解，MedicalNarratives中的100万张图像包含图像像素中的本地化鼠标轨迹，创建了文本与像素之间的空间和时间关联。为了评估MedicalNarratives的实用性，我们使用跨越12个医学领域的数据集训练了GenMedClip，采用类似CLIP的目标。GenMedClip在新构建的医学影像基准上，在所有12个领域中超越了之前的最先进模型。$\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[数据]}$</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of limited medical image datasets by leveraging YouTube&#x27;s extensive collection of open-source medical pedagogical videos to create the MedicalNarratives dataset, which consists of 4.7 million medical image-text pairs. The dataset includes 1 million samples with detailed annotations, such as spatial traces and bounding boxes, and 118,000 videos that provide spatiotemporal grounding for the images. The study evaluates the dataset by training a model called GenMedClip, which employs a CLIP-like objective and demonstrates superior performance over existing state-of-the-art models across all 12 medical domains in a newly established medical imaging benchmark.</div>
<div class="mono" style="margin-top:8px">本研究通过利用YouTube上丰富的开源医学教学视频，解决医学图像数据集有限的问题，创建了MedicalNarratives数据集，该数据集包含470万对医学图像-文本配对。该数据集包括100万个样本，具有详细的注释，如空间轨迹和边界框，允许医学图像的时空定位。研究通过训练名为GenMedClip的模型，采用类似CLIP的目标来评估数据集，该模型在新构建的基准上在所有12个医学领域的测试中表现优于现有的最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models</div>
<div class="meta-line">Authors: Ganxi Xu, Zhao-Rong Lai, Yuting Tang, Yonghao Song, Guoxu Zhou, Boyu wang, Jian Zhu, Jinyi Long</div>
<div class="meta-line">First: 2025-08-31T10:29:58+00:00 · Latest: 2026-01-14T01:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.00787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M/EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present a novel image-to-brain signal framework that generates M/EEG from images by leveraging the diffusion transformer architecture enhanced with cross-attention mechanisms. Specifically, we employ a diffusion transformer (DiT) architecture based on denoising diffusion implicit models (DDIM) to achieve brain signal generation. To realize the goal of image-to-brain signal conversion, we use cross-attention mechanisms to align brain signal embeddings with CLIP image embeddings. Moreover, we leverage large language models (LLMs) to generate image captions, and concatenate the resulting CLIP text embeddings with CLIP image embeddings to form unified embeddings for cross-attention alignment, enabling our model to capture core semantic information. Moreover, to capture core semantic information, we use large language models (LLMs) to generate descriptive and semantically accurate captions for images. Furthermore, we introduce a learnable spatio-temporal position encoding that combines brain region embeddings with temporal embeddings to capture both spatial and temporal characteristics of brain signals. We evaluate the framework on two multimodal benchmark datasets (THINGS-EEG2 and THINGS-MEG) and demonstrate that it generates biologically plausible brain signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP引导的多模态扩散模型的图像到脑信号生成用于视觉假体</div>
<div class="mono" style="margin-top:8px">视觉假体在恢复盲人视力方面具有巨大潜力。尽管研究人员成功利用M/EEG信号在视觉假体的脑解码阶段引发视觉感知，但在脑编码阶段将图像转换为M/EEG信号的互补过程仍然未被充分探索，阻碍了完整功能管道的形成。在本研究中，我们提出了一种新颖的图像到脑信号框架，通过利用增强了交叉注意机制的扩散变换器架构，从图像生成M/EEG信号。具体而言，我们采用基于去噪扩散隐式模型（DDIM）的扩散变换器（DiT）架构来实现脑信号生成。为了实现图像到脑信号转换的目标，我们使用交叉注意机制将脑信号嵌入与CLIP图像嵌入对齐。此外，我们利用大型语言模型（LLMs）生成图像标题，并将生成的CLIP文本嵌入与CLIP图像嵌入连接，形成统一的嵌入以进行交叉注意对齐，使我们的模型能够捕捉核心语义信息。此外，为了捕捉核心语义信息，我们使用大型语言模型（LLMs）为图像生成描述性和语义准确的标题。此外，我们引入了一种可学习的时空位置编码，将脑区嵌入与时间嵌入结合，以捕捉脑信号的空间和时间特征。我们在两个多模态基准数据集（THINGS-EEG2和THINGS-MEG）上评估了该框架，并证明其生成生物学上合理的脑信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance visual prostheses by addressing the underexplored process of converting images into M/EEG signals during the brain encoding stage. The authors propose a novel framework that utilizes a diffusion transformer architecture enhanced with cross-attention mechanisms to generate M/EEG signals from images. Experimental results on two multimodal benchmark datasets, THINGS-EEG2 and THINGS-MEG, indicate that the framework successfully produces biologically plausible brain signals, thereby contributing to a more complete functional pipeline for visual prostheses.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于填补将图像转换为M/EEG信号的空白，这对为盲人恢复视力的视觉假体创建完整的功能管道至关重要。作者提出了一种新颖的框架，利用增强的扩散变换器架构和交叉注意机制，从图像生成M/EEG信号。关键实验结果表明，该框架在两个多模态基准数据集THINGS-EEG2和THINGS-MEG上成功生成生物学上合理的脑信号，证明了其在对齐脑信号嵌入与CLIP图像嵌入方面的有效性，并通过大型语言模型生成的描述性标题捕捉核心语义信息。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型快速发展，但数据在影响运动方面的作用仍不清楚。我们提出了Motive（视频生成的运动归因），这是一个以运动为中心的基于梯度的数据归因框架，能够扩展到现代大型高质量视频数据集和模型。我们利用此框架研究哪些微调片段能改善或恶化时间动态。Motive通过运动加权损失掩码将时间动态与静态外观隔离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型中，Motive识别出强烈影响运动的片段，并指导数据策划，以改善时间一致性和物理合理性。使用Motive选择的高影响数据，我们的方法在VBench上提高了运动平滑度和动态程度，与预训练基础模型相比，获得了74.1%的人工偏好胜率。据我们所知，这是第一个在视频生成模型中归因于运动而非视觉外观的框架，并利用它来策划微调数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to better understand how data influences motion in video generation models, which remains poorly understood despite advancements in the field. The authors introduce Motive, a motion-centric, gradient-based data attribution framework designed to analyze large, high-quality video datasets and models. Their experiments reveal that Motive can effectively identify clips that significantly impact motion dynamics, leading to improved temporal consistency and physical plausibility in text-to-video models, with a notable 74.1% human preference win rate for motion smoothness and dynamic degree when using Motive-selected data compared to the pretrained base model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解数据如何影响视频生成模型中的运动，这是一个尚未充分探讨的领域。作者提出了Motive，一个以运动为中心的基于梯度的数据归因框架，旨在分析大型视频数据集和模型。他们的实验表明，Motive能够有效识别对运动有显著影响的剪辑，从而提高生成视频的时间一致性和物理合理性，使用Motive选择的数据相比于预训练基础模型，在运动平滑性和动态程度上获得了74.1%的人工偏好胜率。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：用于行人重识别的视频超分辨率</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹质量通常被视为事后考虑，绝大多数研究集中于对基础模型的架构修改。这些方法忽视了一个重要的限制，在现实世界的困难场景中部署ReID系统时面临挑战。本文介绍了S3-CLIP，一种基于视频超分辨率的CLIP-ReID框架，旨在2026年WACV的VReID-XFD挑战中开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，适应视频基础的行人重识别设置。据我们所知，这项工作代表了首次系统性研究视频超分辨率作为提高行人ReID轨迹质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法在基线性能上具有竞争力，在空中到地面场景中实现了37.52%的mAP，在地面到空中场景中实现了29.16%的mAP。在地面到空中设置中，S3-CLIP在排名准确性上取得了显著提升，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the often-overlooked quality of tracklets in person re-identification (ReID) systems, which can hinder their effectiveness in real-world scenarios. The authors propose S3-CLIP, a video super-resolution-based framework that combines advancements in super-resolution networks with task-driven pipelines specifically for video-based ReID. Experimental results indicate that S3-CLIP achieves competitive performance, with 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios, and shows significant improvements in ranking accuracy in the ground-to-aerial setting, enhancing Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人脸重识别（ReID）系统中常被忽视的轨迹质量问题，这在现实场景中可能会影响性能。作者提出了S3-CLIP，这是一种将视频超分辨率技术与CLIP-ReID相结合的新框架，专门为VReID-XFD挑战而设计。实验结果表明，S3-CLIP在空中到地面场景中实现了37.52%的平均精度（mAP），在地面到空中场景中实现了29.16%的mAP，并在地面到空中的设置中显著提高了排名准确性，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Visual World with Artificial Intelligence: A Roadmap</div>
<div class="meta-line">Authors: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu</div>
<div class="meta-line">First: 2025-11-11T18:59:50+00:00 · Latest: 2026-01-13T15:42:01+00:00</div>
<div class="meta-line">Comments: Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08585v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08585v2">PDF</a> · <a href="https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://world-model-roadmap.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a &quot;window&quot; into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用人工智能模拟视觉世界：路线图</div>
<div class="mono" style="margin-top:8px">视频生成的格局正在发生变化，从专注于生成视觉吸引力的片段转向构建支持交互并保持物理合理性的虚拟环境。这些发展指向视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐式世界模型，模拟支配真实或想象世界的物理动态、代理-环境交互和任务规划。本调查提供了这一演变的系统概述，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型和视频渲染器。世界模型编码关于世界的结构化知识，包括物理法则、交互动态和代理行为。它作为潜在的模拟引擎，使得连贯的视觉推理、长期时间一致性和目标驱动的规划成为可能。视频渲染器将这种潜在模拟转化为现实的视觉观察，有效地将视频作为“窗口”展示模拟世界。我们通过四个世代追踪视频生成的进展，其中核心能力逐步提升，最终形成一个建立在视频生成模型之上的世界模型，体现内在的物理合理性、实时多模态交互和跨多个时空尺度的规划能力。对于每一代，我们定义其核心特征，突出代表性作品，并考察其应用领域，如机器人技术、自动驾驶和互动游戏。最后，我们讨论下一代世界模型的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作的最新列表可在此链接中查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this paper is to explore the evolution of video generation towards creating interactive virtual environments that maintain physical plausibility. The authors systematically review the development of video foundation models, which integrate an implicit world model that encodes structured knowledge about physical laws and agent interactions with a video renderer that produces realistic visual outputs. Key findings indicate that the progression of video generation has advanced through four generations, culminating in models that support real-time interaction and planning across various scales, with applications in fields such as robotics and gaming, while also identifying challenges and design principles for future advancements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是从生成视觉上吸引人的视频片段转向创建保持物理合理性的互动虚拟环境。作者系统回顾了视频基础模型的发展，这些模型结合了编码物理法则和代理交互的结构化知识的隐式世界模型，以及生成逼真视觉输出的视频渲染器。主要发现表明，这些模型的发展经历了四个阶段，最终形成了一个复杂的世界模型，能够在各种时空尺度上进行实时互动和规划，应用于机器人和游戏等领域，同时识别出未来进展的挑战和设计原则。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts</div>
<div class="meta-line">Authors: Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Loppi, Antti Honkela</div>
<div class="meta-line">First: 2024-06-25T06:04:58+00:00 · Latest: 2026-01-13T15:13:42+00:00</div>
<div class="meta-line">Comments: 19 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.17298v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.17298v3">PDF</a> · <a href="https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无捷径的差分隐私深度学习的高效可扩展实现</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DP-SGD）是基于差分隐私（DP）训练机器学习模型的标准算法。最常见的DP-SGD隐私会计依赖于泊松子采样以确保理论DP保证。使用泊松子采样实现计算高效的DP-SGD并非易事，这导致许多实现通过使用计算更快的子采样走捷径。我们通过实现和基准测试正确的泊松子采样的高效方法来量化在DP下训练深度学习模型的计算成本。我们发现，使用PyTorch中Opacus的DP-SGD的天真实现，其吞吐量比SGD低2.6到8倍。然而，像Ghost Clipping这样的高效梯度裁剪实现可以大致将此成本减半。我们提出了一种使用JAX的DP-SGD的替代计算高效实现，该实现使用泊松子采样，并与基于PyTorch的高效裁剪优化表现相当。我们研究了使用多达80个GPU的扩展行为，发现DP-SGD的扩展性优于SGD。我们在https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL分享我们的库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges in implementing differentially private stochastic gradient descent (DP-SGD) efficiently while maintaining theoretical privacy guarantees. The authors benchmark various methods for training deep learning models under differential privacy, specifically focusing on the computational costs associated with Poisson subsampling. They find that the naive implementation of DP-SGD using Opacus in PyTorch has significantly lower throughput compared to standard SGD, but by employing efficient gradient clipping techniques like Ghost Clipping, they can reduce this cost by approximately half. Additionally, they propose a new implementation of DP-SGD using JAX that maintains the benefits of Poisson subsampling and demonstrates improved scalability when utilizing up to 80 GPUs, outperforming traditional SGD in scaling behavior.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在保持理论隐私保证的同时，如何高效地实现差分隐私随机梯度下降（DP-SGD）的问题。作者对不同方法进行了基准测试，量化了在差分隐私下训练深度学习模型的计算成本，发现使用PyTorch中Opacus的DP-SGD简单实现的速度显著低于标准SGD。他们提出了一种使用JAX的新实现，该实现结合了泊松子采样，并展示了与PyTorch中优化方法的可比性能，同时在使用多达80个GPU时显示出比SGD更好的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</div>
<div class="meta-line">Authors: Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</div>
<div class="meta-line">First: 2026-01-13T13:42:05+00:00 · Latest: 2026-01-13T13:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08557v1">PDF</a> · <a href="https://github.com/Simula/HEDGE#videohedge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHEDGE：基于熵的视觉语言模型视频幻觉检测框架，通过语义聚类和时空扰动</div>
<div class="mono" style="margin-top:8px">在视频能力的视觉语言模型（Video-VLMs）中，幻觉现象仍然频繁且置信度高，而现有的不确定性度量往往无法与正确性对齐。我们提出了VideoHEDGE，这是一个用于视频问答中幻觉检测的模块化框架，将基于熵的可靠性估计从图像扩展到时间结构化输入。给定一个视频-问题对，VideoHEDGE从干净片段和光度及时空扰动的变体中提取基线答案和多个高温生成，然后使用基于自然语言推理（NLI）或嵌入的方法将结果文本输出聚类为语义假设。聚类级别的概率质量产生三个可靠性评分：语义熵（SE）、RadFlag和视觉增强语义熵（VASE）。我们在SoccerChat基准上评估VideoHEDGE，使用LLM作为评判者获得二元幻觉标签。在三个7B Video-VLM（Qwen2-VL、Qwen2.5-VL和一个SoccerChat微调模型）中，VASE始终实现最高的ROC-AUC，尤其是在较大的失真预算下，而SE和RadFlag往往接近随机。我们进一步表明，基于嵌入的聚类在检测性能上与基于NLI的聚类相匹配，但计算成本显著较低，并且领域微调减少了幻觉频率，但在校准方面仅带来了适度的改善。hedge-bench PyPI库支持可重复和可扩展的基准测试，完整代码和实验资源可在https://github.com/Simula/HEDGE#videohedge获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the frequent and high-confidence hallucinations in video-capable vision-language models (Video-VLMs), which existing uncertainty metrics fail to accurately assess. The authors introduce VideoHEDGE, a modular framework that extends entropy-based reliability estimation to video question answering by generating baseline answers and high-temperature outputs from both clean and perturbed video clips, followed by clustering these outputs into semantic hypotheses. Experimental results on the SoccerChat benchmark demonstrate that the Vision-Amplified Semantic Entropy (VASE) score consistently achieves the highest ROC-AUC across three different 7B Video-VLMs, particularly under larger distortion budgets, while other scores like Semantic Entropy and RadFlag perform near chance levels; additionally, embedding-based clustering proves to be as effective as NLI-based clustering at a lower computational cost, and domain fine-tuning shows only modest improvements in hallucination calibration.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视频能力视觉语言模型（Video-VLMs）中频繁且高置信度的幻觉问题，而现有的不确定性度量往往难以准确评估。作者提出了VideoHEDGE，这是一个用于视频问答中检测幻觉的模块化框架，利用基于熵的可靠性估计并将其应用于时间结构化输入。关键实验结果表明，视觉增强语义熵（VASE）分数在多个Video-VLMs中检测幻觉的表现始终优于其他可靠性度量，尤其是在较大的失真预算下，同时还表明基于嵌入的聚类可以以较低的计算成本实现与基于自然语言推理的聚类相似的检测性能。</div>
</details>
</div>
<div class="card">
<div class="title">Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning</div>
<div class="meta-line">Authors: Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano</div>
<div class="meta-line">First: 2025-07-18T17:59:55+00:00 · Latest: 2026-01-13T13:22:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14137v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14137v3">PDF</a> · <a href="https://github.com/valeoai/Franca">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Franca：用于可扩展视觉表示学习的嵌套俄罗斯套娃聚类</div>
<div class="mono" style="margin-top:8px">我们介绍Franca（发音为Fran-ka）：一个免费的开源视觉基础模型，数据、代码和权重完全开放，性能与许多最先进的专有模型（如DINOv2、CLIP、SigLIPv2等）相匹配，并在许多情况下超越它们。我们的方法基于受Web-SSL启发的透明训练流程，使用公开可用的数据：ImageNet-21K和ReLAION-2B的一个子集。除了模型发布外，我们还解决了SSL聚类方法中的关键局限性。现代模型依赖于通过像Sinkhorn-Knopp这样的聚类算法将图像特征分配给大型代码本，但未能考虑聚类语义中的固有模糊性。为了解决这个问题，我们引入了一种基于嵌套俄罗斯套娃表示的参数高效多头聚类投影器。该设计逐步将特征细化为越来越细粒度的聚类，而不增加模型大小，从而实现性能和内存效率。此外，我们提出了一种新颖的位置信息解耦策略，明确消除密集表示中的位置偏差，从而改善语义内容的编码。这在多个下游基准测试中带来了持续的提升，证明了更清晰特征空间的实用性。我们的贡献为透明、高性能的视觉模型建立了新的标准，并为更可重复和可推广的基础模型在更广泛的AI社区中开辟了道路。代码和模型检查点可在https://github.com/valeoai/Franca获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation behind Franca is to create a fully open-source vision foundation model that can match or exceed the performance of leading proprietary models while addressing limitations in self-supervised learning (SSL) clustering methods. The authors developed a parameter-efficient, multi-head clustering projector utilizing nested Matryoshka representations to refine image features into finer clusters without increasing model size, alongside a novel positional disentanglement strategy to eliminate biases in dense representations. Experimental results show that Franca achieves consistent performance improvements on various downstream benchmarks, highlighting its effectiveness in producing cleaner feature spaces and establishing a new standard for high-performance vision models.</div>
<div class="mono" style="margin-top:8px">Franca的研究动机是创建一个完全开源的视觉基础模型，不仅匹配而且在许多情况下超越领先的专有模型，同时解决自监督学习（SSL）聚类方法中的局限性。主要方法涉及基于嵌套马特里奥什卡表示的多头聚类投影器，该方法在不增加模型大小的情况下将图像特征细化为更精细的聚类，并采用位置解耦策略以减轻密集表示中的位置偏差。关键实验结果表明，这种方法在多个下游基准测试中显著提高了性能，突显了更清晰特征空间的有效性，并为AI社区设定了高性能透明视觉模型的新标准。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
