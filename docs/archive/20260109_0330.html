<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-09 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260109_0330</div>
    <div class="row"><div class="card">
<div class="title">Learning Visual Affordance from Audio</div>
<div class="meta-line">Authors: Lidong Lu, Guo Chen, Zhu Wei, Yicheng Liu, Tong Lu</div>
<div class="meta-line">First: 2025-12-01T18:58:56+00:00 · Latest: 2025-12-28T02:41:14+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02005v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.02005v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jscslld.github.io/AVAGFormer/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从音频学习视觉可供性</div>
<div class="mono" style="margin-top:8px">我们介绍了音频-视觉可供性定位（AV-AG），这是一个新任务，旨在从动作声音中分割物体交互区域。与现有依赖文本指令或演示视频的方法不同，这些方法常常受到模糊或遮挡的限制，音频提供了实时、语义丰富且视觉独立的线索，用于可供性定位，从而使交互区域的理解更加直观。为支持此任务，我们构建了第一个AV-AG数据集，包含大量动作声音、物体图像和像素级可供性注释。该数据集还包括一个未见子集，以评估零样本泛化。此外，我们提出了AVAGFormer，一个配备语义条件交叉模态混合器和双头解码器的模型，有效融合音频和视觉信号以进行掩码预测。实验表明，AVAGFormer在AV-AG上实现了最先进的性能，超越了相关任务的基线。全面分析突出了AV-AG与AVS之间的区别、端到端建模的好处以及每个组件的贡献。代码和数据集已发布在https://jscslld.github.io/AVAGFormer/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a more intuitive understanding of object interaction regions using audio cues, as existing methods often suffer from limitations due to ambiguity or occlusion in textual or visual instructions. The authors introduce a new task called Audio-Visual Affordance Grounding (AV-AG) and construct the first AV-AG dataset, which includes action sounds, object images, and pixel-level affordance annotations, along with an unseen subset for zero-shot generalization evaluation. They propose a model named AVAGFormer that utilizes a semantic-conditioned cross-modal mixer and a dual-head decoder to effectively integrate audio and visual signals for mask prediction, achieving state-of-the-art performance on AV-AG and surpassing related task baselines, with analyses revealing the advantages of their approach and the contributions of each model component.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要更直观的方法来使用动作声音分割物体交互区域，因为现有方法在文本或视觉指令中常常面临模糊或遮挡的限制。作者提出了一项名为音频-视觉可供性定位（AV-AG）的新任务，并构建了该任务的第一个数据集，其中包括动作声音、物体图像和像素级可供性注释，以及用于零样本泛化评估的子集。他们提出了一种名为AVAGFormer的模型，该模型利用语义条件的跨模态混合器和双头解码器有效整合音频和视觉信息进行掩膜预测，在AV-AG上实现了最先进的性能，超越了相关任务的基线，分析揭示了其方法和模型组件的优势。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding</div>
<div class="meta-line">Authors: Zhou Chen, Joe Lin, Carson Bulgin, Sathyanarayanan N. Aakur</div>
<div class="meta-line">First: 2025-12-03T19:54:27+00:00 · Latest: 2025-12-03T19:54:27+00:00</div>
<div class="meta-line">Comments: 20 pages. 3 figures, 4 tables. Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04231v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.04231v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT-E：一种用于具身可供性基础的神经符号框架</div>
<div class="mono" style="margin-top:8px">在非结构化环境中操作的辅助机器人必须理解物体不仅是什么，还能用于什么。这需要将基于语言的动作查询与既能提供所请求功能又能被物理获取的物体进行基础连接。现有方法通常依赖于黑箱模型或固定的可供性标签，限制了人机应用的透明性、可控性和可靠性。我们介绍CRAFT-E，这是一种模块化的神经符号框架，它将结构化的动词-属性-物体知识图与视觉-语言对齐和基于能量的抓取推理相结合。该系统生成可解释的基础路径，揭示影响物体选择的因素，并将抓取可行性作为可供性推理的一个组成部分。我们进一步构建了一个具有统一注释的基准数据集，用于动词-物体兼容性、分割和抓取候选，并在物理机器人上部署了完整的管道。CRAFT-E在静态场景、基于ImageNet的功能检索和涉及20个动词和39个物体的真实世界试验中表现出竞争力。该框架在感知噪声下仍然稳健，并提供透明的组件级诊断。通过将符号推理与具身感知相结合，CRAFT-E为基于可供性的物体选择提供了一种可解释和可定制的替代方案，支持辅助机器人系统中的可信决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the ability of assistive robots to understand not only the identity of objects but also their functional uses in unstructured environments. The authors propose CRAFT-E, a neuro-symbolic framework that integrates a structured knowledge graph with visual-language alignment and energy-based grasp reasoning to generate interpretable grounding paths for object selection. Experimental results demonstrate that CRAFT-E achieves competitive performance in various scenarios, including static scenes and real-world trials, while maintaining robustness against perceptual noise and providing transparent diagnostics, thus offering a reliable alternative to traditional black-box models for affordance-grounded object selection in assistive robotics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高助理机器人在非结构化环境中的理解能力，使其不仅能够识别物体，还能理解物体的功能性可用性。作者提出了CRAFT-E，这是一种模块化的神经符号框架，结合了结构化知识图谱、视觉语言对齐和基于能量的抓取推理，以生成可解释的物体选择基础路径。实验结果表明，CRAFT-E在静态场景和现实世界试验中表现出竞争力，且在感知噪声下保持稳健，并提供透明的诊断，从而为助理机器人中的可用性基础物体选择提供了可靠的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Closed-Loop Transfer for Weakly-supervised Affordance Grounding</div>
<div class="meta-line">Authors: Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-10-20T10:21:35+00:00 · Latest: 2025-10-20T10:21:35+00:00</div>
<div class="meta-line">Comments: Accepted at ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17384v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.17384v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can perform previously unexperienced interactions with novel objects simply by observing others engage with them. Weakly-supervised affordance grounding mimics this process by learning to locate object regions that enable actions on egocentric images, using exocentric interaction images with image-level annotations. However, extracting affordance knowledge solely from exocentric images and transferring it one-way to egocentric images limits the applicability of previous works in complex interaction scenarios. Instead, this study introduces LoopTrans, a novel closed-loop framework that not only transfers knowledge from exocentric to egocentric but also transfers back to enhance exocentric knowledge extraction. Within LoopTrans, several innovative mechanisms are introduced, including unified cross-modal localization and denoising knowledge distillation, to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images while enhancing knowledge transfer. Experiments show that LoopTrans achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弱监督可供性定位的闭环传输</div>
<div class="mono" style="margin-top:8px">人类可以通过观察他人与新物体的互动，执行之前未经历过的交互。弱监督可供性定位模仿这一过程，通过学习在自我中心图像中定位能够进行动作的物体区域，使用带有图像级注释的外部交互图像。然而，仅从外部图像提取可供性知识并单向转移到自我中心图像，限制了以往工作在复杂交互场景中的适用性。相反，本研究引入了LoopTrans，一个新颖的闭环框架，不仅将知识从外部转移到自我中心，还能反向转移以增强外部知识提取。在LoopTrans中，引入了多个创新机制，包括统一的跨模态定位和去噪知识蒸馏，以弥合物体中心自我中心图像与交互中心外部图像之间的领域差距，同时增强知识转移。实验表明，LoopTrans在图像和视频基准测试中在所有指标上均实现了一致的改进，甚至能够处理物体交互区域完全被人体遮挡的挑战场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of weakly-supervised affordance grounding, which aims to enable machines to identify object regions for actions based on limited annotations. The authors propose LoopTrans, a closed-loop framework that facilitates bidirectional knowledge transfer between exocentric and egocentric images, enhancing the extraction of affordance knowledge. Experimental results demonstrate that LoopTrans consistently improves performance across various metrics on image and video benchmarks, effectively managing complex scenarios where interaction regions are occluded by the human body.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善弱监督的可供性定位，旨在使机器能够通过观察学习物体交互，类似于人类学习。作者提出了一种名为LoopTrans的新型闭环框架，促进了外观图像和自我中心图像之间的双向知识转移，结合了统一的跨模态定位和去噪知识蒸馏等机制，以增强可供性知识的提取。实验结果表明，LoopTrans在图像和视频基准测试中在各项指标上持续超越了之前的方法，有效应对了物体交互区域被人体遮挡的复杂场景。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Affordance Prediction: Survey and Reproducibility</div>
<div class="meta-line">Authors: Tommaso Apicella, Alessio Xompero, Andrea Cavallaro</div>
<div class="meta-line">First: 2025-05-08T09:10:05+00:00 · Latest: 2025-10-13T19:03:25+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures, 13 tables. Project website at https://apicis.github.io/aff-survey/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05074v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.05074v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://apicis.github.io/aff-survey/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordances are the potential actions an agent can perform on an object, as observed by a camera. Visual affordance prediction is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand pose estimation. This diversity in formulations leads to inconsistent definitions that prevent fair comparisons between methods. In this paper, we propose a unified formulation of visual affordance prediction by accounting for the complete information on the objects of interest and the interaction of the agent with the objects to accomplish a task. This unified formulation allows us to comprehensively and systematically review disparate visual affordance works, highlighting strengths and limitations of both methods and datasets. We also discuss reproducibility issues, such as the unavailability of methods implementation and experimental setups details, making benchmarks for visual affordance prediction unfair and unreliable. To favour transparency, we introduce the Affordance Sheet, a document that details the solution, datasets, and validation of a method, supporting future reproducibility and fairness in the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉可供性预测：调查与可重复性</div>
<div class="mono" style="margin-top:8px">可供性是代理可以对物体执行的潜在动作，通常通过相机观察到。视觉可供性预测在抓取检测、可供性分类、可供性分割和手势估计等任务中有不同的表述。这种表述的多样性导致了不一致的定义，阻碍了方法之间的公平比较。本文提出了一种统一的视觉可供性预测表述，考虑了感兴趣物体的完整信息以及代理与物体的交互以完成任务。这种统一的表述使我们能够全面系统地回顾不同的视觉可供性研究，突出方法和数据集的优缺点。我们还讨论了可重复性问题，例如方法实现和实验设置细节的缺乏，使得视觉可供性预测的基准不公平且不可靠。为了促进透明度，我们引入了可供性表，详细说明了解决方案、数据集和方法的验证，支持未来的可重复性和社区的公平性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inconsistencies in the definitions and formulations of visual affordance prediction, which hinder fair comparisons among various methods. The authors propose a unified formulation that incorporates comprehensive information about objects and their interactions with agents to facilitate a systematic review of existing works. Key findings include the identification of strengths and limitations across different methods and datasets, as well as the introduction of the Affordance Sheet to enhance transparency and reproducibility in the field by providing detailed documentation of methods and experimental setups.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉可供性预测中定义和公式的不一致性，这妨碍了不同方法之间的公平比较。作者提出了一种统一的公式，综合考虑对象的完整信息及其与代理的交互，以便系统地回顾现有的研究成果。主要发现包括识别不同方法和数据集的优缺点，以及引入可供性表以增强该领域的透明度和可重复性，解决与实现细节和实验设置可用性相关的问题。</div>
</details>
</div>
<div class="card">
<div class="title">ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?</div>
<div class="meta-line">Authors: Taewhan Kim, Hojin Bae, Zeming Li, Xiaoqi Li, Iaroslav Ponomarenko, Ruihai Wu, Hao Dong</div>
<div class="meta-line">First: 2024-12-13T11:22:01+00:00 · Latest: 2025-10-09T05:15:53+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.10050v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.10050v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual actionable affordance has emerged as a transformative approach in robotics, focusing on perceiving interaction areas prior to manipulation. Traditional methods rely on pixel sampling to identify successful interaction samples or processing pointclouds for affordance mapping. However, these approaches are computationally intensive and struggle to adapt to diverse and dynamic environments. This paper introduces ManipGPT, a framework designed to predict optimal interaction areas for articulated objects using a large pre-trained vision transformer (ViT). We create a dataset of 9.9k simulated and real images to bridge the visual sim-to-real gap and enhance real-world applicability. By fine-tuning the vision transformer on this small dataset, we significantly improve part-level affordance segmentation, adapting the model&#x27;s in-context segmentation capabilities to robot manipulation scenarios. This enables effective manipulation across simulated and real-world environments by generating part-level affordance masks, paired with an impedance adaptation policy, sufficiently eliminating the need for complex datasets or perception systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ManipGPT：大型视觉模型的可操作性分割是否足以进行关节物体操作？</div>
<div class="mono" style="margin-top:8px">视觉可操作性已成为机器人技术中的一种变革性方法，专注于在操作之前感知交互区域。传统方法依赖于像素采样来识别成功的交互样本或处理点云进行可操作性映射。然而，这些方法计算密集，难以适应多样化和动态环境。本文介绍了ManipGPT，一个旨在使用大型预训练视觉变换器（ViT）预测关节物体的最佳交互区域的框架。我们创建了一个包含9.9k个模拟和真实图像的数据集，以弥合视觉模拟与现实之间的差距，增强现实世界的适用性。通过在这个小数据集上微调视觉变换器，我们显著提高了部件级可操作性分割，将模型的上下文分割能力适应于机器人操作场景。这使得通过生成部件级可操作性掩码，结合阻抗适应策略，有效地在模拟和现实环境中进行操作，充分消除了对复杂数据集或感知系统的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve robotic manipulation by enhancing the perception of actionable affordances in articulated objects, as traditional methods are computationally intensive and less adaptable to dynamic environments. The authors introduce ManipGPT, a framework that utilizes a large pre-trained vision transformer (ViT) to predict optimal interaction areas, supported by a dataset of 9.9k simulated and real images to address the visual sim-to-real gap. The key findings indicate that fine-tuning the vision transformer on this dataset significantly enhances part-level affordance segmentation, enabling effective manipulation in both simulated and real-world scenarios without the need for complex datasets or perception systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过增强对关节物体交互区域的感知来改善机器人操作，因为传统方法计算密集且对动态环境适应性差。作者提出了ManipGPT框架，利用大型预训练视觉变换器预测最佳交互区域，并通过9.9k个模拟和真实图像的数据集来解决视觉模拟与现实之间的差距。主要发现表明，在该数据集上微调视觉变换器显著提高了部件级的可供性分割，使得在模拟和真实世界场景中有效操作成为可能，而无需复杂的数据集或感知系统。</div>
</details>
</div>
<div class="card">
<div class="title">AffordanceSAM: Segment Anything Once More in Affordance Grounding</div>
<div class="meta-line">Authors: Dengyang Jiang, Zanyi Wang, Hengzhuang Li, Sizhe Dang, Teli Ma, Wei Wei, Guang Dai, Lei Zhang, Mengmeng Wang</div>
<div class="meta-line">First: 2025-04-22T07:16:56+00:00 · Latest: 2025-08-25T17:25:44+00:00</div>
<div class="meta-line">Comments: SAM Meets Affordance Grounding</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15650v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.15650v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building a generalized affordance grounding model to identify actionable regions on objects is vital for real-world applications. Existing methods to train the model can be divided into weakly and fully supervised ways. However, the former method requires a complex training framework design and can not infer new actions without an auxiliary prior. While the latter often struggle with limited annotated data and components trained from scratch despite being simpler. This study focuses on fully supervised affordance grounding and overcomes its limitations by proposing AffordanceSAM, which extends SAM&#x27;s generalization capacity in segmentation to affordance grounding. Specifically, we design an affordance-adaption module and curate a coarse-to-fine annotated dataset called C2F-Aff to thoroughly transfer SAM&#x27;s robust performance to affordance in a three-stage training manner. Experimental results confirm that AffordanceSAM achieves state-of-the-art (SOTA) performance on the AGD20K benchmark and exhibits strong generalized capacity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AffordanceSAM：在可供性基础上再次分割任何事物</div>
<div class="mono" style="margin-top:8px">构建一个通用的可供性基础模型以识别物体上的可操作区域对现实世界应用至关重要。现有的模型训练方法可分为弱监督和全监督两种方式。然而，前者需要复杂的训练框架设计，并且无法在没有辅助先验的情况下推断新动作。而后者尽管更简单，但通常在有限的标注数据和从头训练的组件上面临困难。本研究专注于全监督可供性基础，通过提出AffordanceSAM克服其局限性，扩展了SAM在分割中的泛化能力到可供性基础。具体而言，我们设计了一个可供性适应模块，并策划了一个名为C2F-Aff的粗到细标注数据集，以三阶段训练方式彻底转移SAM的强大性能到可供性上。实验结果确认AffordanceSAM在AGD20K基准上实现了最先进的（SOTA）性能，并展现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a generalized affordance grounding model that can effectively identify actionable regions on objects for practical applications. The authors propose a fully supervised method called AffordanceSAM, which enhances the segmentation capabilities of the existing SAM model by incorporating an affordance-adaption module and utilizing a newly curated dataset named C2F-Aff. Experimental results demonstrate that AffordanceSAM achieves state-of-the-art performance on the AGD20K benchmark and shows significant generalization ability in affordance grounding tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个通用的可供性基础模型，能够有效识别物体上的可操作区域，这对实际应用至关重要。作者提出了一种名为AffordanceSAM的全监督方法，通过引入可供性适应模块并利用新创建的C2F-Aff数据集，增强了现有SAM模型的分割能力。实验结果表明，AffordanceSAM在AGD20K基准测试中达到了最先进的性能，并显示出显著的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</div>
<div class="meta-line">Authors: Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma</div>
<div class="meta-line">First: 2025-08-08T10:39:04+00:00 · Latest: 2025-08-16T13:00:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06206v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.06206v3">PDF</a> · <a href="https://github.com/hq-King/Affordance-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on https://github.com/hq-King/Affordance-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Affordance-R1：用于多模态大语言模型的可推广性可供性推理的强化学习</div>
<div class="mono" style="margin-top:8px">可供性基础关注于预测与机器人执行的动作相关的物体特定区域。它在人与机器人交互、人与物体交互、具身操作和具身感知等领域中发挥着至关重要的作用。现有模型往往忽视不同物体之间共享的可供性，因为它们缺乏链式思维（CoT）推理能力，限制了它们的领域外（OOD）泛化和显式推理能力。为了解决这些挑战，我们提出了Affordance-R1，这是第一个统一的可供性基础框架，结合了认知CoT引导的群体相对策略优化（GRPO）与强化学习范式。具体而言，我们设计了一个复杂的可供性函数，其中包含格式、感知和认知奖励，以有效指导优化方向。此外，我们构建了一个高质量的以可供性为中心的推理数据集ReasonAff，以支持训练。Affordance-R1仅通过GRPO进行强化学习训练，而没有显式推理数据，实现了强大的零-shot泛化，并展现出突出的测试时推理能力。全面的实验表明，我们的模型优于成熟的方法，并展现出开放世界泛化。根据我们所知，Affordance-R1是第一个将基于GRPO的强化学习与推理结合到可供性推理中的模型。我们的方法代码和数据集已在https://github.com/hq-King/Affordance-R1上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance affordance grounding, which is crucial for effective human-robot interaction and manipulation, by addressing the limitations of existing models in generalization and reasoning. The authors propose Affordance-R1, a novel framework that combines cognitive Chain-of-Thought guided Group Relative Policy Optimization within a reinforcement learning approach, along with a specially designed affordance function that incorporates various rewards. Experimental results indicate that Affordance-R1 achieves strong zero-shot generalization and demonstrates advanced reasoning capabilities at test time, outperforming established methods and showcasing its potential for open-world applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有模型在泛化和推理方面的局限性，来改善对人机交互等相关领域至关重要的可供性基础。作者提出了Affordance-R1，这是一种结合了认知链式思维引导的群体相对策略优化的强化学习新框架，并开发了专门的可供性函数以优化学习。实验结果表明，Affordance-R1实现了强大的零样本泛化，并在测试中展示了先进的推理能力，超越了现有的方法，并实现了开放世界的泛化。</div>
</details>
</div>
<div class="card">
<div class="title">Selective Contrastive Learning for Weakly Supervised Affordance Grounding</div>
<div class="meta-line">Authors: WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-11T11:49:37+00:00 · Latest: 2025-08-11T11:49:37+00:00</div>
<div class="meta-line">Comments: Accepted to ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07877v1">Abs</a> · <a href="https://arxiv.org/pdf/2508.07877v1">PDF</a> · <a href="http://github.com/hynnsk/SelectiveCL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Facilitating an entity&#x27;s interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弱监督可用性定位的选择性对比学习</div>
<div class="mono" style="margin-top:8px">促进实体与物体的交互需要准确识别能够支持特定动作的部分。弱监督可用性定位（WSAG）旨在模仿人类从第三方示范中学习的过程，人类直观地掌握功能部分，而无需像素级注释。为此，定位通常通过在不同视角的图像中使用共享分类器以及结合部分发现过程的蒸馏策略来学习。然而，由于与可用性相关的部分并不总是容易区分，模型主要依赖分类，往往关注与可用性无关的常见类别特定模式。为了解决这一局限性，我们通过引入选择性原型和像素对比目标，超越孤立的部分级学习，适应性地在部分和物体级别学习与可用性相关的线索，具体取决于可用信息的粒度。最初，我们利用CLIP在自我中心（以物体为中心）和外部中心（第三方示例）图像中找到与动作相关的物体。然后，通过交叉参考互补视角中发现的物体，我们挖掘每个视角中的精确部分级可用性线索。通过持续学习区分与可用性相关的区域和与可用性无关的背景上下文，我们的方法有效地将激活从无关区域转移到有意义的可用性线索。实验结果证明了我们方法的有效性。代码可在github.com/hynnsk/SelectiveCL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve weakly supervised affordance grounding (WSAG) by accurately identifying parts of objects that facilitate specific actions, mimicking human learning from third-person demonstrations without requiring detailed annotations. The authors propose a method that employs selective prototypical and pixel contrastive objectives to adaptively learn affordance-relevant cues at both part and object levels, utilizing CLIP to identify action-associated objects in egocentric and exocentric images. The experimental results indicate that the proposed approach effectively enhances the distinction between affordance-relevant regions and irrelevant background, demonstrating improved performance in grounding tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过准确识别促进特定动作的物体部分来改善弱监督的可供性定位（WSAG），模仿人类学习而无需详细注释。作者提出了一种方法，采用选择性原型和像素对比目标，适应性地学习物体和部分层面的可供性相关线索，利用CLIP识别不同视角中的与动作相关的物体。实验结果表明，该方法成功增强了可供性相关区域与无关背景之间的区分，从而提高了可供性定位任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping</div>
<div class="meta-line">Authors: Dongming Wu, Yanping Fu, Saike Huang, Yingfei Liu, Fan Jia, Nian Liu, Feng Dai, Tiancai Wang, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jianbing Shen</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-07-31T17:17:05+00:00 · Latest: 2025-07-31T17:17:05+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025. The code is at https://github.com/wudongming97/AffordanceNet</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.23734v1">Abs</a> · <a href="https://arxiv.org/pdf/2507.23734v1">PDF</a> · <a href="https://github.com/wudongming97/AffordanceNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General robotic grasping systems require accurate object affordance perception in diverse open-world scenarios following human instructions. However, current studies suffer from the problem of lacking reasoning-based large-scale affordance prediction data, leading to considerable concern about open-world effectiveness. To address this limitation, we build a large-scale grasping-oriented affordance segmentation benchmark with human-like instructions, named RAGNet. It contains 273k images, 180 categories, and 26k reasoning instructions. The images cover diverse embodied data domains, such as wild, robot, ego-centric, and even simulation data. They are carefully annotated with an affordance map, while the difficulty of language instructions is largely increased by removing their category name and only providing functional descriptions. Furthermore, we propose a comprehensive affordance-based grasping framework, named AffordanceNet, which consists of a VLM pre-trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the target. Extensive experiments on affordance segmentation benchmarks and real-robot manipulation tasks show that our model has a powerful open-world generalization ability. Our data and code is available at https://github.com/wudongming97/AffordanceNet.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAGNet：面向通用抓取的大规模基于推理的可用性分割基准</div>
<div class="mono" style="margin-top:8px">通用机器人抓取系统需要在遵循人类指令的多样化开放世界场景中准确感知物体的可用性。然而，目前的研究存在缺乏基于推理的大规模可用性预测数据的问题，这引发了对开放世界有效性的重大担忧。为了解决这一限制，我们构建了一个以抓取为导向的大规模可用性分割基准，命名为RAGNet。它包含273k张图像、180个类别和26k条推理指令。这些图像覆盖了多样化的具身数据领域，如野外、机器人、自我中心和甚至模拟数据。它们经过精心注释，附有可用性图，同时通过去除类别名称并仅提供功能描述，显著增加了语言指令的难度。此外，我们提出了一个综合的基于可用性的抓取框架，命名为AffordanceNet，该框架由在我们的大规模可用性数据上预训练的VLM和一个条件化可用性图以抓取目标的抓取网络组成。在可用性分割基准和真实机器人操作任务上的广泛实验表明，我们的模型具有强大的开放世界泛化能力。我们的数据和代码可在https://github.com/wudongming97/AffordanceNet获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance robotic grasping systems by improving object affordance perception in varied open-world scenarios, addressing the lack of reasoning-based large-scale affordance prediction data. The authors developed RAGNet, a large-scale benchmark for grasping-oriented affordance segmentation that includes 273,000 images across 180 categories and 26,000 reasoning instructions, with images sourced from diverse domains and annotated with affordance maps. Experimental results demonstrate that the proposed AffordanceNet framework, which integrates a vision-language model pre-trained on this extensive dataset, exhibits strong generalization capabilities in both affordance segmentation benchmarks and real-robot manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过增强机器人抓取系统在多样化开放世界场景中基于人类指令感知物体可用性的能力来改善其性能。为了解决缺乏基于推理的大规模可用性预测数据的问题，作者开发了RAGNet，这是一个全面的基准，包含273,000张图像、180个类别和26,000条推理指令，图像来自多种领域，并附有可用性图的注释。所提出的AffordanceNet框架结合了在这一庞大数据集上预训练的视觉-语言模型和利用可用性图的抓取网络，通过在分割基准和真实机器人操作任务上的广泛实验，展示了强大的开放世界泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes</div>
<div class="meta-line">Authors: Wanjun Jia, Fan Yang, Mengfei Duan, Xianchi Chen, Yinxi Wang, Yiming Jiang, Wenrui Chen, Kailun Yang, Zhiyong Li</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2025-03-03T01:34:56+00:00 · Latest: 2025-07-22T08:51:33+00:00</div>
<div class="meta-line">Comments: Accepted to IROS 2025. Source code and benchmark dataset will be publicly available at https://github.com/Dikay1/OS-AGDO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.01092v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.01092v2">PDF</a> · <a href="https://github.com/Dikay1/OS-AGDO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deformable object manipulation in robotics presents significant challenges due to uncertainties in component properties, diverse configurations, visual interference, and ambiguous prompts. These factors complicate both perception and control tasks. To address these challenges, we propose a novel method for One-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric organizing scenes, enabling robots to recognize previously unseen deformable objects with varying colors and shapes using minimal samples. Specifically, we first introduce the Deformable Object Semantic Enhancement Module (DefoSEM), which enhances hierarchical understanding of the internal structure and improves the ability to accurately identify local features, even under conditions of weak component information. Next, we propose the ORB-Enhanced Keypoint Fusion Module (OEKFM), which optimizes feature extraction of key components by leveraging geometric constraints and improves adaptability to diversity and visual interference. Additionally, we propose an instance-conditional prompt based on image data and task context, which effectively mitigates the issue of region ambiguity caused by prompt words. To validate these methods, we construct a diverse real-world dataset, AGDDO15, which includes 15 common types of deformable objects and their associated organizational actions. Experimental results demonstrate that our approach significantly outperforms state-of-the-art methods, achieving improvements of 6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while exhibiting high generalization performance. Source code and benchmark dataset are made publicly available at https://github.com/Dikay1/OS-AGDO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我中心组织场景中可变形物体的一次性可用性定位</div>
<div class="mono" style="margin-top:8px">机器人中的可变形物体操作由于组件属性的不确定性、多样的配置、视觉干扰和模糊提示而面临重大挑战。这些因素使感知和控制任务变得复杂。为了解决这些挑战，我们提出了一种新方法，即在自我中心组织场景中进行可变形物体的一次性可用性定位（OS-AGDO），使机器人能够使用最少的样本识别以前未见过的具有不同颜色和形状的可变形物体。具体而言，我们首先引入了可变形物体语义增强模块（DefoSEM），该模块增强了对内部结构的层次理解，并提高了在弱组件信息条件下准确识别局部特征的能力。接下来，我们提出了ORB增强关键点融合模块（OEKFM），通过利用几何约束优化关键组件的特征提取，并提高对多样性和视觉干扰的适应性。此外，我们提出了一种基于图像数据和任务上下文的实例条件提示，有效缓解了提示词引起的区域模糊问题。为了验证这些方法，我们构建了一个多样化的真实世界数据集AGDDO15，其中包括15种常见的可变形物体及其相关的组织动作。实验结果表明，我们的方法显著优于最先进的方法，在KLD、SIM和NSS指标上分别提高了6.2%、3.2%和2.9%，同时表现出高泛化性能。源代码和基准数据集将在https://github.com/Dikay1/OS-AGDO公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The manipulation of deformable objects in robotics is complicated by uncertainties in their properties and configurations, as well as visual interference and ambiguous prompts. To tackle these issues, the authors introduce a novel method called One-Shot Affordance Grounding of Deformable Objects (OS-AGDO), which utilizes a Deformable Object Semantic Enhancement Module (DefoSEM) for improved hierarchical understanding and an ORB-Enhanced Keypoint Fusion Module (OEKFM) for optimized feature extraction. Experimental validation on a newly created dataset, AGDDO15, shows that OS-AGDO outperforms existing methods, achieving significant improvements in KLD, SIM, and NSS metrics, while demonstrating strong generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器人操作可变形物体时面临的挑战，这些挑战由于物体属性的不确定性和视觉干扰而变得复杂。作者提出了一种名为可变形物体的一次性赋能基础（OS-AGDO）的方法，其中包括可变形物体语义增强模块（DefoSEM），用于改善特征识别，以及基于ORB的关键点融合模块（OEKFM），用于优化特征提取。实验结果表明，OS-AGDO显著优于现有方法，在KLD、SIM和NSS指标上分别提高了6.2%、3.2%和2.9%，同时展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Resource-Efficient Affordance Grounding with Complementary Depth and Semantic Prompts</div>
<div class="meta-line">Authors: Yizhou Huang, Fan Yang, Guoliang Zhu, Gen Li, Hao Shi, Yukun Zuo, Wenrui Chen, Zhiyong Li, Kailun Yang</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2025-03-04T13:20:42+00:00 · Latest: 2025-07-19T15:21:11+00:00</div>
<div class="meta-line">Comments: Accepted to IROS 2025. The source code will be made publicly available at https://github.com/DAWDSE/BiT-Align</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.02600v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.02600v2">PDF</a> · <a href="https://github.com/DAWDSE/BiT-Align">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance refers to the functional properties that an agent perceives and utilizes from its environment, and is key perceptual information required for robots to perform actions. This information is rich and multimodal in nature. Existing multimodal affordance methods face limitations in extracting useful information, mainly due to simple structural designs, basic fusion methods, and large model parameters, making it difficult to meet the performance requirements for practical deployment. To address these issues, this paper proposes the BiT-Align image-depth-text affordance mapping framework. The framework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance (TFG) attention selection mechanism. BPM integrates the auxiliary modality depth image directly as a prompt to the primary modality RGB image, embedding it into the primary modality encoder without introducing additional encoders. This reduces the model&#x27;s parameter count and effectively improves functional region localization accuracy. The TFG mechanism guides the selection and enhancement of attention heads in the image encoder using textual features, improving the understanding of affordance characteristics. Experimental results demonstrate that the proposed method achieves significant performance improvements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset, compared with the current state-of-the-art method, we achieve a 6.0% improvement in the KLD metric, while reducing model parameters by 88.8%, demonstrating practical application values. The source code will be made publicly available at https://github.com/DAWDSE/BiT-Align.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>资源高效的可供性定位与互补深度和语义提示</div>
<div class="mono" style="margin-top:8px">可供性是指代理从环境中感知和利用的功能属性，是机器人执行动作所需的关键感知信息。这些信息本质上丰富且多模态。现有的多模态可供性方法在提取有用信息方面面临限制，主要由于简单的结构设计、基本的融合方法和庞大的模型参数，使得难以满足实际部署的性能要求。为了解决这些问题，本文提出了BiT-Align图像-深度-文本可供性映射框架。该框架包括一个旁路提示模块（BPM）和一个文本特征引导（TFG）注意力选择机制。BPM将辅助模态深度图像直接作为提示集成到主要模态RGB图像中，将其嵌入到主要模态编码器中，而不引入额外的编码器。这减少了模型的参数数量，并有效提高了功能区域定位的准确性。TFG机制利用文本特征引导图像编码器中注意力头的选择和增强，提高了对可供性特征的理解。实验结果表明，所提出的方法在公共AGD20K和HICO-IIF数据集上实现了显著的性能提升。在AGD20K数据集上，与当前最先进的方法相比，我们在KLD指标上提高了6.0%，同时减少了88.8%的模型参数，展示了实际应用价值。源代码将公开发布在https://github.com/DAWDSE/BiT-Align。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the extraction of multimodal affordance information for robots, which is crucial for their action performance but often limited by existing methods due to structural and parameter inefficiencies. The authors propose the BiT-Align framework, which incorporates a Bypass Prompt Module that integrates depth images directly into the RGB image encoder, and a Text Feature Guidance mechanism to optimize attention selection in the image encoder. Experimental results on the AGD20K dataset show a 6.0% improvement in the KLD metric compared to the state-of-the-art, while also reducing model parameters by 88.8%, indicating significant advancements in both efficiency and effectiveness for practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强机器人对多模态可供性信息的提取，这对其执行动作至关重要，但现有方法由于结构和参数效率低下而受到限制。作者提出了BiT-Align框架，该框架结合了旁路提示模块（BPM），将深度图像直接集成到RGB图像编码器中，以及文本特征引导（TFG）机制，以基于文本特征优化注意力选择。实验结果表明，该方法在AGD20K和HICO-IIF数据集上显著提高了性能，在KLD指标上提高了6.0%，同时减少了88.8%的模型参数，表明其实际应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding</div>
<div class="meta-line">Authors: Zhou Chen, Joe Lin, Sathyanarayanan N. Aakur</div>
<div class="meta-line">First: 2025-07-19T01:06:29+00:00 · Latest: 2025-07-19T01:06:29+00:00</div>
<div class="meta-line">Comments: Accepted to NeSy 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14426v1">Abs</a> · <a href="https://arxiv.org/pdf/2507.14426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce CRAFT, a neuro-symbolic framework for interpretable affordance grounding, which identifies the objects in a scene that enable a given action (e.g., &quot;cut&quot;). CRAFT integrates structured commonsense priors from ConceptNet and language models with visual evidence from CLIP, using an energy-based reasoning loop to refine predictions iteratively. This process yields transparent, goal-driven decisions to ground symbolic and perceptual structures. Experiments in multi-object, label-free settings demonstrate that CRAFT enhances accuracy while improving interpretability, providing a step toward robust and trustworthy scene understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：一种用于视觉功能赋能基础的神经符号框架</div>
<div class="mono" style="margin-top:8px">我们介绍CRAFT，一种用于可解释赋能基础的神经符号框架，它识别场景中能够实现特定动作（例如“切割”）的物体。CRAFT将来自ConceptNet的结构化常识先验和语言模型与来自CLIP的视觉证据相结合，使用基于能量的推理循环迭代地优化预测。这个过程产生透明的、目标驱动的决策，以基础符号和感知结构。多物体、无标签设置中的实验表明，CRAFT在提高准确性的同时改善了可解释性，为稳健和可信的场景理解迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to develop a framework that enhances the interpretability of affordance grounding in visual scenes. The authors introduce CRAFT, a neuro-symbolic framework that combines structured commonsense knowledge from ConceptNet and language models with visual evidence from CLIP, employing an energy-based reasoning loop for iterative prediction refinement. Experimental results show that CRAFT improves accuracy and interpretability in multi-object, label-free settings, contributing to more robust and trustworthy scene understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视觉场景中功能赋予的可解释性和准确性，特别是识别能够实现特定动作的物体。作者开发了CRAFT，这是一个神经符号框架，结合了来自ConceptNet的常识知识和语言模型与CLIP的视觉数据，利用基于能量的推理循环进行迭代预测优化。多物体、无标签环境中的实验结果表明，CRAFT显著提高了准确性和可解释性，为更稳健的场景理解做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</div>
<div class="meta-line">Authors: Peiran Xu, Yadong Mu</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-05-30T01:12:39+00:00 · Latest: 2025-05-30T01:12:39+00:00</div>
<div class="meta-line">Comments: ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24103v1">Abs</a> · <a href="https://arxiv.org/pdf/2505.24103v1">PDF</a> · <a href="https://github.com/woyut/WSAG-PLSP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于部分级语义先验的弱监督可用性定位</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们专注于弱监督可用性定位任务，模型通过人机交互图像和自我中心物体图像在没有密集标签的情况下识别物体上的可用性区域。之前的工作大多基于类激活图，这对于语义分割有效，但可能不适合定位动作和功能。利用最近的先进基础模型，我们开发了一种基于伪标签的监督训练流程。伪标签是通过现成的部分分割模型生成的，受可用性到部分名称的映射指导。此外，我们为基线模型引入了三个关键增强：标签精炼阶段、细粒度特征对齐过程和轻量级推理模块。这些技术利用嵌入在现成基础模型中的静态物体的语义知识来改善可用性学习，有效弥合物体与动作之间的差距。大量实验表明，所提模型的性能在现有方法上取得了突破性进展。我们的代码可在 https://github.com/woyut/WSAG-PLSP 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of weakly supervised affordance grounding, aiming to identify affordance regions on objects using limited human-object interaction and egocentric images. The authors propose a supervised training pipeline that utilizes pseudo labels generated from a part segmentation model, guided by a mapping from affordance to part names. Key enhancements to the model include a label refining stage, fine-grained feature alignment, and a lightweight reasoning module, which collectively improve the model&#x27;s performance in linking objects to their actions, resulting in significant advancements over existing methods in extensive experiments.</div>
<div class="mono" style="margin-top:8px">本研究解决了弱监督可供性定位的挑战，旨在利用有限的人物-物体交互和自我中心图像准确识别物体上的可供性区域。作者提出了一种新颖的监督训练流程，利用从部件分割模型生成的伪标签，并通过可供性与部件名称的映射进行指导。模型的关键增强包括标签精炼阶段、细粒度特征对齐和轻量级推理模块，这些共同利用基础模型中的语义知识改善可供性学习。实验结果表明，所提出的方法在该领域显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation</div>
<div class="meta-line">Authors: Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, Xuelong Li</div>
<div class="meta-line">First: 2025-03-14T04:47:38+00:00 · Latest: 2025-03-14T04:47:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.11081v1">Abs</a> · <a href="https://arxiv.org/pdf/2503.11081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://momakitchen.github.io/}{https://momakitchen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, NavAff, for navigation affordance grounding that demonstrates promising performance on the MoMa-Kitchen benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI. Project page: \href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoMa-Kitchen：一个超过10万的基准数据集，用于移动操控中的基于可用性的最后一公里导航</div>
<div class="mono" style="margin-top:8px">在移动操控中，导航和操控通常被视为独立的问题，这导致了仅仅接近物体与有效互动之间的显著差距。许多导航方法主要通过与目标的接近度来定义成功，往往忽视了促进后续操控所需的最佳定位。为了解决这个问题，我们引入了MoMa-Kitchen，一个包含超过10万样本的基准数据集，为模型提供训练数据，以学习无缝过渡到操控的最佳最终导航位置。我们的数据集包括从多样化厨房环境中收集的基于可用性的地面标签，其中不同型号的机器人移动操控器试图在杂乱中抓取目标物体。通过完全自动化的流程，我们模拟了多种真实场景，并生成了最佳操控位置的可用性标签。视觉数据来自于安装在机器人手臂上的第一人称视角相机捕获的RGB-D输入，确保在数据收集过程中的视角一致性。我们还开发了一个轻量级基线模型NavAff，用于导航可用性基础，展示了在MoMa-Kitchen基准上的良好性能。我们的方法使模型能够学习基于可用性的最终定位，适应不同的手臂类型和平台高度，从而为在具身AI中更强大和可推广的导航与操控整合铺平道路。项目页面：\href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to bridge the gap between navigation and manipulation in mobile manipulation tasks, as traditional methods often fail to ensure optimal positioning for effective object engagement. The authors introduce MoMa-Kitchen, a benchmark dataset with over 100,000 samples that provide training data for models to learn optimal navigation positions that facilitate seamless transitions to manipulation. Key findings include the development of a lightweight baseline model, NavAff, which shows promising performance in grounding navigation affordances, thus enabling models to adapt to various robotic arm types and platform heights, ultimately enhancing the integration of navigation and manipulation in embodied AI.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于弥合移动操控任务中导航与操控之间的差距，因为传统方法通常将这两者视为独立问题，导致物体交互效率低下。作者提出了MoMa-Kitchen，一个包含超过10万个样本的基准数据集，旨在训练模型以获得优化的最终导航位置，从而促进有效的操控。关键实验结果表明，他们的轻量级基线模型NavAff在MoMa-Kitchen基准上取得了良好的结果，展示了基于可用性的定位在增强机器人系统中导航与操控整合的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding</div>
<div class="meta-line">Authors: Ji Ha Jang, Hoigi Seo, Se Young Chun</div>
<div class="meta-line">First: 2024-09-10T04:31:51+00:00 · Latest: 2024-09-10T04:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.06210v1">Abs</a> · <a href="https://arxiv.org/pdf/2409.06210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>INTRA：交互关系感知的弱监督可用性定位</div>
<div class="mono" style="margin-top:8px">可用性指的是物体固有的潜在交互。对可用性的感知可以使智能体高效地导航和与新环境互动。弱监督可用性定位在没有昂贵的像素级注释的情况下，通过外部图像教会智能体可用性的概念。尽管最近在弱监督可用性定位方面取得了可喜的成果，但仍然存在一些挑战，包括对配对的外部和自我中心图像数据集的需求，以及为单个物体定位多样化可用性的复杂性。为了解决这些问题，我们提出了交互关系感知的弱监督可用性定位（INTRA）。与之前的研究不同，INTRA将此问题重新表述为表示学习，通过仅使用外部图像的对比学习来识别交互的独特特征，从而消除了对配对数据集的需求。此外，我们利用视觉-语言模型嵌入灵活地执行可用性定位，设计文本条件的可用性图生成，以反映对比学习的交互关系，并通过我们的文本同义词增强提高鲁棒性。我们的方法在AGD20K、IIT-AFF、CAD和UMD等多样化数据集上优于之前的研究。此外，实验结果表明，我们的方法在合成图像/插图方面具有显著的领域可扩展性，并能够为新颖的交互和物体执行可用性定位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the understanding of affordances, which are the potential interactions that objects enable, in order to enhance the navigation and interaction capabilities of intelligent agents in new environments. The authors propose a novel approach called INTRA, which utilizes representation learning and contrastive learning with only exocentric images to address the limitations of existing weakly supervised affordance grounding methods that require paired datasets. Experimental results show that INTRA outperforms previous methods on various datasets, including AGD20K, IIT-AFF, CAD, and UMD, and demonstrates significant scalability for synthesized images and the ability to ground affordances for new interactions and objects.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过增强智能体对物体可供性（affordance）的理解，提高其在新环境中的导航和交互效率，而无需昂贵的像素级注释。作者提出了一种新方法INTRA，该方法利用表征学习和对比学习，仅使用外观图像，从而消除了对配对数据集的需求。实验结果表明，INTRA在多个数据集上优于先前的方法，展示了对合成图像的显著领域可扩展性，以及对新交互和物体进行可供性定位的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</div>
<div class="meta-line">Authors: Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro</div>
<div class="meta-line">Venue: ECCV</div>
<div class="meta-line">First: 2024-09-03T11:54:36+00:00 · Latest: 2024-09-03T11:54:36+00:00</div>
<div class="meta-line">Comments: Paper accepted to Workshop on Assistive Computer Vision and Robotics (ACVR) in European Conference on Computer Vision (ECCV) 2024; 24 pages, 9 figures, 5 tables. Code and trained models are available at https://apicis.github.io/aff-seg/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.01814v1">Abs</a> · <a href="https://arxiv.org/pdf/2409.01814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://apicis.github.io/aff-seg/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物体可供性分割：可重复性和对尺度的敏感性</div>
<div class="mono" style="margin-top:8px">视觉可供性分割识别代理可以与之交互的物体图像区域。现有方法重用并调整基于学习的语义分割架构以适应可供性分割任务，并在小规模数据集上进行评估。然而，实验设置往往不可重复，从而导致不公平和不一致的比较。在本研究中，我们在两个单一物体场景下的可重复设置中对这些方法进行了基准测试，分别是无遮挡的桌面和手持容器，以促进未来的比较。我们包括了最近架构Mask2Former的一个版本，重新训练用于可供性分割，并显示该模型在两个场景的大多数测试集上表现最佳。我们的分析表明，当物体分辨率与训练集中的不同，模型对尺度变化并不稳健。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the reproducibility issues in visual affordance segmentation, which identifies image regions where an agent can interact with objects. The authors benchmark existing methods, including a re-trained version of the Mask2Former architecture, under a reproducible setup across two scenarios: tabletop objects without occlusions and hand-held containers. The key findings indicate that the Mask2Former model outperforms others in most testing sets, but the analysis reveals that these models exhibit sensitivity to scale variations when the object resolutions differ from those used during training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有视觉可供性分割方法缺乏可重复性的问题，这妨碍了研究之间的公平比较。作者在可重复的设置下对这些方法进行了基准测试，使用了两个场景：没有遮挡的桌面物体和手持容器，同时还专门为可供性分割重新训练了Mask2Former架构。主要发现表明，Mask2Former在大多数测试集上优于其他模型，但分析显示这些模型对尺度变化敏感，尤其是在物体分辨率与训练数据集中的不同。</div>
</details>
</div>
<div class="card">
<div class="title">WorldAfford: Affordance Grounding based on Natural Language Instructions</div>
<div class="meta-line">Authors: Changmao Chen, Yuren Cong, Zhen Kan</div>
<div class="meta-line">First: 2024-05-21T02:37:45+00:00 · Latest: 2024-05-21T02:37:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.12461v1">Abs</a> · <a href="https://arxiv.org/pdf/2405.12461v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance grounding aims to localize the interaction regions for the manipulated objects in the scene image according to given instructions. A critical challenge in affordance grounding is that the embodied agent should understand human instructions and analyze which tools in the environment can be used, as well as how to use these tools to accomplish the instructions. Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing to capture complex human objectives. Moreover, these approaches typically identify affordance regions of only a single object in object-centric images, ignoring the object context and struggling to localize affordance regions of multiple objects in complex scenes for practical applications. To address this concern, for the first time, we introduce a new task of affordance grounding based on natural language instructions, extending it from previously using simple labels for complex human instructions. For this new task, we propose a new framework, WorldAfford. We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically. Subsequently, we use SAM and CLIP to localize the objects related to the affordance knowledge in the image. We identify the affordance regions of the objects through an affordance region localization module. To benchmark this new task and validate our framework, an affordance grounding dataset, LLMaFF, is constructed. We conduct extensive experiments to verify that WorldAfford performs state-of-the-art on both the previous AGD20K and the new LLMaFF dataset. In particular, WorldAfford can localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given instruction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldAfford：基于自然语言指令的可供性定位</div>
<div class="mono" style="margin-top:8px">可供性定位旨在根据给定指令定位场景图像中被操控物体的交互区域。可供性定位的一个关键挑战是，具身代理需要理解人类指令并分析环境中可以使用的工具，以及如何使用这些工具来完成指令。最近的研究主要支持简单的动作标签作为定位可供性区域的输入指令，未能捕捉复杂的人类目标。此外，这些方法通常仅识别物体中心图像中单个物体的可供性区域，忽略物体上下文，并在复杂场景中定位多个物体的可供性区域时遇到困难。为了解决这个问题，我们首次引入基于自然语言指令的可供性定位新任务，将其从之前使用简单标签扩展到复杂的人类指令。针对这一新任务，我们提出了一个新框架WorldAfford。我们设计了一种新颖的可供性推理链式思维提示，以更精确和逻辑地推理来自大型语言模型的可供性知识。随后，我们使用SAM和CLIP定位与图像中可供性知识相关的物体。我们通过可供性区域定位模块识别物体的可供性区域。为了基准测试这一新任务并验证我们的框架，构建了一个可供性定位数据集LLMaFF。我们进行了广泛的实验，验证WorldAfford在之前的AGD20K和新的LLMaFF数据集上表现出色。特别是，WorldAfford能够定位多个物体的可供性区域，并在环境中的物体无法完全匹配给定指令时提供替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance affordance grounding by enabling agents to understand complex human instructions and localize interaction regions for multiple objects in complex scenes. The authors introduce a novel framework called WorldAfford, which employs Affordance Reasoning Chain-of-Thought Prompting to leverage large language models for precise reasoning about affordance knowledge. Experimental results demonstrate that WorldAfford achieves state-of-the-art performance on both the AGD20K and the newly constructed LLMaFF dataset, successfully localizing affordance regions for multiple objects and addressing scenarios where environmental objects do not fully align with the instructions provided.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使智能体理解复杂的人类指令来增强可供性定位，从而定位场景中多个物体的交互区域。作者提出了一个新的框架WorldAfford，利用可供性推理链式思维提示，利用大型语言模型对可供性知识进行精确推理。实验结果表明，WorldAfford在AGD20K和新创建的LLMaFF数据集上均实现了最先进的性能，成功定位多个物体的可供性区域，并在物体与指令不完全匹配时提供替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">What does CLIP know about peeling a banana?</div>
<div class="meta-line">Authors: Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, Giuseppe Averta</div>
<div class="meta-line">First: 2024-04-18T09:06:05+00:00 · Latest: 2024-04-18T09:06:05+00:00</div>
<div class="meta-line">Comments: Accepted to MAR Workshop at CVPR2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.12015v1">Abs</a> · <a href="https://arxiv.org/pdf/2404.12015v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans show an innate capability to identify tools to support specific actions. The association between objects parts and the actions they facilitate is usually named affordance. Being able to segment objects parts depending on the tasks they afford is crucial to enable intelligent robots to use objects of daily living. Traditional supervised learning methods for affordance segmentation require costly pixel-level annotations, while weakly supervised approaches, though less demanding, still rely on object-interaction examples and support a closed set of actions. These limitations hinder scalability, may introduce biases, and usually restrict models to a limited set of predefined actions. This paper proposes AffordanceCLIP, to overcome these limitations by leveraging the implicit affordance knowledge embedded within large pre-trained Vision-Language models like CLIP. We experimentally demonstrate that CLIP, although not explicitly trained for affordances detection, retains valuable information for the task. Our AffordanceCLIP achieves competitive zero-shot performance compared to methods with specialized training, while offering several advantages: i) it works with any action prompt, not just a predefined set; ii) it requires training only a small number of additional parameters compared to existing solutions and iii) eliminates the need for direct supervision on action-object pairs, opening new perspectives for functionality-based reasoning of models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP对剥香蕉的了解是什么？</div>
<div class="mono" style="margin-top:8px">人类具有识别工具以支持特定动作的天生能力。物体部件与其促进的动作之间的关联通常被称为可供性。根据任务对物体部件进行分割对于使智能机器人能够使用日常生活中的物体至关重要。传统的可供性分割监督学习方法需要昂贵的像素级注释，而弱监督方法虽然要求较低，但仍依赖于物体交互示例，并支持有限的动作集合。这些限制阻碍了可扩展性，可能引入偏见，并通常将模型限制在一组预定义的动作中。本文提出了AffordanceCLIP，以利用大型预训练视觉-语言模型（如CLIP）中嵌入的隐式可供性知识来克服这些限制。我们通过实验表明，尽管CLIP并未明确针对可供性检测进行训练，但仍保留了该任务的有价值信息。我们的AffordanceCLIP在与经过专门训练的方法相比，取得了具有竞争力的零-shot性能，同时提供了几个优势：i）它可以与任何动作提示一起使用，而不仅仅是预定义集合；ii）与现有解决方案相比，仅需训练少量额外参数；iii）消除了对动作-物体对的直接监督需求，为基于功能的模型推理开辟了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of affordance segmentation in robotics, which is essential for enabling intelligent robots to interact with everyday objects. The authors propose a novel method called AffordanceCLIP that utilizes the implicit knowledge of affordances within large pre-trained Vision-Language models like CLIP, circumventing the need for extensive pixel-level annotations and predefined action sets. Experimental results show that AffordanceCLIP achieves competitive zero-shot performance compared to traditional methods, while requiring minimal additional training and allowing for flexibility in action prompts without direct supervision on action-object pairs.</div>
<div class="mono" style="margin-top:8px">本研究解决了赋能分割的挑战，这对于使智能机器人能够与日常物品互动至关重要。作者提出了AffordanceCLIP，这是一种利用大型预训练视觉-语言模型（如CLIP）中隐含的赋能知识的方法，避免了对广泛像素级注释和预定义动作集的需求。实验结果表明，AffordanceCLIP在零样本性能上与传统方法相比具有竞争力，同时允许任何动作提示，所需的额外训练参数极少，并消除了对动作-物体对的直接监督的需求。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Explainable Affordance Learning with Embodied Caption</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool</div>
<div class="meta-line">First: 2024-04-08T15:22:38+00:00 · Latest: 2024-04-08T15:22:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.05603v1">Abs</a> · <a href="https://arxiv.org/pdf/2404.05603v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the field of visual affordance learning, previous methods mainly used abundant images or videos that delineate human behavior patterns to identify action possibility regions for object manipulation, with a variety of applications in robotic tasks. However, they encounter a main challenge of action ambiguity, illustrated by the vagueness like whether to beat or carry a drum, and the complexities involved in processing intricate scenes. Moreover, it is important for human intervention to rectify robot errors in time. To address these issues, we introduce Self-Explainable Affordance learning (SEA) with embodied caption. This innovation enables robots to articulate their intentions and bridge the gap between explainable vision-language caption and visual affordance learning. Due to a lack of appropriate dataset, we unveil a pioneering dataset and metrics tailored for this task, which integrates images, heatmaps, and embodied captions. Furthermore, we propose a novel model to effectively combine affordance grounding with self-explanation in a simple but efficient manner. Extensive quantitative and qualitative experiments demonstrate our method&#x27;s effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自解释性可供性学习与具身描述</div>
<div class="mono" style="margin-top:8px">在视觉可供性学习领域，以往的方法主要使用大量描绘人类行为模式的图像或视频来识别物体操作的动作可能区域，广泛应用于机器人任务。然而，它们面临着动作模糊性这一主要挑战，例如是否敲打或搬运鼓，以及处理复杂场景所涉及的复杂性。此外，及时的人类干预对于纠正机器人错误至关重要。为了解决这些问题，我们引入了具身描述的自解释性可供性学习（SEA）。这一创新使机器人能够表达其意图，并弥合可解释的视觉-语言描述与视觉可供性学习之间的差距。由于缺乏合适的数据集，我们推出了一个开创性的数据集和针对该任务的指标，整合了图像、热图和具身描述。此外，我们提出了一种新颖的模型，以简单而高效的方式有效结合可供性基础与自我解释。大量定量和定性实验证明了我们方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve visual affordance learning for robotic tasks, particularly addressing the challenges of action ambiguity and the need for human intervention in error correction. The authors introduce Self-Explainable Affordance learning (SEA) with embodied caption, which allows robots to express their intentions and connect vision-language captions with visual affordance learning. They present a new dataset and metrics specifically designed for this task, and develop a model that integrates affordance grounding with self-explanation. Experimental results show that their method is effective in enhancing the understanding of action possibilities in complex scenes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器人任务中视觉可供性学习面临的行动模糊性和人类干预的需求。作者提出了一种名为自解释可供性学习（SEA）与具身描述的新方法，使机器人能够表达其意图，并将视觉-语言描述与视觉可供性学习相结合。他们引入了一个新的数据集和指标，包括图像、热图和具身描述，并开发了一种将可供性定位与自解释相结合的模型。实验结果表明，他们的方法在提高复杂场景中对行动可能性的理解方面是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">One-Shot Open Affordance Learning with Foundation Models</div>
<div class="meta-line">Authors: Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani</div>
<div class="meta-line">First: 2023-11-29T16:23:06+00:00 · Latest: 2023-11-29T16:23:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.17776v1">Abs</a> · <a href="https://arxiv.org/pdf/2311.17776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于基础模型的一次性开放可供性学习</div>
<div class="mono" style="margin-top:8px">我们介绍了一次性开放可供性学习（OOAL），该模型仅用每个基础对象类别的一个示例进行训练，但期望能够识别新颖的对象和可供性。尽管视觉-语言模型在识别新颖对象和场景方面表现出色，但它们往往难以理解更细微的层次，如可供性。为了解决这个问题，我们对现有基础模型进行了全面分析，以探索它们对可供性的内在理解，并评估数据有限的可供性学习的潜力。然后，我们提出了一个视觉-语言框架，采用简单有效的设计，增强视觉特征与可供性文本嵌入之间的对齐。在两个可供性分割基准上的实验表明，所提出的方法在使用不到1%的完整训练数据的情况下超越了最先进的模型，并在未见对象和可供性上表现出合理的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of teaching models to recognize affordances of novel objects with minimal training data. The authors introduce One-shot Open Affordance Learning (OOAL), a vision-language framework designed to enhance the alignment between visual features and affordance text embeddings using only one example per object category. Experimental results demonstrate that OOAL outperforms state-of-the-art models on two affordance segmentation benchmarks while utilizing less than 1% of the full training data and shows reasonable generalization to unseen objects and affordances.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉语言模型在理解可供性方面的挑战，这些模型通常擅长识别新物体，但在处理可供性等细节时表现不佳。作者提出了一种一次性开放可供性学习（OOAL）框架，该框架仅使用每个物体类别的一个示例来训练模型，以识别新物体及其可供性。实验结果表明，该方法在两个可供性分割基准测试中优于最先进的模型，同时利用的训练数据不到1%，并显示出对未见物体和可供性的良好泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Affordance and Situated Meaning in Image Captions: A Multimodal Analysis</div>
<div class="meta-line">Authors: Pin-Er Chen, Po-Ya Angela Wang, Hsin-Yu Chou, Yu-Hsiang Tseng, Shu-Kai Hsieh</div>
<div class="meta-line">First: 2023-05-24T01:30:50+00:00 · Latest: 2023-10-24T11:30:07+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.14616v2">Abs</a> · <a href="https://arxiv.org/pdf/2305.14616v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper explores the grounding issue regarding multimodal semantic representation from a computational cognitive-linguistic view. We annotate images from the Flickr30k dataset with five perceptual properties: Affordance, Perceptual Salience, Object Number, Gaze Cueing, and Ecological Niche Association (ENA), and examine their association with textual elements in the image captions. Our findings reveal that images with Gibsonian affordance show a higher frequency of captions containing &#x27;holding-verbs&#x27; and &#x27;container-nouns&#x27; compared to images displaying telic affordance. Perceptual Salience, Object Number, and ENA are also associated with the choice of linguistic expressions. Our study demonstrates that comprehensive understanding of objects or events requires cognitive attention, semantic nuances in language, and integration across multiple modalities. We highlight the vital importance of situated meaning and affordance grounding in natural language understanding, with the potential to advance human-like interpretation in various scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像标题中的可供性与情境意义探索：一种多模态分析</div>
<div class="mono" style="margin-top:8px">本文从计算认知语言学的角度探讨多模态语义表示的基础问题。我们对Flickr30k数据集中的图像进行了注释，标注了五种感知属性：可供性、感知显著性、物体数量、注视提示和生态位关联（ENA），并考察它们与图像标题中的文本元素的关联。我们的研究发现，具有吉布森可供性的图像，其标题中包含“持有动词”和“容器名词”的频率高于显示目的性可供性的图像。感知显著性、物体数量和ENA也与语言表达的选择相关。我们的研究表明，全面理解物体或事件需要认知注意、语言中的语义细微差别以及多模态的整合。我们强调了情境意义和可供性基础在自然语言理解中的重要性，具有在各种场景中推进类人解释的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the grounding issue of multimodal semantic representation by examining the relationship between visual properties and textual elements in image captions. Using the Flickr30k dataset, the authors annotated images with five perceptual properties and analyzed their correlation with the captions. The results indicate that images with Gibsonian affordance are more likely to have captions featuring &#x27;holding-verbs&#x27; and &#x27;container-nouns&#x27;, while other properties like Perceptual Salience and Object Number also influence linguistic choices, emphasizing the need for cognitive attention and multimodal integration in understanding natural language.</div>
<div class="mono" style="margin-top:8px">本研究探讨了多模态语义表示的基础问题，通过分析视觉属性与图像标题中文本元素之间的关系。作者对Flickr30k数据集中的图像进行了五种感知属性的标注，并分析了它们与标题语言的相关性。结果表明，表现出吉布森可供性（Gibsonian affordance）的图像更可能具有包含“持有动词”和“容器名词”的标题，而感知显著性、物体数量等其他属性也影响语言选择，强调了情境意义和可供性在自然语言理解中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">A large scale multi-view RGBD visual affordance learning dataset</div>
<div class="meta-line">Authors: Zeyad Khalifa, Syed Afaq Ali Shah</div>
<div class="meta-line">First: 2022-03-26T14:31:35+00:00 · Latest: 2023-09-13T01:18:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2203.14092v3">Abs</a> · <a href="https://arxiv.org/pdf/2203.14092v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/afaqshah/dataset">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The physical and textural attributes of objects have been widely studied for recognition, detection and segmentation tasks in computer vision.~A number of datasets, such as large scale ImageNet, have been proposed for feature learning using data hungry deep neural networks and for hand-crafted feature extraction. To intelligently interact with objects, robots and intelligent machines need the ability to infer beyond the traditional physical/textural attributes, and understand/learn visual cues, called visual affordances, for affordance recognition, detection and segmentation. To date there is no publicly available large dataset for visual affordance understanding and learning. In this paper, we introduce a large scale multi-view RGBD visual affordance learning dataset, a benchmark of 47210 RGBD images from 37 object categories, annotated with 15 visual affordance categories. To the best of our knowledge, this is the first ever and the largest multi-view RGBD visual affordance learning dataset. We benchmark the proposed dataset for affordance segmentation and recognition tasks using popular Vision Transformer and Convolutional Neural Networks. Several state-of-the-art deep learning networks are evaluated each for affordance recognition and segmentation tasks. Our experimental results showcase the challenging nature of the dataset and present definite prospects for new and robust affordance learning algorithms. The dataset is publicly available at https://sites.google.com/view/afaqshah/dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模多视角RGBD视觉可供性学习数据集</div>
<div class="mono" style="margin-top:8px">物体的物理和纹理属性在计算机视觉中的识别、检测和分割任务中得到了广泛研究。已经提出了许多数据集，如大规模ImageNet，用于使用数据密集型深度神经网络进行特征学习和手工特征提取。为了智能地与物体互动，机器人和智能机器需要超越传统的物理/纹理属性的推断能力，并理解/学习视觉线索，称为视觉可供性，以进行可供性识别、检测和分割。迄今为止，尚无公开可用的大型视觉可供性理解和学习数据集。在本文中，我们介绍了一个大规模多视角RGBD视觉可供性学习数据集，这是来自37个物体类别的47210张RGBD图像的基准，标注了15个视觉可供性类别。据我们所知，这是首个也是最大的多视角RGBD视觉可供性学习数据集。我们使用流行的视觉变换器和卷积神经网络对所提出的数据集进行可供性分割和识别任务的基准测试。评估了几种最先进的深度学习网络，以进行可供性识别和分割任务。我们的实验结果展示了数据集的挑战性，并为新的和强大的可供性学习算法提供了明确的前景。数据集可在https://sites.google.com/view/afaqshah/dataset公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for robots and intelligent machines to understand visual affordances for better interaction with objects, as existing datasets do not support this aspect of computer vision. The authors introduce a large-scale multi-view RGBD visual affordance learning dataset, comprising 47,210 RGBD images across 37 object categories, annotated with 15 visual affordance categories. Experimental evaluations using Vision Transformer and Convolutional Neural Networks demonstrate the dataset&#x27;s challenging nature and highlight its potential for advancing affordance recognition and segmentation algorithms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于机器人和智能机器需要理解视觉可供性，以便更好地与物体互动，因为现有的数据集无法充分支持这一领域。作者介绍了一个大规模的多视角RGBD视觉可供性学习数据集，包含47,210张RGBD图像，涵盖37个物体类别，并标注了15个视觉可供性类别。使用视觉变换器和卷积神经网络的实验评估展示了该数据集的复杂性及其在推动可供性识别和分割算法方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance segmentation of hand-occluded containers from exocentric images</div>
<div class="meta-line">Authors: Tommaso Apicella, Alessio Xompero, Edoardo Ragusa, Riccardo Berta, Andrea Cavallaro, Paolo Gastaldo</div>
<div class="meta-line">Venue: ICCV</div>
<div class="meta-line">First: 2023-08-22T07:14:29+00:00 · Latest: 2023-08-22T07:14:29+00:00</div>
<div class="meta-line">Comments: Paper accepted to Workshop on Assistive Computer Vision and Robotics (ACVR) in International Conference on Computer Vision (ICCV) 2023; 10 pages, 4 figures, 2 tables. Data, code, and trained models are available at https://apicis.github.io/projects/acanet.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2308.11233v1">Abs</a> · <a href="https://arxiv.org/pdf/2308.11233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://apicis.github.io/projects/acanet.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual affordance segmentation identifies the surfaces of an object an agent can interact with. Common challenges for the identification of affordances are the variety of the geometry and physical properties of these surfaces as well as occlusions. In this paper, we focus on occlusions of an object that is hand-held by a person manipulating it. To address this challenge, we propose an affordance segmentation model that uses auxiliary branches to process the object and hand regions separately. The proposed model learns affordance features under hand-occlusion by weighting the feature map through hand and object segmentation. To train the model, we annotated the visual affordances of an existing dataset with mixed-reality images of hand-held containers in third-person (exocentric) images. Experiments on both real and mixed-reality images show that our model achieves better affordance segmentation and generalisation than existing models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从外部视角图像中分割手遮挡容器的可用性</div>
<div class="mono" style="margin-top:8px">视觉可用性分割识别代理可以与之交互的物体表面。识别可用性面临的常见挑战包括这些表面的几何形状和物理属性的多样性以及遮挡。在本文中，我们专注于被人手持的物体的遮挡。为了解决这个挑战，我们提出了一种可用性分割模型，该模型使用辅助分支分别处理物体和手部区域。所提出的模型通过手和物体分割加权特征图，学习在手遮挡下的可用性特征。为了训练模型，我们对现有数据集进行了注释，使用第三人称（外部视角）图像的手持容器的混合现实图像。对真实和混合现实图像的实验表明，我们的模型在可用性分割和泛化方面优于现有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve visual affordance segmentation, particularly in scenarios where objects are occluded by hands during manipulation. The authors propose a novel segmentation model that utilizes auxiliary branches to separately process the object and hand regions, allowing the model to learn affordance features effectively under occlusion. Experimental results demonstrate that the proposed model outperforms existing methods in both real and mixed-reality images, achieving superior affordance segmentation and generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善视觉可供性分割，特别是在物体被手遮挡的操作场景中。作者提出了一种新颖的分割模型，利用辅助分支分别处理物体和手部区域，从而使模型能够有效地在遮挡情况下学习可供性特征。实验结果表明，所提出的模型在可供性分割和泛化方面优于现有方法，经过对真实和混合现实手持容器图像的测试验证。</div>
</details>
</div>
<div class="card">
<div class="title">Grounded Affordance from Exocentric View</div>
<div class="meta-line">Authors: Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao</div>
<div class="meta-line">First: 2022-08-28T10:32:47+00:00 · Latest: 2023-05-25T05:47:29+00:00</div>
<div class="meta-line">Comments: arXiv admin note: text overlap with arXiv:2203.09905</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2208.13196v2">Abs</a> · <a href="https://arxiv.org/pdf/2208.13196v2">PDF</a> · <a href="https://github.com/lhc1224/Cross-view-affordance-grounding">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance grounding aims to locate objects&#x27; &quot;action possibilities&quot; regions, which is an essential step toward embodied intelligence. Due to the diversity of interactive affordance, the uniqueness of different individuals leads to diverse interactions, which makes it difficult to establish an explicit link between object parts and affordance labels. Human has the ability that transforms the various exocentric interactions into invariant egocentric affordance to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. However, there is some &quot;interaction bias&quot; between personas, mainly regarding different regions and different views. To this end, we devise a cross-view affordance knowledge transfer framework that extracts affordance-specific features from exocentric interactions and transfers them to the egocentric view. Specifically, the perception of affordance regions is enhanced by preserving affordance co-relations. In addition, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from $36$ affordance categories. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality. Code is released at https://github.com/lhc1224/Cross-view-affordance-grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>外部视角的赋能基础</div>
<div class="mono" style="margin-top:8px">赋能基础旨在定位物体的“行动可能性”区域，这是实现具身智能的重要步骤。由于交互赋能的多样性，不同个体的独特性导致多样化的交互，这使得在物体部分与赋能标签之间建立明确的联系变得困难。人类具备将各种外部交互转化为不变的自我中心赋能的能力，以应对交互多样性的影响。为了赋予代理这种能力，本文提出了一项从外部视角进行赋能基础的任务，即在给定外部人-物交互和自我中心物体图像的情况下，学习物体的赋能知识，并仅使用赋能标签作为监督将其转移到自我中心图像。然而，个体之间存在一些“交互偏差”，主要涉及不同区域和不同视角。为此，我们设计了一个跨视角赋能知识转移框架，从外部交互中提取赋能特征并将其转移到自我中心视角。具体而言，通过保留赋能相关性来增强对赋能区域的感知。此外，构建了一个名为AGD20K的赋能基础数据集，通过收集和标注超过2万张来自36个赋能类别的图像。实验结果表明，我们的方法在客观指标和视觉质量方面优于代表性模型。代码已发布在https://github.com/lhc1224/Cross-view-affordance-grounding。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance embodied intelligence by effectively grounding affordances, which are the action possibilities of objects, despite the challenges posed by diverse human interactions. The authors propose a novel method that involves a cross-view affordance knowledge transfer framework, which learns to map affordance knowledge from exocentric human-object interactions to egocentric object images using only affordance labels for supervision. Experimental results indicate that this approach significantly outperforms existing models in terms of both objective metrics and visual quality, supported by a newly constructed dataset, AGD20K, containing over 20,000 labeled images across 36 affordance categories.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效地将可供性（物体的行动可能性）与物体位置关联，从而增强具身智能，尽管多样化的人类互动带来了挑战。作者提出了一种方法，通过从外部视角的人-物互动中学习可供性知识，并利用可供性标签进行监督，将其转移到自我中心图像中。关键实验结果表明，他们的跨视角可供性知识转移框架显著改善了可供性区域的感知，并在客观指标和视觉质量上超越了现有模型，同时支持了一个新构建的数据集AGD20K，该数据集包含超过20,000张标记图像，涵盖36个可供性类别。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance Grounding from Demonstration Video to Target Image</div>
<div class="meta-line">Authors: Joya Chen, Difei Gao, Kevin Qinghong Lin, Mike Zheng Shou</div>
<div class="meta-line">Venue: CVPR 2023</div>
<div class="meta-line">First: 2023-03-26T07:02:41+00:00 · Latest: 2023-03-26T07:02:41+00:00</div>
<div class="meta-line">Comments: CVPR 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.14644v1">Abs</a> · <a href="https://arxiv.org/pdf/2303.14644v1">PDF</a> · <a href="https://github.com/showlab/afformer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans excel at learning from expert demonstrations and solving their own problems. To equip intelligent robots and assistants, such as AR glasses, with this ability, it is essential to ground human hand interactions (i.e., affordances) from demonstration videos and apply them to a target image like a user&#x27;s AR glass view. The video-to-image affordance grounding task is challenging due to (1) the need to predict fine-grained affordances, and (2) the limited training data, which inadequately covers video-image discrepancies and negatively impacts grounding. To tackle them, we propose Affordance Transformer (Afformer), which has a fine-grained transformer-based decoder that gradually refines affordance grounding. Moreover, we introduce Mask Affordance Hand (MaskAHand), a self-supervised pre-training technique for synthesizing video-image data and simulating context changes, enhancing affordance grounding across video-image discrepancies. Afformer with MaskAHand pre-training achieves state-of-the-art performance on multiple benchmarks, including a substantial 37% improvement on the OPRA dataset. Code is made available at https://github.com/showlab/afformer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从演示视频到目标图像的可供性基础</div>
<div class="mono" style="margin-top:8px">人类擅长从专家演示中学习并解决自己的问题。为了使智能机器人和助手（如AR眼镜）具备这种能力，必须从演示视频中将人类手部交互（即可供性）基础化，并将其应用于用户的AR眼镜视图等目标图像。视频到图像的可供性基础任务具有挑战性，原因在于（1）需要预测细粒度的可供性，以及（2）有限的训练数据不足以覆盖视频-图像之间的差异，负面影响基础化。为了解决这些问题，我们提出了可供性变换器（Afformer），它具有基于变换器的细粒度解码器，逐步精炼可供性基础。此外，我们引入了掩码可供性手（MaskAHand），这是一种自监督预训练技术，用于合成视频-图像数据和模拟上下文变化，增强视频-图像差异下的可供性基础。经过MaskAHand预训练的Afformer在多个基准测试中实现了最先进的性能，包括在OPRA数据集上显著提高37%。代码可在https://github.com/showlab/afformer获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enable intelligent robots and assistants to learn from expert demonstrations, particularly in grounding human hand interactions from demonstration videos to target images. The authors propose a method called Affordance Transformer (Afformer), which utilizes a fine-grained transformer-based decoder to refine affordance grounding, and introduce a self-supervised pre-training technique named Mask Affordance Hand (MaskAHand) to synthesize video-image data and simulate context changes. Experimental results show that Afformer with MaskAHand pre-training achieves state-of-the-art performance, including a notable 37% improvement on the OPRA dataset.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于使智能机器人和助手能够从专家演示中学习，特别是在将演示视频中的人手交互基础（即可供性）与目标图像相结合方面。作者提出了一种名为可供性变换器（Afformer）的方法，该方法利用基于变换器的解码器进行细粒度的可供性基础，同时引入了一种名为Mask Affordance Hand（MaskAHand）的自监督预训练技术，以合成视频-图像数据并提高基础准确性。实验结果表明，结合MaskAHand的Afformer在多个基准测试中实现了最先进的性能，包括在OPRA数据集上显著提高了37%。</div>
</details>
</div>
<div class="card">
<div class="title">LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding</div>
<div class="meta-line">Authors: Gen Li, Varun Jampani, Deqing Sun, Laura Sevilla-Lara</div>
<div class="meta-line">Venue: CVPR 2023</div>
<div class="meta-line">First: 2023-03-16T21:47:49+00:00 · Latest: 2023-03-16T21:47:49+00:00</div>
<div class="meta-line">Comments: CVPR 2023, Project page: https://reagan1311.github.io/locate/, Video: https://www.youtube.com/watch?v=RLHansdFxII</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.09665v1">Abs</a> · <a href="https://arxiv.org/pdf/2303.09665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://reagan1311.github.io/locate/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans excel at acquiring knowledge through observation. For example, we can learn to use new tools by watching demonstrations. This skill is fundamental for intelligent systems to interact with the world. A key step to acquire this skill is to identify what part of the object affords each action, which is called affordance grounding. In this paper, we address this problem and propose a framework called LOCATE that can identify matching object parts across images, to transfer knowledge from images where an object is being used (exocentric images used for learning), to images where the object is inactive (egocentric ones used to test). To this end, we first find interaction areas and extract their feature embeddings. Then we learn to aggregate the embeddings into compact prototypes (human, object part, and background), and select the one representing the object part. Finally, we use the selected prototype to guide affordance grounding. We do this in a weakly supervised manner, learning only from image-level affordance and object labels. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a large margin on both seen and unseen objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LOCATE：弱监督可供性定位的物体部件本地化与转移</div>
<div class="mono" style="margin-top:8px">人类通过观察获取知识的能力非常出色。例如，我们可以通过观看演示学习使用新工具。这一技能对于智能系统与世界互动至关重要。获取这一技能的关键步骤是识别物体的哪个部分能够支持每个动作，这被称为可供性定位。本文解决了这一问题，提出了一个名为LOCATE的框架，可以识别图像中匹配的物体部件，将知识从物体被使用的图像（用于学习的外部图像）转移到物体不活动的图像（用于测试的自我中心图像）。为此，我们首先找到交互区域并提取其特征嵌入。然后，我们学习将嵌入聚合成紧凑的原型（人类、物体部件和背景），并选择代表物体部件的原型。最后，我们使用所选原型来指导可供性定位。我们以弱监督的方式进行此操作，仅从图像级别的可供性和物体标签中学习。大量实验表明，我们的方法在已见和未见物体上均大幅超越了最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance intelligent systems&#x27; ability to learn how to use tools by observing demonstrations, specifically by identifying which parts of an object afford specific actions, a process known as affordance grounding. The authors propose a framework called LOCATE, which localizes and transfers knowledge of object parts between images where the object is active and inactive. The method involves finding interaction areas, extracting feature embeddings, aggregating these into compact prototypes, and using the selected prototype to guide affordance grounding in a weakly supervised manner. Experimental results show that LOCATE significantly outperforms existing state-of-the-art methods on both seen and unseen objects.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高智能系统通过识别物体的哪些部分能够支持特定动作来学习与物体交互的能力，这一过程称为赋能基础。作者提出了一种名为LOCATE的框架，该框架通过弱监督学习方法将物体在使用时的图像中识别的部件知识转移到未使用的图像中。实验结果表明，LOCATE在已见和未见物体的赋能基础任务中显著优于现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Deep Learning for Affordance Segmentation in images</div>
<div class="meta-line">Authors: Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Jose J. Guerrero</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2023-03-02T00:01:13+00:00 · Latest: 2023-03-02T00:01:13+00:00</div>
<div class="meta-line">Comments: 2023 IEEE International Conference on Robotics and Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.00871v1">Abs</a> · <a href="https://arxiv.org/pdf/2303.00871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordances are a fundamental concept in robotics since they relate available actions for an agent depending on its sensory-motor capabilities and the environment. We present a novel Bayesian deep network to detect affordances in images, at the same time that we quantify the distribution of the aleatoric and epistemic variance at the spatial level. We adapt the Mask-RCNN architecture to learn a probabilistic representation using Monte Carlo dropout. Our results outperform the state-of-the-art of deterministic networks. We attribute this improvement to a better probabilistic feature space representation on the encoder and the Bayesian variability induced at the mask generation, which adapts better to the object contours. We also introduce the new Probability-based Mask Quality measure that reveals the semantic and spatial differences on a probabilistic instance segmentation model. We modify the existing Probabilistic Detection Quality metric by comparing the binary masks rather than the predicted bounding boxes, achieving a finer-grained evaluation of the probabilistic segmentation. We find aleatoric variance in the contours of the objects due to the camera noise, while epistemic variance appears in visual challenging pixels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像中的贝叶斯深度学习用于可供性分割</div>
<div class="mono" style="margin-top:8px">可供性是机器人学中的一个基本概念，因为它与代理的感知-运动能力和环境相关的可用动作有关。我们提出了一种新颖的贝叶斯深度网络，用于检测图像中的可供性，同时量化空间层面的随机和认知方差分布。我们调整了Mask-RCNN架构，以使用蒙特卡洛丢弃学习概率表示。我们的结果超越了确定性网络的最新技术水平。我们将这一改进归因于编码器上更好的概率特征空间表示和在掩码生成中引入的贝叶斯变异性，这更好地适应了物体轮廓。我们还引入了新的基于概率的掩码质量度量，揭示了概率实例分割模型中的语义和空间差异。我们通过比较二进制掩码而不是预测的边界框，修改了现有的概率检测质量指标，从而实现了对概率分割的更细致评估。我们发现，由于相机噪声，物体轮廓中存在随机方差，而在视觉挑战像素中出现认知方差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance affordance detection in images, which is crucial for robotics as it relates to the actions an agent can take based on its sensory-motor capabilities and the environment. The authors propose a novel Bayesian deep network that modifies the Mask-RCNN architecture to learn a probabilistic representation through Monte Carlo dropout, allowing for the quantification of aleatoric and epistemic variance at a spatial level. The experimental results demonstrate that this approach outperforms state-of-the-art deterministic networks, attributed to improved probabilistic feature space representation and better adaptation to object contours, alongside the introduction of a new Probability-based Mask Quality measure for finer evaluation of probabilistic segmentation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强机器人中的可供性检测，这对于理解基于感知-运动能力和环境上下文的可用动作至关重要。作者提出了一种新颖的贝叶斯深度学习方法，修改了Mask-RCNN架构，以结合蒙特卡洛丢弃法进行概率表示，从而能够量化空间层面的随机和认知方差。实验结果表明，该方法的性能超过了现有的确定性网络，这归因于改进的概率特征表示和对物体轮廓的更好适应，以及引入了一种新的基于概率的掩膜质量度量，以更细致地评估分割质量。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Affordance Grounding from Exocentric Images</div>
<div class="meta-line">Authors: Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao</div>
<div class="meta-line">First: 2022-03-18T12:29:06+00:00 · Latest: 2022-03-18T12:29:06+00:00</div>
<div class="meta-line">Comments: CVPR2022</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2203.09905v1">Abs</a> · <a href="https://arxiv.org/pdf/2203.09905v1">PDF</a> · <a href="http://github.com/lhc1224/Cross-View-AG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordance grounding, a task to ground (i.e., localize) action possibility region in objects, which faces the challenge of establishing an explicit link with object parts due to the diversity of interactive affordance. Human has the ability that transform the various exocentric interactions to invariant egocentric affordance so as to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. To this end, we devise a cross-view knowledge transfer framework that extracts affordance-specific features from exocentric interactions and enhances the perception of affordance regions by preserving affordance correlation. Specifically, an Affordance Invariance Mining module is devised to extract specific clues by minimizing the intra-class differences originated from interaction habits in exocentric images. Besides, an Affordance Co-relation Preserving strategy is presented to perceive and localize affordance by aligning the co-relation matrix of predicted results between the two views. Particularly, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representative models in terms of objective metrics and visual quality. Code: github.com/lhc1224/Cross-View-AG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从外部图像学习可供性基础</div>
<div class="mono" style="margin-top:8px">可供性基础是一项任务，旨在确定物体中动作可能区域的定位，这面临着由于交互可供性的多样性而建立与物体部件之间明确联系的挑战。人类具备将各种外部交互转化为不变的自我中心可供性的能力，以应对交互多样性的影响。为了赋予代理这种能力，本文提出了一项从外部视角进行可供性基础的任务，即在给定外部人-物交互和自我中心物体图像的情况下，学习物体的可供性知识，并仅使用可供性标签作为监督，将其转移到自我中心图像。为此，我们设计了一个跨视角知识转移框架，从外部交互中提取特定于可供性的特征，并通过保持可供性相关性来增强对可供性区域的感知。具体而言，设计了一个可供性不变性挖掘模块，通过最小化源自外部图像中交互习惯的类内差异来提取特定线索。此外，提出了一种可供性相关性保持策略，通过对齐两个视角之间预测结果的相关性矩阵来感知和定位可供性。特别地，构建了一个名为AGD20K的可供性基础数据集，通过收集和标注来自36个可供性类别的超过2万张图像。实验结果表明，我们的方法在客观指标和视觉质量方面优于代表性模型。代码：github.com/lhc1224/Cross-View-AG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of affordance grounding, which involves localizing action possibility regions in objects despite the diversity of interactive affordance. The authors propose a method that learns affordance knowledge from exocentric human-object interactions and transfers it to egocentric images using only affordance labels for supervision. Their approach includes a cross-view knowledge transfer framework that enhances affordance perception by minimizing intra-class differences and aligning correlation matrices between views, leading to superior performance compared to existing models as demonstrated by experimental results on a newly constructed dataset, AGD20K, containing over 20,000 labeled images across 36 affordance categories.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决赋能定位的挑战，即在对象中定位动作可能区域，尽管交互赋能的多样性。作者提出了一种跨视角知识转移框架，该框架从外部人-物交互中学习赋能知识，并利用赋能标签进行监督，将其转移到自我中心图像中。实验结果表明，该方法通过引入赋能不变性挖掘模块和赋能相关性保持策略，在客观指标和视觉质量上显著优于现有模型，并支持一个新构建的数据集AGD20K，该数据集包含36个赋能类别的超过20,000张标记图像。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Visual Affordance Grounding from Demonstration Videos</div>
<div class="meta-line">Authors: Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao</div>
<div class="meta-line">First: 2021-08-12T11:45:38+00:00 · Latest: 2021-08-12T11:45:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2108.05675v1">Abs</a> · <a href="https://arxiv.org/pdf/2108.05675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual affordance grounding aims to segment all possible interaction regions between people and objects from an image/video, which is beneficial for many applications, such as robot grasping and action recognition. However, existing methods mainly rely on the appearance feature of the objects to segment each region of the image, which face the following two problems: (i) there are multiple possible regions in an object that people interact with; and (ii) there are multiple possible human interactions in the same object region. To address these problems, we propose a Hand-aided Affordance Grounding Network (HAGNet) that leverages the aided clues provided by the position and action of the hand in demonstration videos to eliminate the multiple possibilities and better locate the interaction regions in the object. Specifically, HAG-Net has a dual-branch structure to process the demonstration video and object image. For the video branch, we introduce hand-aided attention to enhance the region around the hand in each video frame and then use the LSTM network to aggregate the action features. For the object branch, we introduce a semantic enhancement module (SEM) to make the network focus on different parts of the object according to the action classes and utilize a distillation loss to align the output features of the object branch with that of the video branch and transfer the knowledge in the video branch to the object branch. Quantitative and qualitative evaluations on two challenging datasets show that our method has achieved stateof-the-art results for affordance grounding. The source code will be made available to the public.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从演示视频中学习视觉可供性定位</div>
<div class="mono" style="margin-top:8px">视觉可供性定位旨在从图像/视频中分割出人与物体之间所有可能的交互区域，这对许多应用（如机器人抓取和动作识别）是有益的。然而，现有方法主要依赖物体的外观特征来分割图像的每个区域，面临以下两个问题：（i）物体中人们交互的区域可能有多个；（ii）同一物体区域中可能有多种人类交互。为了解决这些问题，我们提出了一种手辅助可供性定位网络（HAGNet），利用演示视频中手的位置和动作提供的辅助线索，消除多重可能性，更好地定位物体中的交互区域。具体而言，HAG-Net具有双分支结构，用于处理演示视频和物体图像。对于视频分支，我们引入手辅助注意力，以增强每个视频帧中手周围的区域，然后使用LSTM网络聚合动作特征。对于物体分支，我们引入语义增强模块（SEM），使网络根据动作类别关注物体的不同部分，并利用蒸馏损失将物体分支的输出特征与视频分支的特征对齐，将视频分支中的知识转移到物体分支。对两个具有挑战性的数据集进行的定量和定性评估表明，我们的方法在可供性定位方面达到了最先进的结果。源代码将向公众开放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve visual affordance grounding, which involves identifying interaction regions between people and objects in images or videos, a task crucial for applications like robot grasping and action recognition. The authors propose the Hand-aided Affordance Grounding Network (HAGNet), which utilizes hand position and action cues from demonstration videos to refine the segmentation process. Experimental results demonstrate that HAGNet, with its dual-branch structure and innovative attention mechanisms, achieves state-of-the-art performance on two challenging datasets, effectively addressing the complexities of multiple interaction possibilities in object regions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉可供性定位，即识别图像和视频中人与物体之间的交互区域，这对机器人抓取和动作识别等应用至关重要。作者提出了一种手部辅助可供性定位网络（HAGNet），利用演示视频中的手部位置和动作线索来优化交互区域的分割。该方法采用双分支架构，处理视频和物体图像，使用手部辅助注意力和LSTM进行动作特征聚合，并结合语义增强模块聚焦于物体，最终在两个具有挑战性的数据集上实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Are standard Object Segmentation models sufficient for Learning Affordance Segmentation?</div>
<div class="meta-line">Authors: Hugo Caselles-Dupré, Michael Garcia-Ortiz, David Filliat</div>
<div class="meta-line">First: 2021-07-05T15:34:20+00:00 · Latest: 2021-07-05T15:34:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2107.02095v1">Abs</a> · <a href="https://arxiv.org/pdf/2107.02095v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordances are the possibilities of actions the environment offers to the individual. Ordinary objects (hammer, knife) usually have many affordances (grasping, pounding, cutting), and detecting these allow artificial agents to understand what are their possibilities in the environment, with obvious application in Robotics. Proposed benchmarks and state-of-the-art prediction models for supervised affordance segmentation are usually modifications of popular object segmentation models such as Mask R-CNN. We observe that theoretically, these popular object segmentation methods should be sufficient for detecting affordances masks. So we ask the question: is it necessary to tailor new architectures to the problem of learning affordances? We show that applying the out-of-the-box Mask R-CNN to the problem of affordances segmentation outperforms the current state-of-the-art. We conclude that the problem of supervised affordance segmentation is included in the problem of object segmentation and argue that better benchmarks for affordance learning should include action capacities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>标准物体分割模型足以学习可供性分割吗？</div>
<div class="mono" style="margin-top:8px">可供性是环境提供给个体的行动可能性。普通物体（锤子、刀）通常具有多种可供性（抓取、敲打、切割），检测这些可供性使人工智能体能够理解它们在环境中的可能性，显然在机器人技术中有应用。针对监督可供性分割的基准和最先进的预测模型通常是对流行物体分割模型（如Mask R-CNN）的修改。我们观察到，从理论上讲，这些流行的物体分割方法应该足以检测可供性掩膜。因此，我们提出问题：是否有必要为学习可供性的问题量身定制新的架构？我们表明，将现成的Mask R-CNN应用于可供性分割问题的表现优于当前的最先进水平。我们得出结论，监督可供性分割的问题包含在物体分割的问题中，并认为更好的可供性学习基准应包括行动能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether standard object segmentation models are adequate for learning affordance segmentation, which is crucial for enabling artificial agents to understand potential actions in their environment. The authors applied the Mask R-CNN model, a popular object segmentation technique, to affordance segmentation tasks without modifications. The results demonstrated that this approach outperformed existing state-of-the-art methods, suggesting that the challenges of supervised affordance segmentation can be effectively addressed within the framework of object segmentation, and highlighting the need for improved benchmarks that incorporate action capacities.</div>
<div class="mono" style="margin-top:8px">本研究探讨了标准物体分割模型是否足以用于学习可供性分割，这对使人工智能体理解其环境中的潜在动作至关重要。作者将流行的物体分割框架Mask R-CNN应用于可供性分割任务，且未进行修改。实验结果表明，该方法超越了现有的最先进技术，表明可供性分割可以在物体分割的框架内有效解决，并强调需要改进包含动作能力的基准。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
