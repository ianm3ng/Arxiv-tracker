<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-26 03:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260126_0328</div>
    <div class="row"><div class="card">
<div class="title">HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</div>
<div class="meta-line">Authors: Zequn Xie, Xin Liu, Boyun Zhang, Yuxiao Lin, Sihang Cai, Tao Jin</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-22T17:57:42+00:00 · Latest: 2026-01-22T17:57:42+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16155v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &quot;blind&quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HVD：基于人类视觉驱动的视频表示学习用于文本-视频检索</div>
<div class="mono" style="margin-top:8px">CLIP的成功推动了文本-视频检索的重大进展。然而，当前的方法往往遭受“盲目”特征交互的困扰，模型难以从背景噪声中辨别关键视觉信息，原因在于文本查询的稀疏性。为了解决这一问题，我们从人类认知行为中获得灵感，提出了人类视觉驱动（HVD）模型。我们的框架建立了一个粗到细的对齐机制，包括两个关键组件：帧特征选择模块（FFSM）和补丁特征压缩模块（PFCM）。FFSM通过选择关键帧来消除时间冗余，模拟人类的宏观感知能力。随后，PFCM通过先进的注意机制将补丁特征聚合成显著的视觉实体，模拟微观感知，实现精确的实体级匹配。在五个基准上的大量实验表明，HVD不仅捕捉了类人视觉焦点，还达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of current text-video retrieval methods, which often fail to effectively distinguish important visual information from background noise due to sparse textual queries. To address this issue, the authors propose the Human Vision-Driven (HVD) model, which incorporates a coarse-to-fine alignment mechanism through two main components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM selects key frames to reduce temporal redundancy, while PFCM aggregates patch features into salient visual entities using an advanced attention mechanism. Experimental results across five benchmarks indicate that HVD successfully mimics human visual focus and achieves state-of-the-art performance in text-video retrieval tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善当前在特征交互方面存在困难的文本-视频检索方法，这些方法受到背景噪声和稀疏文本查询的影响。作者提出了人类视觉驱动（HVD）模型，该模型采用了一种粗到细的对齐机制，包括帧特征选择模块（FFSM）和补丁特征压缩模块（PFCM）。FFSM选择关键帧以减少时间冗余，而PFCM则通过先进的注意机制将补丁特征聚合成显著的视觉实体。五个基准上的实验结果表明，HVD有效地模拟了人类的视觉聚焦，并在文本-视频检索任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap</div>
<div class="meta-line">Authors: Elisabeth Jüttner, Janelle Pfeifer, Leona Krath, Stefan Korfhage, Hannah Dröge, Matthias B. Hullin, Markus Plack</div>
<div class="meta-line">First: 2025-10-27T16:28:55+00:00 · Latest: 2026-01-22T13:52:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23494v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23494v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yesnt：扩散重光照模型是否准备好用于捕捉阶段合成？一种弥合差距的混合替代方案</div>
<div class="mono" style="margin-top:8px">体积视频重光照对于将捕捉的表演带入虚拟世界至关重要，但当前的方法在提供时间稳定、适合生产的结果方面存在困难。基于扩散的内在分解方法在单帧上显示出潜力，但在扩展到序列时受到随机噪声和不稳定性的影响，而视频扩散模型则受到内存和规模的限制。我们提出了一种混合重光照框架，将扩散衍生的材料先验与时间正则化和物理驱动的渲染相结合。我们的方法将每帧材料属性的多个随机估计聚合为时间一致的阴影组件，使用光流引导的正则化。对于阴影和反射等间接效果，我们从高斯不透明度场中提取网格代理，并在标准图形管线中进行渲染。对真实和合成捕捉的实验表明，这种混合策略在序列中实现了比仅使用扩散的基线更稳定的重光照，同时超越了视频扩散可行的剪辑长度。这些结果表明，平衡学习先验与物理约束的混合方法是实现适合生产的体积视频重光照的实际步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the stability and quality of volumetric video relighting for virtual environments, as existing methods struggle with temporal consistency and production readiness. The authors propose a hybrid relighting framework that integrates diffusion-derived material priors with temporal regularization and physically motivated rendering techniques. Experimental results demonstrate that this approach significantly enhances the stability of relighting across sequences compared to diffusion-only methods, while also accommodating longer clip lengths than those manageable by traditional video diffusion models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善虚拟环境中体积视频重光照的稳定性和质量，因为当前方法在时间一致性和生产准备方面存在困难。作者提出了一种混合重光照框架，将基于扩散的材料先验与时间规整和物理驱动的渲染技术相结合。实验结果表明，该方法显著提高了序列中重光照的稳定性，相较于仅基于扩散的方法，并且支持更长的剪辑长度，表明将学习的先验与物理约束相结合是实现有效体积视频重光照的可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars</div>
<div class="meta-line">Authors: Yarin Benyamin</div>
<div class="meta-line">First: 2026-01-22T12:44:12+00:00 · Latest: 2026-01-22T12:44:12+00:00</div>
<div class="meta-line">Comments: Technical Report benchmarking off-the-shelf CV latencies on commodity CPU hardware for therapeutic VR applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15914v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a &quot;Latency Wall&quot; exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (&lt;23%) or speed (&gt;150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>延迟壁垒：基准测试现成情感识别用于实时虚拟化身</div>
<div class="mono" style="margin-top:8px">在虚拟现实（VR）和人机交互（HCI）领域，实时情感识别在支持自闭症谱系障碍（ASD）个体改善社交技能方面展现出潜力。该任务需要严格的延迟-准确性权衡，运动到光子（MTP）延迟需保持在140毫秒以下以维持应急性。然而，大多数现成的深度学习模型优先考虑准确性，而忽视了商品硬件的严格时间限制。作为可访问VR治疗的第一步，我们基准测试了在虚拟角色上使用UIBVFED数据集的最先进（SOTA）零样本面部表情识别（FER）模型。我们评估了YOLO（v8、v11和v12）的中型和纳米变体用于人脸检测，以及包括CLIP、SigLIP和ViT-FER在内的通用视觉变换器。我们在仅使用CPU的推理结果表明，尽管在风格化化身上的人脸检测是稳健的（100%准确率），但在分类阶段存在“延迟壁垒”。YOLOv11n架构在检测方面提供了最佳平衡（约54毫秒）。然而，像CLIP和SigLIP这样的通用变换器未能在实时循环中实现可行的准确性（&lt;23%）或速度（&gt;150毫秒）。本研究强调了轻量级、特定领域架构的必要性，以实现可访问的实时AI在治疗环境中的应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for real-time emotion recognition in Virtual Reality (VR) to assist individuals with Autism Spectrum Disorder (ASD) in enhancing their social skills, necessitating a strict latency-accuracy trade-off. The authors benchmark various State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) using the UIBVFED dataset, evaluating Medium and Nano variants of YOLO and general-purpose Vision Transformers. The findings reveal that while face detection on stylized avatars achieves 100% accuracy, a significant &#x27;Latency Wall&#x27; is encountered during classification, with YOLOv11n providing the best detection speed (~54 ms), while general-purpose Transformers like CLIP and SigLIP fall short in both accuracy and speed for real-time applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要实时情感识别，以帮助自闭症谱系障碍（ASD）个体提高社交技能，这要求严格的延迟-准确性权衡。研究基于UIBVFED数据集，对多种最新的零样本面部表情识别（FER）模型进行基准测试，评估了YOLO的中型和纳米变体以及通用视觉变换器。结果显示，尽管在风格化虚拟角色上的面部检测达到了100%的准确率，但分类阶段存在显著的“延迟墙”，YOLOv11n在检测速度上表现最佳，约为54毫秒，而通用变换器如CLIP和SigLIP未能满足实时应用所需的准确性和速度。</div>
</details>
</div>
<div class="card">
<div class="title">VideoPro: Adaptive Program Reasoning for Long Video Understanding</div>
<div class="meta-line">Authors: Chenglin Li, Feng Han, Yikun Wang, Ruilin Li, Shuai Dong, Haowen Hou, Haitao Li, Qianglong Chen, Feng Tao, Jingqi Tong, Yin Zhang, Jiaqi Wang</div>
<div class="meta-line">First: 2025-09-22T13:06:17+00:00 · Latest: 2026-01-22T10:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17743v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.17743v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models&#x27; ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoPro：用于长视频理解的自适应程序推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生成视觉任务的程序工作流方面显示出潜力。然而，以前的方法往往依赖于闭源模型，缺乏系统推理，并且在长视频问答（videoQA）中表现不佳。为了解决这些挑战，我们引入了FS-VisPR框架，这是一种自适应视觉程序推理方法，平衡了简单查询的快速推理与困难查询的慢速推理。首先，我们设计了高效的视觉模块（例如，关键片段检索和字幕检索）以支持长视频任务。然后，我们构建了一个多样化且高质量的快慢推理数据集，结合强大的LLM，以对齐开源语言模型生成视觉程序工作流的能力，称为FS-LLM。接下来，我们设计了一个快慢推理框架与FS-LLM：简单查询由VideoLLMs直接解决，而困难查询则调用视觉程序推理，受到类人推理过程的启发。在此过程中，低置信度的快速思考答案将触发第二阶段的慢推理过程，如果程序执行失败，则激活快速推理的回退机制。此外，我们通过在训练和推理期间的参数搜索来改进视觉程序。通过调整程序中视觉模块的参数，生成多个变体：在训练期间，选择产生正确答案的程序，而在推理期间，应用置信度最高的程序。实验表明，FS-VisPR提高了视觉程序工作流的效率和可靠性。在LVBench上取得了50.4%的准确率，超越了GPT-4o，匹配了Qwen2.5VL-72B在VideoMME上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance long-form video question answering (videoQA) by addressing the limitations of previous approaches that often rely on closed-source models and lack systematic reasoning. The authors introduce the FS-VisPR framework, which employs an adaptive visual program reasoning method that balances fast reasoning for simple queries with slow reasoning for more complex ones. Key experimental findings indicate that FS-VisPR improves both efficiency and reliability in visual program workflows, achieving 50.4% accuracy on the LVBench dataset, surpassing GPT-4o and matching the performance of Qwen2.5VL-72B on VideoMME.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决以往依赖闭源模型且缺乏系统推理的局限性，来增强长视频问答（videoQA）的能力。作者提出了FS-VisPR框架，采用一种自适应视觉程序推理方法，将简单查询的快速推理与复杂查询的慢速推理相结合。关键实验结果表明，FS-VisPR显著提高了视觉程序工作流的效率和可靠性，在LVBench数据集上达到了50.4%的准确率，超越了GPT-4o的表现，并与Qwen2.5VL-72B在VideoMME上的表现相匹配。</div>
</details>
</div>
<div class="card">
<div class="title">WavLink: Compact Audio-Text Embeddings with a Global Whisper Token</div>
<div class="meta-line">Authors: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:55:58+00:00 · Latest: 2026-01-22T08:55:20+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15118v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WavLink：带有全局Whisper令牌的紧凑音频-文本嵌入</div>
<div class="mono" style="margin-top:8px">Whisper已成为大型音频语言模型中提取通用音频特征的事实标准编码器，其中30秒的音频片段通常由1500个帧特征表示并投影到LLM中。相比之下，像CLAP模型这样的音频-文本嵌入模型在很大程度上依赖于替代音频编码器（例如HTS-AT、PaSST），并未有效利用Whisper。我们提出了WavLink，这是一种紧凑的音频-文本嵌入模型，通过可学习的全局令牌增强Whisper编码器，并与文本编码器共同训练。通过对设计选择的系统研究，包括预训练文本编码器、损失函数、训练模式和数据混合，我们识别出能够实现最先进检索性能的配置。我们在三种模型规模上的两阶段训练方案，结合马特ryoshka风格的监督，提高了可扩展性，使得嵌入体积缩小8倍且性能损失最小。WavLink在AIR-Bench的多项选择题和零样本分类中也表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more effective audio-text embedding models that leverage the capabilities of the Whisper encoder, which has been underutilized in this context. The authors introduce WavLink, a compact model that enhances the Whisper encoder with a learnable global token, trained in conjunction with a text encoder. Their systematic exploration of various design choices, including different pretrained text encoders and training configurations, leads to the identification of optimal setups that achieve state-of-the-art retrieval performance while enabling significantly smaller embeddings with minimal performance loss, as demonstrated through competitive results on AIR-Bench tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用Whisper编码器的能力来提高音频-文本嵌入模型的效率，而Whisper在这一背景下尚未得到充分利用。作者提出了WavLink，这是一种紧凑的音频-文本嵌入模型，结合了可学习的全局标记，并与文本编码器共同训练。他们对各种设计选择进行了系统探索，得出了实现最先进检索性能的配置，采用两阶段训练方法，使得嵌入体积缩小8倍，同时在AIR-Bench等基准测试中保持竞争力，适用于多项选择题和零样本分类。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework</div>
<div class="meta-line">Authors: Shubham Shukla, Kunal Sonalkar</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-22T07:33:41+00:00 · Latest: 2026-01-22T07:33:41+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, &quot;outer fabric&quot; is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn&#x27;t exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的零样本产品属性标注：三层评估框架</div>
<div class="mono" style="margin-top:8px">细粒度属性预测对于时尚零售应用至关重要，包括目录丰富、视觉搜索和推荐系统。视觉语言模型（VLMs）提供零样本预测，无需特定任务训练，但其在多属性时尚任务上的系统评估仍未得到充分探索。一个关键挑战是时尚属性通常是有条件的。例如，当没有外衣可见时，“外层面料”是未定义的。这要求模型在尝试分类之前检测属性的适用性。我们引入了一个三层评估框架，分解了这一挑战：（1）所有属性的整体任务性能（包括NA类：表示属性不适用），（2）属性适用性检测，以及（3）当属性可确定时的细粒度分类。使用DeepFashion-MultiModal，该数据集在属性标签空间中明确定义NA（表示属性不存在或不可见），我们对九个VLM进行了基准测试，涵盖旗舰（GPT-5，Gemini 2.5 Pro）、高效（GPT-5 Mini，Gemini 2.5 Flash）和超高效（GPT-5 Nano，Gemini 2.5 Flash-Lite）层次，针对在18个属性上对5,000张图像进行预训练的Fashion-CLIP嵌入的分类器。我们的发现表明：（1）零样本VLM的宏观F1达到64.0%，比在预训练Fashion-CLIP嵌入上进行的逻辑回归提高了三倍；（2）VLM在细粒度分类（第3层：70.8% F1）方面表现出色，但在适用性检测（第2层：34.1% NA-F1）方面存在困难，识别出一个关键瓶颈；（3）高效模型在成本较低的情况下实现了超过90%的旗舰性能，提供了实际部署路径。该诊断框架使从业者能够确定错误是源于可见性检测还是分类，从而指导生产系统的针对性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance fine-grained attribute prediction for fashion retail applications, which is crucial for tasks like catalog enrichment and visual search. The authors propose a three-tier evaluation framework to systematically assess Vision-Language Models (VLMs) on multi-attribute fashion tasks, addressing the challenge of conditional fashion attributes. Using the DeepFashion-MultiModal dataset, the study benchmarks nine VLMs against classifiers trained on Fashion-CLIP embeddings, revealing that zero-shot VLMs achieve a macro-F1 score of 64.0%, outperforming logistic regression significantly, while also showing strong performance in fine-grained classification (70.8% F1) but struggling with applicability detection (34.1% NA-F1), highlighting a critical area for improvement. Efficient models demonstrate over 90% of flagship performance at reduced costs, suggesting viable options for practical deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升时尚零售应用中的细粒度属性预测，这对目录丰富和视觉搜索等任务至关重要。作者提出了一个三层评估框架，以系统地评估零样本设置中的视觉语言模型（VLM），解决条件时尚属性的挑战。他们使用DeepFashion-MultiModal数据集对九个VLM进行基准测试，结果显示VLM的宏F1得分为64.0%，显著优于传统方法，但在属性适用性检测方面面临挑战，NA-F1得分仅为34.1%，而高效模型则以超过90%的旗舰性能提供了一种具有成本效益的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection</div>
<div class="meta-line">Authors: Morteza Poudineh, Marc Lalonde</div>
<div class="meta-line">First: 2026-01-21T20:35:51+00:00 · Latest: 2026-01-21T20:35:51+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DevPrompt：基于偏差的提示学习用于单正常样本图像异常检测</div>
<div class="mono" style="margin-top:8px">少量正常样本异常检测（FNSAD）旨在仅使用少量正常训练样本检测图像中的异常区域，由于监督有限和潜在缺陷的多样性，这一任务极具挑战性。最近的方法利用视觉-语言模型，如CLIP，通过基于提示的学习来对齐图像和文本特征。然而，现有方法通常在正常和异常提示之间表现出较弱的可区分性，并且缺乏针对补丁级异常的原则性评分机制。我们提出了一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差的评分的统计可靠性相结合。具体而言，我们用可学习的上下文向量替换固定的提示前缀，这些向量在正常和异常提示之间共享，而特定于异常的后缀标记则实现类感知对齐。为了增强可分离性，我们引入了带有Top-K多实例学习（MIL）的偏差损失，将补丁级特征建模为来自正态分布的高斯偏差。这使得网络能够为具有统计显著偏差的补丁分配更高的异常分数，从而改善定位和可解释性。在MVTecAD和VISA基准上的实验表明，与PromptAD和其他基线相比，像素级检测性能优越。消融研究进一步验证了可学习提示、基于偏差的评分和Top-K MIL策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of few-normal shot anomaly detection (FNSAD), which aims to identify abnormal regions in images using limited normal training samples. The authors propose a deviation-guided prompt learning framework that combines the capabilities of vision-language models with a deviation-based scoring mechanism. Experimental results on the MVTecAD and VISA benchmarks show that this approach significantly improves pixel-level detection performance compared to existing methods, with ablation studies confirming the effectiveness of learnable prompts and the Top-K Multiple Instance Learning strategy in enhancing anomaly localization and interpretability.</div>
<div class="mono" style="margin-top:8px">本研究解决了少量正常样本异常检测（FNSAD）的挑战，该任务涉及在有限的正常训练样本下识别图像中的异常区域。作者提出了一种基于偏差的提示学习框架，通过使用可学习的上下文向量和特定于异常的后缀标记来增强提示的可区分性，并结合偏差损失和Top-K多实例学习（MIL）将补丁级特征建模为高斯偏差。实验结果表明，该方法在MVTecAD和VISA基准测试上显著提高了像素级检测性能，相较于现有方法如PromptAD，消融研究进一步确认了所提技术的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考具身世界的视频生成模型</div>
<div class="mono" style="margin-top:8px">视频生成模型在具身智能方面取得了显著进展，为生成捕捉物理世界中感知、推理和行动的多样化机器人数据开辟了新可能性。然而，合成准确反映现实世界机器人交互的高质量视频仍然具有挑战性，缺乏标准化基准限制了公平比较和进展。为了解决这一问题，我们引入了一个全面的机器人基准RBench，旨在评估五个任务领域和四种不同具身形式的机器人导向视频生成。它通过可重复的子指标评估任务级正确性和视觉保真度，包括结构一致性、物理合理性和行动完整性。对25个代表性模型的评估突显了生成物理现实机器人行为的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。虽然RBench提供了识别这些不足的必要视角，但实现物理现实需要超越评估，解决高质量训练数据的严重短缺。基于这些见解，我们引入了一个精细的四阶段数据管道，最终形成RoVid-X，这是最大的开源机器人视频生成数据集，包含400万个带注释的视频片段，涵盖数千个任务，并配有全面的物理属性注释。总体而言，这一评估和数据的协同生态系统为视频模型的严格评估和可扩展训练奠定了坚实基础，加速了具身人工智能向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video generation models for embodied intelligence, particularly in generating realistic robot interactions in the physical world. The authors introduce RBench, a comprehensive benchmark for evaluating robot-oriented video generation across various tasks and embodiments, focusing on metrics such as structural consistency and physical plausibility. Experimental results from evaluating 25 models reveal significant shortcomings in generating realistic robot behaviors, while RBench demonstrates a high correlation with human evaluations, and the introduction of RoVid-X, a large open-source dataset with 4 million annotated video clips, aims to address the lack of high-quality training data necessary for improving physical realism in video generation models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升用于体现智能的视频生成模型，特别是在生成物理世界中真实的机器人交互方面。作者提出了RBench，一个全面的机器人基准，评估五个任务领域和四种体现形式的视频生成，重点关注任务级正确性和视觉逼真度，通过特定的子指标进行评估。对25个模型的评估揭示了在生成物理真实行为方面的显著不足，RBench与人类评估的高相关性表明其有效性，而RoVid-X的引入，作为一个拥有400万条注释视频片段的大型开源数据集，旨在解决改善这些模型所需的高质量训练数据短缺问题。</div>
</details>
</div>
<div class="card">
<div class="title">OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</div>
<div class="meta-line">Authors: Letian Zhang, Sucheng Ren, Yanqing Liu, Xianhang Li, Zeyu Wang, Yuyin Zhou, Huaxiu Yao, Zeyu Zheng, Weili Nie, Guilin Liu, Zhiding Yu, Cihang Xie</div>
<div class="meta-line">First: 2026-01-21T18:47:12+00:00 · Latest: 2026-01-21T18:47:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15369v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15369v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenVision 3：统一视觉编码器家族，用于理解和生成</div>
<div class="mono" style="margin-top:8px">本文提出了一种先进的视觉编码器家族，名为OpenVision 3，学习一种单一的统一视觉表示，既可用于图像理解，也可用于图像生成。我们的核心架构简单：我们将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持两种互补角色。首先，编码器输出传递给ViT-VAE解码器以重建原始图像，鼓励表示捕捉生成结构。其次，使用对比学习和图像-标题目标优化相同的表示，增强语义特征。通过在共享潜在空间中共同优化重建和语义驱动信号，编码器学习到的表示在两种模式下协同并良好泛化。我们通过广泛的下游评估验证了这种统一设计，编码器保持冻结。对于多模态理解，我们将编码器插入LLaVA-1.5框架：其性能与标准CLIP视觉编码器相当（例如，SeedBench上62.4对62.2，POPE上83.7对82.9）。对于生成，我们在RAE框架下进行测试：我们的表现大幅超越标准CLIP基础编码器（例如，ImageNet上的gFID：1.89对2.54）。我们希望这项工作能激发未来对统一建模的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a unified visual encoder that effectively serves both image understanding and generation tasks. The authors introduce OpenVision 3, which utilizes a simple architecture where VAE-compressed image latents are fed into a ViT encoder, optimizing its output for two roles: reconstructing the original image and enhancing semantic features through contrastive learning and image-captioning objectives. Experimental results demonstrate that the unified encoder performs comparably to standard models in multimodal understanding and significantly outperforms them in image generation tasks, indicating its effectiveness in learning representations that generalize well across different applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的视觉编码器，能够有效处理图像理解和生成任务。作者提出了OpenVision 3，采用简单的架构，将VAE压缩的图像潜变量输入ViT编码器，通过优化输出以实现图像重建和通过对比学习及图像-标题目标提取语义特征。实验结果表明，该编码器在多模态理解任务中表现与现有模型相当，并在图像生成方面显著超越标准的CLIP编码器，表明其在多样化视觉任务中学习共享潜在表示的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</div>
<div class="meta-line">Authors: Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-28T17:57:05+00:00 · Latest: 2026-01-21T16:16:08+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24709v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object binding, the brain&#x27;s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型预训练视觉变换器中对象绑定是否自然出现？</div>
<div class="mono" style="margin-top:8px">对象绑定是大脑将代表一个对象的多个特征结合成一个连贯整体的能力，这对人类认知至关重要。它将低级感知特征分组为高级对象表示，能够高效且组合地存储这些对象，并支持人类对单个对象实例的推理。尽管之前的研究通常明确施加以对象为中心的注意力（例如，插槽注意力）来探讨这些好处，但尚不清楚这种能力是否在预训练的视觉变换器（ViTs）中自然出现。直观上，它们可能会：识别哪些补丁属于同一对象对于下游预测应该是有用的，从而引导注意力。基于自注意力的二次特性，我们假设ViTs表示两个补丁是否属于同一对象，我们称之为IsSameObject。我们使用二次相似性探针从ViT层的补丁嵌入中解码IsSameObject，准确率超过90%。重要的是，这种对象绑定能力在DINO、CLIP和ImageNet监督的ViTs中可靠地出现，但在MAE中明显较弱，这表明绑定不是一个简单的架构伪影，而是通过特定的预训练目标获得的能力。我们进一步发现IsSameObject在对象特征之上的低维子空间中编码，并且该信号积极引导注意力。从模型激活中消除IsSameObject会降低下游性能，并与学习目标相悖，这意味着新兴的对象绑定自然服务于预训练目标。我们的发现挑战了ViTs缺乏对象绑定的观点，并强调了“哪些部分属于一起”的符号知识如何在连接主义系统中自然出现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether object binding, the cognitive ability to integrate various features into coherent object representations, naturally emerges in large pretrained Vision Transformers (ViTs). The authors hypothesize that ViTs can represent whether two patches belong to the same object, termed IsSameObject, and employ a quadratic similarity probe to decode this property from patch embeddings, achieving over 90% accuracy. The findings reveal that this object-binding capability is consistently present in DINO, CLIP, and ImageNet-supervised ViTs, but significantly weaker in MAE, indicating that object binding is not merely an architectural feature but a skill developed through specific pretraining objectives, which also influences attention and downstream performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了对象绑定这一认知能力是否在大型预训练视觉变换器（ViTs）中自然出现，即将各种特征整合为连贯的对象表示。作者假设ViTs能够表示属于同一对象的补丁之间的关系，称为IsSameObject，并使用二次相似性探针从补丁嵌入中解码该属性，准确率超过90%。结果表明，这种对象绑定能力在DINO、CLIP和ImageNet监督的ViTs中始终存在，但在MAE中显著较弱，表明有效的对象绑定与特定的预训练目标相关，而不仅仅是一个架构特征，并且在引导注意力和增强下游性能方面发挥着关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background</div>
<div class="meta-line">Authors: Tianyu Li, Songyue Cai, Zongqian Wu, Ping Hu, Xiaofeng Zhu</div>
<div class="meta-line">First: 2026-01-21T15:12:11+00:00 · Latest: 2026-01-21T15:12:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15065v1">PDF</a> · <a href="https://github.com/lounwb/FoBoR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过前景和背景的细化增强少样本分布外检测</div>
<div class="mono" style="margin-top:8px">基于CLIP的前景-背景（FG-BG）分解方法在提高少样本分布外（OOD）检测性能方面表现出显著的有效性。然而，现有方法仍然存在若干局限性。对于从分解中获得的背景区域，现有方法对所有补丁采用统一的抑制策略，忽视了不同补丁对预测的不同贡献。对于前景区域，现有方法未能充分考虑某些局部补丁可能与其他类别在外观或语义上相似，这可能误导训练过程。为了解决这些问题，我们提出了一种新的即插即用框架。该框架由三个核心组件组成：（1）前景-背景分解模块，遵循以前的FG-BG方法将图像分为前景和背景区域；（2）自适应背景抑制模块，自适应地加权补丁分类熵；（3）可混淆前景校正模块，识别并校正可混淆的前景补丁。大量实验结果表明，所提出的即插即用框架显著提高了现有FG-BG分解方法的性能。代码可在：https://github.com/lounwb/FoBoR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance few-shot out-of-distribution detection performance, which is limited by existing foreground-background decomposition methods. The authors propose a new framework that includes a Foreground-Background Decomposition module, an Adaptive Background Suppression module, and a Confusable Foreground Rectification module to address the shortcomings of uniform suppression and misclassification of local patches. Experimental results show that this framework significantly improves the performance of existing methods in detecting out-of-distribution samples.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高少样本异常检测性能，这对可靠的机器学习应用至关重要。作者提出了一个新框架，包括一个前景-背景分解模块，用于将图像分为前景和背景，一个自适应背景抑制模块，用于加权补丁分类熵，以及一个混淆前景修正模块，以解决误导性局部补丁的问题。实验结果表明，该框架显著提高了现有前景-背景分解方法的性能，解决了当前方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Leakage with Generative Flow Matching Denoiser</div>
<div class="meta-line">Authors: Isaac Baglin, Xiatian Zhu, Simon Hadfield</div>
<div class="meta-line">First: 2026-01-21T14:51:01+00:00 · Latest: 2026-01-21T14:51:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15049v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度泄露与生成流匹配去噪器</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）作为一种强大的去中心化模型训练范式，然而它仍然容易受到深度泄露（DL）攻击，这些攻击通过共享模型更新重建私有客户端数据。尽管先前的DL方法在不同程度上取得了一定成功，但它们通常在现实FL环境下存在不稳定、保真度有限或鲁棒性差的问题。我们提出了一种新的DL攻击，将生成流匹配（FM）先验整合到重建过程中。通过引导优化朝向真实图像的分布（由流匹配基础模型表示），我们的方法在不需要私有数据知识的情况下增强了重建保真度。在多个数据集和目标模型上的广泛实验表明，我们的方法在像素级、感知和基于特征的相似性度量上始终优于最先进的攻击。重要的是，该方法在不同的训练周期、更大的客户端批量大小以及在常见防御措施如噪声注入、裁剪和稀疏化下仍然有效。我们的发现呼吁开发新的防御策略，明确考虑配备强大生成先验的对手。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerability of Federated Learning (FL) to deep leakage attacks, which can reconstruct private client data from shared model updates. The authors propose a novel deep leakage attack that incorporates a generative Flow Matching prior into the reconstruction process, enhancing the fidelity of the reconstructed images without needing access to the private data. Experimental results across various datasets and target models show that this approach consistently outperforms existing state-of-the-art attacks in terms of pixel-level, perceptual, and feature-based similarity metrics, while maintaining effectiveness under different training conditions and common defenses, highlighting the need for new defense strategies against such advanced attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决联邦学习（FL）在共享模型更新中对深度泄露攻击的脆弱性，这种攻击可以重建私有客户端数据。作者提出了一种新颖的深度泄露攻击，将生成的流匹配先验融入重建过程中，从而在不需要访问私有数据的情况下提高重建数据的保真度。跨多个数据集和目标模型的实验结果表明，该方法在像素级、感知和特征相似性指标上始终优于现有的最先进攻击，同时在不同的训练条件和常见防御下保持有效性，突显了针对这种先进对抗技术的新防御策略的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</div>
<div class="meta-line">Authors: Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers</div>
<div class="meta-line">First: 2026-01-21T14:42:33+00:00 · Latest: 2026-01-21T14:42:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于课程的深度强化学习框架用于电动车路径规划问题</div>
<div class="mono" style="margin-top:8px">带时间窗的电动车路径规划问题（EVRPTW）是可持续物流中的复杂优化问题，路径决策必须在满足严格客户时间约束的同时，最小化总旅行距离、车队规模和电池使用。尽管深度强化学习（DRL）作为经典启发式和精确求解器的替代方案显示出巨大潜力，但现有的DRL模型在约束密集时往往难以保持训练稳定性，无法收敛或泛化。在本研究中，我们提出了一种基于课程的深度强化学习（CB-DRL）框架，旨在解决这种不稳定性。该框架利用结构化的三阶段课程，逐步增加问题复杂性：代理首先学习距离和车队优化（阶段A），然后是电池管理（阶段B），最后是完整的EVRPTW（阶段C）。为了确保各阶段的稳定学习，该框架采用了修改后的近端策略优化算法，具有阶段特定的超参数、价值和优势裁剪，以及自适应学习率调度。策略网络基于异构图注意力编码器构建，增强了全局-局部注意力和特征线性调制。这种专门的架构明确捕捉了仓库、客户和充电站的不同特性。模型仅在N=10客户的小实例上训练，显示出对N=5到N=100的未见实例的强泛化能力，在中等规模问题上显著优于标准基线。实验结果确认，这种课程引导的方法在标准DRL基线失败的分布外实例上实现了高可行性率和竞争性解质量，有效弥合了神经速度与操作可靠性之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the electric vehicle routing problem with time windows (EVRPTW), a challenging optimization issue in sustainable logistics that requires minimizing travel distance, fleet size, and battery usage while adhering to strict time constraints. To tackle the instability often seen in deep reinforcement learning (DRL) models, the authors propose a curriculum-based deep reinforcement learning (CB-DRL) framework that employs a structured three-phase curriculum to gradually increase problem complexity. Experimental results show that the CB-DRL framework, trained on small instances, achieves robust generalization to larger unseen instances, significantly outperforming standard baselines in terms of feasibility rates and solution quality, particularly in scenarios where traditional DRL approaches struggle.</div>
<div class="mono" style="margin-top:8px">本研究针对带时间窗口的电动车辆路径规划问题（EVRPTW），这是一个在可持续物流中面临的复杂优化问题，需要在严格的时间约束下平衡旅行距离、车队规模和电池使用。为了应对深度强化学习（DRL）模型中常见的不稳定性，作者提出了一种基于课程的深度强化学习（CB-DRL）框架，该框架引入了一个结构化的三阶段学习过程，首先关注距离和车队优化，然后是电池管理，最后是完整的EVRPTW。实验结果表明，CB-DRL框架在小规模实例上训练后，能够很好地推广到更大且未见过的实例，达到高可行性率，并在中等规模问题上超越标准DRL基线，从而有效提升复杂路径规划场景中的操作可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：用于遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有的视觉语言模型（VLMs）主要在网络抓取的嘈杂图像-文本数据上训练，缺乏对遥感（RS）专业领域的充分接触。这一缺陷导致在RS特定任务上的表现不佳，因为常用数据集往往缺乏详细、科学准确的文本描述，而仅强调日期和位置等属性。为弥补这一关键缺口，我们推出GAIA，这是一个为多尺度、多传感器和多模态RS图像分析设计的新数据集。GAIA包含201,005对精心策划的RS图像-文本对，代表了与不同空间分辨率相关的多样化RS模态。与现有的RS视觉语言数据集不同，GAIA特别关注捕捉多样化的RS应用，提供有关环境变化、自然灾害和其他各种动态现象的独特信息。该数据集提供了空间和时间上平衡的分布，覆盖全球，涵盖过去25年，观察的时间分布也很平衡。GAIA的构建涉及两个阶段的过程：（1）从信誉良好的RS相关来源有针对性地抓取图像及其伴随文本，和（2）为每张图像生成五个高质量、科学基础的合成标题，使用精心设计的提示，利用GPT-4o的先进视觉语言能力。我们的广泛实验，包括对CLIP和BLIP2模型的微调，表明GAIA显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。我们在项目的GitHub仓库上公开了我们的数据集、自动处理框架和微调模型权重：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing Vision-Language Models (VLMs) that are primarily trained on noisy web-scraped data, which inadequately supports remote sensing (RS) tasks due to a lack of scientifically accurate textual descriptions. The authors introduce GAIA, a comprehensive dataset consisting of 201,005 curated RS image-text pairs, created through a two-stage process involving targeted web-scraping and the generation of high-quality synthetic captions using GPT-4o. Experimental results show that fine-tuning models like CLIP and BLIP2 on GAIA leads to significant improvements in RS image classification, cross-modal retrieval, and image captioning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视觉语言模型（VLM）在训练过程中主要依赖于噪声较大的网络抓取数据，导致在遥感（RS）任务中表现不佳的问题，因为缺乏科学准确的文本描述。作者提出了GAIA，一个包含201,005对精心策划的RS图像-文本对的数据集，该数据集通过针对性的网络抓取和使用GPT-4o生成高质量合成标题的两阶段过程创建。实验结果表明，对CLIP和BLIP2等模型进行GAIA微调显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</div>
<div class="meta-line">Authors: Kangcheng Zhou, Jun Jiang, Qing Zhang, Shuang Zheng, Qingli Li, Shugong Xu</div>
<div class="meta-line">First: 2026-01-21T08:21:35+00:00 · Latest: 2026-01-21T08:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14757v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14757v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReinPath：一种用于病理学的多模态强化学习方法</div>
<div class="mono" style="margin-top:8px">可解释性在计算病理学中至关重要，促使从组织病理图像和相应文本数据中开发多模态信息集成。然而，现有的多模态方法由于缺乏支持明确推理和推断的高质量数据集以及简单的推理过程，导致可解释性有限。为了解决上述问题，我们引入了一种具有强大推理能力的新型多模态病理大语言模型。为了提高准确且上下文相关的文本描述的生成，我们设计了一种与组相对策略优化相结合的语义奖励策略。我们构建了一个高质量的病理视觉问答（VQA）数据集，专门设计用于支持复杂推理任务。在该数据集上进行的全面实验表明，我们的方法在性能上优于最先进的方法，即使仅用20%的数据进行训练。我们的方法在下游零样本图像分类任务中也与CLIP相比表现出可比的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance interpretability in computational pathology by integrating multimodal information from histopathological images and corresponding text data. The authors propose a novel multimodal pathology large language model that incorporates a semantic reward strategy and group relative policy optimization to improve the generation of accurate textual descriptions. Experimental results show that this approach outperforms existing state-of-the-art methods on a newly constructed high-quality visual question answering dataset, achieving superior performance even with only 20% of the training data, and demonstrates comparable results in downstream zero-shot image classification tasks when compared to CLIP.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过整合来自组织病理图像和相应文本数据的多模态信息来增强计算病理学中的可解释性。作者提出了一种新颖的多模态病理大语言模型，该模型结合了语义奖励策略和群体相对策略优化，以改善准确文本描述的生成。实验结果表明，该方法在新构建的高质量视觉问答数据集上优于最先进的方法，即使仅使用20%的训练数据也能实现卓越的性能，并且在与CLIP的下游零样本图像分类任务中表现出可比的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</div>
<div class="meta-line">Authors: Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</div>
<div class="meta-line">First: 2025-12-19T01:39:43+00:00 · Latest: 2026-01-21T07:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17160v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成图像能否作为有效且高效的类别原型？</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在零样本图像分类任务中表现出色。然而，现有方法，包括对比语言-图像预训练（CLIP），都依赖于注释的文本-图像对来对齐视觉和文本模态。这种依赖引入了准备高质量数据集的巨大成本和准确性要求。同时，从两种模式处理数据也需要双塔编码器，这也阻碍了它们的轻量化。为了解决这些限制，我们提出了“基于大型语言模型生成的对比语言-图像预训练（LGCLIP）”框架。LGCLIP利用大型语言模型（LLM）生成特定类别的提示，引导扩散模型合成参考图像。随后，这些生成的图像作为视觉原型，真实图像的视觉特征被提取并与这些原型的视觉特征进行比较，以实现比较预测。通过优化LLM的提示生成并仅使用视觉编码器，LGCLIP保持轻量和高效。关键是，我们的框架在整个实验过程中仅需要类别标签作为输入，消除了对手动注释的图像-文本对和额外预处理的需求。实验结果验证了LGCLIP的可行性和效率，在零样本分类任务中表现出色，并建立了分类的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and effectiveness of zero-shot image classification tasks without relying on annotated text-to-image pairs, which are costly and require significant accuracy. The authors propose a new framework called LGCLIP, which utilizes a Large Language Model to generate class-specific prompts that guide a diffusion model in synthesizing reference images, allowing these synthetic images to serve as visual prototypes. Experimental results show that LGCLIP achieves strong performance in zero-shot classification tasks while remaining lightweight and efficient, as it only requires class labels as input, thus eliminating the need for manually annotated datasets.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有视觉语言模型（VLM）在零-shot图像分类中依赖注释文本-图像对的局限性，这种依赖可能成本高且繁琐。作者提出了一种新框架LGCLIP，该框架利用大型语言模型生成特定类别的提示，指导扩散模型合成参考图像，使这些合成图像可以作为视觉原型。实验结果表明，LGCLIP在零-shot分类任务中表现出色，同时保持轻量和高效，因为它只需要类别标签，而无需手动注释的数据集。</div>
</details>
</div>
<div class="card">
<div class="title">RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models</div>
<div class="meta-line">Authors: Sha Luo, Yogesh Prabhu, Timothy Ossowski, Kaiping Chen, Junjie Hu</div>
<div class="meta-line">First: 2026-01-06T19:14:49+00:00 · Latest: 2026-01-21T06:11:42+00:00</div>
<div class="meta-line">Comments: *updated author email in this version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03369v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03369v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RiskCueBench：基于早期风险线索的预期推理基准测试</div>
<div class="mono" style="margin-top:8px">随着以视频为中心的社交媒体的快速增长，从视觉数据中预测风险事件的能力是确保公共安全和防止现实世界事故的一个有前景的方向。先前的研究广泛探讨了在驾驶、抗议和自然灾害等领域的监督视频风险评估。然而，许多现有数据集允许模型访问完整的视频序列，包括事故本身，这大大降低了任务的难度。为了更好地反映现实世界的条件，我们引入了一个新的视频理解基准RiskCueBench，其中视频经过仔细注释，以识别风险信号片段，定义为指示潜在安全隐患的最早时刻。实验结果揭示了当前系统在解释不断变化的情况和从早期视觉信号中预测未来风险事件的能力方面存在显著差距，突显了在实践中部署视频风险预测模型的重要挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance public safety by improving the ability to anticipate risky events from visual data in video-centered social media. The authors introduce RiskCueBench, a new benchmark that focuses on identifying risk signal clips, which are the earliest indicators of potential safety concerns, rather than providing access to full video sequences. Experimental findings indicate that current video language models struggle significantly to interpret evolving situations and predict future risks from these early visual cues, underscoring the challenges faced in deploying effective video risk prediction systems in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提高从视频社交媒体中的视觉数据中预测风险事件的能力来增强公共安全。作者引入了一个新的基准，称为RiskCueBench，重点识别视频中的风险信号片段，代表潜在安全隐患的最早指示，而不是提供完整视频序列的访问。实验结果表明，当前模型在解释不断变化的情况和从早期视觉线索中预测未来风险方面存在显著困难，突显了在实际场景中实施有效视频风险预测系统所面临的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning</div>
<div class="meta-line">Authors: Jiaying Wu, Can Gao, Jinglu Hu, Hui Li, Xiaofeng Cao, Jingcai Guo</div>
<div class="meta-line">First: 2026-01-20T16:06:23+00:00 · Latest: 2026-01-20T16:06:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMCE：基于概率的多粒度语义与标题引导增强的少样本学习</div>
<div class="mono" style="margin-top:8px">少样本学习旨在仅从少量标记样本中识别新类别，其中从稀缺数据估计的原型往往存在偏差且泛化能力差。基于语义的方法通过引入粗略的类别级信息来缓解这一问题，但它们主要应用于支持侧，查询表示保持不变。本文提出了PMCE，一种利用多粒度语义与标题引导增强的概率少样本框架。PMCE构建了一个非参数知识库，存储每个类别的视觉统计信息以及基础类别的CLIP编码类名嵌入。在元测试时，根据每个新类别的类名嵌入的相似性检索最相关的基础类别。这些统计信息随后被聚合为类别特定的先验信息，并通过简单的MAP更新与支持集原型融合。同时，一个冻结的BLIP标题生成器提供无标签的实例级图像描述，而一个在基础类别上训练的轻量级增强器在归纳协议下优化支持原型和查询特征，并通过一致性正则化来稳定噪声标题。在四个基准上的实验表明，PMCE在强基线之上持续改进，在1-shot设置下在MiniImageNet上实现了相对于最强语义竞争者高达7.71%的绝对增益。我们的代码可在https://anonymous.4open.science/r/PMCE-275D获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot learning, which struggles with biased prototypes and poor generalization from limited labeled samples. The authors introduce PMCE, a probabilistic framework that utilizes multi-granularity semantics and caption-guided enhancement, constructing a nonparametric knowledge bank that stores visual statistics and class name embeddings. Experimental results demonstrate that PMCE outperforms strong baselines, achieving an absolute gain of up to 7.71% over the best semantic competitor on the MiniImageNet dataset in the 1-shot learning scenario.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善少样本学习，该领域在有限标记样本中面临偏倚原型和较差泛化的问题。作者提出了PMCE，这是一种利用多粒度语义和基于标题增强的概率框架，构建了一个结合视觉统计和类别名称嵌入的非参数知识库。实验结果表明，PMCE显著优于强基线，在MiniImageNet数据集的1-shot学习场景中，相较于最佳语义竞争者实现了高达7.71%的绝对增益。</div>
</details>
</div>
<div class="card">
<div class="title">Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model</div>
<div class="meta-line">Authors: Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu, Yuyang Zhang, Longxiang Gao, Shuaiguang Li, Xingyu Li, Yanran Xu, Changwei Wang</div>
<div class="meta-line">First: 2026-01-20T15:06:10+00:00 · Latest: 2026-01-20T15:06:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你也需要视觉：利用多模态大语言模型导航分布外检测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测是一项关键任务，受到了广泛关注。CLIP的出现促进了零样本OOD检测的广泛研究，通常采用无训练的方法。目前的方法利用大型语言模型（LLMs）的专家知识来识别潜在的异常值。然而，这些方法往往过于依赖文本空间的知识，忽视了在图像空间中检测分布外样本所面临的固有挑战。本文提出了一种新颖的管道MM-OOD，利用多模态大语言模型（MLLMs）的多模态推理能力及其进行多轮对话的能力来增强异常值检测。我们的方法旨在提高近OOD和远OOD任务的性能。具体而言，(1) 对于近OOD任务，我们直接将ID图像和相应的文本提示输入MLLMs以识别潜在的异常值；(2) 对于远OOD任务，我们引入了草图-生成-详细框架：首先，我们使用文本提示草拟异常值暴露，然后生成相应的视觉OOD样本，最后通过多模态提示进行详细说明。实验表明，我们的方法在广泛使用的多模态数据集（如Food-101）上取得了显著的改进，同时验证了其在ImageNet-1K上的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current out-of-distribution (OOD) detection methods, which often rely heavily on textual knowledge and overlook challenges in the image domain. The authors propose a novel pipeline called MM-OOD that utilizes the multimodal reasoning capabilities of multimodal large language models (MLLMs) to enhance outlier detection through multi-round conversations. Experimental results show that MM-OOD significantly improves performance on near OOD and far OOD tasks across various multimodal datasets, including Food-101 and ImageNet-1K.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决分布外（OOD）检测中的挑战，特别是在多模态数据的背景下，现有方法往往过于依赖文本信息。作者提出了一种名为MM-OOD的新型管道，利用大型语言模型（LLM）的多模态推理能力，通过多轮对话方法增强异常值检测。实验结果表明，MM-OOD在近OOD和远OOD任务上显著提高了性能，在Food-101等数据集上取得了显著进展，并在ImageNet-1K上展示了可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3</div>
<div class="meta-line">Authors: Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li</div>
<div class="meta-line">First: 2026-01-20T12:25:41+00:00 · Latest: 2026-01-20T12:25:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniOVCD：利用SAM 3 简化开放词汇变化检测</div>
<div class="mono" style="margin-top:8px">变化检测（CD）是遥感中的一项基本任务，监测土地覆盖的演变。基于此，开放词汇变化检测（OVCD）提出了新的要求，旨在减少对预定义类别的依赖。现有的无训练OVCD方法主要使用CLIP来识别类别，这些方法还需要额外的模型如DINO来提取特征。然而，结合不同模型往往会导致特征匹配问题，使系统不稳定。最近，推出了Segment Anything Model 3（SAM 3），它将分割和识别能力集成在一个可提示模型中，为OVCD任务提供了新的可能性。本文提出了OmniOVCD，一个为OVCD设计的独立框架。通过利用SAM 3的解耦输出头，我们提出了一种协同融合到实例解耦（SFID）策略。SFID首先融合SAM 3的语义、实例和存在输出以构建土地覆盖掩膜，然后将其分解为单个实例掩膜以进行变化比较。该设计在类别识别中保持高准确性，并在图像间保持实例级一致性。因此，模型能够生成准确的变化掩膜。在四个公共基准（LEVIR-CD、WHU-CD、S2Looking和SECOND）上的实验表明，模型达到了SOTA性能，分别实现了67.2、66.5、24.5和27.1（类别平均）的IoU分数，超越了所有先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Open-Vocabulary Change Detection (OVCD) in remote sensing by reducing reliance on predefined categories. The authors introduce OmniOVCD, a standalone framework that utilizes the Segment Anything Model 3 (SAM 3) to streamline the change detection process. By implementing a Synergistic Fusion to Instance Decoupling (SFID) strategy, the framework effectively combines and decouples outputs from SAM 3 to create accurate land-cover masks and individual instance masks for change comparison. Experimental results on four public benchmarks show that OmniOVCD achieves state-of-the-art performance, with Intersection over Union (IoU) scores of 67.2, 66.5, 24.5, and 27.1, outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过减少对预定义类别的依赖来增强遥感中的开放词汇变化检测（OVCD）。作者提出了OmniOVCD，一个独立的框架，利用Segment Anything Model 3（SAM 3），并引入了协同融合到实例解耦（SFID）策略。该方法有效地融合和解耦SAM 3的输出，以创建准确的土地覆盖掩膜和单个实例掩膜进行变化检测。在四个公共基准上的实验结果表明，OmniOVCD实现了最先进的性能，交并比（IoU）得分分别为67.2、66.5、24.5和27.1，超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍然存在分歧：视觉-语言模型（如CLIP）在全局语义对齐方面表现出色，但缺乏空间精度，而自监督方法（如MAE、DINO）捕捉复杂的局部结构，但在高层语义上下文中表现不佳。我们认为这些范式在根本上是互补的，可以整合到一个有原则的多任务框架中，并通过密集的空间监督进一步增强。我们介绍了MTV，一个多任务视觉预训练框架，联合优化视觉-语言对比、自监督和密集空间目标的共享主干。为了减少对手动标注的需求，我们利用高容量的“专家”模型——如Depth Anything V2和OWLv2——在大规模上合成密集的、结构化的伪标签。除了框架之外，我们还系统地研究了多任务视觉学习的机制，分析：（i）每个目标的边际收益，（ii）任务协同与干扰，以及（iii）在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了“兼具两全其美”的性能，显著增强了细粒度空间推理，而不妨碍全局语义理解。我们的发现表明，多任务学习在高质量伪监督的推动下，是通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current visual representation learning methods, which are often divided between vision-language models that excel in semantic alignment but lack spatial precision, and self-supervised methods that capture local structures but struggle with high-level semantics. The authors propose a multi-task visual pretraining framework called MTV that integrates these complementary paradigms by optimizing a shared backbone across various objectives, including vision-language contrastive, self-supervised, and dense spatial tasks, while using expert models to generate structured pseudo-labels. The experimental results indicate that MTV significantly improves fine-grained spatial reasoning and maintains global semantic understanding, demonstrating the effectiveness of multi-task learning with high-quality pseudo-supervision for developing more general visual encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前视觉表征学习方法的局限性，这些方法要么在全局语义对齐方面表现出色，要么在捕捉局部结构方面表现良好，但无法兼顾两者。作者提出了一种名为MTV的多任务视觉预训练框架，该框架整合了视觉-语言对比、自监督和密集空间目标，同时利用高容量专家模型生成结构化伪标签，无需手动注释。实验结果表明，MTV显著提高了细粒度空间推理能力，并保持了强大的全局语义理解，证明了高质量伪监督的多任务学习可以导致更有效的视觉编码器。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive Generative Augmentation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Wei Liu, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-20T08:09:38+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v3">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应生成增强与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两项协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样但语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它使MoCov2在ImageNet线性分类上提高了2.5%。在视觉-语言学习中，它在十个数据集上使平均零-shot分类准确率比CLIP提高了12.31%，比SLIP提高了5.31%，并进一步使Flickr30k文本检索R@5提高了3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance contrastive learning by addressing the limitations in the construction of high-quality positive pairs and the lack of a quality assessment mechanism in current methods. The authors propose GenView++, a unified framework that employs a multi-source adaptive view generation mechanism to create diverse and semantically coherent views, while also implementing a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ improves MoCov2 by 2.5% on ImageNet linear classification and increases zero-shot classification accuracy by 12.31% over CLIP and 5.31% over SLIP across ten datasets, along with a 3.2% improvement in Flickr30k text retrieval R@5.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决当前方法在构建高质量正样本对和缺乏质量评估机制方面的局限性，来增强对比学习的有效性。作者提出了GenView++，一个统一框架，引入了多源自适应视图生成机制，以合成多样且语义一致的视图，并结合质量驱动的对比学习机制，根据语义对齐和多样性动态调整训练贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的表现2.5%，在十个数据集上平均零样本分类准确率比CLIP提高了12.31%，比SLIP提高了5.31%，同时在Flickr30k文本检索R@5上提高了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</div>
<div class="meta-line">Authors: Donghee Lee, Rui Cai, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-20T05:44:33+00:00 · Latest: 2026-01-20T05:44:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13622v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARPE：通过集成实现的大型视觉语言模型的上下文感知图像表示优先级</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）的最新进展使其更接近成为通用助手。尽管表现强劲，LVLMs在图像分类等视觉中心任务上仍然存在困难，表现不及其基础视觉编码器，后者通常是基于CLIP的模型。为了解决这一限制，我们提出了上下文感知图像表示优先级通过集成（CARPE），这是一种新颖的模型无关框架，引入了视觉集成层和上下文感知集成策略，以识别何时优先考虑图像表示或依赖语言模型的推理能力。该设计增强了模型自适应加权视觉和文本模态的能力，使模型能够捕捉图像表示的各个方面，从而在分类和视觉语言基准测试中实现一致的泛化改进。大量实验表明，CARPE不仅提高了图像分类基准的性能，还增强了各种视觉语言基准的结果。最后，CARPE旨在与大多数由视觉编码器和语言模型组成的开源LVLMs有效集成，确保其在多种架构中的适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Large Vision-Language Models (LVLMs) in vision-centric tasks, where they currently lag behind traditional vision encoders like CLIP. The authors propose a novel framework called Context-Aware Image Representation Prioritization via Ensemble (CARPE), which incorporates vision-integration layers and a context-aware ensemble strategy to optimize the use of image representations and language reasoning. Experimental results show that CARPE significantly improves performance on image classification benchmarks and enhances generalization across various vision-language tasks, demonstrating its effectiveness and adaptability with existing LVLM architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型视觉语言模型（LVLMs）在视觉中心任务中的表现，因为它们目前在这方面的表现不及其基础视觉编码器。作者提出了一种名为上下文感知图像表示优先级排序的集成框架（CARPE），该框架结合了视觉集成层和上下文感知集成策略，以优化图像表示和语言模型推理的使用。实验结果表明，CARPE显著提高了图像分类基准的性能，并改善了各种视觉语言任务的泛化能力，证明了其与现有LVLM架构的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</div>
<div class="meta-line">Authors: Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour</div>
<div class="meta-line">First: 2025-09-03T17:56:46+00:00 · Latest: 2026-01-19T23:50:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03515v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.03515v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Waymo开放运动数据集能否支持现实行为建模？基于自然轨迹的验证研究</div>
<div class="mono" style="margin-top:8px">Waymo开放运动数据集（WOMD）已成为自主车辆（AV）行为数据驱动建模的热门资源。然而，由于专有后处理、缺乏误差量化以及将轨迹分割为20秒片段，其在行为分析中的有效性仍不确定。本研究考察WOMD是否准确捕捉到现实世界AV操作中的动态和交互。利用在亚利桑那州凤凰城（PHX）进行的4级AV操作独立收集的自然数据集，我们在三个典型城市驾驶场景中进行比较分析：在信号交叉口的卸载、跟车和变道行为。在卸载分析中，手动从航拍视频中提取车距，以确保测量误差微乎其微。对于跟车和变道情况，我们应用模拟外推（SIMEX）方法来考虑PHX数据中经验估计的误差，并使用动态时间规整（DTW）距离量化行为差异。所有场景的结果一致表明，PHX中的行为超出了WOMD的行为范围。值得注意的是，WOMD对短车距和突然减速的表现不足。这些发现表明，仅基于WOMD校准的行为模型可能系统性低估自然驾驶的变异性、风险和复杂性。因此，在没有与独立收集的数据进行适当验证的情况下，使用WOMD进行行为建模时应谨慎。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the validity of the Waymo Open Motion Dataset (WOMD) for modeling autonomous vehicle behavior, motivated by concerns over its proprietary processing and lack of error quantification. The researchers compare WOMD with an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona, analyzing three urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing. The results reveal that the behavior observed in Phoenix significantly deviates from WOMD, particularly showing that WOMD underrepresents short headways and abrupt decelerations, indicating that models based solely on WOMD may underestimate the complexity and risk of real-world driving scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨Waymo开放运动数据集（WOMD）在建模自动驾驶汽车行为方面的有效性，动机源于对其专有后处理和缺乏误差量化的担忧。研究人员使用来自亚利桑那州凤凰城的独立收集的自然驾驶数据集进行了比较分析，重点关注三个城市驾驶场景：在信号交叉口的卸载、跟车和变道。结果表明，凤凰城观察到的行为始终超出了WOMD的行为范围，WOMD明显低估了短车距和突然减速，这表明仅基于WOMD的模型可能会低估现实驾驶场景的复杂性和风险。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</div>
<div class="meta-line">Authors: Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</div>
<div class="meta-line">First: 2026-01-19T22:55:30+00:00 · Latest: 2026-01-19T22:55:30+00:00</div>
<div class="meta-line">Comments: 10 pages,4 images</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13440v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的异常分类与分割方法分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM），特别是CLIP，通过实现零样本和少样本缺陷识别，彻底改变了异常检测，无需大量标注数据集。通过学习图像和文本的对齐表示，VLM通过正常和异常状态的自然语言描述促进异常分类和分割，消除了对特定任务训练或缺陷示例的传统要求。本项目对基于VLM的异常分类（AC）和异常分割（AS）方法进行了全面分析。我们系统地研究了关键架构范式，包括基于滑动窗口的密集特征提取（WinCLIP）、具有可学习投影的多阶段特征对齐（AprilLab框架）和组合提示集成策略。我们的分析在关键维度上评估这些方法：特征提取机制、文本-视觉对齐策略、提示工程技术、零样本与少样本的权衡、计算效率和跨领域泛化。通过在MVTec AD和VisA等基准上的严格实验，我们比较了分类准确性、分割精度和推理效率。主要贡献是对VLM在异常检测中成功的基础理解，综合了方法选择的实用见解并识别当前的局限性。本研究旨在促进VLM方法在工业质量控制中的知情采用，并指导未来的研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the effectiveness of Vision-Language Models (VLMs), particularly CLIP, in anomaly detection without the need for extensive labeled datasets. The study employs a comprehensive analysis of various VLM-based approaches for anomaly classification and segmentation, investigating architectural paradigms such as sliding window-based dense feature extraction and multi-stage feature alignment. Key experimental findings reveal that these methods demonstrate significant capabilities in classification accuracy and segmentation precision across benchmarks like MVTec AD and VisA, while also highlighting the trade-offs between zero-shot and few-shot learning, computational efficiency, and cross-domain generalization, ultimately providing insights for practical application in industrial quality control and future research directions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要有效的异常检测方法，而不依赖于大量标注数据集。作者分析了多种视觉语言模型（VLM）方法，包括WinCLIP、AprilLab框架和组合提示集成策略，以评估它们在异常分类和分割中的有效性。关键实验结果表明，这些方法在特征提取、文本-视觉对齐和计算效率等方面存在显著差异，通过在MVTec AD和VisA等基准上的严格测试，展示了它们的分类准确性和分割精度，最终为工业质量控制的实际应用和未来研究方向提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations</div>
<div class="meta-line">Authors: Tim Lachmann, Alexandra Israelsson, Christina Tornberg, Teimuraz Saghinadze, Michal Balazia, Philipp Müller, Petri Laukka</div>
<div class="meta-line">First: 2026-01-19T16:59:45+00:00 · Latest: 2026-01-19T16:59:45+00:00</div>
<div class="meta-line">Comments: Accepted for publication at IEEE Face &amp; Gesture 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有混合情感都是平等的：带有相对显著性注释的混合情感表达BLEMORE数据集</div>
<div class="mono" style="margin-top:8px">人类常常同时体验多种情感的混合，而不仅仅是单一的基本情感，这些情感的显著性各不相同。尽管混合情感的重要性不言而喻，但大多数基于视频的情感识别方法仅设计用于识别单一情感。少数尝试识别混合情感的方法通常无法评估混合情感中各情感的相对显著性。这一局限性主要源于缺乏包含大量带有相对显著性注释的混合情感样本的数据集。为了解决这一问题，我们推出了BLEMORE，这是一个用于多模态（视频、音频）混合情感识别的新数据集，包含每种情感在混合中的相对显著性信息。BLEMORE包含来自58位演员的3000多个片段，表演6种基本情感和10种不同的混合情感，每种混合情感有3种不同的显著性配置（50/50、70/30和30/70）。利用该数据集，我们对两项混合情感预测任务进行了广泛评估： (1) 预测给定样本中情感的存在，(2) 预测混合情感中情感的相对显著性。我们的结果表明，单模态分类器在验证集上实现了最高29%的存在准确率和13%的显著性准确率，而多模态方法则显著改善，ImageBind + WavLM达到了35%的存在准确率，HiCMAE达到了18%的显著性准确率。在保留的测试集上，最佳模型实现了33%的存在准确率（VideoMAEv2 + HuBERT）和18%的显著性准确率（HiCMAE）。总之，BLEMORE数据集为推进考虑混合情感表达复杂性和重要性的情感识别系统研究提供了宝贵资源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inadequacy of existing video-based emotion recognition systems that primarily focus on single emotions, neglecting the complexity of blended emotions with varying salience. To tackle this issue, the authors introduce the BLEMORE dataset, which contains over 3,000 video clips featuring 58 actors expressing 6 basic emotions and 10 distinct blends, each annotated with relative salience configurations. Experimental results demonstrate that while unimodal classifiers achieve up to 29% accuracy in predicting emotion presence and 13% in salience, multimodal approaches significantly improve performance, with the best models reaching 35% presence accuracy and 18% salience accuracy on the validation set.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于识别混合情感的需求，这种情感通常是同时体验的，但现有的情感识别系统主要关注单一情感，未能充分解决这一问题。为了解决这一问题，作者引入了BLEMORE数据集，该数据集包含3000多个视频片段，并对各种混合情感进行了相对显著性的注释。实验结果表明，尽管单模态分类器在检测情感存在方面的准确率最高可达29%，但多模态方法显著提高了性能，最佳模型在测试集上达到了33%的情感存在准确率和18%的显著性准确率，突显了该数据集在增强情感识别系统方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</div>
<div class="meta-line">Authors: Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan</div>
<div class="meta-line">First: 2026-01-19T15:19:28+00:00 · Latest: 2026-01-19T15:19:28+00:00</div>
<div class="meta-line">Comments: Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的可适应自监督学习用于以人为中心的视觉任务</div>
<div class="mono" style="margin-top:8px">以人为中心的视觉分析在监控、医疗保健和人机交互等多种应用中发挥着关键作用。随着大规模无标签人类图像数据集的出现，对能够支持多样化以人为中心的下游任务的一般无监督预训练模型的需求日益增加。为实现这一目标，我们提出了CLASP（基于CLIP的可适应自监督学习），这是一个旨在进行以人为中心的视觉任务的无监督预训练的新框架。CLASP利用强大的视觉-语言模型CLIP生成低级（例如，身体部位）和高级（例如，属性）语义伪标签。这些多层次的语义线索被整合到学习的视觉表示中，丰富了它们的表现力和泛化能力。考虑到不同的下游任务对语义粒度的需求不同，CLASP结合了一个提示控制的专家混合（MoE）模块。MoE根据特定任务的提示动态调整特征提取，减轻潜在的特征冲突并增强可迁移性。此外，CLASP采用多任务预训练策略，其中来自CLIP的部分和属性级伪标签指导表示学习过程。在多个基准上的广泛实验表明，CLASP始终优于现有的无监督预训练方法，推动了以人为中心的视觉分析领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the need for a general unsupervised pre-training model for human-centric visual tasks, which are crucial in various applications such as surveillance and healthcare. The authors propose CLASP, a framework that utilizes the CLIP vision-language model to generate multi-level semantic pseudo-labels, which are integrated into visual representations. Experimental results show that CLASP outperforms existing unsupervised pre-training methods across multiple benchmarks, demonstrating its effectiveness in enhancing the expressiveness and generalizability of learned representations for human-centric tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要一个通用的无监督预训练模型，以支持各种以人为中心的视觉任务，特别是在大规模未标记的人类图像数据集可用的情况下。作者提出了CLASP框架，该框架利用CLIP视觉-语言模型生成多层次的语义伪标签，并将其整合到视觉表示中。实验结果表明，CLASP在多个基准测试中优于现有的无监督预训练方法，证明了其在增强以人为中心的视觉分析中学习表示的表现力和泛化能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</div>
<div class="meta-line">Authors: Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</div>
<div class="meta-line">First: 2025-10-11T10:17:38+00:00 · Latest: 2026-01-19T15:00:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10150v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process carries a critical risk: entropy collapse. This phenomenon is a rapid decrease in policy entropy, which severely limits exploration and diminishes learning effectiveness. Recent methods attempt to mitigate this collapse via heuristic entropy interventions, yet the underlying mechanisms governing entropy remain unclear. In this work, we conduct a theoretical and quantitative analysis of GRPO&#x27;s entropy dynamics, revealing that token-level entropy change in each update step is jointly governed by four key factors: clipping strategy, advantage, token probability, and token entropy. These findings not only explain the mechanisms of existing methods, but also reveal their limitations: they rely on heuristic adjustments to only one or two factors, leaving other relevant factors unconsidered and reducing their effectiveness. This motivates us to propose a new method, STEER, which adaptively reweights tokens based on their estimated entropy change to regulate entropy in a principled manner. Experiments on both math and coding benchmarks demonstrate that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考RLVR中的熵干预：熵变化视角</div>
<div class="mono" style="margin-top:8px">尽管具有可验证奖励的强化学习（RLVR）可以增强大型语言模型（LLM）的推理能力，但其训练过程存在一个关键风险：熵崩溃。这一现象是策略熵的快速下降，严重限制了探索并降低了学习效果。最近的方法试图通过启发式熵干预来减轻这种崩溃，但控制熵的基本机制仍不清楚。在本研究中，我们对GRPO的熵动态进行了理论和定量分析，揭示了每个更新步骤中令牌级熵变化由四个关键因素共同决定：裁剪策略、优势、令牌概率和令牌熵。这些发现不仅解释了现有方法的机制，还揭示了它们的局限性：它们依赖于对一两个因素的启发式调整，忽略了其他相关因素，从而降低了有效性。这促使我们提出了一种新方法STEER，该方法根据估计的熵变化自适应地重新加权令牌，以原则性地调节熵。在数学和编码基准上的实验表明，STEER有效减轻了熵崩溃，并始终优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the critical issue of entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR), which hampers exploration and learning effectiveness. The authors conduct a theoretical and quantitative analysis of the entropy dynamics in GRPO, identifying four key factors that influence token-level entropy change during updates. Their findings highlight the limitations of existing heuristic methods that only consider a subset of these factors. To overcome this, they propose a new method called STEER, which adaptively reweights tokens based on estimated entropy changes. Experimental results on math and coding benchmarks show that STEER effectively mitigates entropy collapse and outperforms current state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决可验证奖励的强化学习（RLVR）中熵崩溃的关键问题，这一问题妨碍了探索和学习的有效性。作者对GRPO中的熵动态进行了理论和定量分析，识别出在更新过程中影响令牌级熵变化的四个关键因素。他们的发现突显了现有启发式方法的局限性，这些方法仅调整一两个因素，因此促使他们开发了一种新的方法STEER，该方法根据估计的熵变化自适应地重新加权令牌。数学和编码基准上的实验结果表明，STEER有效地缓解了熵崩溃，并且在性能上超越了当前的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Proxy Robustness in Vision Language Models is Effortlessly Transferable</div>
<div class="meta-line">Authors: Xiaowei Fu, Fuxiang Huang, Lei Zhang</div>
<div class="meta-line">First: 2026-01-19T09:23:11+00:00 · Latest: 2026-01-19T09:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12865v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12865v1">PDF</a> · <a href="http://github.com/fxw13/HPT-GPD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的代理鲁棒性易于转移</div>
<div class="mono" style="margin-top:8px">作为提高深度模型防御能力的关键技术，通过蒸馏实现对抗鲁棒性转移在传统图像分类任务中取得了显著成功。然而，当应用于视觉语言模型（VLM）（例如CLIP）时，这一范式面临重大挑战：为大规模多模态模型构建对抗鲁棒教师需要极高的计算资源。我们通过揭示一个有趣的现象来弥补这一差距：普通CLIP（未经过对抗训练）对由不同架构的另一个CLIP生成的对抗样本表现出内在的防御能力。我们正式将其定义为代理对抗鲁棒性，并自然提出了一个异构代理转移（HPT）框架，该框架在CLIP变体之间建立跨架构鲁棒性蒸馏通道，轻松实现从代理到目标模型的VLM鲁棒性转移。然而，这种代理转移范式容易导致严重的过拟合，导致零-shot自然泛化的急剧下降。为了解决这个问题，我们通过利用学习率调度的差异设计了泛化-支点解耦（GPD）。这将代理转移过程解耦为一个保持泛化的泛化锚定预热和一个促进对抗鲁棒性的泛化拉动HPT，以实现自然泛化与对抗鲁棒性之间的平衡。在15个零-shot数据集上的大量实验证明了我们HPT-GPD方法的有效性。代码可在github.com/fxw13/HPT-GPD网站上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the challenge of enhancing adversarial robustness in vision-language models (VLMs) like CLIP, which typically require substantial computational resources for constructing robust teacher models. The authors introduce a Heterogeneous Proxy Transfer (HPT) framework that leverages the intrinsic defensive capabilities of vanilla CLIP against adversarial examples from differently architected CLIPs, facilitating robustness transfer without extensive computational demands. To address the issue of overfitting that arises from this proxy transfer, they propose Generalization-Pivot Decoupling (GPD), which balances natural generalization and adversarial robustness through a tailored learning rate scheduling. Experimental results across 15 zero-shot datasets validate the effectiveness of the HPT-GPD method in achieving this balance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在视觉语言模型（VLM）中转移对抗鲁棒性的挑战，该过程在构建鲁棒教师时资源消耗巨大。作者提出了一种异构代理转移（HPT）框架，利用普通CLIP模型对来自不同架构的对抗样本的内在防御能力。为了减轻过拟合并保持零-shot自然泛化，他们提出了泛化-支点解耦（GPD），该方法平衡了泛化的预热阶段与对抗鲁棒性转移。对15个零-shot数据集的实验结果验证了HPT-GPD方法在实现这种平衡方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</div>
<div class="meta-line">Authors: Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</div>
<div class="meta-line">First: 2026-01-19T08:16:11+00:00 · Latest: 2026-01-19T08:16:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成空间关系数据训练的CLIP风格视觉-语言模型中的左右对称破缺</div>
<div class="mono" style="margin-top:8px">空间理解仍然是视觉-语言模型中的一个关键挑战。然而，目前尚不清楚这种理解是否真正获得，以及如果获得，是通过什么机制。我们提出了一个可控的1D图像-文本测试平台，以探究在使用CLIP风格对比目标训练的基于Transformer的视觉和文本编码器中，左右关系理解是如何出现的。我们在一对一和二物体场景的描述上端到端训练轻量级的基于Transformer的视觉和文本编码器，并在系统性变化标签和布局多样性的同时评估对未见物体对的泛化能力。我们发现对比训练学习了左右关系，并且在这种情况下，标签多样性比布局多样性更是泛化的主要驱动因素。为了获得机制理解，我们进行了注意力分解，显示位置和标记嵌入之间的交互引发了一个水平注意力梯度，破坏了编码器中的左右对称性；消除这一贡献会显著降低左右区分能力。我们的结果提供了CLIP风格模型何时以及如何获得关系能力的机制性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the mechanisms behind spatial understanding in vision-language models, specifically focusing on left-right relational understanding. The authors developed a controllable 1D image-text testbed and trained lightweight Transformer-based encoders using a CLIP-style contrastive objective on paired descriptions of one- and two-object scenes. The findings reveal that contrastive training effectively learns left-right relations, with label diversity being the key factor influencing generalization, while an attention decomposition analysis indicates that interactions between positional and token embeddings create a horizontal attention gradient that disrupts left-right symmetry in the encoders, highlighting the importance of this mechanism for relational competence in CLIP-style models.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉语言模型中空间理解的挑战，特别关注左右关系理解的发展。作者创建了一个可控的1D图像-文本测试平台，以研究在使用CLIP风格对比目标训练的基于Transformer的编码器中这一现象，使用了一对一和一对二物体场景的描述。研究结果表明，对比训练有效地学习了左右关系，其中标签多样性对泛化的影响大于布局多样性；此外，注意力分解揭示了位置嵌入和标记嵌入之间的相互作用产生了破坏左右对称的水平注意力梯度，消融研究表明这一贡献对左右区分至关重要。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
