<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-25 04:00</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260225_0400</div>
    <div class="row"><div class="card">
<div class="title">Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device</div>
<div class="meta-line">Authors: Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar, Senmao Li, Hisham Cholakkal, Ian Reid, Eric P. Xing, Salman Khan, Fahad Shahbaz Khan</div>
<div class="meta-line">First: 2026-02-23T18:59:58+00:00 · Latest: 2026-02-23T18:59:58+00:00</div>
<div class="meta-line">Comments: Project page: https://amshaker.github.io/Mobile-O/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20161v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://amshaker.github.io/Mobile-O/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mobile-O：移动设备上的统一多模态理解与生成</div>
<div class="mono" style="margin-top:8px">统一多模态模型可以在单一架构内理解和生成视觉内容。然而，现有模型仍然对数据需求较高，并且在边缘设备上部署过于庞大。我们提出了Mobile-O，这是一种紧凑的视觉-语言-扩散模型，将统一多模态智能带入移动设备。其核心模块Mobile Conditioning Projector (MCP)通过深度可分离卷积和逐层对齐融合视觉-语言特征与扩散生成器。该设计以最小的计算成本实现高效的跨模态条件化。Mobile-O仅在几百万个样本上训练，并在一种新颖的四元组格式（生成提示、图像、问题、答案）中进行后训练，联合增强视觉理解和生成能力。尽管效率高，Mobile-O在GenEval上达到了74%的竞争性或优越性能，分别比Show-O和JanusFlow快5%和11%，运行速度分别为6倍和11倍。对于视觉理解，Mobile-O在七个基准测试中平均超越它们15.3%和5.1%。在iPhone上每512x512图像仅需约3秒，Mobile-O建立了边缘设备上实时统一多模态理解与生成的第一个实用框架。我们希望Mobile-O能促进未来在完全依赖设备而非云的情况下进行实时统一多模态智能的研究。我们的代码、模型、数据集和移动应用程序可在https://amshaker.github.io/Mobile-O/公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Unified multimodal models can both understand and generate visual content within a single architecture.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种能够高效理解和生成视觉内容的统一多模态模型，以解决现有数据需求大和模型过于庞大的问题。作者提出了Mobile-O，这是一种紧凑的视觉-语言-扩散模型，利用移动条件投影器（MCP）通过深度可分离卷积和逐层对齐将视觉-语言特征与扩散生成器整合，从而实现高效的跨模态条件处理。实验结果表明，Mobile-O在GenEval上得分74%，并且在性能上超过了Show-O和JanusFlow，分别提高了5%和11%，同时处理速度显著更快，在iPhone上处理512x512的图像仅需约3秒，从而为边缘设备上的实时多模态理解和生成建立了实用框架。</div>
</details>
</div>
<div class="card">
<div class="title">OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</div>
<div class="meta-line">Authors: Akashah Shabbir, Muhammad Umer Sheikh, Muhammad Akhtar Munir, Hiyam Debary, Mustansar Fiaz, Muhammad Zaigham Zaheer, Paolo Fraccaro, Fahad Shahbaz Khan, Muhammad Haris Khan, Xiao Xiang Zhu, Salman Khan</div>
<div class="meta-line">First: 2026-02-19T18:59:54+00:00 · Latest: 2026-02-23T18:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17665v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17665v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenEarthAgent：一个统一的工具增强地理空间代理框架</div>
<div class="mono" style="margin-top:8px">最近在多模态推理方面的进展使得代理能够解释图像，将其与语言连接，并执行结构化分析任务。将这些能力扩展到遥感领域仍然具有挑战性，因为模型必须在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步骤逻辑。为了解决这一问题，OpenEarthAgent引入了一个统一的框架，用于开发基于卫星图像、自然语言查询和详细推理轨迹训练的工具增强地理空间代理。训练管道依赖于对结构化推理轨迹的监督微调，使模型与各种分析上下文中的经过验证的多步骤工具交互对齐。伴随的语料库包括14,538个训练实例和1,169个评估实例，训练集中的推理步骤超过10万，评估集中的推理步骤超过7千。它涵盖城市、环境、灾害和基础设施领域，并结合GIS基础操作以及NDVI、NBR和NDBI等指数分析。基于明确的推理轨迹，学习到的代理展示了结构化推理、稳定的空间理解和可解释的行为，通过工具驱动的地理空间交互在多种条件下表现出色。我们报告了相对于强基线的一致性改进，以及与最近的开源和闭源模型相比的竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of agents in the remote sensing domain, particularly in interpreting satellite imagery and performing complex analytical tasks. The authors propose OpenEarthAgent, a unified framework that employs supervised fine-tuning on structured reasoning trajectories to train tool-augmented geospatial agents. Experimental results show that the developed agent exhibits improved structured reasoning, stable spatial understanding, and interpretable behavior, achieving consistent performance enhancements over a strong baseline and competitive results compared to existing models across various domains such as urban, environmental, and disaster analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升遥感领域中智能体的能力，特别是在解释卫星图像和执行复杂分析任务方面。作者提出了OpenEarthAgent，这是一个统一框架，通过对结构化推理轨迹进行监督微调来训练工具增强的地理空间智能体。实验结果表明，所开发的智能体在结构化推理、稳定的空间理解和可解释行为方面表现出改善，相较于强基线取得了一致的性能提升，并在城市、环境和灾害分析等多个领域与现有模型相比具有竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">A Very Big Video Reasoning Suite</div>
<div class="meta-line">Authors: Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, Thaddäus Wiedemer, Qingying Gao, Dezhi Luo, Yaoyao Qian, Lianyu Huang, Zelong Hong, Jiahui Ge, Qianli Ma, Hang He, Yifan Zhou, Lingzi Guo, Lantao Mei, Jiachen Li, Hanwen Xing, Tianqi Zhao, Fengyuan Yu, Weihang Xiao, Yizheng Jiao, Jianheng Hou, Danyang Zhang, Pengcheng Xu, Boyang Zhong, Zehong Zhao, Gaoyun Fang, John Kitaoka, Yile Xu, Hua Xu, Kenton Blacutt, Tin Nguyen, Siyuan Song, Haoran Sun, Shaoyue Wen, Linyang He, Runming Wang, Yanzhi Wang, Mengyue Yang, Ziqiao Ma, Raphaël Millière, Freda Shi, Nuno Vasconcelos, Daniel Khashabi, Alan Yuille, Yilun Du, Ziming Liu, Bo Li, Dahua Lin, Ziwei Liu, Vikash Kumar, Yijiang Li, Lei Yang, Zhongang Cai, Hokin Deng</div>
<div class="meta-line">First: 2026-02-23T18:59:41+00:00 · Latest: 2026-02-23T18:59:41+00:00</div>
<div class="meta-line">Comments: Homepage: https://video-reason.com/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个非常大的视频推理套件</div>
<div class="mono" style="margin-top:8px">视频模型的快速进展主要集中在视觉质量上，推理能力尚未得到充分探索。视频推理将智能建立在时空一致的视觉环境中，这超出了文本自然捕捉的范围，使得对时空结构（如连续性、互动和因果关系）进行直观推理成为可能。然而，系统研究视频推理及其扩展行为受到缺乏大规模训练数据的限制。为了解决这一问题，我们引入了非常大的视频推理（VBVR）数据集，这是一个前所未有的大规模资源，涵盖200个经过精心策划的推理任务，遵循原则性分类法，并包含超过一百万个视频片段，规模大约是现有数据集的三个数量级。我们进一步提出了VBVR-Bench，这是一个可验证的评估框架，通过结合基于规则的、与人类对齐的评分者，超越了基于模型的评判，使视频推理能力的可重复和可解释的诊断成为可能。利用VBVR套件，我们进行了一项大规模视频推理扩展研究的初步探索，并观察到对未见推理任务的早期泛化迹象。VBVR为可推广视频推理研究的下一个阶段奠定了基础。数据、基准工具包和模型可在https://video-reason.com/上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of video models, which have primarily focused on visual quality. To tackle the challenge of limited large-scale training data for video reasoning, the authors introduce the Very Big Video Reasoning (VBVR) Dataset, which includes over one million video clips and 200 curated reasoning tasks. The key experimental findings indicate that using the VBVR suite allows for a large-scale scaling study of video reasoning, revealing early signs of emergent generalization to previously unseen reasoning tasks, thus establishing a foundation for future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强对视频推理能力的理解，尽管视频模型在视觉质量方面取得了进展，但推理能力仍未得到充分探索。为了填补大规模训练数据的缺乏，作者引入了非常大的视频推理（VBVR）数据集，该数据集包含超过一百万个视频片段和200个精心策划的推理任务，规模远超现有数据集。该研究采用VBVR-Bench评估框架，结合了基于规则和人类对齐的评分方法，允许对视频推理进行可重复的评估。研究结果表明，出现了对未见推理任务的早期泛化迹象，为未来可推广视频推理的研究奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</div>
<div class="meta-line">Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</div>
<div class="meta-line">First: 2026-02-23T18:59:27+00:00 · Latest: 2026-02-23T18:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20156v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skill-Inject：测量智能体对技能文件攻击的脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）智能体正在迅速发展，得益于代码执行、工具和最近引入的智能体技能功能。技能允许用户通过专门的第三方代码、知识和指令扩展LLM应用程序。尽管这可以将智能体能力扩展到新领域，但也创造了一个日益复杂的智能体供应链，为提示注入攻击提供了新的表面。我们将基于技能的提示注入识别为一个重大威胁，并引入SkillInject，一个评估广泛使用的LLM智能体对技能文件注入的易受攻击性的基准。SkillInject包含202个注入任务对，攻击范围从明显恶意的注入到隐藏在其他合法指令中的微妙、依赖上下文的攻击。我们在SkillInject上评估前沿LLM，测量在有害指令避免方面的安全性和在合法指令合规性方面的效用。我们的结果表明，今天的智能体高度脆弱，前沿模型的攻击成功率高达80%，经常执行极具危害性的指令，包括数据外泄、破坏性行为和类似勒索软件的行为。此外，它们还表明，这个问题不会通过模型扩展或简单的输入过滤来解决，而是需要上下文感知的授权框架来实现稳健的智能体安全。我们的基准可在https://www.skill-inject.com/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing vulnerability of LLM agents to skill-based prompt injection attacks, which arise from the integration of third-party code and instructions. The authors introduce SkillInject, a benchmark designed to evaluate the susceptibility of popular LLM agents to these attacks, consisting of 202 injection-task pairs that range from overtly malicious to subtle context-dependent threats. Experimental results reveal that current LLM agents exhibit a high vulnerability, with attack success rates reaching up to 80%, leading to the execution of harmful instructions such as data exfiltration and destructive actions, indicating that effective security measures will require more than just model scaling or input filtering.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于LLM代理的快速发展以及代理技能的引入，这虽然增强了能力，但也增加了对提示注入攻击的脆弱性。作者开发了SkillInject，一个基准测试，旨在评估流行LLM代理对基于技能的提示注入的易受攻击性，包含202个注入任务对，涵盖从明显恶意到微妙攻击的范围。对领先的LLM进行评估显示出高脆弱性，攻击成功率高达80%，导致执行数据外泄和破坏性行为等有害指令，表明提高代理安全性不仅需要模型扩展或输入过滤，而是需要上下文感知的授权框架。</div>
</details>
</div>
<div class="card">
<div class="title">JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks</div>
<div class="meta-line">Authors: Jakob Heiss, Sören Lambrecht, Jakob Weissteiner, Hanna Wutte, Žan Žurič, Josef Teichmann, Bin Yu</div>
<div class="meta-line">First: 2026-02-23T18:59:10+00:00 · Latest: 2026-02-23T18:59:10+00:00</div>
<div class="meta-line">Comments: 11 pages + appendix. Preliminary version of an ongoing project that will be expanded with furhter evaluations</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20153v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models&#x27; internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JUCAL：联合校准分类任务中的随机和认知不确定性</div>
<div class="mono" style="margin-top:8px">我们研究训练好的分类器集的后校准不确定性。具体而言，我们考虑随机（标签噪声）和认知（模型）不确定性。在分类中，最流行和广泛使用的校准方法包括温度缩放（即，先汇聚再校准）和保形方法。然而，这些校准方法的主要缺点是它们没有平衡随机和认知不确定性的比例。不平衡这些不确定性可能严重误导预测不确定性，导致某些输入区域的过度自信预测，而在其他区域则表现出不足的自信。为了解决这个缺点，我们提出了一种简单但强大的校准算法联合不确定性校准（JUCAL），它联合校准随机和认知不确定性。JUCAL通过优化验证/校准数据集上的负对数似然（NLL）来联合校准两个常数，以加权和缩放认知和随机不确定性。JUCAL可以应用于任何训练好的分类器集（例如，变换器、卷积神经网络或基于树的方法），计算开销最小，无需访问模型的内部参数。我们在各种文本分类任务上对JUCAL进行了实验评估，针对不同规模和不同集成策略的分类器集。实验结果表明，JUCAL在所有考虑的分类任务中显著优于SOTA校准方法，分别将NLL和预测集大小减少了多达15%和20%。有趣的是，即使将JUCAL应用于规模为5的集成，其NLL和预测集大小也能超越规模高达50的温度缩放集成，导致推理成本减少多达10倍。因此，我们提出JUCAL作为校准分类集成的新方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of misrepresenting predictive uncertainty in classification tasks due to the imbalance between aleatoric and epistemic uncertainties in existing calibration methods. The authors propose a new calibration algorithm called Joint Uncertainty Calibration (JUCAL), which optimizes the negative log-likelihood on validation datasets to jointly calibrate these uncertainties. Experimental results demonstrate that JUCAL significantly outperforms state-of-the-art calibration methods, achieving reductions in negative log-likelihood and predictive set size by up to 15% and 20%, respectively, even showing that smaller ensembles calibrated with JUCAL can outperform larger temperature-scaled ensembles in terms of efficiency and cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善分类任务中不确定性的校准，特别是解决可能导致预测不确定性失真的随机不确定性和模型不确定性之间的不平衡。作者提出了一种名为联合不确定性校准（JUCAL）的校准算法，通过优化验证数据集上的负对数似然来联合校准这些不确定性，适用于任何训练好的分类器集成。实验结果表明，JUCAL显著优于最先进的校准方法，负对数似然和预测集大小分别减少了高达15%和20%，并且即使是小型集成经过JUCAL校准后也能超越大型温度缩放集成，从而显著降低推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data</div>
<div class="meta-line">Authors: Zhenyao Ma, Yue Liang, Dongxu Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-23T18:59:04+00:00 · Latest: 2026-02-23T18:59:04+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20152v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20152v1">PDF</a> · <a href="https://github.com/MoonYLiang/Behavior-Learning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为学习（BL）：从数据中学习层次优化结构</div>
<div class="mono" style="margin-top:8px">受行为科学启发，我们提出了行为学习（BL），一种新颖的通用机器学习框架，能够从数据中学习可解释和可识别的优化结构，涵盖从单一优化问题到层次组合的范围。它统一了预测性能、内在可解释性和可识别性，广泛适用于涉及优化的科学领域。BL对由内在可解释的模块块构建的组合效用函数进行参数化，从而诱导出用于预测和生成的数据分布。每个模块块可以用符号形式表示为效用最大化问题（UMP），这是行为科学中的基础范式和优化的通用框架。BL支持从单一UMP到层次组合的架构，后者建模层次优化结构。其平滑单调变体（IBL）保证了可识别性。从理论上讲，我们建立了BL的通用逼近性质，并分析了IBL的M估计性质。从经验上看，BL展示了强大的预测性能、内在可解释性和对高维数据的可扩展性。代码：https://github.com/MoonYLiang/Behavior-Learning；通过pip install blnetwork安装。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a machine learning framework that can learn interpretable and identifiable optimization structures from data, applicable across various scientific domains. The proposed Behavior Learning (BL) framework utilizes a compositional utility function constructed from interpretable modular blocks, enabling it to model both single optimization problems and hierarchical compositions. Experimental results show that BL achieves strong predictive performance, intrinsic interpretability, and scalability to high-dimensional data, while its smooth variant, IBL, ensures identifiability and demonstrates the universal approximation property.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要一个能够从数据中学习可解释和可识别优化结构的机器学习框架，适用于各种科学领域。所提出的行为学习（BL）框架利用由可解释模块块构建的组合效用函数，使其能够建模单一优化问题和层次组合。实验结果表明，BL在预测性能、内在可解释性和高维数据的可扩展性方面表现出色，而其平滑变体IBL确保了可识别性并展示了普遍逼近性质。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Risk Control for Non-Monotonic Losses</div>
<div class="meta-line">Authors: Anastasios N. Angelopoulos</div>
<div class="meta-line">First: 2026-02-23T18:58:54+00:00 · Latest: 2026-02-23T18:58:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非单调损失的符合风险控制</div>
<div class="mono" style="margin-top:8px">符合风险控制是符合预测的扩展，用于控制超出误覆盖的风险函数。原始算法控制在一维参数中单调损失的期望值。在这里，我们为应用于可能非单调损失的通用算法提供风险控制保证，参数为多维。保证依赖于算法的稳定性——不稳定的算法具有较松的保证。我们将该技术应用于选择性图像分类、肿瘤分割的FDR和IOU控制，以及在重叠种族和性别组中使用经验风险最小化进行的再犯预测的多组去偏差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to extend conformal risk control to manage risk functions that are not limited to monotonic losses, addressing a gap in existing methodologies. The authors propose a framework that provides risk control guarantees for generic algorithms dealing with potentially non-monotonic losses in multidimensional settings, emphasizing the importance of algorithm stability on the strength of these guarantees. Key experimental findings demonstrate the applicability of this approach in various contexts, including selective image classification, controlling false discovery rates and intersection over union in tumor segmentations, and debiasing recidivism predictions across different demographic groups using empirical risk minimization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是将符合风险控制扩展到管理不局限于单调损失的风险函数，适用于多维参数空间。作者提出了一种方法，为通用算法提供风险控制保证，强调算法稳定性的重要性，因为不稳定的算法会导致较弱的保证。主要实验结果展示了该技术在多个领域的应用，包括选择性图像分类、肿瘤分割中的假发现率和交并比控制，以及使用经验风险最小化对重叠人口群体的再犯预测进行去偏差处理。</div>
</details>
</div>
<div class="card">
<div class="title">Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization</div>
<div class="meta-line">Authors: Wei-Cheng Huang, Jiaheng Han, Xiaohan Ye, Zherong Pan, Kris Hauser</div>
<div class="meta-line">First: 2026-02-23T18:58:24+00:00 · Latest: 2026-02-23T18:58:24+00:00</div>
<div class="meta-line">Comments: 15 pages, 13 figures, in submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理感知的联合形状与姿态优化的仿真准备杂乱场景估计</div>
<div class="mono" style="margin-top:8px">从现实世界观察中估计仿真准备场景对于下游规划和策略学习任务至关重要。遗憾的是，现有方法在杂乱环境中表现不佳，通常面临高昂的计算成本、较差的鲁棒性以及在多个交互对象扩展时的局限性。我们提出了一种统一的基于优化的真实到仿真场景估计公式，能够在物理约束下联合恢复多个刚性对象的形状和姿态。我们的方法基于两个关键技术创新。首先，我们利用最近提出的形状可微接触模型，其全局可微性允许在建模对象间接触的同时，对对象几何形状和姿态进行联合优化。其次，我们利用增广拉格朗日海森矩阵的结构稀疏性，推导出一种高效的线性系统求解器，其计算成本随着场景复杂度的增加而有利地扩展。在此基础上，我们开发了一个端到端的真实到仿真场景估计管道，集成了基于学习的对象初始化、物理约束的联合形状-姿态优化和可微纹理细化。在最多包含5个对象和22个凸包的杂乱场景上的实验表明，我们的方法能够稳健地重建物理有效的、仿真准备的对象形状和姿态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the estimation of simulation-ready scenes from real-world observations, particularly in cluttered environments where existing methods face challenges such as high computational costs and limited robustness. The authors propose a unified optimization-based approach that simultaneously recovers the shapes and poses of multiple rigid objects while adhering to physical constraints. Key experimental results indicate that their method effectively reconstructs physically valid, simulation-ready object shapes and poses in cluttered scenes containing up to five objects and 22 convex hulls, demonstrating enhanced robustness and efficiency compared to previous techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善从现实世界观察中估计模拟准备场景的能力，特别是在杂乱环境中，现有方法面临高计算成本和有限鲁棒性等挑战。作者提出了一种统一的优化方法，同时恢复多个刚性物体的形状和姿态，并遵循物理约束，利用形状可微接触模型和基于结构稀疏性的高效线性系统求解器。实验结果表明，他们的方法在包含多达五个物体和22个凸包的杂乱场景中有效重建了物理有效且准备好的模拟物体形状和姿态。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Scalable and Robust Optical Systems Control</div>
<div class="meta-line">Authors: Zehao Wang, Mingzhe Han, Wei Cheng, Yue-Kai Huang, Philip Ji, Denton Wu, Mahdi Safari, Flemming Holtorf, Kenaish AlQubaisi, Norbert M. Linke, Danyang Zhuo, Yiran Chen, Ting Wang, Dirk Englund, Tingjun Chen</div>
<div class="meta-line">First: 2026-02-23T18:54:32+00:00 · Latest: 2026-02-23T18:54:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展和稳健的光学系统控制的自主AI</div>
<div class="mono" style="margin-top:8px">我们提出了AgentOptics，这是一个基于模型上下文协议（MCP）的高保真、自主光学系统控制的自主AI框架。AgentOptics解释自然语言任务，并通过结构化工具抽象层在异构光学设备上执行符合协议的操作。我们在8个代表性光学设备上实现了64个标准化的MCP工具，并构建了一个410任务的基准，以评估请求理解、角色感知响应、多步骤协调、对语言变异的鲁棒性和错误处理。我们评估了两种部署配置——商业在线LLM和本地托管的开源LLM，并将其与基于LLM的代码生成基线进行比较。AgentOptics实现了87.7%至99.0%的平均任务成功率，显著优于代码生成方法，其成功率最高可达50%。我们通过五个案例研究进一步展示了更广泛的适用性，这些案例超越了设备级控制，扩展到系统编排、监控和闭环优化。这些包括DWDM链路配置和对相干400 GbE和模拟光纤无线电（ARoF）通道的协调监控；对承载5G前传流量的宽带ARoF链路的自主表征和偏置优化；多跨距通道配置与发射功率优化；闭环光纤偏振稳定；以及基于分布式声学传感（DAS）的光纤监测与LLM辅助事件检测。这些结果确立了AgentOptics作为异构光学系统自主控制和编排的可扩展、稳健的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a robust and scalable framework for autonomous control of optical systems using agentic AI. The authors introduce AgentOptics, which utilizes the Model Context Protocol (MCP) to interpret natural language tasks and execute actions on various optical devices through a structured tool abstraction layer. Experimental results show that AgentOptics achieves an average task success rate of 87.7% to 99.0%, significantly outperforming traditional code-generation approaches that only reach up to 50% success, and the framework&#x27;s applicability is further validated through five case studies involving complex optical system tasks such as DWDM link provisioning and fiber polarization stabilization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个稳健且可扩展的框架，以实现光学系统的自主控制，采用代理人工智能。作者提出了AgentOptics，利用模型上下文协议（MCP）来解释自然语言任务，并通过结构化工具抽象层在各种光学设备上执行操作。实验结果表明，AgentOptics的平均任务成功率达到87.7%至99.0%，显著优于传统代码生成方法的最高50%成功率，并且通过涉及复杂光学系统任务的五个案例研究进一步验证了该框架的适用性，如DWDM链路配置和光纤偏振稳定。</div>
</details>
</div>
<div class="card">
<div class="title">TROLL: Trust Regions improve Reinforcement Learning for Large Language Models</div>
<div class="meta-line">Authors: Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-04T14:14:20+00:00 · Latest: 2026-02-23T18:54:13+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03817v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.03817v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model&#x27;s most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model&#x27;s inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TROLL：信任区域改善大型语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">使用类似PPO的剪切目标的强化学习（RL）已成为基于奖励的大型语言模型（LLM）微调的标准选择。尽管最近的工作探索了改进的优势估计器和归一化，但剪切机制本身仍未改变。最初作为基于原则的KL信任区域的代理引入，剪切是一种粗略的近似，常常导致不稳定的更新和次优的性能。我们用一种新颖的离散可微信任区域投影替代剪切目标，提供基于原则的令牌级KL约束。该投影在模型最重要的令牌logits的稀疏子集上操作，以平衡计算成本和投影效果。我们的方法，针对大型语言模型的信任区域优化（TROLL），在训练期间直接替代类似PPO的剪切，并且不改变模型的推理行为。在数学推理和代码生成任务、模型家族以及优势估计方法中，TROLL在训练速度、稳定性和最终成功率方面始终优于类似PPO的剪切。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of the clipping mechanism in reinforcement learning (RL) for fine-tuning large language models (LLMs), which often leads to unstable updates and suboptimal performance. The authors introduce a novel method called Trust Region Optimization for Large Language models (TROLL), which replaces the traditional PPO-like clipping objective with a discrete differentiable trust region projection that imposes token-level KL constraints. Experimental results demonstrate that TROLL outperforms PPO-like clipping across various tasks, including mathematical reasoning and code generation, showing improvements in training speed, stability, and final success rates.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在大语言模型（LLMs）微调中，强化学习（RL）中剪切机制的局限性，这往往导致不稳定的更新和次优的性能。作者提出了一种新方法，称为大语言模型的信任区域优化（TROLL），该方法用离散可微的信任区域投影替代传统的PPO类剪切目标，从而强制执行基于token的KL约束。实验结果表明，TROLL在数学推理和代码生成等各种任务中，相比于标准剪切方法，显著提高了训练速度、稳定性和最终成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Recurrent Structural Policy Gradient for Partially Observable Mean Field Games</div>
<div class="meta-line">Authors: Clarisse Wibault, Johannes Forkel, Sebastian Towers, Tiphaine Wibault, Juan Duque, George Whittle, Andreas Schaab, Yucheng Yang, Chiyuan Wang, Michael Osborne, Benjamin Moll, Jakob Foerster</div>
<div class="meta-line">First: 2026-02-23T18:53:09+00:00 · Latest: 2026-02-23T18:53:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20141v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20141v1">PDF</a> · <a href="https://github.com/CWibault/mfax">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>部分可观测均场博弈的递归结构策略梯度</div>
<div class="mono" style="margin-top:8px">均场博弈（MFGs）为大规模人口模型中的交互建模提供了一个原则性框架：在规模上，人口动态变得确定性，只有通过聚合冲击或共同噪声引入不确定性。然而，由于无模型方法方差过高和精确方法扩展性差，算法进展有限。最近的混合结构方法（HSMs）结合了对共同噪声的蒙特卡洛回放和基于这些样本的期望回报的精确估计。然而，HSMs尚未扩展到部分可观测的环境。我们提出了递归结构策略梯度（RSPG），这是第一个针对涉及公共信息的环境的历史感知HSM。我们还介绍了MFAX，这是我们基于JAX的均场博弈框架。通过利用已知的转移动态，RSPG实现了最先进的性能以及数量级更快的收敛，并首次解决了具有异质代理、共同噪声和历史感知策略的宏观经济均场博弈。MFAX可在以下网址公开获取：https://github.com/CWibault/mfax。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing methods in Mean Field Games (MFGs), particularly in partially observable settings where algorithmic progress has been hindered by high variance in model-free methods and poor scalability of exact methods. The authors propose a novel approach called Recurrent Structural Policy Gradient (RSPG), which integrates history-awareness into Hybrid Structural Methods (HSMs) to effectively handle public information. Experimental results demonstrate that RSPG achieves state-of-the-art performance with significantly faster convergence, successfully solving a macroeconomic MFG involving heterogeneous agents and common noise for the first time.</div>
<div class="mono" style="margin-top:8px">本研究解决了在部分可观测环境中，均值场博弈（MFGs）算法进展有限的问题，传统方法在高方差和可扩展性差方面存在困难。作者提出了一种新方法，称为递归结构策略梯度（RSPG），该方法结合了历史感知的混合结构方法（HSMs），有效处理公共信息。实验结果表明，RSPG在性能上达到了最先进水平，并且收敛速度显著加快，首次成功解决了涉及异质代理和共同噪声的宏观经济MFG。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Science of AI Agent Reliability</div>
<div class="meta-line">Authors: Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</div>
<div class="meta-line">First: 2026-02-18T18:05:44+00:00 · Latest: 2026-02-23T18:49:07+00:00</div>
<div class="meta-line">Comments: Interactive dashboard available at: https://hal.cs.princeton.edu/reliability</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16666v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16666v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hal.cs.princeton.edu/reliability">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向人工智能代理可靠性的科学</div>
<div class="mono" style="margin-top:8px">人工智能代理越来越多地被部署来执行重要任务。尽管在标准基准上的准确性评分不断上升，表明了快速进展，但许多代理在实践中仍然失败。这种差异突显了当前评估的一个基本局限性：将代理行为压缩为单一成功指标掩盖了关键的操作缺陷。特别是，它忽视了代理在不同运行中的一致性、对扰动的抵抗能力、可预测的失败或有限的错误严重性。基于安全关键工程，我们通过提出十二个具体指标，提供了一个整体性能概况，这些指标从一致性、鲁棒性、可预测性和安全性四个关键维度分解代理可靠性。在两个互补基准上评估14个模型后，我们发现最近的能力提升仅带来了可靠性的小幅改善。通过揭示这些持续的局限性，我们的指标补充了传统评估，同时提供了推理代理如何表现、退化和失败的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap between the increasing accuracy of AI agents on benchmarks and their real-world performance failures. The authors propose a method that introduces twelve metrics to evaluate agent reliability across four dimensions: consistency, robustness, predictability, and safety. Their evaluation of 14 AI models on two benchmarks reveals that despite recent advancements in capabilities, there have only been minor improvements in reliability, highlighting the need for a more comprehensive understanding of agent performance and failure modes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决AI代理在标准基准测试中高准确率与其在实际应用中表现不佳之间的差距。作者提出了一种方法，开发了十二个具体指标，以评估代理在一致性、鲁棒性、可预测性和安全性四个维度上的可靠性。他们对14个模型在两个基准上的评估显示，尽管AI能力有了最近的进展，但可靠性仅有微小改善，这突显了对代理性能和失败模式更细致理解的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Do Large Language Models Understand Data Visualization Rules?</div>
<div class="meta-line">Authors: Martin Sinnona, Valentin Bonas, Emmanuel Iarussi, Viviana Siless</div>
<div class="meta-line">First: 2026-02-23T18:47:51+00:00 · Latest: 2026-02-23T18:47:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco&#x27;s constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 &lt; 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型理解数据可视化规则吗？</div>
<div class="mono" style="margin-top:8px">数据可视化规则源于数十年的设计和感知研究，确保图表沟通的可信性。虽然之前的研究表明大型语言模型（LLMs）可以生成图表或标记误导性图形，但尚不清楚它们是否能够直接推理和执行可视化规则。基于约束的系统如Draco将这些规则编码为逻辑约束，以进行精确的自动检查，但维护符号编码需要专家的努力，这促使使用LLMs作为灵活的规则验证者。本文首次系统评估LLMs对可视化规则的表现，使用从答案集编程（ASP）派生的硬验证真相。我们将Draco的一部分约束翻译为自然语言陈述，并生成了一个包含2000个Vega-Lite规范的受控数据集，标注了明确的规则违规。LLMs在检测违规的准确性和提示遵循性方面进行了评估，后者衡量输出是否遵循所需的结构化格式。结果表明，前沿模型实现了高遵循性（Gemma 3 4B / 27B: 100%，GPT-oss 20B: 98%），并可靠地检测常见违规（F1高达0.82），但对于更微妙的感知规则（某些类别F1 &lt; 0.15）和从技术ASP公式生成的输出，性能下降。将约束翻译为自然语言使较小模型的性能提高了多达150%。这些发现展示了LLMs作为灵活的、语言驱动的验证者的潜力，同时突显了它们与符号求解器相比的当前局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether large language models (LLMs) can understand and apply data visualization rules, which are crucial for effective chart communication. The authors employed a systematic evaluation method using hard-verification ground truth from Answer Set Programming (ASP), translating a subset of visualization constraints into natural language and creating a dataset of 2,000 Vega-Lite specifications with annotated rule violations. The results indicate that advanced LLMs demonstrate high adherence to structured output formats and can reliably detect common visualization violations, achieving an F1 score of up to 0.82, although their performance declines for more subtle perceptual rules and outputs derived from technical formulations, with improvements noted for smaller models when constraints were expressed in natural language.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估大型语言模型（LLMs）是否能够有效理解和执行数据可视化规则，这些规则对准确的图表沟通至关重要。作者采用了一种系统评估方法，将Draco系统中的可视化约束翻译成自然语言，并创建了一个包含2000个带有注释规则违规的Vega-Lite规范的数据集。实验结果表明，尽管先进的LLMs在结构化输出格式上表现出高度遵循性，并能够可靠地检测常见的可视化违规，但它们在处理更微妙的感知规则和技术公式生成的输出时，性能显著下降，尽管将约束翻译成自然语言使较小模型的性能提高了多达150%。</div>
</details>
</div>
<div class="card">
<div class="title">A Benchmark of Causal vs. Correlation AI for Predictive Maintenance</div>
<div class="meta-line">Authors: Shaunak Dhande, Chutian Ma, Giacinto Paolo Saggese, Paul Smith, Krishna Taduri</div>
<div class="meta-line">First: 2025-11-30T23:59:37+00:00 · Latest: 2026-02-23T18:46:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01149v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01149v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果与相关AI在预测性维护中的基准测试</div>
<div class="mono" style="margin-top:8px">制造环境中的预测性维护呈现出一个具有挑战性的优化问题，其特征是极端的成本不对称，漏报故障的成本大约是误报的五十倍。传统的机器学习方法通常优化统计准确性指标，这些指标并不能反映这一操作现实，且无法可靠地区分因果关系与虚假相关性。本研究对八种预测模型进行了基准测试，从基线统计方法到贝叶斯结构因果方法，数据集包含10,000台CNC机器，故障发生率为3.3%。虽然基于集成相关的模型如随机森林（L4）实现了最高的原始成本节省（减少70.8%），但贝叶斯结构因果模型（L7）在财务表现上也具有竞争力（减少66.4%成本），并具备故障归因的内在能力，而基于相关的模型则不易提供。该模型对HDF、PWF和OSF故障类型实现了完美的归因。这些结果表明，因果方法结合领域知识和贝叶斯推理，在预测性维护应用中提供了预测性能与操作可解释性之间的潜在有利权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the challenges of predictive maintenance in manufacturing, where the costs of missed failures are significantly higher than those of false alarms. The researchers benchmark eight predictive models, including both traditional statistical methods and Bayesian structural causal approaches, using a dataset of 10,000 CNC machines with a low failure prevalence. The findings reveal that while correlation-based models like Random Forest achieve the highest cost savings, the Bayesian Structural Causal Model provides competitive financial performance and excels in failure attribution, demonstrating that causal methods can enhance both predictive accuracy and operational interpretability in predictive maintenance scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决制造业中预测性维护的挑战，特别是错过故障的成本远高于误报的成本。研究人员使用包含10,000台CNC机器、故障发生率为3.3%的数据集，对八种预测模型进行了基准测试，包括基线统计方法和贝叶斯结构因果方法。结果显示，虽然像随机森林这样的集成相关模型实现了最高的原始成本节约（70.8%），但贝叶斯结构因果模型提供了66.4%的竞争性成本降低，并在故障归因方面表现优异，表明因果方法可以在此背景下提高预测性能和操作可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation</div>
<div class="meta-line">Authors: Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</div>
<div class="meta-line">First: 2025-05-22T11:37:39+00:00 · Latest: 2026-02-23T18:46:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16547v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16547v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找水果：针对遮挡感知植物操作的零样本Sim2Real强化学习</div>
<div class="mono" style="margin-top:8px">开放环境中的自主采摘呈现出复杂的操作问题。在大多数情况下，自主系统必须处理显著的遮挡，并在存在较大结构不确定性的情况下进行交互（每种植物都是不同的）。感知和建模的不确定性使得设计可靠的采摘操作控制器变得具有挑战性，导致部署时性能不佳。我们提出了一种针对遮挡感知植物操作的sim2real强化学习（RL）框架，其中策略完全在仿真中学习，以重新定位茎和叶子以揭示目标水果。在我们提出的方法中，我们将高层运动规划与低层顺应控制解耦，从而简化了sim2real转移。这种分解使得学习到的策略能够在具有不同刚度和形态的多种植物之间进行泛化。在多个真实植物设置的实验中，我们的系统在暴露目标水果方面达到了86.7%的成功率，展示了对遮挡变化和结构不确定性的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of autonomous harvesting in environments with significant occlusion and structural uncertainties. The authors propose a sim2real reinforcement learning framework that learns a policy in simulation to manipulate plant structures, specifically to reposition stems and leaves to expose target fruits. Experimental results show that the system achieves a success rate of up to 86.7% in revealing target fruits across various real-world plant setups, indicating its robustness to variations in occlusion and plant morphology.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在植物存在显著遮挡和结构不确定性的环境中进行自主采摘的挑战。作者提出了一种sim2real强化学习框架，在模拟中学习策略，以操纵植物的茎和叶子以揭示目标水果。实验结果表明，该系统在不同的真实植物设置中实现了高达86.7%的成功率，表明其对遮挡和植物形态变化的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration</div>
<div class="meta-line">Authors: Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari, Mahdi Noori, Farhan Farsi, Behnam Bahrak</div>
<div class="meta-line">First: 2026-02-23T18:46:27+00:00 · Latest: 2026-02-23T18:46:27+00:00</div>
<div class="meta-line">Comments: Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20135v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20135v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>骑士：基于知识图谱的多项选择题生成与自适应难度校准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的兴起，它们在检索增强生成（RAG）等应用中变得至关重要。然而，评估这些系统仍然受到构建专门评估数据集的时间和成本的制约。我们介绍了骑士，一个基于LLM的、知识图谱驱动的框架，用于从外部来源生成多项选择题（MCQ）数据集。骑士构建了一个主题特定的知识图谱，这是一个结构化且简洁的实体和关系摘要，可以重复使用以生成教师控制的难度级别，包括多跳问题，而无需反复输入完整的源文本。这个知识图谱充当了一个压缩的、可重用的状态，使得问题生成成为对图谱的廉价读取。我们在维基百科/维基数据上实例化骑士，同时保持框架的领域和本体无关性。作为案例研究，骑士在历史、生物和数学领域生成了六个MCQ数据集。我们在五个标准上评估质量：流畅性、明确性（单一正确答案）、主题相关性、选项唯一性和在提供的来源下的可回答性（作为幻觉的代理）。结果表明，骑士能够从可重用的图表示中实现高效的令牌和成本生成，在这些标准上达到高质量，并产生与MMLU风格基准一致的模型排名，同时支持主题特定和难度控制的评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of evaluating large language models (LLMs) due to the high costs and time associated with creating specialized assessment datasets. The authors present KNIGHT, a framework that utilizes knowledge graphs to generate multiple-choice question (MCQ) datasets from external sources, allowing for adaptive difficulty levels without the need to repeatedly process the full source text. Experimental results demonstrate that KNIGHT produces high-quality MCQs across various criteria, including fluency and relevance, while being efficient in terms of token usage and costs, and it successfully supports topic-specific evaluations in History, Biology, and Mathematics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决评估大型语言模型（LLMs）时面临的高成本和时间问题，这使得创建专业评估数据集变得困难。作者提出了KNIGHT，一个利用知识图谱从外部来源生成多项选择题（MCQ）数据集的框架，能够在不重复处理完整源文本的情况下实现自适应难度水平。实验结果表明，KNIGHT能够高效地生成各种学科的高质量MCQ，在流畅性、明确性、主题相关性、选项独特性和可回答性等方面获得良好评估，同时与已建立的模型性能基准相一致。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Epidemiological Dynamics Under Adversarial Data and User Deception</div>
<div class="meta-line">Authors: Yiqi Su, Christo Kurisummoottil Thomas, Walid Saad, Bud Mishra, Naren Ramakrishnan</div>
<div class="meta-line">First: 2026-02-23T18:45:55+00:00 · Latest: 2026-02-23T18:45:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20134v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在对抗性数据和用户欺骗下建模流行病动态</div>
<div class="mono" style="margin-top:8px">流行病模型越来越依赖于自我报告的行为数据，如疫苗接种状态、口罩使用和社交距离遵守，以预测疾病传播并评估非药物干预措施（NPI）的影响。虽然这些数据提供了有价值的实时洞察，但它们常常受到战略性错误报告的影响，个体出于避免惩罚、获取利益或对公共卫生当局的不信任而驱动。为了考虑这种人类行为，本文引入了一个博弈论框架，将人口与公共卫生当局之间的互动建模为信号博弈。个体（发送者）选择如何报告他们的行为，而公共卫生当局（接收者）根据可能扭曲的信号更新其流行病模型。我们专注于围绕口罩和疫苗接种的欺骗，分析性地表征博弈均衡结果，并评估在通过政策干预保持疫情控制的同时，欺骗可以被容忍的程度。我们的结果表明，分离均衡——在最小欺骗下——能使感染率随着时间推移接近零。值得注意的是，即使在普遍不诚实的聚合均衡下，设计良好的发送者和接收者策略仍然可以维持有效的疫情控制。这项工作推动了对流行病学中对抗性数据的理解，并提供了在战略用户行为存在的情况下设计更强健公共卫生模型的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve epidemiological models that rely on self-reported behavioral data, which can be distorted due to strategic misreporting by individuals. The authors introduce a game-theoretic framework to model the interaction between the population and public health authorities as a signaling game, where individuals report their behaviors and authorities update their models based on these reports. The key findings indicate that separating equilibria, characterized by minimal deception, can drive infections to near zero, and even in scenarios with significant dishonesty, effective epidemic control can still be achieved through well-designed strategies for both senders and receivers.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决自我报告行为数据中的战略性误报所带来的挑战，这可能会妨碍准确的疾病传播预测和非药物干预措施的评估。作者提出了一个博弈论框架，将个体与公共卫生当局之间的互动建模为一个信号博弈，其中个体报告其行为，公共卫生当局根据这些可能扭曲的信号更新其模型。关键发现表明，特征为最小欺骗的分离均衡可以使感染率在一段时间内降至接近零，即使在普遍不诚实的情况下，通过精心设计的发送者和接收者策略，仍然可以实现有效的疫情控制。</div>
</details>
</div>
<div class="card">
<div class="title">AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization</div>
<div class="meta-line">Authors: Mert Cemri, Shubham Agrawal, Akshat Gupta, Shu Liu, Audrey Cheng, Qiuyang Mang, Ashwin Naren, Lutfi Eren Erdogan, Koushik Sen, Matei Zaharia, Alex Dimakis, Ion Stoica</div>
<div class="meta-line">First: 2026-02-23T18:45:31+00:00 · Latest: 2026-02-23T18:45:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an &quot;accumulated improvement signal&quot; to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaEvolve：自适应LLM驱动的零阶优化</div>
<div class="mono" style="margin-top:8px">自动程序生成的范式正从一次性生成转向推理时搜索，其中大型语言模型（LLM）在进化循环中充当语义变异操作符。尽管有效，这些系统目前受静态调度的限制，未能考虑搜索过程的非平稳动态。这种僵化导致了大量计算浪费，因为资源被无差别地分配给停滞的人群，而有前景的前沿仍未得到充分开发。我们引入了AdaEvolve，一个将LLM驱动的进化重新表述为分层自适应优化问题的框架。AdaEvolve使用“累积改进信号”在三个层面上统一决策：局部适应，动态调节解候选人群体内的探索强度；全局适应，通过基于强盗的调度在不同解候选人群体中分配全球资源预算；以及元指导，当进展停滞时，根据先前生成的解决方案及其相应的改进生成新解决策略。我们证明，AdaEvolve在185个不同的开放式优化问题中始终优于开源基线，包括组合、系统优化和算法设计问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation behind this study is to enhance the efficiency of automated program generation by addressing the limitations of static schedules in LLM-driven evolutionary systems. The authors introduce AdaEvolve, a framework that reformulates the optimization process as a hierarchical adaptive problem, utilizing an accumulated improvement signal to guide decision-making at three levels: Local Adaptation, Global Adaptation, and Meta-Guidance. Experimental results show that AdaEvolve significantly outperforms existing open-sourced baselines across 185 diverse optimization problems, including combinatorial and algorithm design challenges.</div>
<div class="mono" style="margin-top:8px">本研究解决了当前依赖静态调度的自动程序生成方法的局限性，这导致在搜索过程中资源分配效率低下。作者提出了AdaEvolve框架，将LLM驱动的进化视为一个分层自适应优化问题，利用累积改进信号在局部适应、全局适应和元指导三个层面增强决策。实验结果表明，AdaEvolve在185个不同的开放式优化问题上显著优于现有的开源基线，包括组合优化和系统优化任务。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges</div>
<div class="meta-line">Authors: Minh Dinh, Stéphane Deny</div>
<div class="meta-line">First: 2026-02-20T18:14:05+00:00 · Latest: 2026-02-23T18:44:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18406v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18406v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training$\unicode{x2013}$for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to learn equivariant operators in a latent space, from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒物体识别的潜在等变算子：前景与挑战</div>
<div class="mono" style="margin-top:8px">尽管深度学习在计算机视觉中取得了成功，但在识别经历过训练中少见的群对称变换的物体时仍然存在困难——例如，姿势、尺度、位置或其组合不寻常的物体。等变神经网络是解决跨对称变换泛化问题的方案，但需要事先了解变换。另一类架构提议在潜在空间中从对称变换的示例中学习等变算子。在这里，我们使用简单的旋转和平移的噪声MNIST数据集，展示了如何成功利用这些架构进行分布外分类，从而克服传统网络和等变网络的局限性。尽管在概念上引人入胜，我们讨论了在将这些架构扩展到更复杂数据集时面临的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in object recognition posed by group-symmetric transformations that are infrequently encountered during training, such as unusual poses and scales. The authors propose a method that utilizes latent equivariant operators, which learn to recognize these transformations from examples in a latent space. The experimental results demonstrate that this approach effectively enables out-of-distribution classification on simple datasets like rotated and translated noisy MNIST, highlighting its potential to surpass the limitations of traditional and equivariant networks, although the authors also note significant challenges in scaling these architectures for more complex datasets.</div>
<div class="mono" style="margin-top:8px">本研究解决了识别经历群对称变换的物体的挑战，这些变换在训练数据中往往没有体现，例如不寻常的姿势或尺度。作者提出了一种使用潜在等变算子的方式，从对称变换的示例中学习，从而在简单数据集（如旋转和位移的噪声MNIST）上改善了分布外分类。研究结果表明，这些架构能够有效地跨变换进行泛化，但作者也强调了将这些方法扩展到更复杂数据集时面临的重大挑战。</div>
</details>
</div>
<div class="card">
<div class="title">LAD: Learning Advantage Distribution for Reasoning</div>
<div class="meta-line">Authors: Wendi Li, Sharon Li</div>
<div class="meta-line">First: 2026-02-23T18:44:10+00:00 · Latest: 2026-02-23T18:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAD：推理的学习优势分布</div>
<div class="mono" style="margin-top:8px">当前大型模型推理的强化学习目标主要集中在最大化期望奖励。这种范式可能导致对主导奖励信号的过拟合，同时忽视其他有效的推理轨迹，从而限制了多样性和探索。为了解决这个问题，我们引入了学习优势分布（LAD），这是一种分布匹配框架，替代优势最大化，学习优势引导的分布。通过建立最优策略更新与基于优势的目标分布之间的等价关系，我们推导出一个实用的LAD目标，形式化为最小化策略引导分布与优势引导分布之间的$f$-散度。这产生了一个梯度更新，增加高优势响应的可能性，同时抑制过于自信的概率增长，防止崩溃而无需额外的熵正则化。与GRPO相比，LAD没有额外的训练成本，并且自然适应LLM后训练。在一个受控的赌博者设置中，LAD忠实地恢复了多模态优势分布，验证了理论公式。在多个LLM骨干上的数学和代码推理任务实验表明，LAD可靠地提高了准确性和生成多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current reinforcement learning objectives in large-model reasoning, which often focus on maximizing expected rewards and can lead to overfitting. The authors propose Learning Advantage Distributions (LAD), a framework that shifts from advantage maximization to learning the advantage-induced distribution by minimizing an $f$-divergence between policy-induced and advantage-induced distributions. Experimental results demonstrate that LAD effectively recovers the multimodal advantage distribution in a controlled bandit setting and significantly enhances both accuracy and generative diversity in math and code reasoning tasks across various large language model backbones.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前大型模型推理中强化学习目标的局限性，这些目标通常专注于最大化期望奖励，可能导致过拟合。作者提出了学习优势分布（LAD），这一框架将重点从优势最大化转向学习优势诱导分布，采用$f$-散度最小化的方法。实验结果表明，LAD在受控的赌博机设置中有效地恢复了多模态优势分布，并在多个大型语言模型基础上显著提高了数学和代码推理任务的准确性和生成多样性。</div>
</details>
</div>
<div class="card">
<div class="title">To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering</div>
<div class="meta-line">Authors: Zaifu Zhan, Min Zeng, Shuang Zhou, Yiran Song, Xiaoyi Chen, Yu Hou, Yifan Wu, Yang Ruan, Rui Zhang</div>
<div class="meta-line">First: 2026-02-23T18:42:50+00:00 · Latest: 2026-02-23T18:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20130v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.
  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.
  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\leq$4\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.
  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.
  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理与否：医学问答中的选择性思维链</div>
<div class="mono" style="margin-top:8px">目标：通过避免不必要的推理，同时保持准确性，提高大型语言模型（LLMs）在医学问答（MedQA）中的效率。
方法：我们提出了选择性思维链（Selective CoT），这是一种推理时策略，首先预测问题是否需要推理，仅在需要时生成理由。评估了两个开源LLM（Llama-3.1-8B和Qwen-2.5-7B）在四个生物医学QA基准（HeadQA、MedQA-USMLE、MedMCQA和PubMedQA）上的表现。指标包括准确性、生成的总令牌数和推理时间。
结果：选择性CoT将推理时间减少了13-45%，令牌使用量减少了8-47%，且准确性损失最小（$\leq$4\%）。在某些模型-任务对中，其准确性和效率均高于标准CoT。与固定长度的CoT相比，选择性CoT在显著较低的计算成本下达到了相似或更优的准确性。
讨论：选择性CoT通过仅在有利时调用显式推理，动态平衡推理深度和效率，减少了对回忆型问题的冗余，同时保持了解释性。
结论：选择性CoT为医学QA提供了一种简单、模型无关且具有成本效益的方法，将推理努力与问题复杂性对齐，以增强基于LLM的临床系统在现实世界中的可部署性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the efficiency of medical question answering using large language models by minimizing unnecessary reasoning while ensuring accuracy. The authors introduce a method called Selective Chain-of-Thought (Selective CoT), which determines if reasoning is required for a question and generates a rationale only when necessary. Experimental results show that Selective CoT reduces inference time by 13-45% and token usage by 8-47% with a minimal accuracy loss of up to 4%, and in some cases, it outperforms standard reasoning methods in both accuracy and efficiency, demonstrating its potential for practical application in clinical systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过减少不必要的推理来提高大型语言模型（LLMs）在医学问答（MedQA）中的效率，同时确保准确性。作者提出了一种称为选择性推理链（Selective CoT）的方法，该方法判断问题是否需要推理，仅在必要时生成推理依据。实验结果表明，选择性推理链将推理时间减少了13-45%，令令牌使用量减少了8-47%，且准确性损失不超过4%，在某些情况下，它在准确性和效率上均优于标准推理链，展示了其作为医学问答系统成本效益解决方案的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptation to Intrinsic Dependence in Diffusion Language Models</div>
<div class="meta-line">Authors: Yunxiao Zhao, Changxiao Cai</div>
<div class="meta-line">First: 2026-02-23T18:41:34+00:00 · Latest: 2026-02-23T18:41:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20126v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K&lt;L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散语言模型中的内在依赖适应</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）最近作为自回归（AR）方法的有前景替代方案出现，使得超越严格的从左到右顺序的并行标记生成成为可能。尽管经验成功日益增长，但对去掩码调度——即在采样过程中指定去掩码标记的顺序和大小——如何影响生成质量的理论理解仍然有限。在本研究中，我们引入了一种与分布无关的去掩码调度，适应目标数据分布的（未知）依赖结构，而无需任何先验知识或超参数调整。与之前固定去掩码大小的确定性程序不同，我们的方法在每次迭代中随机化揭示的标记数量。我们展示了对于两个特定的参数选择，采样收敛保证——通过Kullback-Leibler（KL）散度测量——分别按$\widetilde O(\mathsf{TC}/K)$和$\widetilde O(\mathsf{DTC}/K)$缩放。这里，$K$是迭代次数，$\mathsf{TC}$和$\mathsf{DTC}$是目标分布的总相关性和双重总相关性，捕捉数据的内在依赖结构。重要的是，我们的保证在实际相关的并行采样范围$K&lt;L$内成立，其中$L$是标记序列长度。这些结果显著改善了之前的收敛理论，并为低复杂度分布提供了实质性的采样加速。总体而言，我们的发现揭示了DLMs对内在数据结构的适应性，并阐明了随机去掩码大小在推理调度设计中的好处。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the theoretical understanding of unmasking schedules in diffusion language models (DLMs) and their impact on generation quality. The authors propose a distribution-agnostic unmasking schedule that adapts to the unknown dependence structure of the target data without requiring prior knowledge or hyperparameter tuning, differing from previous methods that used fixed unmasking sizes. Experimental results demonstrate that the sampling convergence guarantees improve significantly, with scaling behaviors of Kullback-Leibler divergence measured as \widetilde O(\mathsf{TC}/K) and \widetilde O(\mathsf{DTC}/K), indicating substantial sampling acceleration for low-complexity distributions in the parallel-sampling regime where the number of iterations is less than the token sequence length.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强对扩散语言模型（DLMs）中解掩码调度的理论理解，这些模型在与自回归方法相比，显示出并行生成标记的潜力。作者提出了一种与分布无关的解掩码调度，该调度能够适应目标数据的未知依赖结构，而无需先验知识或超参数调整，这与先前的确定性方法不同。关键实验结果表明，采样收敛保证显著改善，收敛速率分别为\widetilde O(\mathsf{TC}/K)和\widetilde O(\mathsf{DTC}/K)，展示了对低复杂度分布的显著采样加速，并揭示了DLMs对内在数据结构的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">NanoKnow: How to Know What Your Language Model Knows</div>
<div class="meta-line">Authors: Lingwei Gu, Nour Jedidi, Jimmy Lin</div>
<div class="meta-line">First: 2026-02-23T18:37:49+00:00 · Latest: 2026-02-23T18:37:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20122v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20122v1">PDF</a> · <a href="https://github.com/castorini/NanoKnow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model&#x27;s parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat&#x27;s pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow&#x27;s utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NanoKnow：如何了解你的语言模型知道什么</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）如何知道它们所知道的内容？回答这个问题一直很困难，因为预训练数据通常是一个“黑箱”——未知或无法访问。最近发布的nanochat——一系列具有完全开放预训练数据的小型LLMs——解决了这个问题，因为它提供了模型参数知识来源的透明视图。为了理解LLMs如何编码知识，我们发布了NanoKnow，这是一个基准数据集，将来自Natural Questions和SQuAD的问题根据其答案是否存在于nanochat的预训练语料库中进行划分。利用这些划分，我们现在可以正确区分LLMs在生成输出时所依赖的知识来源。为了展示NanoKnow的实用性，我们使用八个nanochat检查点进行了实验。我们的发现表明：（1）闭卷准确性受到预训练数据中答案频率的强烈影响，（2）提供外部证据可以减轻这种频率依赖性，（3）即使有外部证据，当答案在预训练期间被看到时，模型的准确性更高，表明参数知识和外部知识是互补的，以及（4）不相关信息是有害的，准确性根据不相关上下文的位置和数量而下降。我们在https://github.com/castorini/NanoKnow发布所有NanoKnow文档。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates how large language models (LLMs) acquire and utilize knowledge, addressing the challenge posed by the opaque nature of pre-training data. The authors introduce NanoKnow, a benchmark dataset that categorizes questions from Natural Questions and SQuAD based on the presence of answers in the pre-training corpus of the nanochat models, enabling a clearer understanding of knowledge sources. Experimental results reveal that closed-book accuracy is significantly affected by answer frequency in the pre-training data, that external evidence can reduce this dependency, and that while external evidence improves accuracy, models perform better when answers were included in pre-training, indicating a complementary relationship between parametric and external knowledge, with the presence of non-relevant information negatively impacting accuracy based on its position and quantity.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决理解大型语言模型（LLMs）如何获取知识的挑战，特别是由于其预训练数据的不透明性。作者引入了NanoKnow，一个基准数据集，该数据集根据答案在nanochat模型的预训练语料库中的存在情况，将Natural Questions和SQuAD中的问题进行分类，这些模型具有完全开放的数据。对八个nanochat检查点的实验表明，闭卷准确性受到预训练数据中答案频率的显著影响，外部证据可以减少这种频率依赖性，并且当答案包含在预训练中时，模型的表现更好，表明参数知识和外部知识之间存在互补关系。此外，非相关信息的存在对准确性产生负面影响，准确性随着非相关上下文的位置和数量的增加而下降。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Unifying Perceptual Reasoning and Logical Reasoning</div>
<div class="meta-line">Authors: Hiroyuki Kido</div>
<div class="meta-line">First: 2022-06-27T10:32:47+00:00 · Latest: 2026-02-23T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2206.13174v2">Abs</a> · <a href="https://arxiv.org/pdf/2206.13174v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">An increasing number of scientific experiments support the view of perception as Bayesian inference, which is rooted in Helmholtz&#x27;s view of perception as unconscious inference. Recent study of logic presents a view of logical reasoning as Bayesian inference. In this paper, we give a simple probabilistic model that is applicable to both perceptual reasoning and logical reasoning. We show that the model unifies the two essential processes common in perceptual and logical systems: on the one hand, the process by which perceptual and logical knowledge is derived from another knowledge, and on the other hand, the process by which such knowledge is derived from data. We fully characterise the model in terms of logical consequence relations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一感知推理与逻辑推理的探索</div>
<div class="mono" style="margin-top:8px">越来越多的科学实验支持将感知视为贝叶斯推断的观点，这一观点源于赫尔姆霍兹对感知作为无意识推断的看法。最近的逻辑研究提出了将逻辑推理视为贝叶斯推断的观点。本文给出了一个适用于感知推理和逻辑推理的简单概率模型。我们展示了该模型统一了感知和逻辑系统中两个基本过程：一方面是从其他知识中推导感知和逻辑知识的过程，另一方面是从数据中推导此类知识的过程。我们全面表征了该模型的逻辑后果关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the relationship between perceptual reasoning and logical reasoning, both of which are increasingly viewed through the lens of Bayesian inference. The authors propose a simple probabilistic model that integrates these two reasoning processes, demonstrating how perceptual and logical knowledge can be derived from existing knowledge and data. The key experimental findings indicate that this model effectively unifies the essential processes of knowledge derivation in both perceptual and logical systems, characterized through logical consequence relations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨感知推理与逻辑推理之间的联系，这两者越来越多地被视为贝叶斯推理。作者提出了一个简单的概率模型，整合了这两种推理过程，展示了感知和逻辑知识如何从现有知识和数据中推导出来。主要发现表明，该模型有效地统一了感知和逻辑系统的基本过程，并通过逻辑蕴涵关系进行了表征。</div>
</details>
</div>
<div class="card">
<div class="title">NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning</div>
<div class="meta-line">Authors: Jiahui Fu, Junyu Nan, Lingfeng Sun, Hongyu Li, Jianing Qian, Jennifer L. Barry, Kris Kitani, George Konidaris</div>
<div class="meta-line">First: 2026-02-23T18:35:18+00:00 · Latest: 2026-02-23T18:35:18+00:00</div>
<div class="meta-line">Comments: 25 pages, 15 figures. Project webpage: https://nova-plan.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20119v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nova-plan.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NovaPlan：通过闭环视频语言规划实现零样本长时间操作</div>
<div class="mono" style="margin-top:8px">解决长时间任务需要机器人将高层语义推理与低层物理交互相结合。尽管视觉语言模型（VLMs）和视频生成模型可以分解任务并想象结果，但它们通常缺乏现实世界执行所需的物理基础。我们介绍了NovaPlan，一个将闭环VLM和视频规划与几何基础的机器人执行统一的分层框架，用于零样本长时间操作。在高层，VLM规划器将任务分解为子目标，并在闭环中监控机器人执行，使系统能够通过自主重新规划从单步失败中恢复。为了计算低层机器人动作，我们从生成的视频中提取并利用与任务相关的物体关键点和人手姿势作为运动学先验，并采用切换机制选择更好的一个作为机器人动作的参考，即使在严重遮挡或深度不准确的情况下也能保持稳定执行。我们在三个长时间任务和功能操作基准（FMB）上展示了NovaPlan的有效性。我们的结果表明，NovaPlan能够执行复杂的组装任务，并表现出灵巧的错误恢复行为，而无需任何先前的演示或训练。项目页面：https://nova-plan.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enable robots to effectively perform long-horizon manipulation tasks by integrating high-level semantic reasoning with low-level physical interactions, addressing the limitations of existing vision-language models and video generation models in real-world applications. The authors propose NovaPlan, a hierarchical framework that combines closed-loop vision-language modeling with video planning and geometrically grounded robot execution, allowing for zero-shot manipulation. Experimental results demonstrate that NovaPlan successfully executes complex assembly tasks and shows robust error recovery behaviors across three long-horizon tasks and the Functional Manipulation Benchmark, all without requiring prior demonstrations or training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于使机器人能够有效执行长时间跨度的操作任务，通过将高层语义推理与低层物理交互相结合，解决现有视觉语言模型和视频生成模型的局限性。作者提出了NovaPlan，这是一个将闭环视频语言建模和视频规划与几何基础的机器人执行相结合的分层框架，允许零样本操作。实验结果表明，NovaPlan成功执行复杂的组装任务，并在三个长时间跨度的任务和功能操作基准上展示了有效的错误恢复行为，且无需先前的演示或训练。</div>
</details>
</div>
<div class="card">
<div class="title">ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models</div>
<div class="meta-line">Authors: Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel, Sam Bayless, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-02-23T18:34:29+00:00 · Latest: 2026-02-23T18:34:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReSyn：用于推理模型的自主扩展合成环境</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）已成为训练推理语言模型（RLMs）的有前景的方法，通过利用验证者的监督。尽管对于许多任务，验证者的实现比解决方案注释更容易，但现有的合成数据生成方法仍然主要以解决方案为中心，而基于验证者的方法依赖于少量手工制作的程序环境。在这项工作中，我们通过引入ReSyn来扩展RLVR，这是一条生成多样化推理环境的管道，配备实例生成器和验证者，涵盖约束满足、算法难题和空间推理等任务。使用ReSyn数据进行RL训练的Qwen2.5-7B-Instruct模型在推理基准和域外数学基准上均取得了一致的提升，包括在具有挑战性的BBEH基准上相对提高27%。消融实验表明，基于验证者的监督和任务多样性的增加都显著贡献，提供了实证证据，表明大规模生成推理环境可以增强RLMs的推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the training of reasoning language models (RLMs) through reinforcement learning with verifiable rewards (RLVR), addressing the limitations of existing synthetic data generation methods. The authors introduce ReSyn, a pipeline that autonomously generates diverse reasoning environments using instance generators and verifiers, which cover various tasks such as constraint satisfaction and algorithmic puzzles. Experimental results demonstrate that a Qwen2.5-7B-Instruct model trained on ReSyn data shows significant performance improvements across reasoning benchmarks, including a 27% relative increase on the BBEH benchmark, highlighting the effectiveness of verifier-based supervision and task diversity in enhancing RLM capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过可验证奖励的强化学习（RLVR）来增强推理语言模型（RLMs）的训练，解决现有合成数据生成方法通常以解决方案为中心的局限性。作者提出了ReSyn，一个自动生成多样化推理环境的管道，使用实例生成器和验证器，适用于约束满足和算法难题等多种任务。实验结果表明，基于ReSyn生成的数据训练的Qwen2.5-7B-Instruct模型在推理基准测试中表现出显著改善，包括在具有挑战性的BBEH基准上相对提高27%，突显了基于验证者的监督和任务多样性在增强RLM能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Unlearning for Vision Transformers</div>
<div class="meta-line">Authors: Kairan Zhao, Iurie Luca, Peter Triantafillou</div>
<div class="meta-line">First: 2026-02-23T18:33:16+00:00 · Latest: 2026-02-23T18:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20114v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉变换器的去学习基准测试</div>
<div class="mono" style="margin-top:8px">机器去学习（MU）研究获得了强劲的动力：MU现在被广泛视为构建安全和公平AI的关键能力。同时，计算机视觉任务的变换器架构研究也取得了很大成功：视觉变换器（VTs）越来越成为卷积神经网络（CNNs）的强有力替代品。然而，针对视觉任务的MU研究主要集中在CNNs上，而非VTs。尽管针对大型语言模型（LLMs）、扩散模型和CNNs的MU基准测试已有所涉及，但VTs尚无相关研究。本研究首次尝试这一点，基准测试不同VT家族（ViT和Swin-T）及不同容量下的MU算法性能。该研究采用（i）不同的数据集，以评估数据集规模和复杂性的影响；（ii）不同的MU算法，以代表根本不同的MU方法；以及（iii）单次和持续去学习协议。此外，研究重点基准测试利用训练数据记忆的MU算法，因为最近发现利用记忆显著提高了之前的SOTA算法性能。在此过程中，研究描述了VTs相对于CNNs如何记忆训练数据，并评估不同记忆代理对性能的影响。基准测试使用统一的评估指标，捕捉遗忘质量的两个互补概念，以及在未见（测试）数据和保留数据上的准确性。总体而言，本研究提供了一个基准测试基础，使现有（和未来）VTs上的MU算法能够进行可重复、公平和全面的比较。并且首次揭示了现有算法在VT环境中的表现，为建立有前景的参考性能基线提供了依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the growing importance of machine unlearning (MU) in ensuring safe and fair AI, particularly as Vision Transformers (VTs) become prominent in computer vision tasks. The study benchmarks the performance of various MU algorithms across different families of VTs, specifically ViT and Swin-T, using diverse datasets to evaluate the effects of scale and complexity, as well as employing both single-shot and continual unlearning protocols. Key findings indicate that leveraging training data memorization significantly enhances the performance of MU algorithms in VTs, and the research provides a foundational benchmarking framework for future comparisons of MU methods in this context.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于机器遗忘（MU）作为确保安全和公平人工智能的重要能力日益受到重视，尤其是在视觉变换器（VTs）领域，而这一领域相比于卷积神经网络（CNNs）尚未得到充分研究。该研究对不同VT家族（包括ViT和Swin-T）中的各种MU算法性能进行了基准测试，使用多样化的数据集来评估规模和复杂性的影响，并采用单次和持续遗忘协议。主要发现揭示了VTs相较于CNNs在记忆训练数据方面的表现，并强调了不同记忆代理对MU性能的显著影响，最终为VT环境下未来的MU研究提供了基础基准。</div>
</details>
</div>
<div class="card">
<div class="title">StyleStream: Real-Time Zero-Shot Voice Style Conversion</div>
<div class="meta-line">Authors: Yisi Liu, Nicholas Lee, Gopala Anumanchipalli</div>
<div class="meta-line">First: 2026-02-23T18:32:59+00:00 · Latest: 2026-02-23T18:32:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20113v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://berkeley-speech-group.github.io/StyleStream/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Voice style conversion aims to transform an input utterance to match a target speaker&#x27;s timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StyleStream：实时零样本语音风格转换</div>
<div class="mono" style="margin-top:8px">语音风格转换旨在将输入的语句转换为匹配目标说话者的音色、口音和情感，主要挑战在于将语言内容与风格分离。尽管之前的研究探讨了这个问题，但转换质量仍然有限，且尚未解决实时语音风格转换。我们提出了StyleStream，这是第一个可流式传输的零样本语音风格转换系统，达到了最先进的性能。StyleStream由两个组件组成：Destylizer，它在保留语言内容的同时去除风格属性；Stylizer，一个扩散变换器（DiT），根据参考语音重新引入目标风格。通过文本监督和高度受限的信息瓶颈，强有力地执行内容与风格的分离。这一设计使得完全非自回归架构成为可能，实现了1秒的端到端延迟的实时语音风格转换。样本和实时演示： https://berkeley-speech-group.github.io/StyleStream/.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve voice style conversion, which involves transforming an utterance to match a target speaker&#x27;s characteristics while addressing the limitations of previous methods in conversion quality and real-time processing. The authors introduce StyleStream, a novel system that employs a Destylizer to eliminate style attributes while retaining linguistic content, and a Stylizer based on a diffusion transformer to reintroduce the desired style based on reference speech. The experimental results demonstrate that StyleStream achieves state-of-the-art performance in real-time voice style conversion, with an end-to-end latency of just 1 second, facilitated by robust content-style disentanglement through text supervision and a constrained information bottleneck.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高语音风格转换的质量和效率，该过程涉及将话语转换为匹配目标说话者风格，同时将语言内容与风格属性分离。作者提出了StyleStream，这是一种新颖的零样本语音风格转换系统，能够实时操作，包含一个去风格器用于去除风格属性，以及一个风格化器（扩散变换器），用于根据参考语音重新引入目标风格。实验结果表明，StyleStream以仅1秒的端到端延迟实现了最先进的性能，显著增强了语音风格转换应用的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds</div>
<div class="meta-line">Authors: Ezra Edelman, Surbhi Goel</div>
<div class="meta-line">First: 2026-02-23T18:30:48+00:00 · Latest: 2026-02-23T18:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20111v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\ from an unknown distribution $\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\tilde{O}(\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.
  We resolve this question by proving a matching $Ω(\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \emph{inference dimension}, yielding combined error $\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗性注入下的可靠弃权：紧的下界和新的上界</div>
<div class="mono" style="margin-top:8px">我们研究了由[Goel等人2017]提出的对抗性注入模型中的在线学习，其中一系列标记示例主要是从未知分布$\mathcal{D}$中独立同分布（i.i.d.）抽取，但可能会夹杂着对抗性选择的实例，而学习者不知道哪些轮次是对抗性的。关键是，标签始终与固定目标概念一致（干净标签设置）。学习者还被允许放弃预测，总错误计数在学习者决定预测时的错误和在i.i.d.轮次中放弃时的错误。或许令人惊讶的是，先前的工作表明，对底层分布的oracle访问在VC维度$d$下产生$O(d^2 \log T)$的综合错误，而与分布无关的算法在受限类别中仅实现$\tilde{O}(\sqrt{T})$，这使得这一差距是否是根本性的问题仍然悬而未决。我们通过证明VC维度为$1$的匹配$Ω(\sqrt{T})$下界来解决这个问题，建立了两种信息机制之间的明确分离。在算法方面，我们引入了一个基于潜力的框架，驱动因素是\emph{鲁棒见证}，即小的标记示例子集，它们在保持对抗性污染的韧性同时证明预测的有效性。我们使用两个组合维度实例化该框架：（1）\emph{推理维度}，对于推理维度为$k$的类别，产生综合错误$\tilde{O}(T^{1-1/k})$；（2）\emph{证书维度}，这是我们引入的一种新松弛。作为应用，我们展示了$\mathbb{R}^2$中的半空间具有证书维度$3$，为该类别获得了第一个与分布无关的界限$\tilde{O}(T^{2/3})$。这很重要，因为[Blum等人2021]表明，在没有弃权的情况下，半空间在干净标签攻击下不是鲁棒可学习的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of online learning in the adversarial injection model, where labeled examples are mostly drawn from an unknown distribution but can include adversarial instances. The study proves a lower bound of Ω(√T) for VC dimension 1, highlighting a significant gap between oracle access and distribution-agnostic algorithms. The authors propose a potential-based framework utilizing robust witnesses to certify predictions, achieving a combined error of ŷO(T^{1-1/k}) for classes with inference dimension k, and introduce a new concept called certificate dimension, demonstrating that halfspaces in ℝ² have a certificate dimension of 3, leading to a distribution-agnostic bound of ŷO(T^{2/3}).</div>
<div class="mono" style="margin-top:8px">该研究解决了在对抗注入模型中在线学习的挑战，其中标记示例主要来自未知分布，但可能包含对抗实例。作者证明了VC维度为1时的下界为Ω(√T)，突显了oracle访问与分布无关算法之间的显著差距。他们提出了一种基于潜力的框架，使用鲁棒见证来认证预测，对于推理维度类实现了组合误差率为ŷO(T^{1-1/k})，并引入了一个新的概念称为证书维度，这导致了在ℝ²中半空间的分布无关界限为ŷO(T^{2/3})。</div>
</details>
</div>
<div class="card">
<div class="title">VillageNet: Graph-based, Easily-interpretable, Unsupervised Clustering for Broad Biomedical Applications</div>
<div class="meta-line">Authors: Aditya Ballal, Gregory A. DePaul, Esha Datta, Asuka Hatano, Erik Carlsson, Ye Chen-Izu, Javier E. López, Leighton T. Izu</div>
<div class="meta-line">First: 2025-01-16T06:56:43+00:00 · Latest: 2026-02-23T18:26:51+00:00</div>
<div class="meta-line">Comments: Software available at https://villagenet.streamlit.app/ Github Link: https://github.com/lordareicgnon/VillageNet</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10471v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.10471v2">PDF</a> · <a href="https://github.com/lordareicgnon/VillageNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering large high-dimensional datasets with diverse variable is essential for extracting high-level latent information from these datasets. Here, we developed an unsupervised clustering algorithm, we call &quot;Village-Net&quot;. Village-Net is specifically designed to effectively cluster high-dimension data without priori knowledge on the number of existing clusters. The algorithm operates in two phases: first, utilizing K-Means clustering, it divides the dataset into distinct subsets we refer to as &quot;villages&quot;. Next, a weighted network is created, with each node representing a village, capturing their proximity relationships. To achieve optimal clustering, we process this network using a community detection algorithm called Walk-likelihood Community Finder (WLCF), a community detection algorithm developed by one of our team members. A salient feature of Village-Net Clustering is its ability to autonomously determine an optimal number of clusters for further analysis based on inherent characteristics of the data. We present extensive benchmarking on extant real-world datasets with known ground-truth labels to showcase its competitive performance, particularly in terms of the normalized mutual information (NMI) score, when compared to other state-of-the-art methods. The algorithm is computationally efficient, boasting a time complexity of O(N*k*d), where N signifies the number of instances, k represents the number of villages and d represents the dimension of the dataset, which makes it well suited for effectively handling large-scale datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VillageNet：基于图的、易于解释的、无监督聚类方法在广泛生物医学应用中的应用</div>
<div class="mono" style="margin-top:8px">对具有多样变量的大型高维数据集进行聚类对于从这些数据集中提取高层次潜在信息至关重要。在此，我们开发了一种无监督聚类算法，称为“Village-Net”。Village-Net专门设计用于有效聚类高维数据，而无需事先了解现有聚类的数量。该算法分为两个阶段：首先，利用K-Means聚类将数据集划分为我们称之为“村庄”的不同子集。接下来，创建一个加权网络，每个节点代表一个村庄，捕捉它们的邻近关系。为了实现最佳聚类，我们使用一种称为Walk-likelihood Community Finder (WLCF)的社区检测算法来处理该网络，该算法由我们团队的一名成员开发。Village-Net聚类的一个显著特点是其能够根据数据的固有特征自主确定进一步分析的最佳聚类数量。我们在现有的具有已知真实标签的真实世界数据集上进行了广泛的基准测试，以展示其竞争性能，特别是在与其他最先进方法相比时的归一化互信息（NMI）得分。该算法计算效率高，时间复杂度为O(N*k*d)，其中N表示实例数量，k表示村庄数量，d表示数据集的维度，这使其非常适合有效处理大规模数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the clustering of large high-dimensional biomedical datasets to extract meaningful latent information without prior knowledge of the number of clusters. The authors developed an unsupervised clustering algorithm called Village-Net, which operates in two phases: first, it employs K-Means clustering to create subsets termed &#x27;villages,&#x27; and then it constructs a weighted network to analyze the proximity relationships between these villages using the Walk-likelihood Community Finder (WLCF) algorithm. The experimental results demonstrate that Village-Net autonomously determines the optimal number of clusters and shows competitive performance, particularly in normalized mutual information (NMI) scores, when benchmarked against existing methods on real-world datasets with known labels, while maintaining computational efficiency with a time complexity of O(N*k*d).</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效地对大型高维生物医学数据集进行聚类，以提取有意义的潜在信息。作者开发了一种名为Village-Net的无监督聚类算法，该算法分为两个阶段：首先使用K均值聚类创建称为“村庄”的不同子集，然后构建这些村庄的加权网络，以使用步态似然社区发现（WLCF）算法分析它们的邻近关系。实验结果表明，Village-Net能够自主确定最佳聚类数量，并在基于真实世界数据集的基准测试中，特别是在归一化互信息（NMI）得分方面表现出与现有方法的竞争力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0335.html">20260223_0335</a>
<a href="archive/20260222_0334.html">20260222_0334</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0348.html">20260220_0348</a>
<a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
