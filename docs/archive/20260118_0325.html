<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_0325</div>
    <div class="row"><div class="card">
<div class="title">An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses</div>
<div class="meta-line">Authors: Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-25T02:05:41+00:00 · Latest: 2026-01-15T17:33:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures, accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17772v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.17772v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model&#x27;s utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进的有界域和平滑损失下差分隐私SGD的隐私和效用分析</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DPSGD）广泛用于在机器学习模型训练过程中保护敏感数据，但其隐私保证往往以模型性能的大幅下降为代价，因为缺乏量化隐私损失的紧密理论界限。尽管最近的努力实现了更准确的隐私保证，但仍然施加了一些禁止实际应用的假设，如凸性和复杂的参数要求，并且很少深入研究隐私机制对模型效用的影响。本文为具有一般L-平滑和非凸损失函数的DPSGD提供了严格的隐私特征，揭示了在有界域情况下迭代的隐私损失收敛性。具体而言，我们跟踪多个迭代中的隐私损失，利用噪声平滑减少特性，并进一步在不同场景中建立全面的收敛分析。特别地，我们展示了对于具有有界域的DPSGD，(i) 在没有凸性假设的情况下隐私损失仍然可以收敛，(ii) 在某些条件下，较小的有界直径可以同时改善隐私和效用，以及(iii) 对于具有梯度裁剪的DPSGD（DPSGD-GC）和具有有界域的DPSGD-GC（DPSGD-DC）及μ-强凸人口风险函数，隐私效用权衡的可达大O阶。通过在实际环境中进行的成员推断攻击（MIA）实验验证了从理论结果中获得的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of Differentially Private Stochastic Gradient Descent (DPSGD), which is commonly used to protect sensitive data in machine learning but often suffers from significant performance degradation due to inadequate privacy loss quantification. The authors provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, analyzing privacy loss over multiple iterations and establishing convergence analysis in bounded-domain scenarios. Key findings indicate that privacy loss can converge without convexity assumptions, that a smaller bounded diameter can enhance both privacy and utility under certain conditions, and that the privacy-utility trade-off can be quantified for DPSGD with gradient clipping and bounded domains, supported by experimental validation through membership inference attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强差分隐私随机梯度下降（DPSGD）的隐私保证，同时尽量减少对模型性能的负面影响，因为现有方法通常依赖于不切实际的假设。作者采用严格的隐私特征化方法，针对具有一般L-光滑和非凸损失函数的DPSGD，分析了在有界域场景中多次迭代的隐私损失。主要发现表明，隐私损失在没有凸性假设的情况下仍然可以收敛，较小的有界直径在某些条件下可以同时改善隐私和效用，并且他们建立了带有梯度裁剪的DPSGD和有界域设置的DPSGD的隐私-效用权衡的big-O阶，实验通过成员推断攻击验证了他们的理论见解。</div>
</details>
</div>
<div class="card">
<div class="title">Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2026-01-15T15:16:15+00:00 · Latest: 2026-01-15T15:16:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>投影微批量累积为强化学习提供无参考的近端策略更新</div>
<div class="mono" style="margin-top:8px">本文介绍了投影微批量累积（PROMA），一种用于大型语言模型微调的近端策略更新方法。PROMA通过在微批量聚合之前投影序列级梯度分量，累积微批量的策略梯度。该投影在反向传播过程中逐层应用，实现了高效的实现，无需额外的前向或反向传播。实证结果表明，PROMA对局部KL散度的控制比GRPO更严格，从而实现了更稳定的策略学习。与PPO和GRPO不同，PROMA在不引起熵崩溃的情况下实现近端更新，并且不依赖于参考策略或似然比裁剪。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more stable policy learning in reinforcement learning, particularly in the context of fine-tuning large language models. The authors introduce Projected Microbatch Accumulation (PROMA), a method that accumulates policy gradients across microbatches by projecting out sequence-wise gradient components during the backward pass, allowing for efficient implementation without extra passes. Experimental results demonstrate that PROMA provides tighter control of local KL divergence compared to existing methods like GRPO, leading to improved stability in policy updates without the issues of entropy collapse or reliance on a reference policy.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于提高强化学习中策略学习的稳定性，特别是在大语言模型微调的背景下。作者提出了投影微批量累积（PROMA）方法，该方法通过在反向传播过程中投影序列级梯度分量来累积微批量的策略梯度，从而实现高效的实现，而无需额外的前向或反向传播。实验结果表明，与现有方法如GRPO相比，PROMA对局部KL散度提供了更严格的控制，从而实现了更稳定的策略更新，且没有熵崩溃或依赖参考策略的问题。</div>
</details>
</div>
<div class="card">
<div class="title">mergetune: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-15T15:15:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v1">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mergetune：视觉-语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉-语言模型（VLMs），如CLIP，往往会导致预训练知识的灾难性遗忘。之前的工作主要旨在减轻适应过程中的遗忘；然而，在此过程中，遗忘往往是不可避免的。我们引入了一种新范式，\emph{持续微调（CFT）}，旨在在零-shot模型已经适应后恢复预训练知识。我们提出了一种简单的、与模型无关的CFT策略（称为MERGETUNE），由线性模式连接（LMC）指导，可以在现有微调模型上事后应用，而无需架构更改。给定一个微调模型，我们继续微调其可训练参数（例如，软提示或线性头），以寻找一个具有两个低损失路径的持续模型，分别指向零-shot（例如，CLIP）和微调（例如，CoOp）解决方案。通过利用损失景观的几何特性，持续模型隐式地合并了这两种解决方案，恢复了在微调对应模型中丢失的预训练知识。一个挑战是，普通的LMC约束需要从预训练任务中重放数据。我们通过二阶代理近似这一约束，消除了对大规模数据重放的需求。实验表明，MERGETUNE在不增加参数的情况下，提高了CoOp在基础-新颖泛化上的调和平均值+5.6\%。我们首次在跨数据集迁移中展示了在DTD和EuroSAT上优于CLIP的表现。在稳健微调评估中，来自MERGETUNE的LMC合并模型以更低的推理成本超越了集成基线，与零-shot模型集成时实现了进一步的增益和最先进的结果。我们的代码可在\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of catastrophic forgetting in vision-language models (VLMs) like CLIP during fine-tuning. The authors introduce a method called continued fine-tuning (CFT) through a model-agnostic strategy named MERGETUNE, which aims to recover pretrained knowledge after adaptation without requiring changes to the model architecture. Experimental results demonstrate that MERGETUNE enhances the harmonic mean of CoOp by 5.6% on base-novel generalization and achieves superior performance compared to CLIP on cross-dataset transfer, while also surpassing ensemble baselines with lower inference costs in robust fine-tuning evaluations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉语言模型（VLMs）如CLIP在微调过程中出现的灾难性遗忘问题。作者提出了一种名为持续微调（CFT）的新方法，使用一种模型无关的策略MERGETUNE，允许在适应后恢复预训练知识，而无需更改架构。实验结果表明，MERGETUNE在基础-新颖泛化上提高了CoOp的调和平均值5.6%，在跨数据集迁移任务中表现优于CLIP，并且在较低推理成本下超越了集成基线，从而在与零样本模型结合时设定了新的最先进结果。</div>
</details>
</div>
<div class="card">
<div class="title">DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset</div>
<div class="meta-line">Authors: Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang</div>
<div class="meta-line">First: 2026-01-15T11:28:58+00:00 · Latest: 2026-01-15T11:28:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 11 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10305v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DanQing：最新的大规模中文视觉-语言预训练数据集</div>
<div class="mono" style="margin-top:8px">视觉-语言预训练（VLP）模型通过对大规模图像-文本对进行对比预训练，在各种下游任务中表现出强大的性能。大量英语图像-文本数据集（如COYO-700M和LAION-400M）的发布，使得CLIP和SigLIP等模型在跨模态检索和图像描述等任务中得到广泛应用。然而，由于高质量中文图像-文本数据的稀缺，中文视觉-语言预训练的发展显著滞后。为了解决这一问题，我们开发了一套全面的管道来构建高质量的中文跨模态数据集。因此，我们提出了DanQing，包含从Common Crawl收集的1亿个图像-文本对。与现有数据集不同，DanQing通过更严格的选择过程进行策划，数据质量更高。此外，DanQing主要基于2024-2025年的网络数据构建，使模型能够更好地捕捉不断变化的语义趋势，从而提供更大的实际效用。我们通过对SigLIP2模型的持续预训练，将DanQing与现有数据集进行比较。实验结果表明，DanQing在一系列中文下游任务中始终实现了优越的性能，包括零样本分类、跨模态检索和基于LMM的评估。为了促进中文视觉-语言预训练的进一步研究，我们将根据创意共享CC-BY 4.0许可证开源DanQing数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lag in Chinese vision-language pre-training due to a lack of high-quality image-text data compared to extensive English datasets. The authors developed DanQing, a large-scale dataset containing 100 million image-text pairs sourced from Common Crawl, employing a rigorous selection process to ensure superior data quality and relevance to contemporary semantic trends. Experimental results demonstrate that models pre-trained on DanQing, specifically the SigLIP2 model, outperform existing datasets in various Chinese downstream tasks, including zero-shot classification and cross-modal retrieval.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于缺乏高质量图像-文本数据集而导致的中文视觉语言预训练滞后问题。作者开发了一套全面的流程来构建一个大规模数据集，命名为DanQing，该数据集包含来自Common Crawl的1亿对图像-文本，并通过严格的筛选过程确保数据质量。实验结果表明，基于DanQing进行预训练的模型，特别是SigLIP2模型，在各种中文下游任务中表现优于现有数据集，包括零样本分类和跨模态检索。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍然是盲点。目前的VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器。训练方案通常将图像展平为一维补丁序列，忽略了进行空间推理所需的二维结构。我们认为，这种缺乏空间意识是VLM设计中的一个缺失维度，也是需要空间基础的应用（如机器人技术和具身人工智能）的瓶颈。为了解决这个问题，我们研究了（i）使用替代目标训练的图像编码器和（ii）二维位置编码。我们的实验表明，这些架构选择可以在多个基准上改善空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of vision-language models (VLMs) in capturing spatial relationships, which is crucial for applications like robotics and embodied AI. The authors propose an investigation into alternative training objectives for image encoders and the incorporation of 2D positional encodings to enhance spatial awareness. Experimental results demonstrate that these modifications significantly improve spatial reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉语言模型（VLMs）在捕捉空间关系方面的局限性，这对于机器人技术和具身人工智能等应用至关重要。作者探讨了替代训练方法，特别关注具有不同目标的图像编码器和二维位置编码的结合。实验结果表明，这些架构修改显著提高了在多个基准测试中的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP</div>
<div class="meta-line">Authors: Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-14T20:38:36+00:00 · Latest: 2026-01-14T20:38:36+00:00</div>
<div class="meta-line">Comments: Submitted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09859v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09859v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破开放权重CLIP的限制：自监督微调CLIP的优化框架</div>
<div class="mono" style="margin-top:8px">CLIP已成为多模态表示学习的基石，但提高其性能通常需要在数十亿样本上从头开始训练，这一过程成本高昂。我们提出一个不同的问题：是否可以仅使用现有的自监督数据集来提高开放权重CLIP模型在各种下游任务中的性能？与将预训练模型适应单一下游任务的监督微调不同，我们的设置旨在提高在各种任务中的整体性能。然而，正如我们的实验和先前研究所揭示的，从开放权重CLIP模型开始简单地应用标准训练协议往往会失败，导致性能下降。本文介绍了TuneCLIP，一个克服性能下降的自监督微调框架。TuneCLIP有两个关键组成部分：（1）一个恢复优化统计的热身阶段，以减少冷启动偏差，灵感来自理论分析；（2）一个优化新的对比损失的微调阶段，以减轻对假负对的惩罚。我们的广泛实验表明，TuneCLIP在模型架构和规模上始终提高性能。值得注意的是，它提升了领先的开放权重模型，如SigLIP（ViT-B/16），在ImageNet及相关的分布外基准上实现了高达+2.5%的增益，在竞争激烈的DataComp基准上实现了+1.2%的增益，为高效的后预训练适应设定了新的强基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of open-weight CLIP models in multimodal representation learning without the need for extensive training on large datasets. The authors propose TuneCLIP, a self-supervised fine-tuning framework that includes a warm-up stage to recover optimization statistics and a fine-tuning stage that optimizes a new contrastive loss to address performance degradation. Experimental results demonstrate that TuneCLIP significantly improves the performance of various model architectures, achieving notable gains of up to +2.5% on ImageNet and +1.2% on the DataComp benchmark, thereby establishing a new baseline for efficient adaptation post-pretraining.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于在不需要对大型数据集进行广泛重训练的情况下，提高开放权重CLIP模型在多模态表示学习中的性能。作者提出了TuneCLIP，这是一种自监督微调框架，包括一个恢复优化统计的热身阶段和一个优化新对比损失的微调阶段，以解决性能下降问题。实验结果表明，TuneCLIP显著提高了各种模型架构的性能，在ImageNet上获得了高达+2.5%的增益，在DataComp基准上获得了+1.2%的增益，从而为开放权重模型的高效适应建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Normalize Filters! Classical Wisdom for Deep Vision</div>
<div class="meta-line">Authors: Gustavo Perez, Stella X. Yu</div>
<div class="meta-line">First: 2025-06-04T19:32:42+00:00 · Latest: 2026-01-14T19:43:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04401v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.04401v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>归一化滤波器！深度视觉的经典智慧</div>
<div class="mono" style="margin-top:8px">经典图像滤波器，如平均或差分滤波器，经过精心归一化，以确保一致性、可解释性，并避免强度偏移、光晕或振铃等伪影。相比之下，在深度网络中端到端学习的卷积滤波器缺乏这种约束。尽管它们可能类似于小波和斑点/边缘检测器，但它们并未以相同或任何方式进行归一化。因此，当图像经历大气传输时，它们的响应会失真，导致错误的结果。我们通过提出滤波器归一化，随后进行可学习的缩放和偏移，类似于批量归一化，来解决这一限制。这一简单而有效的修改确保滤波器具有大气等变性，实现共域对称性。通过将经典滤波原理融入深度学习（适用于卷积神经网络和依赖卷积的视觉变换器），我们的方法在人工和自然强度变化基准上取得了显著改进。我们的ResNet34甚至可以大幅超越CLIP。我们的分析表明，未归一化的滤波器会降低性能，而滤波器归一化则规范学习，促进多样性，提高鲁棒性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of convolutional filters in deep learning, which lack normalization and can lead to distorted responses during atmospheric transfer. The authors propose a method that incorporates filter normalization followed by learnable scaling and shifting, similar to batch normalization, to ensure that filters are atmosphere-equivariant. The experimental results demonstrate that this approach significantly enhances performance on benchmarks involving artificial and natural intensity variations, with the ResNet34 model outperforming CLIP by a substantial margin, indicating that normalized filters improve robustness, generalization, and learning diversity compared to unnormalized filters.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决深度学习中卷积滤波器的局限性，这些滤波器缺乏归一化，可能导致图像在经历大气传输时响应失真。作者提出了一种方法，结合滤波器归一化和可学习的缩放与偏移，类似于批量归一化，以确保滤波器具有大气等变性。实验结果表明，该方法在人工和自然强度变化基准测试中显著提高了性能，其中ResNet34模型的表现甚至大幅超越了CLIP，突显出未归一化滤波器会降低性能，而归一化则增强了鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适应于稀有类别</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型如CLIP在零-shot识别中表现出色，但在预训练期间很少见的类别（包括新出现的实体和文化特定类别）上表现不佳。我们介绍了LiteEmbed，这是一个轻量级框架，用于CLIP的少量样本个性化，使新类别能够在不重新训练编码器的情况下添加。LiteEmbed在CLIP的词汇中对文本嵌入进行子空间引导优化，利用基于PCA的分解，将粗略语义方向与细粒度变化分离。两个互补目标，粗对齐和细分离，共同保持全局语义一致性，同时增强视觉相似类别之间的可区分性。一旦优化，嵌入可以即插即用，顺利替代CLIP的原始文本特征，适用于分类、检索、分割和检测任务。大量实验表明，相较于先前的方法，LiteEmbed在适应于代表性不足、稀有或未见类别方面取得了显著提升，确立了其作为有效方法的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of large-scale vision-language models like CLIP in recognizing rarely seen classes, including new entities and culturally specific categories. The authors propose LiteEmbed, a lightweight framework that allows for few-shot personalization of CLIP without the need to retrain its encoders, utilizing subspace-guided optimization of text embeddings through a PCA-based decomposition. Experimental results show that LiteEmbed significantly improves performance over previous methods, effectively adapting CLIP to underrepresented and unseen classes across various tasks such as classification, retrieval, segmentation, and detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型视觉语言模型（如CLIP）在预训练期间未充分代表的稀有类别上的表现。作者提出了LiteEmbed，这是一种轻量级框架，允许在不重新训练模型编码器的情况下对CLIP进行少量样本个性化，通过优化文本嵌入来实现。实验结果表明，LiteEmbed显著增强了模型识别未充分代表、稀有或未见类别的能力，在分类、检索、分割和检测等各种任务中优于先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：用于科学复合图的视觉条件面板检测与标题生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个标记面板合并为单个图像，但在实际流程中，标题往往缺失或仅提供图形级别的摘要，使得面板级别的理解变得困难。本文提出了FigEx2，一种视觉条件框架，能够从复合图中定位面板并直接生成面板级标题。为了减轻开放式标题生成中多样化措辞的影响，我们引入了一种噪声感知门控融合模块，能够自适应过滤标记级特征，以稳定检测查询空间。此外，我们采用了一种分阶段优化策略，将监督学习与强化学习（RL）相结合，利用基于CLIP的对齐和基于BERTScore的语义奖励来强制执行严格的多模态一致性。为了支持高质量的监督，我们策划了BioSci-Fig-Cap，这是一个经过精炼的面板级基础数据集，并配有物理和化学领域的跨学科测试套件。实验结果表明，FigEx2在检测方面达到了优越的0.726 mAP@0.5:0.95，并在METEOR上比Qwen3-VL-8B高出0.51，在BERTScore上高出0.24。值得注意的是，FigEx2在没有任何微调的情况下，展现出显著的零-shot迁移能力，能够适应分布外的科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the understanding of scientific compound figures, which often lack detailed panel-level captions. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates specific captions for each panel using a noise-aware gated fusion module to enhance feature stability. Experimental results show that FigEx2 achieves a mean Average Precision of 0.726 for detection and outperforms the Qwen3-VL-8B model by 0.51 in METEOR and 0.24 in BERTScore, demonstrating strong zero-shot transferability to different scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高对科学复合图的理解，因为这些图通常缺乏详细的面板级标题。作者提出了FigEx2，这是一种视觉条件框架，利用噪声感知门控融合模块和结合监督学习与强化学习的分阶段优化策略，定位面板并为每个面板生成特定标题。实验结果表明，FigEx2在面板检测中达到了0.726的平均精度，并在METEOR和BERTScore指标上超越了基线模型Qwen3-VL-8B，同时在其他科学领域表现出强大的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Differentially private federated learning for localized control of infectious disease dynamics</div>
<div class="meta-line">Authors: Raouf Kerkouche, Henrik Zunker, Mario Fritz, Martin J. Kühn</div>
<div class="meta-line">First: 2025-09-17T14:28:04+00:00 · Latest: 2026-01-14T15:45:09+00:00</div>
<div class="meta-line">Comments: 26 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14024v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: R2 around 0.94 (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; R2 around 0.88 (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于传染病动态局部控制的差分隐私联邦学习</div>
<div class="mono" style="margin-top:8px">在流行病时期，迅速反应是减缓疫情传播的必要措施。局部方法在此反应中具有多种优势，限制了所需资源并减少了干预对更大范围的影响。然而，由于可用数据有限，在地方范围内训练单独的机器学习（ML）模型通常不可行。集中数据也面临挑战，因为其高度敏感性和隐私限制。在本研究中，我们考虑了一种基于德国县和社区的局部策略，由相关地方卫生机构（LHA）管理。为了在不妨碍详细情境数据可用性的情况下保护隐私，我们提出了一种隐私保护的预测方法，可以协助公共卫生专家和决策者。使用联邦学习（FL）的ML方法在不集中原始数据的情况下训练共享模型。将县、社区或LHA视为客户端，并在效用和隐私之间找到平衡，我们研究了具有客户端差分隐私（DP）的FL框架。我们在最近病例数的滑动窗口上训练共享的多层感知器，以预测病例数量，同时客户端仅交换规范裁剪的更新，服务器则聚合带有DP噪声的更新。我们在两个阶段对县级COVID-19数据评估该方法。如预期，严格的隐私会导致不稳定且不可用的预测。在适度强的隐私水平下，DP模型与非DP模型接近：2020年11月R2约为0.94（对比0.95），平均绝对百分比误差（MAPE）为26%；2022年3月R2约为0.88（对比0.93），MAPE为21%。总体而言，客户端级DP-FL可以提供有用的县级预测，并具有强隐私保障，且可行的隐私预算取决于流行病阶段，允许卫生机构之间进行隐私合规的合作以进行地方预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to enhance the response to epidemics through localized control strategies that minimize resource use and intervention impacts while addressing privacy concerns associated with sensitive health data. The authors propose a privacy-preserving forecasting method using federated learning (FL) that allows local health authorities to collaboratively train a machine learning model without centralizing raw data, employing client-level differential privacy (DP) to ensure data confidentiality. Experimental results demonstrate that while very strict privacy settings lead to unstable forecasts, a moderately strong DP level yields predictions closely aligned with non-DP models, achieving R2 values of approximately 0.94 and 0.88 and mean absolute percentage errors of 26% and 21% during two evaluation phases, indicating that client-level DP-FL can effectively provide reliable county-level predictions while maintaining strong privacy guarantees.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过局部控制策略增强对流行病的响应，同时解决与敏感健康数据相关的隐私问题。作者提出了一种隐私保护的预测方法，使用联邦学习（FL），使地方卫生当局能够在不集中原始数据的情况下协作训练共享机器学习模型。实验结果表明，虽然非常严格的隐私设置会导致不稳定的预测，但适度强的差分隐私水平能够产生与非隐私模型相近的预测结果，在2020年11月达到约0.94的R2和26%的平均绝对百分比误差，在2022年3月达到约0.88的R2和21%的MAPE，表明客户端级差分隐私能够有效支持地方流行病预测，并提供强有力的隐私保障。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity</div>
<div class="meta-line">Authors: Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal</div>
<div class="meta-line">First: 2026-01-14T14:03:11+00:00 · Latest: 2026-01-14T14:03:11+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09497v1">PDF</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr">Code1</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git">Code2</a> · <a href="https://github.com/Ritabrata04/cdod-icpr">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在领域特异性下实现稳健的跨数据集目标检测泛化</div>
<div class="mono" style="margin-top:8px">目标检测器在分布内通常表现良好，但在不同基准上会急剧下降。我们通过设置特异性的视角研究跨数据集目标检测（CD-OD）。我们将基准分组为具有多样日常场景的设置无关数据集和与狭窄环境相关的设置特定数据集，并在所有训练-测试对上评估标准检测器系列。这揭示了CD-OD中的明确结构：在相同设置类型内的迁移相对稳定，而跨设置类型的迁移显著下降且通常是不对称的。最严重的崩溃发生在从特定源迁移到无关目标时，并且在开放标签对齐后仍然存在，表明领域转移在最困难的情况下占主导地位。为了将领域转移与标签不匹配分开，我们比较了闭合标签迁移与开放标签协议，后者使用CLIP相似性将预测类别映射到最近的目标标签。开放标签评估产生了一致但有限的增益，许多修正案例对应于图像证据支持的语义近失。总体而言，我们提供了在设置特异性下对CD-OD的原则性表征以及在分布转移下评估检测器的实用指导。代码将发布在\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation of object detectors when applied to different benchmarks, particularly in cross-dataset object detection (CD-OD). The authors categorize datasets into setting-agnostic and setting-specific groups and evaluate a standard family of detectors across various train-test pairs. The findings indicate that while transfer within the same setting type is stable, transferring across different setting types leads to significant performance drops, especially when moving from specific sources to agnostic targets, highlighting the dominance of domain shift in challenging scenarios. Additionally, the study compares closed-label and open-label transfer methods, revealing that open-label evaluation provides consistent but limited improvements, often correcting cases that are semantically close to the target labels based on image evidence, thus offering a structured understanding of CD-OD under setting specificity and guidance for evaluating detectors amidst distribution shifts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决物体检测器在应用于不同基准时性能显著下降的问题，特别是在跨数据集物体检测（CD-OD）中。作者将数据集分为设置无关和设置特定两类，并在各种训练-测试对上评估标准检测器家族。研究结果表明，同一设置类型内的迁移性能相对稳定，但在不同类型之间迁移时显著下降，尤其是从特定设置到无关设置，表明领域转移是一个关键挑战。该研究还比较了闭标签和开放标签的迁移方法，发现开放标签评估提供了一致的改进，特别是在预测类别与目标标签语义接近的情况下，从而为在分布转移下更好地评估检测器提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models</div>
<div class="meta-line">Authors: Yizhi Chen, Ahmed Hemani</div>
<div class="meta-line">First: 2026-01-14T12:52:08+00:00 · Latest: 2026-01-14T12:52:08+00:00</div>
<div class="meta-line">Comments: Accepted to DATE Late Breaking Results 2026, Verona, Italy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09451v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破性结果：Quamba-SE：状态空间模型中激活的软边量化器</div>
<div class="mono" style="margin-top:8px">我们提出了Quamba-SE，一种用于状态空间模型（SSM）激活量化的软边量化器。与现有方法使用标准INT8操作不同，Quamba-SE采用三种自适应尺度：小值的高精度、正常值的标准尺度和异常值的低精度。这保留了异常值信息，而不是硬剪切，同时保持其他值的精度。我们在Mamba-130M上评估了6个零样本基准。结果表明，Quamba-SE在各个基准上始终优于Quamba，在单个基准上最高提高了+2.68%，在6个数据集的平均准确率上提高了+0.83%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve activation quantization in State Space Models (SSMs) by addressing the limitations of existing methods that rely on standard INT8 operations. The authors introduce Quamba-SE, a soft-edge quantizer that utilizes three adaptive scales to handle different value ranges: high-precision for small values, standard scale for normal values, and low-precision for outliers, thereby preserving outlier information while maintaining overall precision. Experimental results on the Mamba-130M model across six zero-shot benchmarks demonstrate that Quamba-SE consistently outperforms the previous Quamba method, achieving improvements of up to +2.68% on individual benchmarks and an average accuracy increase of +0.83% across all datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善状态空间模型（SSM）中激活量化的效果，以更好地处理异常值信息。作者提出了Quamba-SE，这是一种软边量化器，采用三种自适应尺度进行量化：小值使用高精度，正常值使用标准尺度，异常值使用低精度。实验结果表明，Quamba-SE在多个基准测试中优于之前的Quamba方法，单个基准测试的提升可达+2.68%，在六个零样本数据集上的平均准确率提高了+0.83%。</div>
</details>
</div>
<div class="card">
<div class="title">Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</div>
<div class="meta-line">Authors: Jiachen Li, Xiaojin Gong</div>
<div class="meta-line">First: 2023-10-26T08:12:53+00:00 · Latest: 2026-01-14T09:17:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.17218v3">Abs</a> · <a href="https://arxiv.org/pdf/2310.17218v3">PDF</a> · <a href="https://github.com/RikoLi/PCL-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原型对比学习的CLIP微调用于物体重识别</div>
<div class="mono" style="margin-top:8px">本研究旨在将大规模预训练的视觉-语言模型（如对比语言-图像预训练（CLIP））适应于提高物体重识别（Re-ID）在各种监督设置下的性能。尽管提示学习使得名为CLIP-ReID的近期工作取得了良好的性能，但由于ReID任务中缺乏语义标签，提示学习的基本机制和必要性仍不清楚。在本研究中，我们首先分析了提示学习在CLIP-ReID中的作用，并识别其局限性。基于我们的调查，我们提出了一种简单而有效的方法来适应CLIP用于监督物体Re-ID。我们的方法直接使用原型对比学习（PCL）损失微调CLIP的图像编码器，消除了对提示学习的需求。在人和车的Re-ID数据集上的实验结果表明，我们的方法与CLIP-ReID相比具有竞争力。此外，我们将基于PCL的CLIP微调方法扩展到无监督场景，在这些场景中我们实现了最先进的性能。代码可在https://github.com/RikoLi/PCL-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adaptation of large-scale pre-trained vision-language models, specifically CLIP, to improve object re-identification (Re-ID) performance under various supervision settings. The authors analyze the limitations of prompt learning in the existing CLIP-ReID framework and propose a novel method that fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, thereby eliminating the need for prompt learning. Experimental results on person and vehicle Re-ID datasets show that this new approach is competitive with CLIP-ReID and achieves state-of-the-art performance in unsupervised scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过使用大规模预训练的视觉-语言模型（如CLIP）来提高物体重识别（Re-ID）的性能，特别关注现有方法中提示学习的作用不明确。作者提出了一种新方法，通过原型对比学习（PCL）损失对CLIP的图像编码器进行微调，从而消除了对提示学习的需求。在人员和车辆Re-ID数据集上的实验结果表明，该方法与CLIP-ReID具有竞争力，并在无监督场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion</div>
<div class="meta-line">Authors: Jialu Li, Taiyan Zhou</div>
<div class="meta-line">First: 2026-01-14T06:38:12+00:00 · Latest: 2026-01-14T06:38:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpikeVAEDiff：基于神经脉冲的自然视觉场景重建通过VD-VAE和多功能扩散</div>
<div class="mono" style="margin-top:8px">从神经活动重建自然视觉场景是神经科学和计算机视觉中的一个关键挑战。我们提出了SpikeVAEDiff，一个新颖的两阶段框架，结合了非常深的变分自编码器（VDVAE）和多功能扩散模型，从神经脉冲数据生成高分辨率和语义丰富的图像重建。在第一阶段，VDVAE通过将神经脉冲信号映射到潜在表示，生成低分辨率的初步重建。在第二阶段，回归模型将神经脉冲信号映射到CLIP-Vision和CLIP-Text特征，使多功能扩散能够通过图像到图像生成来细化图像。我们在艾伦视觉编码-神经像素数据集上评估了我们的方法，并分析了不同的脑区。我们的结果表明，VISI区域表现出最显著的激活，并在重建质量中发挥关键作用。我们展示了成功和不成功的重建示例，反映了解码神经活动的挑战。与基于fMRI的方法相比，脉冲数据提供了更优越的时间和空间分辨率。我们进一步验证了VDVAE模型的有效性，并进行消融研究，表明来自特定脑区的数据显著提高了重建性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing natural visual scenes from neural activity, which is significant in both neuroscience and computer vision. The authors propose SpikeVAEDiff, a two-stage framework that integrates a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to achieve high-resolution image reconstructions from neural spike data. Experimental results on the Allen Visual Coding-Neuropixels dataset reveal that the VISI region is crucial for reconstruction quality, with the study highlighting both successful and unsuccessful reconstructions, and demonstrating that spike data outperforms fMRI in terms of temporal and spatial resolution, while also validating the VDVAE model&#x27;s effectiveness through ablation studies that show the importance of specific brain region data for improved performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从神经活动重建自然视觉场景的挑战，这在神经科学和计算机视觉中都具有重要意义。作者提出了SpikeVAEDiff，这是一个两阶段框架，结合了非常深的变分自编码器（VDVAE）和多功能扩散模型，以实现从神经尖峰数据中生成高分辨率图像重建。对Allen视觉编码-神经像素数据集的实验结果表明，VISI区域表现出最高的激活，并且对重建质量至关重要，研究突出了成功和不成功的重建案例，并表明尖峰数据在时间和空间分辨率方面优于fMRI，同时通过消融研究确认了VDVAE模型的有效性，显示特定脑区数据在提高重建性能方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</div>
<div class="meta-line">Authors: Youngmin Kim, Giyeong Oh, Kwangsoo Youm, Youngjae Yu</div>
<div class="meta-line">First: 2025-07-14T11:33:47+00:00 · Latest: 2026-01-14T06:25:00+00:00</div>
<div class="meta-line">Comments: Accepted to Automation in Construction. Our project page: https://winston1214.github.io/SlumpGuard/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10171v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.10171v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://winston1214.github.io/SlumpGuard/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SlumpGuard：基于AI的实时自动混凝土坍落度预测系统</div>
<div class="mono" style="margin-top:8px">混凝土的可加工性对建筑质量至关重要，坍落度测试是最广泛使用的现场评估方法。然而，传统的坍落度测试是手动的，耗时且高度依赖操作员，不适合在浇筑过程中进行连续或实时监测。为了解决这些局限性，我们提出了SlumpGuard，这是一种基于AI的视觉系统，通过单个固定摄像头分析搅拌车漏斗的自然排放流。该系统执行自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，实现了无需传感器、硬件安装或人工干预的质量监测。我们介绍了系统设计，构建了一个包含6000多个视频片段的现场复制数据集，并报告了广泛的评估，证明了在多种现场条件下可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测。专家研究进一步揭示了人类视觉估计之间的显著分歧，突显了自动评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the assessment of concrete workability, which is crucial for construction quality, by addressing the limitations of traditional manual slump testing. The authors developed SlumpGuard, an AI-powered vision system that utilizes a single fixed camera to analyze the discharge flow from a mixer-truck chute, enabling automatic chute detection, pouring-event identification, and video-based slump classification. Experimental results demonstrate that SlumpGuard reliably localizes the chute, accurately detects pouring events, and predicts slump under various field conditions, highlighting the system&#x27;s potential for real-time monitoring without the need for additional sensors or manual intervention.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决传统手动坍落度测试的局限性，来改善混凝土工作性评估，这对建筑质量至关重要。作者开发了SlumpGuard，这是一种利用单个固定摄像头分析搅拌车漏斗排放流的人工智能视觉系统，能够自动检测漏斗、识别浇筑事件并分类坍落度，而无需额外传感器或手动干预。实验结果表明，SlumpGuard能够在各种现场条件下可靠地定位漏斗、准确检测浇筑事件并预测坍落度，同时专家研究显示人类视觉估计存在显著差异，强调了自动评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-14T06:01:44+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v2">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应视图生成与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两个协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样且语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能，提升幅度为+2.5%。在视觉-语言学习中，它在十个数据集上将平均零-shot分类准确率提高了+12.31%（相较于CLIP）和+5.31%（相较于SLIP），并进一步将Flickr30k文本检索的R@5提高了+3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of contrastive learning by addressing the limitations in the construction of high-quality positive pairs and the lack of a quality assessment mechanism in current methods. The authors propose GenView++, a unified framework that introduces a multi-source adaptive view generation mechanism to create diverse and semantically coherent views, along with a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ improves MoCov2 by 2.5% on ImageNet linear classification and increases zero-shot classification accuracy by 12.31% over CLIP and 5.31% over SLIP across ten datasets, while also enhancing Flickr30k text retrieval R@5 by 3.2%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决当前方法在构建高质量正样本对和缺乏质量评估机制方面的局限性，来提高对比学习的有效性。作者提出了GenView++，一个统一框架，结合了多源自适应视图生成机制，以创建多样且语义一致的视图，以及质量驱动的对比学习机制，根据每对样本的语义对齐和多样性评估并重新加权其贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的性能2.5%，在十个数据集上相较于CLIP和SLIP分别提高了12.31%和5.31%的零样本分类准确率，并在Flickr30k文本检索R@5上提高了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-14T04:42:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了分层语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0\%的图像-AUROC和92.2\%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that struggle with balancing semantic generalization and structural discriminability. The authors propose a novel method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through a Hierarchical Semantic-Visual Synergy (HSVS) mechanism and employs a Vision-Conditioned Prompt Generator (VCPG) for dynamic prompt generation. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临局限，这些网络在语义泛化和结构可区分性之间难以有效平衡。作者提出了一种新方法，称为协同语义-视觉提示（SSVP），通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，并采用视觉条件提示生成器（VCPG）进行动态提示生成。实验结果表明，SSVP在七个工业基准测试中实现了最先进的性能，在MVTec-AD上达到了93.0%的图像AUROC和92.2%的像素AUROC，显著超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives</div>
<div class="meta-line">Authors: Wisdom O. Ikezogwo, Kevin Zhang, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Linda Shapiro, Ranjay Krishna</div>
<div class="meta-line">First: 2025-01-07T23:32:05+00:00 · Latest: 2026-01-14T03:02:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.04184v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.04184v3">PDF</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data">Code1</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to $\textit{think-aloud}$ studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial and temporal association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. $\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data]}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedicalNarratives：将医学视觉与语言连接到本地化叙事</div>
<div class="mono" style="margin-top:8px">多模态模型对数据需求量大。虽然自然图像的数据集丰富，但医学图像数据集无法享受同样的奢侈。为了大规模实现医学图像的表示学习，我们转向YouTube，这是一个拥有大量开源医学教学视频的平台。我们整理了MedicalNarratives，一个包含470万对医学图像-文本的数据库，其中100万样本包含以空间轨迹（和边界框）形式的密集注释，118K个视频集中于轨迹事件（带对齐文本），使得超越单帧的时空基础成为可能。类似于$\textit{think-aloud}$研究，讲师在相关图像区域上悬停鼠标光标时进行口述，MedicalNarratives中的100万张图像包含图像像素中的本地化鼠标轨迹，创建了文本与像素之间的空间和时间关联。为了评估MedicalNarratives的实用性，我们使用跨越12个医学领域的数据集训练了GenMedClip，采用类似CLIP的目标。GenMedClip在新构建的医学影像基准上，在所有12个领域的表现超过了之前的最先进模型。$\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[数据]}$</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the scarcity of large-scale medical image datasets necessary for training multi-modal models. The authors developed MedicalNarratives, a dataset comprising 4.7 million medical image-text pairs sourced from YouTube, which includes 1 million samples with detailed spatial annotations and 118,000 videos that provide spatiotemporal grounding. The experimental results demonstrate that the model GenMedClip, trained using this dataset with a CLIP-like objective, significantly outperforms existing state-of-the-art models across all 12 medical domains evaluated on a newly constructed benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决训练多模态模型所需的大规模医学图像数据集的稀缺问题。作者开发了MedicalNarratives数据集，该数据集包含来自YouTube的470万对医学图像-文本对，其中包括100万样本具有详细的空间注释和118,000个提供时空基础的视频。实验结果表明，使用该数据集在12个医学领域上训练的GenMedClip模型，在新建立的医学影像基准测试中，显著超越了现有的最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models</div>
<div class="meta-line">Authors: Ganxi Xu, Zhao-Rong Lai, Yuting Tang, Yonghao Song, Guoxu Zhou, Boyu wang, Jian Zhu, Jinyi Long</div>
<div class="meta-line">First: 2025-08-31T10:29:58+00:00 · Latest: 2026-01-14T01:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.00787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M/EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present a novel image-to-brain signal framework that generates M/EEG from images by leveraging the diffusion transformer architecture enhanced with cross-attention mechanisms. Specifically, we employ a diffusion transformer (DiT) architecture based on denoising diffusion implicit models (DDIM) to achieve brain signal generation. To realize the goal of image-to-brain signal conversion, we use cross-attention mechanisms to align brain signal embeddings with CLIP image embeddings. Moreover, we leverage large language models (LLMs) to generate image captions, and concatenate the resulting CLIP text embeddings with CLIP image embeddings to form unified embeddings for cross-attention alignment, enabling our model to capture core semantic information. Moreover, to capture core semantic information, we use large language models (LLMs) to generate descriptive and semantically accurate captions for images. Furthermore, we introduce a learnable spatio-temporal position encoding that combines brain region embeddings with temporal embeddings to capture both spatial and temporal characteristics of brain signals. We evaluate the framework on two multimodal benchmark datasets (THINGS-EEG2 and THINGS-MEG) and demonstrate that it generates biologically plausible brain signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP引导的多模态扩散模型的图像到脑信号生成用于视觉假体</div>
<div class="mono" style="margin-top:8px">视觉假体在恢复盲人视力方面具有巨大潜力。尽管研究人员成功利用M/EEG信号在视觉假体的脑解码阶段引发视觉感知，但在脑编码阶段将图像转换为M/EEG信号的互补过程仍然未被充分探索，阻碍了完整功能管道的形成。在本研究中，我们提出了一种新颖的图像到脑信号框架，通过利用增强了交叉注意机制的扩散变换器架构从图像生成M/EEG信号。具体而言，我们采用基于去噪扩散隐式模型（DDIM）的扩散变换器（DiT）架构来实现脑信号生成。为了实现图像到脑信号转换的目标，我们使用交叉注意机制将脑信号嵌入与CLIP图像嵌入对齐。此外，我们利用大型语言模型（LLMs）生成图像标题，并将生成的CLIP文本嵌入与CLIP图像嵌入连接，形成统一的嵌入以进行交叉注意对齐，使我们的模型能够捕捉核心语义信息。此外，为了捕捉核心语义信息，我们使用大型语言模型（LLMs）为图像生成描述性和语义准确的标题。此外，我们引入了一种可学习的时空位置编码，将脑区嵌入与时间嵌入结合，以捕捉脑信号的空间和时间特征。我们在两个多模态基准数据集（THINGS-EEG2和THINGS-MEG）上评估了该框架，并证明其生成生物学上合理的脑信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance visual prostheses by addressing the underexplored process of converting images into M/EEG signals, which is essential for a complete functional pipeline. The authors propose a novel framework that utilizes a diffusion transformer architecture enhanced with cross-attention mechanisms to generate M/EEG signals from images. Experimental results on two multimodal benchmark datasets, THINGS-EEG2 and THINGS-MEG, show that the proposed method successfully generates biologically plausible brain signals, thereby contributing to the development of effective visual prosthetic systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于填补将图像转换为M/EEG信号的空白，这对于为盲人恢复视力的视觉假体的完整功能流程至关重要。作者提出了一种新颖的框架，利用增强的扩散变换器架构和交叉注意机制，从图像生成M/EEG信号。对两个多模态基准数据集的实验结果表明，该框架成功生成生物学上合理的脑信号，表明其在视觉假体的脑编码阶段的潜在有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型快速发展，但数据在影响运动方面的作用仍不清楚。我们提出了Motive（视频生成的运动归因），这是一个以运动为中心的基于梯度的数据归因框架，能够扩展到现代大型高质量视频数据集和模型。我们利用此框架研究哪些微调片段能改善或恶化时间动态。Motive通过运动加权损失掩码将时间动态与静态外观隔离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型中，Motive识别出强烈影响运动的片段，并指导数据策划，以改善时间一致性和物理合理性。使用Motive选择的高影响数据，我们的方法在VBench上提高了运动平滑性和动态程度，与预训练基础模型相比，获得了74.1%的人工偏好胜率。据我们所知，这是第一个在视频生成模型中归因于运动而非视觉外观的框架，并利用它来策划微调数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to better understand how data influences motion in video generation models, an area that has been inadequately explored. The authors introduce Motive, a motion-centric, gradient-based data attribution framework designed to analyze the impact of fine-tuning clips on temporal dynamics in large video datasets. Experimental results demonstrate that Motive effectively isolates temporal dynamics from static appearance, leading to improved motion smoothness and dynamic degree in text-to-video models, achieving a 74.1% human preference win rate over the pretrained base model by guiding data curation towards high-influence clips.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于更好地理解数据如何影响视频生成模型中的运动。作者提出了Motive，这是一个以运动为中心的基于梯度的数据归因框架，旨在分析大型视频数据集和模型。实验结果表明，Motive有效识别出对运动有显著影响的剪辑，从而提高了时间一致性和物理合理性，在VBench上使用Motive选择的数据进行微调时，运动平滑度和动态程度的人工偏好胜率达到74.1%，相比于预训练的基础模型有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：用于行人重识别的视频超分辨率</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹质量通常被视为事后考虑，绝大多数研究集中于对基础模型的架构修改。这些方法忽视了一个重要的限制，在现实世界的困难场景中部署ReID系统时面临挑战。本文介绍了S3-CLIP，一种基于视频超分辨率的CLIP-ReID框架，旨在2026年WACV的VReID-XFD挑战中开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，适应视频基础的行人重识别设置。据我们所知，这项工作代表了首次系统性研究视频超分辨率作为提高行人ReID轨迹质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法在基线性能上具有竞争力，在空中到地面场景中实现了37.52%的mAP，在地面到空中场景中实现了29.16%的mAP。在地面到空中设置中，S3-CLIP在排名准确性上取得了显著提升，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the often-overlooked quality of tracklets in person re-identification (ReID) systems, which can hinder their effectiveness in real-world scenarios. The authors propose S3-CLIP, a novel framework that combines video super-resolution techniques with task-driven pipelines specifically for video-based ReID. Experimental results show that S3-CLIP achieves competitive performance, with a mean Average Precision (mAP) of 37.52% in aerial-to-ground and 29.16% in ground-to-aerial scenarios, along with significant improvements in ranking accuracy in the ground-to-aerial setting, enhancing Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人脸再识别（ReID）系统中常被忽视的轨迹质量问题，这可能在现实场景中影响性能。作者提出了S3-CLIP，这一新框架结合了视频超分辨率技术与CLIP-ReID，以增强轨迹质量，特别针对VReID-XFD挑战。实验结果表明，S3-CLIP在空中到地面场景中实现了37.52%的平均精度（mAP），在地面到空中场景中实现了29.16%的mAP，并在地面到空中设置中显著提高了排名准确性，Rank-1、Rank-5和Rank-10的性能分别提升了11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Visual World with Artificial Intelligence: A Roadmap</div>
<div class="meta-line">Authors: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu</div>
<div class="meta-line">First: 2025-11-11T18:59:50+00:00 · Latest: 2026-01-13T15:42:01+00:00</div>
<div class="meta-line">Comments: Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08585v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08585v2">PDF</a> · <a href="https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://world-model-roadmap.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a &quot;window&quot; into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用人工智能模拟视觉世界：路线图</div>
<div class="mono" style="margin-top:8px">视频生成的格局正在发生变化，从专注于生成视觉吸引力的片段转向构建支持交互并保持物理合理性的虚拟环境。这些发展指向视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐式世界模型，模拟支配真实或想象世界的物理动态、代理-环境交互和任务规划。本调查提供了这一演变的系统概述，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型和视频渲染器。世界模型编码关于世界的结构化知识，包括物理法则、交互动态和代理行为。它作为潜在的模拟引擎，使得连贯的视觉推理、长期时间一致性和目标驱动的规划成为可能。视频渲染器将这种潜在模拟转化为现实的视觉观察，有效地将视频作为“窗口”展示模拟世界。我们通过四个世代追踪视频生成的进展，其中核心能力逐步提升，最终形成一个建立在视频生成模型之上的世界模型，体现内在的物理合理性、实时多模态交互和跨多个时空尺度的规划能力。对于每一代，我们定义其核心特征，突出代表性作品，并考察其应用领域，如机器人技术、自动驾驶和互动游戏。最后，我们讨论下一代世界模型的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作的最新列表可在此链接查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance the field of video generation from merely creating visually appealing clips to developing interactive virtual environments that maintain physical plausibility. The authors systematically survey the evolution of video foundation models, conceptualizing them as a combination of an implicit world model and a video renderer. Key findings indicate that these models encode structured knowledge about the world, enabling coherent visual reasoning and goal-driven planning, with applications in areas such as robotics and autonomous driving, while also identifying open challenges for future development in agent intelligence and system evaluation.</div>
<div class="mono" style="margin-top:8px">本文的动机在于探讨视频生成向创建保持物理合理性的互动虚拟环境的演变。作者系统性地概述了现代视频基础模型，这些模型由隐式世界模型和视频渲染器组成，前者编码关于物理法则和代理行为的结构化知识，后者生成逼真的视觉输出。主要发现表明，这些模型的发展经历了四个阶段，最终形成了一个能够实时进行多模态互动和跨多时空尺度进行规划的系统，应用于机器人技术和自动驾驶等领域。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts</div>
<div class="meta-line">Authors: Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Loppi, Antti Honkela</div>
<div class="meta-line">First: 2024-06-25T06:04:58+00:00 · Latest: 2026-01-13T15:13:42+00:00</div>
<div class="meta-line">Comments: 19 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.17298v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.17298v3">PDF</a> · <a href="https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无捷径的差分隐私深度学习的高效可扩展实现</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DP-SGD）是基于差分隐私（DP）训练机器学习模型的标准算法。最常见的DP-SGD隐私会计依赖于泊松子采样以确保理论上的DP保证。使用泊松子采样实现计算高效的DP-SGD并非易事，这导致许多实现通过使用计算更快的子采样走捷径。我们通过实现和基准测试使用正确的泊松子采样的高效方法来量化在DP下训练深度学习模型的计算成本。我们发现，使用PyTorch中Opacus的DP-SGD的简单实现，其吞吐量比SGD低2.6到8倍。然而，像Ghost Clipping这样的高效梯度裁剪实现可以大致将这一成本减半。我们提出了一种使用JAX的DP-SGD替代计算高效实现，采用泊松子采样，并与基于PyTorch的高效裁剪优化表现相当。我们研究了使用多达80个GPU的扩展行为，发现DP-SGD的扩展性优于SGD。我们在https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL分享我们的库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for efficient implementations of differentially private stochastic gradient descent (DP-SGD) that adhere to theoretical privacy guarantees while avoiding shortcuts. The authors benchmark various methods for training deep learning models under differential privacy, focusing on the computational costs associated with Poisson subsampling. They find that the naive implementation of DP-SGD using Opacus in PyTorch has significantly lower throughput compared to standard SGD, but efficient gradient clipping methods like Ghost Clipping can reduce this cost by approximately half. Additionally, they propose a new implementation of DP-SGD using JAX that maintains the benefits of Poisson subsampling and demonstrates better scalability with up to 80 GPUs compared to SGD.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高在差分隐私下训练深度学习模型的效率，使用的标准算法是差分隐私随机梯度下降（DP-SGD），该算法传统上依赖于泊松子采样。作者实现并基准测试了正确利用泊松子采样的方法，发现使用PyTorch中Opacus的DP-SGD简单实现的吞吐量比标准SGD低2.6到8倍。然而，他们发现像Ghost Clipping这样的高效梯度裁剪方法可以显著降低这一成本，并提出了一种使用JAX的新实现，该实现在采用泊松子采样的同时保持性能。实验表明，DP-SGD在使用多达80个GPU时的扩展性优于SGD。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</div>
<div class="meta-line">Authors: Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</div>
<div class="meta-line">First: 2026-01-13T13:42:05+00:00 · Latest: 2026-01-13T13:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08557v1">PDF</a> · <a href="https://github.com/Simula/HEDGE#videohedge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHEDGE：基于熵的视觉语言模型视频幻觉检测框架，通过语义聚类和时空扰动</div>
<div class="mono" style="margin-top:8px">在视频能力的视觉语言模型（Video-VLMs）中，幻觉现象仍然频繁且置信度高，而现有的不确定性度量往往无法与正确性对齐。我们提出了VideoHEDGE，这是一个用于视频问答中幻觉检测的模块化框架，将基于熵的可靠性估计从图像扩展到时间结构化输入。给定一个视频-问题对，VideoHEDGE从干净片段和光度及时空扰动变体中提取基线答案和多个高温生成，然后使用基于自然语言推理（NLI）或嵌入的方法将结果文本输出聚类为语义假设。聚类级别的概率质量产生三个可靠性评分：语义熵（SE）、RadFlag和视觉增强语义熵（VASE）。我们在SoccerChat基准上评估VideoHEDGE，使用LLM作为评判者获得二元幻觉标签。在三个7B Video-VLM（Qwen2-VL、Qwen2.5-VL和一个SoccerChat微调模型）中，VASE始终实现最高的ROC-AUC，尤其是在较大的失真预算下，而SE和RadFlag往往接近随机。我们进一步表明，基于嵌入的聚类在检测性能上与基于NLI的聚类相匹配，但计算成本显著较低，并且领域微调减少了幻觉频率，但在校准方面仅带来了适度的改善。hedge-bench PyPI库支持可重复和可扩展的基准测试，完整代码和实验资源可在https://github.com/Simula/HEDGE#videohedge获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the frequent and high-confidence hallucinations in video-capable vision-language models (Video-VLMs), which existing uncertainty metrics fail to accurately assess. The authors introduce VideoHEDGE, a modular framework for detecting hallucinations in video question answering that employs entropy-based reliability estimation adapted for temporally structured inputs. Experimental results on the SoccerChat benchmark demonstrate that the Vision-Amplified Semantic Entropy (VASE) score outperforms other metrics in detecting hallucinations across three different Video-VLMs, particularly under larger distortion budgets, while also showing that embedding-based clustering can achieve similar detection performance to Natural Language Inference-based clustering at a lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频能力视觉语言模型（Video-VLMs）中频繁且高置信度的幻觉问题，而现有的不确定性度量无法准确评估。作者提出了VideoHEDGE，这是一个模块化框架，通过利用语义聚类和时空扰动，采用基于熵的可靠性估计来检测视频问答中的幻觉。对SoccerChat基准的实验结果表明，VideoHEDGE的视觉增强语义熵（VASE）在三个7B Video-VLMs中始终实现了最高的ROC-AUC，特别是在较大的失真预算下，而其他指标如语义熵和RadFlag的表现接近随机水平；此外，基于嵌入的聚类在计算成本较低的情况下提供了与基于自然语言推理的聚类相当的检测性能，而领域微调在减少幻觉频率方面仅显示出适度的改善。</div>
</details>
</div>
<div class="card">
<div class="title">Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning</div>
<div class="meta-line">Authors: Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano</div>
<div class="meta-line">First: 2025-07-18T17:59:55+00:00 · Latest: 2026-01-13T13:22:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14137v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14137v3">PDF</a> · <a href="https://github.com/valeoai/Franca">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Franca：用于可扩展视觉表示学习的嵌套俄罗斯套娃聚类</div>
<div class="mono" style="margin-top:8px">我们介绍Franca（发音为Fran-ka）：一个免费的开源视觉基础模型，完全开放（数据、代码、权重），其性能与最先进的专有模型（如DINOv2、CLIP、SigLIPv2等）相匹配，并在许多情况下超越它们。我们的方法基于受Web-SSL启发的透明训练流程，使用公开可用的数据：ImageNet-21K和ReLAION-2B的一个子集。除了模型发布外，我们还解决了SSL聚类方法中的关键限制。现代模型依赖于通过像Sinkhorn-Knopp这样的聚类算法将图像特征分配给大型代码本，但未能考虑聚类语义中的固有模糊性。为了解决这个问题，我们引入了一种基于嵌套俄罗斯套娃表示的参数高效多头聚类投影器。该设计逐步将特征细化为越来越细粒度的聚类，而不增加模型大小，从而实现性能和内存效率。此外，我们提出了一种新颖的位置信息解耦策略，明确消除密集表示中的位置偏差，从而改善语义内容的编码。这在多个下游基准测试中带来了持续的提升，证明了更清晰特征空间的实用性。我们的贡献为透明、高性能的视觉模型建立了新的标准，并为更可重复和可推广的基础模型开辟了道路，服务于更广泛的AI社区。代码和模型检查点可在https://github.com/valeoai/Franca获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to develop a fully open-source vision foundation model that can match or exceed the performance of leading proprietary models while addressing limitations in self-supervised learning (SSL) clustering methods. The authors introduce Franca, which employs a transparent training pipeline using publicly available datasets like ImageNet-21K and a subset of ReLAION-2B, and features a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations to refine image features into fine-grained clusters. The experimental results show that this approach, along with a novel positional disentanglement strategy, leads to significant improvements in encoding semantic content and consistent gains across various downstream benchmarks, establishing a new standard for high-performance vision models in the AI community.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个完全开源的视觉基础模型，该模型不仅匹配而且通常超越现有专有模型的性能。作者开发了Franca，采用了受Web-SSL启发的透明训练流程，并利用了公开可用的数据集，如ImageNet-21K和ReLAION-2B的子集。关键实验结果表明，Franca的创新多头聚类投影器基于嵌套的马特里奥什卡表示，有效地将图像特征细化为详细的聚类，同时保持模型效率，此外，引入的位置信息解耦策略增强了语义内容编码，导致在多个下游基准测试中性能的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Autoregressive Generation</div>
<div class="meta-line">Authors: Stepan Maschan, Haoxuan Qu, Jun Liu</div>
<div class="meta-line">First: 2026-01-06T17:07:27+00:00 · Latest: 2026-01-13T11:19:48+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03184v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去中心化自回归生成</div>
<div class="mono" style="margin-top:8px">我们对自回归生成的去中心化进行了理论分析。我们通过将概率生成速度表示为专家流的线性组合，定义了去中心化离散流匹配目标。我们还进行了实验，展示了在多模态语言模型的不同基准测试中，去中心化和中心化训练设置之间的等价性。具体而言，我们比较了两种不同的范式：LLaVA和InternVL 2.5-1B，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调（ViT+MLP+LLM）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to analyze the decentralization of autoregressive generation in multimodal language models. The authors introduce the Decentralized Discrete Flow Matching objective, which represents probability generating velocity as a linear combination of expert flows. Experimental results show that decentralized and centralized training settings yield equivalent performance across various benchmarks when comparing the LLaVA and InternVL 2.5-1B models, which utilize a fixed CLIP vision encoder and undergo full-parameter fine-tuning during instruction tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是分析多模态语言模型中自回归生成的去中心化。作者提出了去中心化离散流匹配目标，该目标将概率生成速度表示为专家流的线性组合。实验结果表明，在比较LLaVA和InternVL 2.5-1B模型时，去中心化和中心化训练设置在不同基准测试中表现出等效的性能，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调。</div>
</details>
</div>
<div class="card">
<div class="title">MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</div>
<div class="meta-line">Authors: Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra, Biplab Banerjee</div>
<div class="meta-line">First: 2026-01-13T10:44:37+00:00 · Latest: 2026-01-13T10:44:37+00:00</div>
<div class="meta-line">Comments: Accepted at InGARSS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08420v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08420v1">PDF</a> · <a href="https://github.com/AdityaChaudhary2913/CLIP_HSI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMLGNet：使用CLIP进行遥感数据的跨模态对齐</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的多模态框架——多模态语言引导网络（MMLGNet），旨在使用视觉语言模型（如CLIP）将异构遥感模态（如高光谱成像（HSI）和激光雷达（LiDAR））与自然语言语义对齐。随着多模态地球观测数据的日益丰富，迫切需要有效融合光谱、空间和几何信息的方法，同时实现语义层面的理解。MMLGNet采用特定模态的编码器，通过双向对比学习在共享潜在空间中将视觉特征与手工制作的文本嵌入对齐。受到CLIP训练范式的启发，我们的方法弥合了高维遥感数据与语言引导解释之间的差距。值得注意的是，MMLGNet在简单的基于CNN的编码器上表现出色，在两个基准数据集上超越了几种已建立的仅视觉多模态方法，展示了语言监督的显著优势。代码可在https://github.com/AdityaChaudhary2913/CLIP_HSI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing need to effectively fuse heterogeneous remote sensing modalities, such as Hyperspectral Imaging and LiDAR, with natural language semantics to enhance semantic-level understanding of Earth observation data. The authors propose a novel framework called Multimodal Language-Guided Network (MMLGNet), which utilizes modality-specific encoders and bi-directional contrastive learning to align visual features with textual embeddings in a shared latent space. Experimental results show that MMLGNet, using simple CNN-based encoders, outperforms several established multimodal visual-only methods on two benchmark datasets, highlighting the advantages of incorporating language supervision in remote sensing data analysis.</div>
<div class="mono" style="margin-top:8px">本研究解决了将异构遥感模态（如高光谱成像和激光雷达）与自然语言语义对齐的挑战，原因在于多模态地球观测数据的日益增加。提出的多模态语言引导网络（MMLGNet）利用特定模态的编码器和双向对比学习，将视觉特征与共享潜在空间中的文本嵌入对齐。实验结果表明，MMLGNet使用简单的CNN编码器在两个基准数据集上超越了几种已建立的多模态视觉方法，突显了在遥感数据解释中引入语言监督的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care</div>
<div class="meta-line">Authors: Jorge García-Torres, Øyvind Meinich-Bache, Sara Brunner, Siren Rettedal, Vilde Kolstad, Kjersti Engan</div>
<div class="meta-line">First: 2025-03-05T07:52:52+00:00 · Latest: 2026-01-13T10:18:52+00:00</div>
<div class="meta-line">Comments: This work has been accepted at IEEE 25th International Conference on Digital Signal Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03244v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03244v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Around 10% of newborns require some help to initiate breathing, and 5\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双流热成像融合技术用于增强新生儿护理中的出生时间检测</div>
<div class="mono" style="margin-top:8px">约10%的新生儿需要帮助以启动呼吸，5%需要通气支持。准确的出生时间（ToB）记录对于优化新生儿护理至关重要，因为及时干预对适当复苏至关重要。然而，目前记录ToB的临床方法往往依赖手动过程，容易出现不准确。在本研究中，我们提出了一种新颖的双流融合系统，结合图像和视频分析的优势，准确检测产房和手术室中的热成像记录的ToB。通过整合静态和动态流，我们的方法捕捉到更丰富的与出生相关的时空特征，从而实现更强大和精确的ToB估计。我们证明了数据模态之间的协同作用提升了性能，优于单流方法。我们的系统在短视频片段中检测出生的精确度达到95.7%，召回率为84.8%。此外，借助评分聚合模块，它成功识别100%的测试案例中的ToB，较手动标注的中位绝对误差为2秒，绝对平均偏差为4.5秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of Time of Birth (ToB) documentation in neonatal care, which is crucial for timely interventions during resuscitation. The authors developed a two-stream fusion system that integrates image and video analysis to detect ToB from thermal recordings in delivery rooms and operating theaters. The experimental results show that this method achieves 95.7% precision and 84.8% recall in detecting birth events within short video clips, and it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于新生儿护理中准确记录出生时间（ToB）的必要性，因为及时干预对需要帮助的新生儿至关重要。作者开发了一种双流融合系统，结合图像和视频分析，以增强从分娩室和手术室的热成像记录中检测ToB的能力。实验结果表明，该方法在短视频片段中检测出生事件的精度达到95.7%，召回率为84.8%，并且在所有测试案例中成功识别ToB，且与手动标注相比，绝对中位误差仅为2秒。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment</div>
<div class="meta-line">Authors: Hua Ye, Hang Ding, Siyuan Chen, Yiyang Jiang, Changyuan Zhang, Xuan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-11T16:15:15+00:00 · Latest: 2026-01-13T06:56:09+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08399v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过不对齐进行对齐：面向边界的多模态对齐课程学习</div>
<div class="mono" style="margin-top:8px">大多数多模态模型将每个负样本视为相同，忽略了与正样本仅在细节上有所不同的模糊负样本。我们提出了边界感知课程学习与局部注意力（BACL），这是一个轻量级的附加模块，将这些边界案例转化为课程信号。边界感知负样本采样器逐步提高难度，而对比局部注意力损失则突出不匹配发生的地方。这两个模块都是完全可微分的，并且可以与任何现成的双编码器配合使用。理论预测错误率快速下降至O(1/n)；实践表明在CLIP上提高了高达32%的R@1，并在四个大规模基准上达到了新的SOTA，且无需额外标签。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in multimodal models where negative pairs are treated uniformly, neglecting those that are only slightly different from positive pairs. The authors introduce the Boundary-Aware Curriculum with Local Attention (BACL), which incorporates a Boundary-aware Negative Sampler to incrementally increase the difficulty of these borderline cases and a Contrastive Local Attention loss to pinpoint mismatches. Experimental results demonstrate that this approach achieves up to a 32% improvement in R@1 over CLIP and sets new state-of-the-art performance on four large-scale benchmarks, all without requiring additional labels.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决模态模型对模糊负样本的处理来改善多模态模型。作者提出了边界感知课程学习与局部注意力（BACL），其中包括一个边界感知负样本采样器，逐步提高难度，以及一个对比局部注意力损失，以准确定位不匹配之处。实验结果表明，BACL在R@1上比CLIP提高了32%，并在四个大规模基准上创下了新的最先进性能，且无需额外标签。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向预测的深度强化学习在奶牛场高效电力负荷调度中的应用</div>
<div class="mono" style="margin-top:8px">奶牛养殖是一个能源密集型行业，严重依赖电网电力。随着可再生能源的不断整合，可持续能源管理已成为减少对电网依赖和支持联合国可持续发展目标7（可负担和清洁能源）的关键。然而，可再生能源的间歇性特征在实时平衡供需方面带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面显示出潜力。然而，大多数基于RL的调度方法假设对未来价格或发电有完全的了解，这在动态环境中是不现实的。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，通常导致在可变电价下训练不稳定。为了解决这些挑战，本研究提出了一种深度强化学习框架，用于在奶牛场进行高效负荷调度，重点关注电池储存和水加热，并考虑现实的操作约束。所提出的面向预测的PPO结合了基于日时和月份的残差校准的短期需求和可再生发电预测，而PID KL PPO变体则采用比例-积分-微分控制器自适应调节KL散度，以实现稳定的策略更新。该方法在真实奶牛场数据上训练，电力成本比PPO低1%，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO将电网进口减少了13.1%，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance energy management in dairy farming, which is heavily reliant on grid electricity and faces challenges due to the intermittent nature of renewable energy sources. The study proposes a Deep Reinforcement Learning framework, specifically the Forecast Aware PPO, which utilizes short-term forecasts of demand and renewable generation to improve load scheduling while addressing the limitations of traditional RL methods that assume complete knowledge of future conditions. Experimental results indicate that the proposed method achieves up to 1% lower electricity costs compared to standard PPO, 4.8% compared to DQN, and 1.5% compared to SAC, while also reducing grid imports for battery scheduling by 13.1%, demonstrating its effectiveness in sustainable energy management for dairy farms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善奶牛养殖中的能源管理，该行业严重依赖电网电力，并面临可再生能源间歇性带来的挑战。研究提出了一种深度强化学习框架，特别是预测感知PPO，结合了需求和可再生发电的短期预测，以改善负载调度，同时解决传统强化学习方法假设对未来条件完全了解的局限性。实验结果表明，该方法在电力成本上比标准PPO低1%，比DQN低4.8%，比SAC低1.5%，同时在电池调度方面减少了13.1%的电网进口，证明了其在奶牛养殖可持续能源管理中的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
