<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-16 03:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260216_0336</div>
    <div class="row"><div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&#x27;&#x27; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &quot;boot-time compute&quot; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩放验证比缩放策略学习更有效于视觉-语言-动作对齐</div>
<div class="mono" style="margin-top:8px">通用机器人的长期愿景依赖于它们理解和执行自然语言指令的能力。视觉-语言-动作（VLA）模型在实现这一目标方面取得了显著进展，但它们生成的动作仍可能与给定指令不一致。本文探讨了测试时验证作为缩小“意图-动作差距”的手段。我们首先描述了具身指令跟随的测试时缩放法则，并证明联合缩放重述指令和生成动作的数量大大增加了测试时样本多样性，通常比独立缩放每个维度更有效地恢复正确动作。为了利用这些缩放法则，我们提出了CoVer，一个用于视觉-语言-动作对齐的对比验证器，并展示了我们的架构在额外计算资源和数据下的良好扩展性。然后，我们引入了“启动时计算”和一个层次化的VLA验证推理管道。在部署时，我们的框架从视觉-语言模型（VLM）预计算一组多样化的重述指令，为每个指令重复生成动作候选，然后使用验证器选择最佳的高层提示和低层动作片段。与在相同数据上进行的策略预训练相比，我们的验证方法在SIMPLER基准上在分布内获得22%的增益，在分布外获得13%的增益，并在真实世界实验中进一步提高了45%。在PolaRiS基准上，CoVer在任务进展上获得14%的增益，在成功率上获得9%的增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the alignment of actions generated by Vision-Language-Action (VLA) models with natural language instructions, addressing the persistent issue of misalignment. The authors propose a method called CoVer, a contrastive verifier that utilizes test-time verification and scaling laws to improve the diversity of generated actions and instructions. Experimental results demonstrate that this approach outperforms traditional scaling policy pre-training, achieving a 22% improvement in-distribution and a 13% improvement out-of-distribution on the SIMPLER benchmark, along with a 45% enhancement in real-world scenarios, and a 14% increase in task progress and 9% in success rate on the PolaRiS benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强视觉-语言-动作（VLA）模型生成的动作与其应遵循的自然语言指令之间的对齐。作者提出了一种名为CoVer的对比验证器，利用测试时验证来缩小意图与动作之间的差距，通过同时扩展重述指令和生成动作来实现。实验结果表明，该方法优于传统的策略预训练，在SIMPLER基准上实现了22%的分布内提升和13%的分布外提升，在实际场景中提升了45%，并在PolaRiS基准上实现了14%的任务进展和9%的成功率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</div>
<div class="meta-line">Authors: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu</div>
<div class="meta-line">First: 2026-02-12T18:59:54+00:00 · Latest: 2026-02-12T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://stroke-of-surprise.github.io/ Code: https://github.com/stroke-of-surprise/Stroke-Of-Surprise</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12280v1">PDF</a> · <a href="https://github.com/stroke-of-surprise/Stroke-Of-Surprise">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://stroke-of-surprise.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &quot;dual-constraint&quot;: initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a &quot;common structural subspace&quot; valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>惊喜的笔触：向量素描中的渐进语义幻觉</div>
<div class="mono" style="margin-top:8px">视觉幻觉传统上依赖于空间操作，如多视图一致性。在这项工作中，我们引入了渐进语义幻觉，这是一项新颖的向量素描任务，其中单个素描通过逐步添加笔触经历戏剧性的语义转变。我们提出了惊喜的笔触，这是一个生成框架，优化向量笔触以满足不同绘制阶段的独特语义解释。核心挑战在于“双重约束”：初始前缀笔触必须形成一个连贯的对象（例如，一只鸭子），同时在添加增量笔触时作为第二个概念（例如，一只羊）的结构基础。为了解决这个问题，我们提出了一种序列感知的联合优化框架，驱动于双分支评分蒸馏采样（SDS）机制。与冻结初始状态的顺序方法不同，我们的方法动态调整前缀笔触，以发现对两个目标有效的“共同结构子空间”。此外，我们引入了一种新颖的叠加损失，强制空间互补，确保结构整合而非遮挡。大量实验表明，我们的方法在可识别性和幻觉强度上显著优于最先进的基线，成功地将视觉字谜从空间维度扩展到时间维度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore visual illusions through a novel vector sketching task that allows for dramatic semantic transformations via sequential stroke additions. The authors introduce a generative framework called Stroke of Surprise, which employs a dual-branch Score Distillation Sampling mechanism to optimize vector strokes while satisfying dual constraints: the initial strokes must represent a coherent object and also serve as a foundation for a second concept. Experimental results show that this method significantly outperforms existing techniques in terms of recognizability and illusion strength, effectively extending visual anagrams into the temporal dimension.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索超越传统空间操控的视觉幻觉，特别是通过一种新的矢量素描任务，使得语义转变变得戏剧化。作者提出了一种名为“惊喜笔画”的生成框架，该框架采用序列感知的联合优化方法，利用双分支评分蒸馏采样机制来管理保持初始笔画一致性与添加新语义解释之间的双重约束。实验结果表明，该方法在可识别性和幻觉强度方面显著优于现有方法，有效地将视觉双关语扩展到时间维度。</div>
</details>
</div>
<div class="card">
<div class="title">UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</div>
<div class="meta-line">Authors: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu</div>
<div class="meta-line">First: 2026-02-12T18:59:49+00:00 · Latest: 2026-02-12T18:59:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12279v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniT：统一多模态思维链测试时扩展</div>
<div class="mono" style="margin-top:8px">统一模型可以在单一架构内处理多模态理解和生成，但通常在单次传递中操作，而不迭代地优化输出。许多多模态任务，特别是涉及复杂空间组合、多个交互对象或不断变化的指令的任务，需要分解指令、验证中间结果并进行迭代修正。尽管测试时扩展（TTS）已证明为迭代推理分配额外推理计算可以显著提高语言模型性能，但将这一范式扩展到统一多模态模型仍然是一个开放挑战。我们介绍了UniT，一个用于多模态思维链测试时扩展的框架，使单一统一模型能够在多个回合中推理、验证和优化。UniT结合了主动数据合成、统一模型训练和灵活的测试时推理，以引发包括验证、子目标分解和内容记忆在内的认知行为。我们的主要发现是：（1）在短推理轨迹上训练的统一模型在测试时能够推广到更长的推理链；（2）顺序思维链推理提供了一种比并行采样更具可扩展性和计算效率的TTS策略；（3）在生成和编辑轨迹上训练可以改善分布外视觉推理。这些结果确立了多模态测试时扩展作为推动统一模型生成和理解的有效范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of unified multimodal models, which typically lack the ability to iteratively refine outputs during complex tasks. The authors introduce UniT, a framework that implements multimodal chain-of-thought test-time scaling, allowing a single model to reason, verify, and refine outputs through multiple rounds of inference. Key experimental findings indicate that unified models trained on shorter reasoning paths can generalize to longer inference chains, that sequential reasoning is more efficient than parallel sampling for test-time scaling, and that training on both generation and editing trajectories enhances visual reasoning capabilities in out-of-distribution scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型的性能，这些模型通常缺乏对复杂任务所需的迭代输出精炼。作者提出了UniT框架，促进多模态思维链测试时间扩展，使单一模型能够通过多次迭代进行推理、验证和精炼输出。主要实验结果表明，经过短推理路径训练的统一模型能够有效地推广到更长的推理链，顺序推理在测试时间扩展中比并行采样更高效，并且在生成和编辑轨迹上的训练增强了模型在分布外场景中的视觉推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">AttentionRetriever: Attention Layers are Secretly Long Document Retrievers</div>
<div class="meta-line">Authors: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang</div>
<div class="meta-line">First: 2026-02-12T18:59:35+00:00 · Latest: 2026-02-12T18:59:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12278v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12278v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AttentionRetriever：注意力层是秘密的长文档检索器</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）已被广泛采用，以帮助大型语言模型（LLMs）处理涉及长文档的任务。然而，现有的检索模型并未针对长文档检索进行设计，未能解决长文档检索的几个关键挑战，包括上下文意识、因果依赖和检索范围。在本文中，我们提出了AttentionRetriever，一种新颖的长文档检索模型，利用注意力机制和基于实体的检索来构建上下文感知的长文档嵌入，并确定检索范围。通过大量实验，我们发现AttentionRetriever在长文档检索数据集上能够大幅超越现有的检索模型，同时保持与密集检索模型一样的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve long document retrieval in retrieval augmented generation (RAG) for Large Language Models (LLMs), as existing models struggle with key challenges such as context-awareness and causal dependence. The authors propose AttentionRetriever, a novel model that utilizes attention mechanisms and entity-based retrieval to create context-aware embeddings and define the retrieval scope. Experimental results demonstrate that AttentionRetriever significantly outperforms current retrieval models on long document datasets while maintaining efficiency comparable to dense retrieval models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善大型语言模型（LLMs）对长文档的检索能力，因为现有模型在上下文意识和因果依赖等挑战上表现不佳。作者提出了AttentionRetriever，这是一种新型检索模型，利用注意力机制和基于实体的检索来创建上下文感知的嵌入并定义检索范围。实验结果表明，AttentionRetriever在长文档数据集上的表现显著优于当前的检索模型，同时保持与密集检索模型相当的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Test-Time Scaling for WebAgents</div>
<div class="meta-line">Authors: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</div>
<div class="meta-line">First: 2026-02-12T18:58:30+00:00 · Latest: 2026-02-12T18:58:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent&#x27;s own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebAgents的代理测试时间缩放</div>
<div class="mono" style="margin-top:8px">测试时间缩放已成为提高神经网络模型性能和增强可靠性的标准方法。然而，它在代理的多步骤任务上的表现仍然不够清楚：每步的小错误可能在长时间内累积；我们发现，简单的策略均匀增加采样会导致收益递减。在这项工作中，我们提出了CATTS，这是一种为多步骤代理动态分配计算的简单技术。我们首先对Web代理的推理时间缩放进行了实证研究。我们发现，在长时间环境中，均匀增加每步计算会迅速饱和。然后，我们研究了更强的聚合策略，包括基于LLM的仲裁者，它可以超越简单投票，但可能会推翻高共识决策。我们展示了从代理自身投票分布（熵和前1/前2边际）得出的不确定性统计与下游成功相关，并为动态计算分配提供了实用信号。基于这些发现，我们引入了信心感知测试时间缩放（CATTS），它利用投票衍生的不确定性仅在决策真正有争议时分配计算。CATTS在WebArena-Lite和GoBrowse上的性能提高了多达9.1%，同时使用的token比均匀缩放少了多达2.3倍，提供了效率提升和可解释的决策规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance and reliability of neural network models in agentic, multi-step tasks, where small errors can accumulate over time. The authors introduce Confidence-Aware Test-Time Scaling (CATTS), a technique for dynamically allocating compute resources based on the uncertainty of decisions made by web agents. Through empirical studies, they find that simply increasing compute uniformly leads to diminishing returns in long-horizon tasks, and they explore more effective aggregation strategies, including an LLM-based Arbiter. Their findings reveal that uncertainty statistics from the agent&#x27;s vote distribution correlate with success, allowing CATTS to improve performance on WebArena-Lite and GoBrowse by up to 9.1% while using 2.3x fewer tokens than uniform scaling, thus achieving both efficiency and interpretability in decision-making.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高神经网络模型在代理性多步骤任务中的性能和可靠性，因为小错误可能会随着时间的推移而累积。作者提出了一种基于决策不确定性的动态计算资源分配方法——信心感知测试时间缩放（CATTS）。通过实证研究，他们发现均匀增加每一步的计算在长时间任务中会导致收益递减，并且他们证明了该方法可以在WebArena-Lite和GoBrowse上提高性能，最高可达9.1%，同时使用的令牌数量比传统均匀缩放少2.3倍，从而在决策中提供了效率和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</div>
<div class="meta-line">Authors: Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen</div>
<div class="meta-line">First: 2026-02-12T18:58:12+00:00 · Latest: 2026-02-12T18:58:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12274v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12274v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于碳捕集与储存的正向与反向建模的函数空间解耦扩散</div>
<div class="mono" style="margin-top:8px">准确表征地下流动对碳捕集与储存（CCS）至关重要，但由于稀疏观测的逆问题具有不适定性，仍面临挑战。我们提出了Fun-DDPS，这是一种生成框架，将函数空间扩散模型与可微神经算子代理结合，用于正向和反向建模。我们的方法使用单通道扩散模型学习地质参数（地质模型）的先验分布，然后利用局部神经算子（LNO）代理为动态场的跨场条件提供物理一致的指导。这种解耦使得扩散先验能够稳健地恢复参数空间中的缺失信息，而代理则为数据同化提供高效的基于梯度的指导。我们在合成CCS建模数据集上演示了Fun-DDPS，取得了两个关键结果：（1）在仅有25%观测的正向建模中，Fun-DDPS的相对误差为7.7%，而标准代理为86.9%（提高了11倍），证明了其处理极端数据稀疏的能力，而确定性方法则失败。（2）我们首次对基于扩散的逆解算器进行了严格验证，与渐近精确的拒绝采样（RS）后验进行比较。Fun-DDPS和联合状态基线（Fun-DPS）对真实值的詹森-香农散度均小于0.06。重要的是，Fun-DDPS生成的物理一致的实现不受联合状态基线中观察到的高频伪影的影响，其样本效率比拒绝采样提高了4倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the characterization of subsurface flow in Carbon Capture and Storage (CCS), which is complicated by the challenges of inverse problems with limited data. The authors introduce Fun-DDPS, a generative framework that integrates function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. The key experimental findings indicate that Fun-DDPS significantly outperforms standard methods, achieving a 7.7% relative error in forward modeling with only 25% observations, compared to 86.9% for traditional surrogates, and it also validates diffusion-based inverse solvers against exact Rejection Sampling posteriors, demonstrating improved sample efficiency and physically consistent results without high-frequency artifacts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善碳捕集与储存（CCS）中地下流动的表征，这一过程受到稀疏观测下逆问题挑战的影响。作者提出了Fun-DDPS，这是一种将函数空间扩散模型与可微神经算子代理结合的生成框架，用于正向和反向建模。关键实验结果表明，Fun-DDPS在仅有25%观测数据的情况下，正向建模的相对误差为7.7%，显著优于标准代理，并且对扩散基础的逆解算子进行了严格验证，Jensen-Shannon散度小于0.06，同时确保了物理一致的实现和更高的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs</div>
<div class="meta-line">Authors: Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</div>
<div class="meta-line">First: 2026-02-12T18:57:43+00:00 · Latest: 2026-02-12T18:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12273v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12273v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习控制：用于线性偏微分方程非光滑最优控制的iUzawa-Net</div>
<div class="mono" style="margin-top:8px">我们提出了一种优化信息驱动的深度神经网络方法，称为iUzawa-Net，旨在成为第一个能够实时解决一类线性偏微分方程（PDE）非光滑最优控制问题的求解器。iUzawa-Net展开了一种不精确的Uzawa方法用于鞍点问题，用特定设计的可学习神经网络替代经典的预处理器和PDE求解器。我们证明了其通用逼近性质，并建立了iUzawa-Net的渐近$\varepsilon$-最优性，通过非光滑椭圆和抛物线最优控制问题验证了其良好的数值效率。我们的方法为设计和分析各种优化信息驱动的深度学习方法提供了一个多功能框架，适用于最优控制和其他PDE约束的优化问题。所提出的学习控制方法将基于模型的优化算法与数据驱动的深度学习技术相结合，继承了两种方法的优点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for real-time solutions to nonsmooth optimal control problems in linear partial differential equations (PDEs). The authors introduce the iUzawa-Net, an optimization-informed deep neural network that employs an inexact Uzawa method to address saddle point problems, replacing traditional preconditioners and PDE solvers with learnable neural networks. Experimental results demonstrate the iUzawa-Net&#x27;s universal approximation capabilities and its asymptotic ε-optimality, showing promising numerical efficiency in solving nonsmooth elliptic and parabolic optimal control problems.</div>
<div class="mono" style="margin-top:8px">本研究通过引入iUzawa-Net，一种优化信息驱动的深度神经网络，解决线性偏微分方程（PDE）中非光滑最优控制问题的实时求解挑战。该方法采用不精确的Uzawa方法展开鞍点问题，利用可学习的神经网络替代传统的预处理器和PDE求解器。实验结果表明，iUzawa-Net具有普适逼近性质和渐近ε最优性，在解决非光滑椭圆和抛物最优控制问题时展现出显著的数值效率。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupled Diffusion Sampling for Inverse Problems on Function Spaces</div>
<div class="meta-line">Authors: Thomas Y. L. Lin, Jiachen Yao, Lufang Chiang, Julius Berner, Anima Anandkumar</div>
<div class="meta-line">First: 2026-01-30T18:54:49+00:00 · Latest: 2026-02-12T18:56:52+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23280v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23280v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>功能空间中逆问题的解耦扩散采样</div>
<div class="mono" style="margin-top:8px">我们提出了一种数据高效、物理感知的生成框架，用于逆偏微分方程（PDE）问题。现有的即插即用扩散后验采样器通过联合系数-解建模隐式地表示物理，需大量配对监督。相比之下，我们的解耦扩散逆求解器（DDIS）采用解耦设计：无条件扩散学习系数先验，而神经算子显式建模前向PDE以提供指导。这种解耦使得数据效率更高，物理信息学习更有效，同时自然支持解耦退火后验采样（DAPS），以避免扩散后验采样（DPS）中的过平滑。理论上，我们证明了DDIS在训练数据稀缺时避免了联合模型的指导衰减失败。实证上，DDIS在稀疏观测下实现了最先进的性能，平均提高了11%的$l_2$误差和54%的谱误差；当数据限制在1%时，DDIS在$l_2$误差上相比联合模型保持了40%的准确性优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of data efficiency in solving inverse PDE problems, which typically require substantial paired supervision in existing methods. The authors introduce the Decoupled Diffusion Inverse Solver (DDIS), which separates the learning of coefficient priors through an unconditional diffusion from the explicit modeling of the forward PDE using a neural operator. This approach enhances data efficiency and supports Decoupled Annealing Posterior Sampling to mitigate over-smoothing issues. Experimental results demonstrate that DDIS outperforms traditional joint models, achieving an average improvement of 11% in $l_2$ error and 54% in spectral error under sparse observations, and maintains a 40% advantage in $l_2$ error when data is limited to 1%.</div>
<div class="mono" style="margin-top:8px">本研究针对逆偏微分方程问题中的数据低效性挑战，提出了一种新的生成框架，称为解耦扩散逆求解器（DDIS）。该方法采用解耦设计，其中无条件扩散模型学习系数先验，而神经算子明确建模正向偏微分方程，从而提高数据效率和物理信息学习。实验结果表明，DDIS显著优于现有模型，在稀疏观测下实现了$ l_2 $误差降低11%和光谱误差降低54%的效果，并且在数据仅限于1%时，相较于联合模型保持了$ l_2 $误差40%的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Creative Ownership in the Age of AI</div>
<div class="meta-line">Authors: Annie Liang, Jay Lu</div>
<div class="meta-line">First: 2026-02-12T18:56:42+00:00 · Latest: 2026-02-12T18:56:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12270v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12270v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Copyright law focuses on whether a new work is &quot;substantially similar&quot; to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能时代的创造性所有权</div>
<div class="mono" style="margin-top:8px">版权法关注新作品是否与现有作品&quot;实质相似&quot;，但生成性人工智能可以在不复制内容的情况下紧密模仿风格，这一能力现在是持续诉讼的核心。我们认为现有的侵权定义不适合这种情况，并提出一个新标准：如果生成性人工智能的输出无法在其训练语料库中生成，则该输出侵犯了现有作品。为了实现这一定义，我们将生成系统建模为将现有作品语料库映射到新作品输出的闭包算子。如果根据我们的标准，人工智能生成的输出不侵犯任何现有作品，则被视为\emph{允许}。我们的结果表征了允许生成的结构特性，并揭示了一个明显的渐近二分法：当有机创作过程是轻尾的时，依赖于个别作品最终消失，因此监管对人工智能生成没有限制；而在重尾创作中，监管可能持续约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inadequacy of existing copyright law in dealing with generative AI, which can imitate styles without directly copying content, leading to ongoing legal disputes. The authors propose a new criterion for infringement, suggesting that a generative AI output infringes on an existing work if it could not have been generated without that work in its training data. Their findings reveal structural properties of permissible AI generation and demonstrate a significant dichotomy: in cases of light-tailed organic creations, reliance on individual works diminishes, allowing for unrestricted AI generation, whereas in heavy-tailed scenarios, regulatory constraints remain impactful.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前版权法在生成性人工智能背景下的不足，生成性人工智能能够模仿风格而不直接复制内容，导致持续的法律争议。作者提出了一种新的侵权标准，基于生成性人工智能输出是否无法在其训练数据中存在的现有作品的情况下产生。通过将生成系统建模为闭包算子，他们对允许的人工智能输出进行了表征，并发现基础创作的性质显著影响监管约束，揭示了轻尾创作允许无限制的人工智能生成，而重尾创作则施加持久的监管限制。</div>
</details>
</div>
<div class="card">
<div class="title">CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use</div>
<div class="meta-line">Authors: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang</div>
<div class="meta-line">First: 2026-02-12T18:55:09+00:00 · Latest: 2026-02-12T18:55:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12268v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12268v1">PDF</a> · <a href="https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn&#x27;s intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CM2：使用检查表奖励的多轮多步代理工具使用强化学习</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地被用于通过推理多轮用户交互和调用外部工具来解决现实世界的任务。然而，将强化学习应用于这种环境仍然困难：现实目标往往缺乏可验证的奖励，而强调开放式行为；此外，多轮多步代理工具使用的强化学习仍然未被充分探索；构建和维护可执行的工具环境成本高昂，限制了规模和覆盖范围。我们提出CM2，一个将可验证结果奖励替换为检查表奖励的强化学习框架。CM2将每轮的预期行为分解为具有明确证据基础和结构化元数据的细粒度二元标准，将开放式判断转变为更稳定的分类风格决策。为了平衡稳定性和信息量，我们的方法采用稀疏奖励分配但密集评估标准的策略。训练在可扩展的LLM模拟工具环境中进行，避免了对大型工具集的重型工程。实验表明，CM2在监督微调上持续改进。从一个8B基础模型开始，并在一个8k示例的强化学习数据集上训练，CM2在tau^-Bench上比SFT对照组提高了8分，在BFCL-V4上提高了10分，在ToolSandbox上提高了12分。结果与同样规模的开源基线相匹配，甚至超越，包括判断模型。因此，CM2提供了一种可扩展的优化多轮多步工具使用代理的方案，而无需依赖可验证的奖励。代码由开源社区提供：https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the application of reinforcement learning (RL) in multi-turn and multi-step interactions involving AI agents and external tools, which often face challenges due to the lack of verifiable rewards and the complexity of building tool environments. The authors propose CM2, an RL framework that utilizes checklist rewards by breaking down intended behaviors into binary criteria with explicit evidence and structured metadata, allowing for more stable decision-making. Experimental results demonstrate that CM2 significantly outperforms supervised fine-tuning, achieving improvements of 8 points on tau^-Bench, 10 points on BFCL-V4, and 12 points on ToolSandbox, while also matching or exceeding the performance of comparable open-source models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升强化学习（RL）在多轮和多步骤代理工具使用中的应用，但由于缺乏可验证的奖励和构建可执行工具环境的复杂性，这一领域面临挑战。作者提出了CM2，一个利用检查表奖励的RL框架，通过将每次交互分解为具有明确证据和结构化元数据的二元标准，从而实现更稳定的决策。实验结果表明，CM2在多个基准测试中优于监督微调，在tau^-Bench上提高了8分，在ToolSandbox上提高了12分，同时也匹配或超越了类似开源模型的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</div>
<div class="meta-line">Authors: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar</div>
<div class="meta-line">First: 2026-02-12T18:54:57+00:00 · Latest: 2026-02-12T18:54:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12267v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO&#x27;s robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过流引导神经算子进行自监督学习的时间序列数据</div>
<div class="mono" style="margin-top:8px">自监督学习（SSL）是一种强大的从未标记时间序列数据中学习的范式。然而，流行的方法如掩码自编码器（MAEs）依赖于从固定的、预先确定的掩码比例重建输入。我们提出将损坏程度视为表示学习的新自由度，以增强灵活性和性能。为此，我们引入流引导神经算子（FGNO），这是一个将算子学习与流匹配结合的自监督学习训练的新框架。FGNO通过使用短时傅里叶变换在功能空间中学习映射，以统一不同的时间分辨率。我们通过利用不同网络层和流时间提取丰富的特征层次，这些层次对输入数据施加不同强度的噪声。这使得从低级模式到高级全局特征的多样化表示的提取成为可能，使用一个适应特定任务的单一模型。与之前在推理过程中使用噪声输入的生成自监督学习方法不同，我们建议在表示提取时使用干净输入，同时用噪声学习表示；这消除了随机性并提高了准确性。我们在三个生物医学领域评估FGNO，结果表明其始终优于已建立的基线。我们的方法在神经信号解码（BrainTreeBank）中实现了高达35%的AUROC增益，在皮肤温度预测（DREAMT）中减少了16%的RMSE，并在低数据情况下的SleepEDF上提高了20%以上的准确性和宏观F1。这些结果突显了FGNO对数据稀缺的鲁棒性及其学习多样时间序列的表达性表示的优越能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance self-supervised learning (SSL) for unlabeled time-series data by introducing flexibility in the corruption level used during training. The authors propose the Flow-Guided Neural Operator (FGNO), which combines operator learning with flow matching to learn representations in functional spaces through Short-Time Fourier Transform. Experimental results demonstrate that FGNO significantly outperforms existing methods across three biomedical domains, achieving up to 35% AUROC gains in neural signal decoding, 16% RMSE reductions in skin temperature prediction, and over 20% improvement in accuracy and macro-F1 scores on SleepEDF, showcasing its effectiveness in learning robust representations even with limited data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善无标签时间序列数据的自监督学习（SSL），解决现有方法依赖固定掩蔽比例的局限性。作者提出了流引导神经算子（FGNO），这是一种将算子学习与流匹配相结合的新框架，通过将损坏水平视为变量来增强表示学习。实验结果表明，FGNO在三个生物医学领域显著优于现有基准，在神经信号解码中获得高达35%的AUROC增益，在皮肤温度预测中减少16%的RMSE，并在SleepEDF上提高超过20%的准确率和宏F1值，展示了其在有限数据下学习鲁棒表示的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</div>
<div class="meta-line">Authors: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2026-02-12T18:52:35+00:00 · Latest: 2026-02-12T18:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12262v1">PDF</a> · <a href="https://github.com/Tyrion58/T3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model&#x27;s own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T3D：通过轨迹自蒸馏与直接判别优化实现的少步扩散语言模型</div>
<div class="mono" style="margin-top:8px">扩散大型语言模型（DLLMs）有潜力通过并行解码多个标记来实现快速文本生成。然而，在实践中，它们的推理效率受到许多精炼步骤的限制，而大幅减少步骤数量会导致生成质量显著下降。为了解决这个问题，我们提出了一种轨迹自蒸馏框架，通过蒸馏模型自身的生成轨迹来改善少步解码。我们结合了直接判别优化（DDO），这是一种促进模式寻求蒸馏的反向KL目标，鼓励学生集中于高概率教师模式。在基准测试中，我们的方法在严格的步骤预算下始终优于强大的少步基线和标准训练。尽管全步解码仍然优越，但我们大幅缩小了差距，为实用的少步DLLMs奠定了坚实基础。源代码可在 https://github.com/Tyrion58/T3D 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inefficiencies in inference for diffusion large language models (DLLMs), which require many refinement steps for quality text generation. The authors propose a trajectory self-distillation framework that enhances few-step decoding by leveraging the model&#x27;s own generative trajectories, incorporating Direct Discriminative Optimization (DDO) to focus on high-probability outputs. Experimental results demonstrate that this method consistently outperforms existing few-step baselines and standard training methods, significantly narrowing the performance gap with full-step decoding and laying the groundwork for more practical few-step DLLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高扩散大语言模型（DLLMs）的推理效率，以实现快速文本生成，而这一效率通常受到需要多次精炼步骤的限制。作者提出了一种轨迹自蒸馏框架，通过利用模型自身的生成轨迹来改善少步解码，并结合直接判别优化（DDO）以促进模式寻求蒸馏。实验结果表明，该方法在严格的步骤预算下，始终优于强少步基线和标准训练方法，显著缩小了与全步解码的性能差距，为实用的少步DLLMs奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</div>
<div class="meta-line">Authors: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu</div>
<div class="meta-line">First: 2026-02-12T18:49:27+00:00 · Latest: 2026-02-12T18:49:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像科学家一样思考：物理引导的LLM代理用于方程发现</div>
<div class="mono" style="margin-top:8px">通过符号化、可解释的公式解释观察到的现象是科学的基本目标。最近，大型语言模型（LLMs）因其广泛的领域知识和强大的推理能力而成为符号方程发现的有前景工具。然而，大多数现有的基于LLM的系统直接从数据中猜测方程，而没有建模科学家通常遵循的多步骤推理过程：首先推断物理属性，如对称性，然后使用这些作为先验来限制候选方程的空间。我们介绍了KeplerAgent，一个明确遵循这一科学推理过程的代理框架。该代理协调基于物理的工具提取中间结构，并利用这些结果配置符号回归引擎，如PySINDy和PySR，包括它们的函数库和结构约束。在一系列物理方程基准测试中，KeplerAgent在符号准确性和对噪声数据的鲁棒性方面显著高于LLM和传统基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the process of symbolic equation discovery in science by modeling the multi-step reasoning that scientists typically employ. The authors introduce KeplerAgent, a framework that integrates physics-based tools to extract intermediate structures and utilizes these insights to optimize symbolic regression engines like PySINDy and PySR. Experimental results demonstrate that KeplerAgent significantly outperforms both large language models and traditional methods in terms of symbolic accuracy and robustness to noisy data across various physical equation benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过模拟科学家通常采用的多步骤推理过程来增强符号方程发现的过程。作者提出了KeplerAgent，一个集成物理基础工具以提取中间结构的框架，并利用这些见解来配置符号回归引擎，如PySINDy和PySR。实验结果表明，KeplerAgent在多个物理方程基准测试中，在符号准确性和对噪声数据的鲁棒性方面显著优于大型语言模型和传统方法。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference</div>
<div class="meta-line">Authors: Nicolas Johansson, Tobias Olsson, Daniel Nilsson, Johan Östman, Fazeleh Hoseini</div>
<div class="meta-line">First: 2025-09-04T12:43:45+00:00 · Latest: 2026-02-12T18:46:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04169v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04169v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Membership inference attacks (MIAs) aim to determine whether specific data were used to train a model. While extensively studied on classification models, their impact on time series forecasting remains largely unexplored. We address this gap by introducing two new attacks: (i) an adaptation of multivariate LiRA, a state-of-the-art MIA originally developed for classification models, to the time-series forecasting setting, and (ii) a novel end-to-end learning approach called Deep Time Series (DTS) attack. We benchmark these methods against adapted versions of other leading attacks from the classification setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD datasets, targeting two strong forecasting architectures, LSTM and the state-of-the-art N-HiTS, under both record- and user-level threat models. Our results show that forecasting models are vulnerable, with user-level attacks often achieving perfect detection. The proposed methods achieve the strongest performance in several settings, establishing new baselines for privacy risk assessment in time series forecasting. Furthermore, vulnerability increases with longer prediction horizons and smaller training populations, echoing trends observed in large language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间序列预测中的隐私风险：用户级和记录级成员推断</div>
<div class="mono" style="margin-top:8px">成员推断攻击（MIA）旨在确定特定数据是否用于训练模型。尽管在分类模型上进行了广泛研究，但其对时间序列预测的影响仍然未被充分探索。我们通过引入两种新攻击来填补这一空白：（i）将多变量LiRA的适应版本（最先进的MIA，最初为分类模型开发）应用于时间序列预测场景，以及（ii）一种称为深度时间序列（DTS）攻击的新型端到端学习方法。我们将这些方法与分类场景中其他领先攻击的适应版本进行基准测试。
我们在TUH-EEG和ELD数据集的现实环境中评估所有攻击，针对两种强大的预测架构，LSTM和最先进的N-HiTS，在记录级和用户级威胁模型下。我们的结果表明，预测模型存在脆弱性，用户级攻击通常实现完美检测。所提出的方法在多个设置中表现最强，为时间序列预测中的隐私风险评估建立了新的基准。此外，随着预测时间范围的延长和训练人群的减少，脆弱性增加，反映了在大型语言模型中观察到的趋势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in understanding membership inference attacks (MIAs) in time series forecasting, a topic that has been extensively studied in classification models. The authors introduce two new attack methods: an adaptation of the multivariate LiRA attack for time series and a novel end-to-end learning approach called Deep Time Series (DTS) attack. Evaluating these methods on the TUH-EEG and ELD datasets with LSTM and N-HiTS architectures, the findings reveal that forecasting models are significantly vulnerable, with user-level attacks achieving perfect detection in many cases, and the vulnerability increasing with longer prediction horizons and smaller training populations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了时间序列预测中的隐私风险，特别关注于成员推断攻击（MIA），该攻击在分类模型中得到了广泛研究，但在预测中尚未得到充分探讨。作者提出了两种新型攻击方法：一种是将多变量LiRA攻击适应于时间序列，另一种是名为深度时间序列（DTS）攻击的新端到端学习方法。对TUH-EEG和ELD数据集的实验评估表明，预测模型，特别是LSTM和N-HiTS架构，对这些攻击高度脆弱，用户级攻击的检测率达到完美，且脆弱性随着预测时间的延长和训练样本的减少而增加，从而为该领域的隐私风险评估建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">On the implicit regularization of Langevin dynamics with projected noise</div>
<div class="meta-line">Authors: Govind Menon, Austin J. Stromme, Adrien Vacher</div>
<div class="meta-line">First: 2026-02-12T18:45:42+00:00 · Latest: 2026-02-12T18:45:42+00:00</div>
<div class="meta-line">Comments: 30 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12257v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12257v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于带投影噪声的朗之万动力学的隐式正则化</div>
<div class="mono" style="margin-top:8px">我们研究了带有投影噪声的朗之万动力学，该噪声投影到与等距群作用正交的方向上。引入这一数学模型旨在揭示对称性对过参数化模型的随机梯度下降的影响。我们的主要结果识别出一种新型的隐式正则化形式：当初始密度和目标密度在群作用下均不变时，带投影噪声的朗之万动力学在法律上等价于具有各向同性扩散的朗之万动力学，但具有一个额外的漂移项，该漂移项与群轨道的负对数体积成正比。我们通过构造一个通过群本身的第三个过程来耦合这两个过程，证明了这一结果，并将额外的漂移识别为轨道的平均曲率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the effects of symmetry on stochastic gradient descent in over-parametrized models through the lens of Langevin dynamics with projected noise. The authors introduce a mathematical model that reveals a new form of implicit regularization, demonstrating that when both the initial and target densities are invariant under a group action, the Langevin dynamics with projected noise behaves similarly to Langevin dynamics with isotropic diffusion, but includes an additional drift term related to the negative log volume of the group orbit. This finding is established by constructing a coupling between the two processes using a third process on the group, highlighting the additional drift as the mean curvature of the orbits.</div>
<div class="mono" style="margin-top:8px">本研究通过投影噪声的朗之万动力学探讨对称性对过参数化模型随机梯度下降的影响。作者引入了一种数学模型，分析正交于等距群作用方向的噪声如何影响动力学。主要发现揭示了一种新的隐式正则化形式，表明当初始密度和目标密度在群作用下均保持不变时，带有投影噪声的朗之万动力学在法律上等同于各向同性扩散，但包含一个与群轨道的负对数体积相关的附加漂移项，该结果是通过在群上构建过程的耦合得出的。</div>
</details>
</div>
<div class="card">
<div class="title">Is Online Linear Optimization Sufficient for Strategic Robustness?</div>
<div class="meta-line">Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</div>
<div class="meta-line">First: 2026-02-12T18:41:55+00:00 · Latest: 2026-02-12T18:41:55+00:00</div>
<div class="meta-line">Comments: 26 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#x27;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.
  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\sqrt{T (\log K+\log(T/δ)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线线性优化是否足以实现战略鲁棒性？</div>
<div class="mono" style="margin-top:8px">我们考虑在重复的贝叶斯第一价格拍卖中进行竞标。实现最优遗憾的竞标算法已被广泛研究，但其对卖方操控的战略鲁棒性仍然相对未被深入探讨。基于无交换遗憾算法的竞标算法同时具备理想特性，但在统计和计算效率方面是次优的。相比之下，在线梯度上升是唯一一个实现$O(\sqrt{TK})$遗憾和战略鲁棒性的算法[KSS24]，其中$T$表示拍卖次数，$K$表示竞标次数。
在本文中，我们探讨简单的在线线性优化（OLO）算法是否足以用于同时具备理想特性的竞标算法。我们的主要结果表明，次线性线性化遗憾足以实现战略鲁棒性。具体而言，我们构建了简单的黑箱归约，将任何OLO算法转换为在已知和未知价值分布设置下的战略鲁棒无遗憾竞标算法。对于已知价值分布的情况，我们的归约产生了一个实现$O(\sqrt{T \log K})$遗憾和战略鲁棒性的竞标算法（在$K$依赖性上相比[KSS24]有指数改善）。对于未知价值分布的情况，我们的归约提供了一个具有高概率$O(\sqrt{T (\log K+\log(T/δ)})$遗憾和战略鲁棒性的竞标算法，同时去除了[KSS24]中做出的有界密度假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the effectiveness of online linear optimization (OLO) algorithms in achieving strategic robustness in bidding for repeated Bayesian first-price auctions, an area that has seen limited exploration regarding the seller&#x27;s manipulation. The authors propose a method that constructs black-box reductions to convert any OLO algorithm into a no-regret bidding algorithm that maintains strategic robustness, addressing both known and unknown value distributions. The key findings indicate that sublinear linearized regret is sufficient for strategic robustness, resulting in a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret for known distributions and $O(\sqrt{T (\log K + \log(T/δ))})$ regret for unknown distributions, significantly improving upon previous work in terms of K-dependence and removing certain assumptions.</div>
<div class="mono" style="margin-top:8px">本研究解决了在重复的贝叶斯第一价格拍卖中，竞标算法在卖方操控下的战略稳健性理解的不足。作者探讨了在线线性优化（OLO）算法是否能够同时提供最佳的遗憾和战略稳健性。研究结果表明，次线性线性化遗憾足以实现战略稳健性，进而开发了黑箱还原方法，将任何OLO算法转化为无遗憾竞标算法，在已知和未知价值分布场景中，相较于之前的研究，改善了遗憾界限。</div>
</details>
</div>
<div class="card">
<div class="title">EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph</div>
<div class="meta-line">Authors: Nan Jiang, Ziyi Wang, Yexiang Xue</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-08T04:39:11+00:00 · Latest: 2026-02-12T18:38:11+00:00</div>
<div class="meta-line">Comments: Camera-ready version accepted for ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05849v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05849v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nan-jiang-group.github.io/egg-sr">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the search space and accelerating training lies in *symbolic equivalence*: many expressions, although syntactically different, define the same function -- for example, $\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates symbolic equivalence into a class of modern symbolic regression methods, including Monte Carlo Tree Search (MCTS), Deep Reinforcement Learning (DRL), and Large Language Models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module (via equality graphs), accelerating learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalent generated sequences in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Theoretically, we show the benefit of embedding EGG into learning: it tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances a class of symbolic regression models across several benchmarks, discovering more accurate expressions within the same time limit. Project page is at: https://nan-jiang-group.github.io/egg-sr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EGG-SR：通过等式图将符号等价嵌入符号回归</div>
<div class="mono" style="margin-top:8px">符号回归旨在通过搜索封闭形式表达式从实验数据中揭示物理定律，这是人工智能驱动的科学发现中的一项重要任务。然而，表达式搜索空间的指数增长使得这一任务在计算上具有挑战性。一个有前景但尚未深入探索的方向是*符号等价*：许多表达式虽然在语法上不同，但定义相同的函数——例如，$\log(x_1^2x_2^3)$、$\log(x_1^2)+\log(x_2^3)$和$2\log(x_1)+3\log(x_2)$。现有算法将这些变体视为不同的输出，导致冗余探索和学习缓慢。我们提出EGG-SR，这是一个统一框架，将符号等价集成到一类现代符号回归方法中，包括蒙特卡洛树搜索（MCTS）、深度强化学习（DRL）和大型语言模型（LLMs）。EGG-SR通过所提出的EGG模块（通过等式图）紧凑地表示等价表达式，通过以下方式加速学习：（1）在EGG-MCTS中修剪冗余子树探索，（2）在EGG-DRL中聚合等价生成序列的奖励，以及（3）在EGG-LLM中丰富反馈提示。从理论上讲，我们展示了将EGG嵌入学习的好处：它收紧了MCTS的遗憾界限，并减少了DRL梯度估计器的方差。从经验上看，EGG-SR在多个基准测试中持续增强了一类符号回归模型，在相同的时间限制内发现更准确的表达式。项目页面在：https://nan-jiang-group.github.io/egg-sr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of symbolic regression, which aims to derive physical laws from experimental data but faces challenges due to the exponential growth of the search space for expressions. The authors propose EGG-SR, a framework that incorporates symbolic equivalence into various symbolic regression methods, including Monte Carlo Tree Search, Deep Reinforcement Learning, and Large Language Models, by using equality graphs to represent equivalent expressions. Experimental results demonstrate that EGG-SR significantly enhances the performance of symbolic regression models, allowing for the discovery of more accurate expressions within the same time constraints compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高符号回归的效率，符号回归旨在从实验数据中推导物理定律，但由于表达式搜索空间的指数增长而面临挑战。作者提出了EGG-SR框架，将符号等价性融入多种符号回归方法，如蒙特卡洛树搜索、深度强化学习和大型语言模型，使用等式图模块表示等价表达式。实验结果表明，EGG-SR显著提升了符号回归模型的性能，使其能够在相同的计算时间限制内发现更准确的表达式。</div>
</details>
</div>
<div class="card">
<div class="title">A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</div>
<div class="meta-line">Authors: Ralph Krüger</div>
<div class="meta-line">First: 2026-02-12T18:37:23+00:00 · Latest: 2026-02-12T18:37:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 1 figure, EAMT 2026, TAITT Workshop</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12251v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向语言的人工智能在翻译和专业交流中的技术课程</div>
<div class="mono" style="margin-top:8px">本文提出了一个面向语言的人工智能（AI）在语言和翻译（L&amp;T）行业中的技术课程。该课程旨在通过以易于理解的方式让利益相关者接触现代面向语言的AI的概念和技术/算法基础，培养翻译和专业交流领域的特定领域技术AI素养。核心课程重点包括1）向量嵌入，2）神经网络的技术基础，3）分词和4）变换器神经网络。它旨在帮助用户发展计算思维、算法意识和算法能力，最终促进他们在AI驱动的工作环境中的数字韧性。该课程的教学适宜性在科隆应用科技大学翻译与多语言交流研究所的一个以AI为重点的硕士课程中进行了测试。结果表明该课程的教学有效性，但参与者反馈表明应将其嵌入更高层次的教学支架中，例如以讲师支持的形式，以便实现最佳学习条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance technical AI literacy among stakeholders in the translation and specialized communication sectors. The authors developed a curriculum that covers key concepts such as vector embeddings, neural networks, tokenization, and transformer neural networks, aiming to improve computational thinking and algorithmic awareness. Experimental results from an AI-focused MA course indicate that while the curriculum is didactically effective, participant feedback suggests the need for additional support from lecturers to optimize learning outcomes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提供一个结构化的课程来提高翻译和专业沟通领域利益相关者的技术人工智能素养，该课程专注于语言导向的人工智能。主要方法是开发一个核心课程，涵盖向量嵌入、神经网络、分词和变换器神经网络，并在科隆应用技术大学的翻译与多语言沟通研究所的人工智能硕士课程中实施。主要实验结果表明，尽管该课程在教学上有效，但参与者反馈建议需要额外的高级支持以优化学习条件。</div>
</details>
</div>
<div class="card">
<div class="title">Community Concealment from Unsupervised Graph Learning-Based Clustering</div>
<div class="meta-line">Authors: Dalyapraz Manatova, Pablo Moriano, L. Jean Camp</div>
<div class="meta-line">First: 2026-02-12T18:36:19+00:00 · Latest: 2026-02-12T18:36:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12250v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无监督图学习的聚类中的社区隐匿</div>
<div class="mono" style="margin-top:8px">图神经网络（GNN）旨在利用属性图学习表示。这些表示在无监督聚类学习和社区检测中具有重要意义。然而，这种推断可能会揭示敏感群体、聚类系统或集体行为，引发对群体隐私的担忧。例如，社交和关键基础设施网络中的社区归属可能暴露协调的资产群体、操作层级和系统依赖关系，这些信息可能被用于画像或情报收集。我们研究了一种防御设置，其中数据发布者（防御者）试图在网络中进行有限的、关注效用的更改，以隐匿感兴趣的社区。我们的分析表明，社区隐匿受到两个可量化因素的强烈影响：社区边界的连通性和受保护社区与相邻社区之间的特征相似性。基于这些发现，我们提出了一种扰动策略，通过重新连接一组选定的边并修改节点特征，以减少GNN消息传递所利用的独特性。我们在合成基准和真实网络图上的实验表明，所提方法在相同扰动预算下优于DICE。总体而言，它在评估设置中实现了约20-45%的中位相对隐匿改进。这些发现展示了一种针对基于GNN的社区学习的缓解策略，并突显了图学习中固有的群体隐私风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the privacy concerns associated with community detection in graph neural networks (GNNs), particularly how such methods can expose sensitive group information. The authors propose a defensive strategy where a data publisher aims to conceal a community of interest by making limited changes to the network structure and node features. Experimental results show that their perturbation strategy significantly improves community concealment, achieving median relative concealment enhancements of approximately 20-45% compared to existing methods like DICE, thereby highlighting the privacy risks inherent in GNN-based clustering approaches.</div>
<div class="mono" style="margin-top:8px">本研究关注图神经网络（GNN）中社区检测所带来的隐私问题，揭示敏感群体信息可能导致的剖析风险。作者提出了一种扰动策略，通过重连边缘和修改节点特征来隐蔽感兴趣的社区，同时保持网络的实用性。实验结果表明，该方法在相同扰动预算下，相较于现有的DICE方法，社区隐蔽性提高了约20-45%。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Sorry, I Didn&#x27;t Catch That&quot;: How Speech Models Miss What Matters Most</div>
<div class="meta-line">Authors: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou</div>
<div class="meta-line">First: 2026-02-12T18:36:09+00:00 · Latest: 2026-02-12T18:36:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12249v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;抱歉，我没听清&quot;: 语音模型如何忽视最重要的内容</div>
<div class="mono" style="margin-top:8px">尽管语音识别系统在标准基准测试中实现了低字错误率，但在实际应用中，它们常常在短小且高风险的发言中失败。在这里，我们研究了这一失败模式在一个高风险任务中的表现：转录美国街道名称，发言者为美国参与者。我们评估了来自OpenAI、Deepgram、Google和Microsoft的15个模型，基于来自语言多样性的美国发言者的录音，发现平均转录错误率为44%。我们通过地理位置量化失败转录的下游影响，并显示错误转录系统性地导致所有发言者的错误，但非英语母语发言者的路线距离错误是英语母语发言者的两倍。为了减轻这种损害，我们引入了一种合成数据生成方法，使用开源文本到语音模型生成命名实体的多样化发音。使用不到1000个合成样本进行微调，使非英语母语发言者的街道名称转录准确率提高了近60%（相对于基础模型）。我们的结果突显了基准性能与语音系统在现实世界中的可靠性之间的关键差距，并展示了一条简单、可扩展的路径，以减少高风险转录错误。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the significant shortcomings of speech recognition systems, particularly their failure to accurately transcribe high-stakes utterances like U.S. street names. The study evaluates 15 models from various companies on recordings from a linguistically diverse group of U.S. speakers, revealing an average transcription error rate of 44%. The findings indicate that mis-transcriptions disproportionately affect non-English primary speakers, with routing distance errors being twice as large compared to English speakers. To improve accuracy, the authors propose a synthetic data generation method that enhances model training with diverse pronunciations, resulting in nearly a 60% improvement in transcription accuracy for non-English primary speakers when fine-tuned with fewer than 1,000 synthetic samples.</div>
<div class="mono" style="margin-top:8px">本研究探讨了语音识别系统的不足之处，特别是在准确转录高风险语句（如美国街道名称）方面的失败。研究评估了来自主要公司的15个模型在多样化美国说话者的录音上的表现，发现平均转录错误率为44%。研究发现，非英语母语者的路线距离错误是英语母语者的两倍。为了解决这个问题，作者提出了一种合成数据生成方法来创建多样化的发音，证明使用不到1000个合成样本进行微调可以将非英语母语者的转录准确率提高近60%，从而突显了基准性能与语音系统实际应用之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction</div>
<div class="meta-line">Authors: Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen</div>
<div class="meta-line">First: 2026-02-12T18:31:37+00:00 · Latest: 2026-02-12T18:31:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12247v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12247v1">PDF</a> · <a href="https://github.com/ContextualAI/extract-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExtractBench：复杂结构提取的基准和评估方法</div>
<div class="mono" style="margin-top:8px">非结构化文档如PDF包含有价值的结构化信息，但下游系统需要以可靠、标准化的格式提供这些数据。大型语言模型（LLMs）越来越多地被用于自动化这一提取过程，使得准确性和可靠性至关重要。然而，进展受到两个瓶颈的制约。首先，没有端到端的基准评估企业级架构广度下的PDF到JSON提取。其次，没有原则性的方法论捕捉嵌套提取的语义，其中字段要求不同的正确性概念（标识符的精确匹配、数量的容忍、名称的语义等价），数组需要对齐，遗漏必须与幻觉区分。我们通过ExtractBench解决这两个问题，这是一个开源的PDF到JSON结构提取基准和评估框架。该基准将35个PDF文档与JSON架构和人类注释的金标准标签配对，涵盖经济价值领域，产生了12,867个可评估字段，跨越从几十到几百个字段的架构复杂性。评估框架将架构视为可执行规范：每个字段声明其评分指标。基线评估显示，前沿模型（GPT-5/5.2、Gemini-3 Flash/Pro、Claude 4.5 Opus/Sonnet）在现实架构上仍然不可靠。随着架构广度的增加，性能急剧下降，在所有测试模型中，369字段的财务报告架构的有效输出为0%。我们在https://github.com/ContextualAI/extract-bench发布ExtractBench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the extraction of structured information from unstructured documents like PDFs, which is crucial for downstream systems that require reliable data formats. The authors developed ExtractBench, an open-source benchmark and evaluation framework specifically designed for PDF-to-JSON structured extraction, addressing the lack of comprehensive evaluation methods for complex schema extraction. Key experimental findings indicate that current leading models, such as GPT-5 and Gemini-3, show significant unreliability when handling realistic schemas, with performance deteriorating sharply as schema complexity increases, resulting in 0% valid output for a 369-field financial reporting schema across all tested models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善从非结构化文档（如PDF）中提取结构化信息的能力，这对需要标准化数据格式的下游系统至关重要。作者开发了ExtractBench，这是一个开源基准和评估框架，旨在评估PDF到JSON提取在广泛企业级模式下的表现。主要实验结果表明，当前领先模型（如GPT-5和Gemini-3）在面对复杂模式时存在显著的可靠性问题，尤其是在369字段的财务报告模式上，完全无法生成有效输出。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</div>
<div class="meta-line">Authors: Anthony Kobanda, Waris Radji</div>
<div class="meta-line">First: 2026-02-12T18:30:27+00:00 · Latest: 2026-02-12T18:30:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内在能量联合嵌入预测架构诱导准度量空间</div>
<div class="mono" style="margin-top:8px">联合嵌入预测架构（JEPAs）旨在通过从上下文嵌入预测目标嵌入来学习表示，在潜在空间中诱导标量兼容能量。相比之下，准度量强化学习（QRL）通过支持在不对称动态下达到目标的有向距离值（到达成本）研究目标条件控制。在这篇短文中，我们通过限制关注于一类原则性的JEPA能量函数：内在（最小作用）能量，将这些观点联系起来，内在能量被定义为在两个状态之间可接受轨迹上累积局部努力的下确界。在温和的闭合性和可加性假设下，任何内在能量都是准度量。在目标达成控制中，最优到达成本函数恰好具有这种内在形式；反之，训练以建模内在能量的JEPAs位于QRL所针对的准度量值类中。此外，我们观察到对称有限能量在单向可达性上结构不匹配，这促使在方向性重要时使用不对称（准度量）能量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the connection between Joint-Embedding Predictive Architectures (JEPAs) and Quasimetric Reinforcement Learning (QRL), motivated by the need to understand goal-conditioned control in asymmetric dynamics. The authors propose a principled class of JEPA energy functions, specifically intrinsic energies, which are defined as the infima of accumulated local effort over admissible trajectories. The key findings reveal that under certain assumptions, intrinsic energies can be classified as quasimetric, and that optimal cost-to-go functions in goal-reaching control align with this intrinsic form, indicating that JEPAs trained on intrinsic energies correspond to the quasimetric value class in QRL, while highlighting the limitations of symmetric finite energies in one-way reachability scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于将联合嵌入预测架构（JEPA）与准度量强化学习（QRL）联系起来，以增强在不对称动态下的目标条件控制。作者提出了一种方法，专注于内在能量，定义为可接受轨迹上累积局部努力的下确界，在某些假设下被证明是准度量的。主要发现表明，目标达成控制中的最优成本函数与内在能量形式一致，而训练以建模这些能量的JEPA适合于QRL的准度量值类，强调了在方向性重要的场景中不对称能量的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Autonomous Mathematics Research</div>
<div class="meta-line">Authors: Tony Feng, Trieu H. Trinh, Garrett Bingham, Dawsen Hwang, Yuri Chervonyi, Junehyuk Jung, Joonkyung Lee, Carlo Pagano, Sang-hyun Kim, Federico Pasqualotto, Sergei Gukov, Jonathan N. Lee, Junsu Kim, Kaiying Hou, Golnaz Ghiasi, Yi Tay, YaGuang Li, Chenkai Kuang, Yuan Liu, Hanzhao Lin, Evan Zheran Liu, Nigamaa Nayakanti, Xiaomeng Yang, Heng-Tze Cheng, Demis Hassabis, Koray Kavukcuoglu, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-02-10T18:50:15+00:00 · Latest: 2026-02-12T18:27:29+00:00</div>
<div class="meta-line">Comments: 35 pages. Accompanied blog post https://deepmind.google/blog/accelerating-mathematical-and-scientific-discovery-with-gemini-deep-think/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10177v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10177v2">PDF</a> · <a href="https://github.com/google-deepmind/superhuman/tree/main/aletheia">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom&#x27;s Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest quantifying standard levels of autonomy and novelty of AI-assisted results, as well as propose a novel concept of human-AI interaction cards for transparency. We conclude with reflections on human-AI collaboration in mathematics and share all prompts as well as model outputs at https://github.com/google-deepmind/superhuman/tree/main/aletheia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自主数学研究</div>
<div class="mono" style="margin-top:8px">最近基础模型的进展产生了能够在国际数学奥林匹克中达到金牌标准的推理系统。然而，从竞赛级问题解决转向专业研究需要浏览大量文献并构建长期证明。在这项工作中，我们介绍了Aletheia，一个数学研究代理，能够端到端地以自然语言迭代生成、验证和修订解决方案。具体而言，Aletheia由一个高级版本的Gemini Deep Think驱动，适用于具有挑战性的推理问题，采用了一种超越奥林匹克级问题的新颖推理时间扩展法则，并通过密集的工具使用来应对数学研究的复杂性。我们展示了Aletheia从奥林匹克问题到博士级练习的能力，特别是通过几个在AI辅助数学研究中的独特里程碑：(a) 一篇由AI生成的研究论文（Feng26），在计算算术几何中称为特征权重的某些结构常数时没有任何人类干预；(b) 一篇研究论文（LeeSeo26），展示了人类与AI合作证明称为独立集的相互作用粒子系统的界限；(c) 对布隆的厄尔德什猜想数据库中700个开放问题的广泛半自主评估（Feng等，2026a），包括对四个开放问题的自主解决方案。为了帮助公众更好地理解与AI和数学相关的发展，我们建议量化AI辅助结果的标准自主性和新颖性水平，并提出一种新的透明度人类-AI互动卡片概念。我们最后反思了人类与AI在数学中的合作，并在https://github.com/google-deepmind/superhuman/tree/main/aletheia分享所有提示和模型输出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to bridge the gap between competition-level mathematical problem-solving and professional research, which involves extensive literature navigation and long-horizon proofs. The authors introduce Aletheia, a math research agent that utilizes an advanced version of Gemini Deep Think to iteratively generate, verify, and revise mathematical solutions in natural language. Key findings include the generation of a research paper on eigenweights without human intervention, a collaborative paper on independent sets, and an evaluation of 700 open problems, with autonomous solutions to four questions, demonstrating significant advancements in AI-assisted mathematics research and proposing new frameworks for understanding human-AI interactions in this field.</div>
<div class="mono" style="margin-top:8px">本研究的动机是推动人工智能在数学领域的能力，从竞赛级问题解决转向专业研究，这涉及到广泛文献的导航和复杂证明的构建。作者介绍了Aletheia，一个数学研究代理，利用高级版本的Gemini Deep Think以自然语言迭代生成、验证和修订解决方案。主要发现包括在没有人类干预的情况下生成关于特征权重的研究论文、关于独立集的协作论文，以及对700个开放问题的评估，其中对四个问题提供了自主解决方案，突显了人工智能在数学研究中的重要贡献潜力以及人机交互透明度的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</div>
<div class="meta-line">Authors: Manjunath Kudlur, Evan King, James Wang, Pete Warden</div>
<div class="meta-line">First: 2026-02-12T18:20:45+00:00 · Latest: 2026-02-12T18:20:45+00:00</div>
<div class="meta-line">Comments: 7 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12241v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent &quot;encode-the-whole-utterance&quot; latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Moonshine v2：用于延迟关键语音应用的遍历流编码器ASR</div>
<div class="mono" style="margin-top:8px">延迟关键的语音应用（例如，实时转录、语音命令和实时翻译）需要低的首次令牌时间（TTFT）和高的转录准确性，特别是在资源受限的边缘设备上。全注意力Transformer编码器仍然是自动语音识别（ASR）的强准确性基准，因为每一帧可以直接关注每一其他帧，这利用远程词汇上下文解决了局部模糊的声学问题。然而，这种全局依赖在序列长度上引入了二次复杂性，导致固有的“编码整个话语”延迟特征。对于流式用例，这导致TTFT随着话语长度线性增长，因为编码器必须处理整个前缀才能发出任何解码器令牌。为了更好地满足设备上流式ASR用例的需求，我们引入了Moonshine v2，这是一种遍历流编码器ASR模型，采用滑动窗口自注意力实现有界、低延迟推理，同时保留强大的局部上下文。我们的模型在标准基准测试中实现了最先进的字错误率，达到与其大小6倍的模型相当的准确性，同时运行速度显著更快。这些结果表明，精心设计的局部注意力在大小和延迟成本的很小一部分中与全注意力的准确性具有竞争力，为边缘设备上的交互式语音接口开辟了新的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for low-latency and high-accuracy automatic speech recognition (ASR) in latency-critical applications, particularly on resource-constrained edge devices. The authors introduce Moonshine v2, an ergodic streaming-encoder ASR model that utilizes sliding-window self-attention to reduce time-to-first-token while maintaining strong local context. Experimental results show that Moonshine v2 achieves state-of-the-art word error rates on standard benchmarks, matching the accuracy of larger models while operating significantly faster, indicating that local attention can effectively compete with full attention methods in terms of performance and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在需要快速响应和高转录准确性的语音应用中的延迟问题，特别是在资源有限的边缘设备上。作者提出了Moonshine v2，这是一种采用滑动窗口自注意力的遍历流式编码自动语音识别（ASR）模型，旨在减少延迟，同时保持强大的局部上下文。实验结果表明，Moonshine v2在标准基准测试中实现了最先进的字错误率，准确性与更大模型相当，同时运行速度显著更快，从而证明局部注意力在准确性和效率方面可以有效与全注意力竞争。</div>
</details>
</div>
<div class="card">
<div class="title">Hyperparameter Transfer with Mixture-of-Expert Layers</div>
<div class="meta-line">Authors: Tianze Jiang, Blake Bordelon, Cengiz Pehlevan, Boris Hanin</div>
<div class="meta-line">First: 2026-01-28T03:02:30+00:00 · Latest: 2026-02-12T18:19:47+00:00</div>
<div class="meta-line">Comments: 25 Pages, 18 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20205v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20205v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合专家层的超参数转移</div>
<div class="mono" style="margin-top:8px">混合专家（MoE）层已成为扩展现代神经网络的重要工具，通过将总可训练参数与每个标记在前向传播中激活的参数解耦。然而，稀疏的MoE增加了训练的复杂性，原因有：(i) 新的可训练参数（路由权重），与所有其他参数组一样，需要超参数（HP）调优；(ii) 必须选择的新架构规模维度（专家的数量和大小），并可能需要增大。为了使超参数选择便宜且可靠，我们提出了一种新的参数化方法，用于具有MoE层的变换器模型，在扩展模型宽度、深度、专家数量和专家（隐藏）大小时。我们的参数化方法通过一种新颖的动态平均场理论（DMFT）分析得到了证明。当在固定的标记预算下变化不同的模型维度时，我们实证发现我们的参数化方法能够在从5100万到超过20亿总参数的模型之间可靠地转移超参数。我们进一步利用在短标记范围内对小模型进行扫描识别的超参数，来训练在更长范围内的大模型，并报告了良好的模型表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to simplify hyperparameter tuning for Mixture-of-Experts (MoE) layers in neural networks, which complicate training due to the introduction of new parameters and architecture scales. The authors propose a novel parameterization for transformer models incorporating MoE layers, supported by a dynamical mean-field theory analysis. Experimental results demonstrate that this parameterization allows for effective hyperparameter transfer across models ranging from 51M to over 2B parameters, enabling the use of hyperparameters identified from smaller models to train larger models efficiently.</div>
<div class="mono" style="margin-top:8px">本研究解决了混合专家（MoE）层中超参数调优的挑战，由于引入了新参数和架构维度，训练变得复杂。作者提出了一种新的参数化方法，用于具有MoE层的变换器模型，并通过动态平均场理论分析支持该方法，以便在扩展模型维度时促进高效的超参数选择。实验结果表明，该参数化方法使得在51M到超过2B参数的模型之间实现可靠的超参数转移成为可能，从而能够有效地利用在短时间内训练的小模型中识别出的超参数来训练更大的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Olmix: A Framework for Data Mixing Throughout LM Development</div>
<div class="meta-line">Authors: Mayee F. Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher Ré, Luca Soldaini, Kyle Lo</div>
<div class="meta-line">First: 2026-02-12T18:16:05+00:00 · Latest: 2026-02-12T18:16:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12237v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Olmix：语言模型开发中的数据混合框架</div>
<div class="mono" style="margin-top:8px">数据混合——确定来自不同领域的数据比例——是训练语言模型（LM）的首要问题。虽然现有的混合方法显示出潜力，但在实际的LM开发中应用时却不尽如人意。我们提出了Olmix，一个解决两个此类挑战的框架。首先，开发混合方法的配置空间尚不清楚——现有方法的设计选择缺乏合理性或共识，并忽视了数据约束等实际问题。我们对这一空间进行了全面的实证研究，识别出哪些设计选择能导致强大的混合方法。其次，在实践中，随着数据集的添加、删除、划分和修订，领域集在LM开发过程中不断演变——这一问题在现有工作中基本未得到解决，现有工作假设领域是固定的。我们研究了如何在领域集更新后高效地重新计算混合，利用过去混合的信息。我们引入了混合重用机制，该机制重用现有比例，仅对受更新影响的领域重新计算比例。在模拟真实世界LM开发的五次领域集更新中，混合重用在计算量减少74%的情况下，匹配了每次更新后完全重新计算混合的性能，并在下游任务中比不混合训练提高了11.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve data mixing methods for training language models, which are crucial yet poorly understood in real-world applications. The authors introduce Olmix, a framework that systematically explores the configuration space of mixing methods and addresses the evolving nature of domain sets during model development. Their key findings demonstrate that the mixture reuse mechanism can efficiently recompute data ratios after domain updates, achieving the same performance as full recomputation while reducing computational costs by 74% and enhancing downstream task performance by 11.6% compared to training without mixing.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善用于训练语言模型的数据混合方法，这在现实场景中至关重要但尚未得到充分解决。作者提出了Olmix框架，系统地探索混合方法的配置空间，并通过实证研究识别有效的设计选择。主要发现表明，他们提出的混合重用机制能够在领域集更新后高效地重新计算数据比例，达到与完全重新计算相当的性能，同时将计算成本降低74%，并在与不混合训练相比的下游任务中提高了11.6%的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</div>
<div class="meta-line">Authors: Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia</div>
<div class="meta-line">First: 2026-02-12T18:15:32+00:00 · Latest: 2026-02-12T18:15:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向神经形态视觉的脉冲神经网络持续学习的能量感知脉冲预算</div>
<div class="mono" style="margin-top:8px">基于脉冲神经网络（SNN）的神经形态视觉系统为事件驱动和帧驱动相机提供超低功耗感知，但灾难性遗忘仍然是持续演变环境中部署的关键障碍。现有的持续学习方法主要为人工神经网络开发，鲜有同时优化准确性和能量效率的研究，尤其在事件驱动数据集上的探索非常有限。我们提出了一种能量感知脉冲预算框架，用于持续SNN学习，集成了经验重放、可学习的漏积分脉冲神经元参数和自适应脉冲调度器，以在训练过程中强制执行特定数据集的能量约束。我们的方法表现出模态依赖性：在帧驱动数据集（MNIST, CIFAR-10）上，脉冲预算作为稀疏性诱导正则化器，提升准确性，同时将脉冲率降低多达47%；在事件驱动数据集（DVS-Gesture, N-MNIST, CIFAR-10-DVS）上，受控预算放松使准确性提高最多17.45个百分点，且计算开销最小。在涵盖两种模态的五个基准测试中，我们的方法在最小化动态功耗的同时，展现出一致的性能提升，推动了神经形态视觉系统中持续学习的实际可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of catastrophic forgetting in spiking neural networks (SNNs) used for neuromorphic vision systems, which is critical for their deployment in dynamic environments. The authors propose an energy-aware spike budgeting framework that combines experience replay, adjustable neuron parameters, and an adaptive spike scheduler to meet energy constraints during continual learning. Experimental results show that on frame-based datasets, the method improves accuracy while reducing spike rates by up to 47%, and on event-based datasets, it achieves accuracy gains of up to 17.45 percentage points with minimal computational overhead, demonstrating enhanced performance and reduced power consumption across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决用于神经形态视觉系统的脉冲神经网络（SNNs）中的灾难性遗忘问题，这对于在动态环境中部署至关重要。作者提出了一种能量感知的脉冲预算框架，结合了经验重放、可调节的神经元参数和自适应脉冲调度器，以管理训练过程中的能量约束。实验结果表明，在基于帧的数据集上，该方法在提高准确率的同时将脉冲率降低了多达47%，而在基于事件的数据集上，准确率提高了最多17.45个百分点，且计算开销最小，展示了在多个基准测试中性能的提升和功耗的降低。</div>
</details>
</div>
<div class="card">
<div class="title">Categorical Flow Maps</div>
<div class="meta-line">Authors: Daan Roos, Oscar Davis, Floor Eijkelboom, Michael Bronstein, Max Welling, İsmail İlkan Ceylan, Luca Ambrogioni, Jan-Willem van de Meent</div>
<div class="meta-line">First: 2026-02-12T18:10:46+00:00 · Latest: 2026-02-12T18:10:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12233v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分类流图</div>
<div class="mono" style="margin-top:8px">我们介绍了分类流图，这是一种通过自蒸馏加速生成分类数据的流匹配方法。基于最近的流匹配变分公式和在扩散及基于流的模型中加速推理的更广泛趋势，我们定义了一种流图，朝向单纯形传输概率质量到预测的终点，从而产生一种自然约束模型预测的参数化。由于我们的轨迹是连续的而非离散的，分类流图可以使用现有的蒸馏技术进行训练，以及基于终点一致性的新目标。这种连续的公式还自动解锁了测试时推理：我们可以直接重用现有的引导和重加权技术，在分类设置中引导采样朝向下游目标。在实证上，我们在图像、分子图和文本上实现了最先进的少步结果，即使在单步生成中也表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the generation of categorical data through an efficient flow-matching method called Categorical Flow Maps, leveraging self-distillation techniques. The authors propose a continuous flow map that transports probability mass towards a predicted endpoint, allowing for a parametrization that constrains model predictions effectively. Experimental results demonstrate that this method achieves state-of-the-art performance in few-step generation across various domains, including images, molecular graphs, and text, with notable success even in single-step generation scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过一种称为分类流图的高效流匹配方法来增强分类数据的生成，利用自蒸馏技术。作者提出了一种流图，能够将概率质量引导至预测的终点，采用连续轨迹的方法，使其能够与现有的蒸馏方法和新的终点一致性目标进行训练。实验结果表明，该方法在图像、分子图和文本等多个领域的少步生成中达到了最先进的性能，甚至在单步生成场景中也表现出显著的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating LLM Reasoning Beyond Correctness and CoT</div>
<div class="meta-line">Authors: Soheil Abbasloo</div>
<div class="meta-line">First: 2025-10-20T22:08:59+00:00 · Latest: 2026-02-12T18:07:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18134v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18134v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">What does it truly mean for a language model to &quot;reason&quot;? Current evaluations reward models&#x27; correct standalone answers-but correctness alone reveals little about the process that produced them. We argue that reasoning should be understood not as a static chain of steps but as a dynamic trajectory in which ideas interact, clash, and evolve into integrated insights. Building on the philosophical tradition of dialectics, we introduce SIEV, a structured evaluation framework that assesses reasoning through explicit thesis-antithesis-synthesis interactions. SIEV produces interpretable trajectories that highlight key properties of reasoning-robustness to challenge, adaptability under conflict, and synthesis across competing viewpoints-dimensions that conventional correctness-based metrics cannot capture. Empirical results on GSM and MMLU demonstrate substantial gaps in the reasoning abilities of state-of-the-art models: for example, GPT-5-chat loses more than 40 points (out of 100) on GSM when evaluated through SIEV&#x27;s process-oriented lens. By shifting focus from what answer a model gives to how it arrives there, SIEV enables a more transparent and principled distinction between structured reasoning and surface-level pattern generation offering a clearer foundation for assessing and understanding the reasoning capabilities of LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越正确性和链式推理评估LLM推理</div>
<div class="mono" style="margin-top:8px">语言模型的“推理”究竟意味着什么？当前的评估奖励模型的正确独立答案，但仅仅正确性并不能揭示产生这些答案的过程。我们认为，推理应被理解为一个动态轨迹，其中思想相互作用、冲突并演变为综合见解，而不是静态的步骤链。基于辩证法的哲学传统，我们引入了SIEV，一个结构化评估框架，通过明确的论点-反论点-综合互动来评估推理。SIEV产生可解释的轨迹，突出推理的关键特性——对挑战的鲁棒性、在冲突下的适应性以及跨竞争观点的综合——这些维度是传统基于正确性的度量无法捕捉的。对GSM和MMLU的实证结果显示，最先进模型的推理能力存在显著差距：例如，当通过SIEV的过程导向视角进行评估时，GPT-5-chat在GSM上损失超过40分（满分100分）。通过将焦点从模型给出的答案转向其如何得出答案，SIEV使得结构化推理与表层模式生成之间的区别更加透明和原则化，为评估和理解LLM的推理能力提供了更清晰的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to redefine the evaluation of reasoning in language models beyond mere correctness, emphasizing the dynamic nature of reasoning as an evolving process. The authors introduce SIEV, a structured evaluation framework based on dialectical philosophy, which assesses reasoning through the interactions of thesis, antithesis, and synthesis. Experimental results on datasets like GSM and MMLU reveal significant deficiencies in the reasoning capabilities of advanced models, such as GPT-5-chat, which loses over 40 points when evaluated using the SIEV framework, highlighting the need for a more nuanced understanding of reasoning in language models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于重新定义语言模型推理的评估，不仅仅依赖于正确性，因为传统指标无法捕捉到推理过程的内在机制。作者提出了SIEV，一个基于辩证哲学的结构化评估框架，通过论题、对立和综合的互动来评估推理。对GSM和MMLU等数据集的实验证明，先进模型的推理能力存在显著不足，例如GPT-5-chat在使用SIEV评估时失去了超过40分，这突显了对语言模型推理更细致理解的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</div>
<div class="meta-line">Authors: Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li</div>
<div class="meta-line">First: 2026-02-12T18:06:03+00:00 · Latest: 2026-02-12T18:06:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12229v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12229v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越KL的扩散对齐：方差最小化作为有效的策略优化器</div>
<div class="mono" style="margin-top:8px">扩散对齐将预训练的扩散模型调整为沿去噪轨迹从奖励倾斜分布中采样。这个过程自然地接受了序列蒙特卡洛（SMC）解释，其中去噪模型充当提议，奖励引导产生重要性权重。基于这一观点，我们引入了方差最小化策略优化（VMPO），将扩散对齐表述为最小化对数重要性权重的方差，而不是直接优化基于Kullback-Leibler（KL）的目标。我们证明了方差目标由奖励倾斜的目标分布最小化，并且在策略采样下，其梯度与标准的基于KL的对齐梯度一致。这一视角为理解扩散对齐提供了一个共同的视角。在不同的潜在函数和方差最小化策略选择下，VMPO恢复了各种现有方法，同时也提出了超越KL的新设计方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve diffusion alignment in pretrained models for sampling from reward-tilted distributions. The authors introduce Variance Minimisation Policy Optimisation (VMPO), which reformulates diffusion alignment by minimizing the variance of log importance weights instead of using a Kullback-Leibler (KL) objective. Experimental results demonstrate that the variance objective aligns with the reward-tilted target distribution and that the gradient under on-policy sampling matches that of traditional KL-based methods, providing a unified framework for understanding diffusion alignment and suggesting new design directions beyond KL-based approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善预训练扩散模型中扩散对齐，以便从奖励倾斜分布中进行采样。作者提出了方差最小化策略优化（VMPO），该方法通过关注最小化对数重要性权重的方差来重新构造扩散对齐，而不是直接优化基于Kullback-Leibler（KL）的目标。主要实验结果表明，方差目标与奖励倾斜目标分布一致，并且在策略采样下，其梯度与传统的基于KL的对齐相匹配，为理解扩散对齐提供了统一框架，并提出了超越基于KL方法的新设计方向。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
