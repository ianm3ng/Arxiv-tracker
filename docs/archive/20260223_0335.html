<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-23 03:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260223_0335</div>
    <div class="row"><div class="card">
<div class="title">OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</div>
<div class="meta-line">Authors: Akashah Shabbir, Muhammad Umer Sheikh, Muhammad Akhtar Munir, Hiyam Debary, Mustansar Fiaz, Muhammad Zaigham Zaheer, Paolo Fraccaro, Fahad Shahbaz Khan, Muhammad Haris Khan, Xiao Xiang Zhu, Salman Khan</div>
<div class="meta-line">First: 2026-02-19T18:59:54+00:00 · Latest: 2026-02-19T18:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17665v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenEarthAgent：一个统一的工具增强地理空间代理框架</div>
<div class="mono" style="margin-top:8px">最近在多模态推理方面的进展使得代理能够解读图像，将其与语言连接，并执行结构化分析任务。将这些能力扩展到遥感领域仍然具有挑战性，因为模型必须在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步骤逻辑。为了解决这一问题，OpenEarthAgent引入了一个统一的框架，用于开发基于卫星图像、自然语言查询和详细推理轨迹训练的工具增强地理空间代理。训练管道依赖于对结构化推理轨迹的监督微调，使模型与各种分析上下文中的经过验证的多步骤工具交互对齐。伴随的语料库包括14,538个训练实例和1,169个评估实例，训练集中的推理步骤超过10万，评估集中的推理步骤超过7千。它涵盖城市、环境、灾害和基础设施领域，并结合GIS基础操作以及NDVI、NBR和NDBI等指数分析。基于明确的推理轨迹，学习到的代理展示了结构化推理、稳定的空间理解和可解释的行为，通过工具驱动的地理空间交互在多种条件下表现出色。我们报告了相对于强基线的一致性改进，以及与最近的开源和闭源模型相比的竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of agents in the remote sensing domain, particularly in interpreting satellite imagery and executing complex analytical tasks. The authors developed OpenEarthAgent, a unified framework for tool-augmented geospatial agents, which utilizes supervised fine-tuning on structured reasoning trajectories to align the model with verified multistep tool interactions. Key experimental findings indicate that the learned agent exhibits structured reasoning, stable spatial understanding, and interpretable behavior, achieving consistent improvements over a strong baseline and competitive performance against recent models across various analytical contexts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强遥感领域中代理的能力，特别是在解释卫星图像和执行复杂分析任务方面。作者开发了OpenEarthAgent，这是一个统一框架，通过对结构化推理轨迹进行监督微调来训练工具增强的地理空间代理。主要实验结果表明，该代理展示了改进的结构化推理、稳定的空间理解和可解释的行为，在涉及城市、环境、灾害和基础设施领域的各种分析上下，取得了相对于强基线的一致性能提升，并与其他模型相比表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Sink-Aware Pruning for Diffusion Language Models</div>
<div class="meta-line">Authors: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen</div>
<div class="meta-line">First: 2026-02-19T18:59:50+00:00 · Latest: 2026-02-19T18:59:50+00:00</div>
<div class="meta-line">Comments: Code at: https://github.com/VILA-Lab/Sink-Aware-Pruning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17664v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17664v1">PDF</a> · <a href="https://github.com/VILA-Lab/Sink-Aware-Pruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向汇聚的扩散语言模型剪枝</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）由于迭代去噪而产生高推理成本，这促使了高效剪枝的需求。现有的剪枝启发式方法主要继承自自回归（AR）LLMs，通常保留注意力汇聚标记，因为AR汇聚作为稳定的全局锚点。我们表明这一假设在DLMs中并不成立：注意力汇聚位置在整个生成轨迹中表现出显著更高的方差（通过主导汇聚位置在时间步之间的变化来衡量），表明汇聚通常是短暂的，并且在结构上不如AR模型中重要。基于这一观察，我们提出了${\bf \texttt{面向汇聚的剪枝}}$，该方法自动识别并剪除DLMs中的不稳定汇聚（之前的研究通常保留AR LLMs的汇聚）。在不重新训练的情况下，我们的方法实现了更好的质量效率权衡，并在匹配计算下超越了强大的先前剪枝基线。我们的代码可在https://github.com/VILA-Lab/Sink-Aware-Pruning获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high inference costs associated with Diffusion Language Models (DLMs) due to their iterative denoising process, leading to the need for efficient pruning methods. The authors propose a novel approach called Sink-Aware Pruning, which identifies and removes unstable attention sink tokens that are not structurally essential, contrasting with traditional methods that retain these tokens based on their importance in autoregressive models. Experimental results demonstrate that this method achieves a superior quality-efficiency trade-off compared to existing pruning techniques without requiring retraining, thus enhancing performance under similar computational constraints.</div>
<div class="mono" style="margin-top:8px">本研究针对扩散语言模型（DLMs）因迭代去噪过程而导致的高推理成本，提出了更高效的剪枝方法。作者质疑了通常基于自回归模型的保留注意力汇聚标记的传统做法，表明在DLMs中，这些汇聚标记往往是短暂的，并且在生成过程中表现出显著的方差。他们提出了一种名为汇聚标记感知剪枝的新方法，该方法能够有效识别并去除不稳定的汇聚标记，无需重新训练，从而实现了更好的质量效率权衡，并在相同计算条件下超越了现有的剪枝技术。</div>
</details>
</div>
<div class="card">
<div class="title">CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</div>
<div class="meta-line">Authors: Juri Opitz, Corina Raclé, Emanuela Boros, Andrianos Michail, Matteo Romanello, Maud Ehrmann, Simon Clematide</div>
<div class="meta-line">First: 2026-02-19T18:59:44+00:00 · Latest: 2026-02-19T18:59:44+00:00</div>
<div class="meta-line">Comments: ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at https://hipe-eval.github.io/HIPE-2026/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hipe-eval.github.io/HIPE-2026/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (&quot;Has the person ever been at this place?&quot;) and $isAt$ (&quot;Is the person located at this place around publication time?&quot;) - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLEF HIPE-2026：评估从多语言历史文本中准确高效地提取人-地关系</div>
<div class="mono" style="margin-top:8px">HIPE-2026是一个CLEF评估实验室，专注于从嘈杂的多语言历史文本中提取人-地关系。基于HIPE-2020和HIPE-2022活动，它将系列扩展到语义关系提取，目标是识别多种语言和时间段中的人-地关联。系统需要对两种类型的关系进行分类 - $at$（“这个人曾经在这个地方吗？”）和$isAt$（“这个人在出版时是否位于这个地方？”），这需要对时间和地理线索进行推理。该实验室引入了三重评估标准，联合评估准确性、计算效率和领域泛化。通过将关系提取与大规模历史数据处理相结合，HIPE-2026旨在支持知识图谱构建、历史传记重建和数字人文学科中的空间分析等下游应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind HIPE-2026 is to enhance the extraction of person-place relations from noisy, multilingual historical texts, building on previous campaigns. The main method involves classifying relations into two types, $at$ and $isAt$, which require reasoning about temporal and geographical contexts. Key experimental findings indicate that the evaluation framework assesses systems based on accuracy, computational efficiency, and domain generalization, thereby facilitating applications in knowledge-graph construction and historical analysis.</div>
<div class="mono" style="margin-top:8px">HIPE-2026的动机是提高从嘈杂的多语言历史文本中提取人-地点关系的能力，基于之前的HIPE活动。该方法涉及将关系分类为两种类型：$at$和$isAt$，这需要对时间和地理背景进行推理。主要发现表明，该评估框架有效地评估了准确性、计算效率和领域泛化，从而促进了知识图谱构建和历史分析等应用。</div>
</details>
</div>
<div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当视觉超越语言：评估和缓解视觉语言行动模型中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动模型（VLA）承诺将语言指令与机器人控制相结合，但在实践中往往无法忠实地遵循语言。当面临缺乏强场景特定监督的指令时，VLA会遭遇反事实失败：它们基于数据集偏差引发的视觉捷径进行操作，重复执行已学会的行为，并选择在训练中频繁看到的物体，而不考虑语言意图。为了系统地研究这一问题，我们引入了LIBERO-CF，这是第一个针对VLA的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令来评估语言跟随能力。我们的评估揭示了反事实失败在最先进的VLA中普遍存在但尚未被深入探讨。我们提出了反事实行动指导（CAG），这是一种简单而有效的双分支推理方案，明确地对VLA中的语言条件进行正则化。CAG将标准VLA策略与无语言条件的视觉-行动（VA）模块相结合，使得在行动选择过程中能够进行反事实比较。这一设计减少了对视觉捷径的依赖，提高了在观察不足任务上的鲁棒性，并且不需要额外的演示或对现有架构或预训练模型的修改。大量实验表明其在多种VLA中的即插即用集成和一致改进。例如，在LIBERO-CF上，CAG在语言跟随准确性上提高了9.7%的$π_{0.5}$，在观察不足任务上的任务成功率提高了3.6%，在与VA模型配对时，进一步提高了15.5%和8.5%。在现实世界评估中，CAG平均减少了9.4%的反事实失败，并提高了17.2%的任务成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the counterfactual failures in Vision-Language-Action models (VLAs), which often misinterpret language instructions due to biases in training data. The authors introduce LIBERO-CF, a benchmark designed to evaluate the language following capabilities of VLAs by presenting alternative instructions in visually plausible scenarios. They propose a dual-branch inference method called Counterfactual Action Guidance (CAG) that combines a standard VLA policy with a language-unconditioned Vision-Action module to mitigate reliance on visual shortcuts. Experimental results show that CAG significantly enhances language following accuracy by 9.7% and task success by 3.6% on under-observed tasks, with even greater improvements when paired with a VA model, and reduces counterfactual failures by 9.4% in real-world evaluations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉-语言-行动模型（VLA）中的反事实失败问题，这些模型常因训练数据中的偏见而误解语言指令。作者引入了LIBERO-CF，这是一个旨在通过在视觉上合理的场景中提供替代指令来评估VLA语言跟随能力的基准。研究提出了一种名为反事实行动指导（CAG）的方法，该方法集成了双分支推理方案，以增强语言条件，而无需额外的演示或对现有模型进行更改。实验结果表明，CAG在观察不足的任务上显著提高了语言跟随准确率9.7%和任务成功率3.6%，并且与视觉-行动模型结合时，改进幅度更大，同时在真实世界评估中将反事实失败减少了9.4%。</div>
</details>
</div>
<div class="card">
<div class="title">MARS: Margin-Aware Reward-Modeling with Self-Refinement</div>
<div class="meta-line">Authors: Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon</div>
<div class="meta-line">First: 2026-02-19T18:59:03+00:00 · Latest: 2026-02-19T18:59:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17658v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17658v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model&#x27;s estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARS：基于边际的自我优化奖励建模</div>
<div class="mono" style="margin-top:8px">奖励建模是现代对齐管道的核心组成部分，包括RLHF和RLAIF，支撑着包括PPO和TRPO在内的策略优化方法。然而，训练可靠的奖励模型在很大程度上依赖于人工标注的偏好数据，这些数据成本高且有限，因此激励了数据增强的使用。现有的增强方法通常在表示或语义层面上操作，并对奖励模型的估计难度保持无知。在本文中，我们提出了MARS，一种自适应的、基于边际的增强和采样策略，明确针对奖励模型的模糊和失败模式。我们提出的框架MARS将增强集中在低边际（模糊）偏好对上，在这些地方奖励模型最不确定，并通过困难样本增强迭代优化训练分布。我们提供了理论保证，表明该策略增加了损失函数的平均曲率，从而增强了信息并改善了条件，同时提供了实证结果，证明相较于均匀增强在稳健奖励建模上具有一致的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the reliability of reward models used in alignment pipelines, which are often limited by the availability of human-labeled preference data. The authors propose MARS, a margin-aware augmentation and sampling strategy that focuses on ambiguous and failure modes of reward models, enhancing training efficiency. Experimental results indicate that MARS significantly outperforms traditional uniform augmentation methods by increasing the average curvature of the loss function, leading to improved information gain and better conditioning for robust reward modeling.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高奖励模型的可靠性，这些模型在对齐管道中使用，严重依赖于昂贵的人类标记偏好数据。作者提出了MARS，这是一种自适应的边际感知增强和采样策略，专注于奖励模型的模糊和失败模式。实验结果表明，MARS提高了损失函数的平均曲率，从而改善了信息和条件，并在稳健的奖励建模中始终优于传统的均匀增强方法。</div>
</details>
</div>
<div class="card">
<div class="title">Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</div>
<div class="meta-line">Authors: Jiaqi Xi, Raghav Saboo, Luming Chen, Martin Wang, Sudeep Das</div>
<div class="meta-line">First: 2026-02-19T18:56:36+00:00 · Latest: 2026-02-19T18:56:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17654v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17654v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a two-stage &quot;Mine and Refine&quot; contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>挖掘与精炼：优化电子商务搜索检索中的分级相关性</div>
<div class="mono" style="margin-top:8px">我们提出了一种两阶段的“挖掘与精炼”对比训练框架，用于语义文本嵌入，以增强多类别电子商务搜索检索。大规模电子商务搜索需要能够适应长尾、嘈杂查询的嵌入，同时遵循与产品和政策约束兼容的可扩展监督。一个实际挑战是相关性通常是分级的：用户接受超出精确匹配的替代品或补充品，生产系统从这些相关性层次之间的相似性评分的清晰分离中受益，以实现稳定的混合和阈值处理。为了获得可扩展的政策一致监督，我们在三层相关性指导下对人类注释进行了轻量级LLM的微调，并通过参与驱动的审计进一步减少残余噪声。在第一阶段，我们训练了一个多语言的Siamese双塔检索器，采用标签感知的监督对比目标，塑造一个稳健的全球语义空间。在第二阶段，我们通过ANN挖掘困难样本，并使用与政策对齐的LLM重新注释它们，引入了一个多类扩展的圆形损失，明确地锐化相关性水平之间的相似性边界，以进一步精炼和丰富嵌入空间。通过附加的拼写增强和合成查询生成，稳健性得到了进一步提高。大量离线评估和生产A/B测试表明，我们的框架改善了检索相关性，并在参与度和商业影响上带来了统计显著的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enhancing multi-category e-commerce search retrieval by optimizing graded relevance in semantic text embeddings. The authors propose a two-stage &quot;Mine and Refine&quot; contrastive training framework, which includes training a multilingual Siamese two-tower retriever and refining embeddings through hard sample mining and re-annotation with a lightweight LLM. Key experimental results demonstrate that this framework significantly improves retrieval relevance and leads to notable increases in user engagement and business impact, as evidenced by extensive offline evaluations and production A/B tests.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过优化语义文本嵌入中的分级相关性来增强多类别电子商务搜索检索的效果。作者提出了一个两阶段的“挖掘与精炼”对比训练框架，第一阶段训练多语言的Siamese双塔检索器，第二阶段通过困难样本挖掘和用轻量级LLM重新标注来精炼嵌入。实验结果表明，该框架显著提高了检索相关性，并在用户参与度和商业影响上带来了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Round Human-AI Collaboration with User-Specified Requirements</div>
<div class="meta-line">Authors: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas</div>
<div class="meta-line">First: 2026-02-19T18:54:34+00:00 · Latest: 2026-02-19T18:54:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17646v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用户指定要求的多轮人机协作</div>
<div class="mono" style="margin-top:8px">随着人类越来越依赖多轮对话AI进行高风险决策，需要有原则的框架来确保这种互动可靠地提高决策质量。我们采用以人为中心的视角，遵循两个原则：反事实伤害，确保AI不削弱人类的优势；互补性，确保AI在人的易错领域增加价值。我们通过用户定义的规则形式化这些概念，允许用户准确指定其特定任务中伤害和互补性的含义。然后，我们引入了一种在线的、无分布假设的算法，具有有限样本保证，强制执行用户指定的协作动态约束。我们在两个互动设置中评估我们的框架：在医疗诊断任务上的LLM模拟协作和在图像推理任务上的人类众包研究。我们表明，即使在非平稳的互动动态下，我们的在线程序也能保持规定的反事实伤害和互补性违反率。此外，收紧或放宽这些约束会产生可预测的下游人类准确性变化，确认这两个原则作为引导多轮协作朝向更好决策质量的实用杠杆，而无需建模或约束人类行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for reliable frameworks in multi-round human-AI interactions, particularly in high-stakes decision-making contexts. The authors propose a human-centric approach based on the principles of counterfactual harm and complementarity, allowing users to define specific requirements for their tasks. Their online, distribution-free algorithm demonstrates that it can maintain user-specified constraints during collaboration, and experimental results from a medical diagnostic task and a pictorial reasoning task show that adjusting these constraints leads to predictable changes in human accuracy, validating the effectiveness of the proposed framework in enhancing decision quality.</div>
<div class="mono" style="margin-top:8px">本研究解决了在高风险决策环境中多轮人机协作中对可靠框架的需求。作者提出了一种以人为中心的方法，基于两个原则：反事实伤害和互补性，通过用户定义的规则形式化这些原则，以指定特定任务的期望结果。他们的在线无分布算法在两个设置中进行了评估，证明其有效地维持了指定的反事实伤害和互补性违反率，同时还表明对这些约束的调整可以可预测地影响人类决策准确性，从而在不需要复杂建模人类行为的情况下增强协作结果。</div>
</details>
</div>
<div class="card">
<div class="title">Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</div>
<div class="meta-line">Authors: Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen</div>
<div class="meta-line">First: 2026-02-19T18:54:32+00:00 · Latest: 2026-02-19T18:54:32+00:00</div>
<div class="meta-line">Comments: Code at: https://github.com/vila-lab/M-Attack-V2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17645v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17645v1">PDF</a> · <a href="https://github.com/vila-lab/M-Attack-V2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过细粒度细节定位推动黑箱LVLM攻击的前沿</div>
<div class="mono" style="margin-top:8px">对大型视觉语言模型（LVLM）的黑箱对抗攻击因缺乏梯度和复杂的多模态边界而具有挑战性。尽管之前的最先进的基于转移的方法如M-Attack通过源图像和目标图像之间的局部裁剪级匹配表现良好，但我们发现这会导致迭代间高方差、几乎正交的梯度，违反一致的局部对齐并使优化不稳定。我们将其归因于（i）ViT翻译敏感性导致的尖峰梯度和（ii）源裁剪与目标裁剪之间的结构不对称。我们将局部匹配重新表述为对源变换和目标语义的非对称期望，并对M-Attack进行了梯度去噪升级。在源侧，多裁剪对齐（MCA）通过每次迭代从多个独立采样的局部视图平均梯度以减少方差。在目标侧，辅助目标对齐（ATA）用来自语义相关分布的小辅助集替代激进的目标增强，产生更平滑、低方差的目标流形。我们进一步将动量重新解释为补丁动量，重放历史裁剪梯度；结合精细的补丁大小集成（PE+），这增强了可转移方向。这些模块共同形成M-Attack-V2，这是对M-Attack的简单模块化增强，显著改善了对前沿LVLM的基于转移的黑箱攻击：将Claude-4.0的成功率从8%提升至30%，将Gemini-2.5-Pro从83%提升至97%，将GPT-5从98%提升至100%，超越了之前的黑箱LVLM攻击。代码和数据可在以下网址公开获取：https://github.com/vila-lab/M-Attack-V2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve black-box adversarial attacks on Large Vision-Language Models (LVLMs), which face challenges due to the absence of gradients and complex multimodal boundaries. The authors reformulate local matching techniques by introducing Multi-Crop Alignment (MCA) and Auxiliary Target Alignment (ATA) to reduce gradient variance and enhance optimization stability. Experimental results demonstrate that their method, M-Attack-V2, significantly increases success rates for attacks on various LVLMs, achieving improvements from 8% to 30% on Claude-4.0, from 83% to 97% on Gemini-2.5-Pro, and maintaining a 100% success rate on GPT-5, surpassing previous black-box LVLM attack methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高对大型视觉语言模型（LVLM）的黑箱对抗攻击的有效性，这些攻击面临着缺乏梯度和复杂多模态边界的挑战。作者提出了一种名为M-Attack-V2的方法，通过多裁剪对齐和辅助目标对齐重新构建局部匹配，以减少梯度方差并改善优化稳定性，从而增强现有的M-Attack。实验结果表明，M-Attack-V2显著提高了对各种LVLM的攻击成功率，在Claude-4.0上从8%提高到30%，在Gemini-2.5-Pro上从83%提高到97%，在GPT-5上从98%提高到100%，超越了之前的黑箱LVLM攻击方法。</div>
</details>
</div>
<div class="card">
<div class="title">A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning</div>
<div class="meta-line">Authors: Dhruv Talwar, Harsh Desai, Wendong Yin, Goutam Mohanty, Rafael Reveles</div>
<div class="meta-line">First: 2026-02-19T18:54:06+00:00 · Latest: 2026-02-19T18:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17642v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A.R.I.S.: 基于深度学习的电子废物分类自动回收识别系统</div>
<div class="mono" style="margin-top:8px">传统电子回收过程由于材料分离和识别能力不足，导致显著的资源损失，限制了材料回收。我们提出了A.R.I.S.（自动回收识别系统），这是一种低成本、便携的碎片电子废物分拣机，解决了这一效率差距。该系统采用YOLOx模型实时分类金属、塑料和电路板，实现了低推理延迟和高检测精度。实验评估显示整体精度达到90%，平均精度均值（mAP）为82.2%，分拣纯度为84%。通过将深度学习与现有分拣方法相结合，A.R.I.S.提高了材料回收效率，降低了先进回收采用的门槛。这项工作补充了延长产品生命周期、支持以旧换新和回收计划以及减少供应链环境影响的更广泛倡议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve electronic recycling processes, which currently suffer from significant resource loss due to poor material separation and identification. The authors developed A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter that utilizes a YOLOx deep learning model to classify metals, plastics, and circuit boards in real time. Experimental results demonstrated that A.R.I.S. achieved 90% overall precision, 82.2% mean average precision, and 84% sortation purity, thereby enhancing material recovery efficiency and facilitating the adoption of advanced recycling methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善电子废物回收过程，目前由于材料分离和识别能力不足，导致资源损失严重。作者开发了A.R.I.S.（自动回收识别系统），这是一种低成本、便携式的分拣机，利用YOLOx深度学习模型实时分类金属、塑料和电路板。实验结果表明，A.R.I.S.的整体精度达到90%，平均精度均值（mAP）为82.2%，分拣纯度为84%，从而提高了材料回收效率，促进了先进回收实践。</div>
</details>
</div>
<div class="card">
<div class="title">FAMOSE: A ReAct Approach to Automated Feature Discovery</div>
<div class="meta-line">Authors: Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li</div>
<div class="meta-line">First: 2026-02-19T18:53:15+00:00 · Latest: 2026-02-19T18:53:15+00:00</div>
<div class="meta-line">Comments: 23 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17641v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE&#x27;s strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAMOSE：一种基于ReAct的自动特征发现方法</div>
<div class="mono" style="margin-top:8px">特征工程在机器学习中仍然是一个关键但具有挑战性的瓶颈，特别是对于表格数据，因为从指数级大的特征空间中识别最佳特征通常需要大量的领域专业知识。为了解决这一挑战，我们引入了FAMOSE（特征增强与最优选择代理），这是一个新颖的框架，利用ReAct范式自主探索、生成和优化特征，同时在代理架构中集成特征选择和评估工具。据我们所知，FAMOSE是首次将代理ReAct框架应用于自动特征工程，特别是回归和分类任务。大量实验表明，FAMOSE在分类任务上达到了或接近最先进水平（尤其是在超过10K实例的任务中，ROC-AUC平均提高0.23%），并在回归任务中通过平均减少2.0%的RMSE达到了最先进水平，同时比其他算法对错误更具鲁棒性。我们假设FAMOSE的强大性能是因为ReAct允许LLM上下文窗口记录（通过迭代特征发现和评估步骤）哪些特征有效或无效。这类似于少量示例提示，并引导LLM发明更好、更具创新性的特征。我们的工作提供了证据，表明AI代理在解决需要高度创造性解决方案的问题（如特征工程）方面非常有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of feature engineering in machine learning, particularly for tabular data, where optimal feature identification typically requires extensive domain expertise. The authors propose FAMOSE, a framework that utilizes the ReAct paradigm to autonomously explore, generate, and refine features while incorporating feature selection and evaluation within an agent architecture. Experimental results indicate that FAMOSE achieves state-of-the-art performance in classification tasks, with an average ROC-AUC increase of 0.23% for datasets with over 10K instances, and it also excels in regression tasks by reducing RMSE by 2.0% on average, demonstrating greater robustness to errors compared to other algorithms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器学习中特征工程的挑战，尤其是对于表格数据，通常需要显著的领域专业知识来从庞大的特征空间中识别最佳特征。作者提出了FAMOSE，一个利用ReAct范式的框架，能够自主探索、生成和优化特征，同时在代理架构中整合特征选择和评估工具。实验结果表明，FAMOSE在分类任务上达到了最先进的性能，对于超过10,000个实例的数据集，平均ROC-AUC提高了0.23%，在回归任务中平均减少了2.0%的RMSE，并且相比其他算法表现出更强的错误鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">IntRec: Intent-based Retrieval with Contrastive Refinement</div>
<div class="meta-line">Authors: Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Yue Lu</div>
<div class="meta-line">First: 2026-02-19T18:50:53+00:00 · Latest: 2026-02-19T18:50:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17639v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IntRec：基于意图的对比精炼检索</div>
<div class="mono" style="margin-top:8px">从复杂场景中检索用户指定的对象仍然是一项具有挑战性的任务，尤其是在查询模糊或涉及多个相似对象时。现有的开放词汇检测器以一次性方式操作，缺乏根据用户反馈精炼预测的能力。为了解决这个问题，我们提出了IntRec，一个交互式对象检索框架，基于用户反馈精炼预测。其核心是意图状态（IS），维护正锚点（确认线索）和负约束（拒绝假设）的双重记忆集。对比对齐函数通过最大化与正线索的相似性并惩罚被拒绝的对象来对候选对象进行排名，从而在杂乱场景中实现细粒度的消歧。我们的交互式框架在没有额外监督的情况下显著提高了检索准确性。在LVIS上，IntRec达到了35.4 AP，分别比OVMR、CoDet和CAKE提高了+2.3、+3.7和+0.5。在具有挑战性的LVIS-Ambiguous基准上，它在一次纠正反馈后比其一次性基线提高了+7.9 AP，每次交互增加的延迟不到30毫秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the retrieval of user-specified objects from complex scenes, particularly when queries are ambiguous or involve similar objects. The authors propose IntRec, an interactive object retrieval framework that utilizes an Intent State to maintain dual memory sets for positive anchors and negative constraints, allowing for refined predictions based on user feedback. Experimental results show that IntRec achieves a 35.4 AP on the LVIS dataset, outperforming existing methods, and demonstrates a significant improvement of 7.9 AP on the LVIS-Ambiguous benchmark after a single corrective feedback, with minimal added latency per interaction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善从复杂场景中检索用户指定对象的能力，尤其是在面对模糊查询或多个相似对象时。作者提出了IntRec，这是一种基于用户反馈的交互式对象检索框架，利用意图状态维护正锚点和负约束的双重记忆集。实验结果表明，IntRec显著提高了检索准确性，在LVIS数据集上达到了35.4 AP，超越了现有方法，同时在LVIS-Ambiguous基准上经过一次用户交互后，性能提高了7.9 AP，且增加的延迟不到30毫秒。</div>
</details>
</div>
<div class="card">
<div class="title">Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</div>
<div class="meta-line">Authors: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim</div>
<div class="meta-line">First: 2026-02-19T18:48:08+00:00 · Latest: 2026-02-19T18:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Reverso：高效的时间序列基础模型用于零样本预测</div>
<div class="mono" style="margin-top:8px">学习时间序列基础模型已被证明是零样本时间序列预测的有前景的方法，适用于多种时间序列领域。由于扩展性是其他模态（如语言和视觉）中基础模型性能的关键驱动因素，最近关于时间序列基础建模的许多工作都集中在扩展性上。这导致了具有数亿参数的时间序列基础模型，尽管性能良好，但在实践中使用效率低且成本高。本文描述了一种学习高效基础模型用于零样本时间序列预测的简单方法，这些模型的规模小得多。我们展示了大规模变换器并非必要：小型混合模型通过交错长卷积和线性RNN层（特别是DeltaNet层）可以匹配更大变换器模型的性能，同时体积小于百倍。我们还描述了几种数据增强和推理策略，进一步提高性能。该方法产生了Reverso，一个高效的时间序列基础模型系列，用于零样本预测，显著推动了性能-效率的Pareto前沿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of time series foundation models for zero-shot forecasting, as existing models are often large and costly to use. The authors propose a method that utilizes small hybrid models combining long convolution and linear RNN layers, specifically DeltaNet layers, which can achieve performance comparable to larger transformer-based models while being significantly smaller in size. The experimental results demonstrate that the Reverso models not only maintain high forecasting accuracy but also enhance performance-efficiency, thereby advancing the state-of-the-art in time series forecasting.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高零-shot时间序列预测的基础模型的效率，因为现有模型往往体积庞大且使用成本高。作者提出了一种方法，利用结合长卷积和线性RNN层的小型混合模型，特别是DeltaNet层，这些模型在体积显著减小的同时能够达到与大型基于变换器的模型相当的性能。实验结果表明，Reverso模型不仅保持了高预测准确性，还通过额外的数据增强和推理策略提高了性能效率。</div>
</details>
</div>
<div class="card">
<div class="title">When to Trust the Cheap Check: Weak and Strong Verification for Reasoning</div>
<div class="meta-line">Authors: Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani</div>
<div class="meta-line">First: 2026-02-19T18:47:38+00:00 · Latest: 2026-02-19T18:47:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17633v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时信任廉价检查：推理的弱验证与强验证</div>
<div class="mono" style="margin-top:8px">使用大型语言模型（LLMs）进行推理越来越多地在更广泛的验证循环中展开。内部，系统使用廉价检查，如自一致性或代理奖励，我们称之为弱验证。外部，用户检查输出并通过反馈引导模型，直到结果值得信赖，我们称之为强验证。这些信号在成本和可靠性上有明显差异：强验证可以建立信任，但资源密集，而弱验证快速且可扩展，但嘈杂且不完美。我们通过弱-强验证策略形式化这种紧张关系，这些策略决定何时基于弱验证接受或拒绝，何时依赖强验证。我们引入捕捉错误接受、错误拒绝和强验证频率的指标。在总体上，我们展示了最佳策略具有双阈值结构，并且校准和清晰度决定了弱验证者的价值。在此基础上，我们开发了一种在线算法，能够在不对查询流、语言模型或弱验证者做假设的情况下，证明性地控制接受和拒绝错误。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the balance between weak and strong verification methods in reasoning with large language models (LLMs), motivated by the need for efficient yet reliable verification processes. The authors formalize this tension through weak-strong verification policies and introduce metrics to evaluate incorrect acceptance, incorrect rejection, and strong-verification frequency. Experimental results demonstrate that optimal policies exhibit a two-threshold structure, revealing that calibration and sharpness significantly influence the effectiveness of weak verifiers, and an online algorithm is developed to manage acceptance and rejection errors effectively without prior assumptions about the query stream or the model used.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于平衡弱验证方法的效率与强验证的可靠性，以便在大型语言模型（LLMs）中进行推理。作者通过弱-强验证策略形式化这种平衡，决定何时依赖快速但噪声较大的弱验证，何时使用资源密集型的强验证。实验结果表明，最佳验证策略呈现出双阈值结构，并且校准和清晰度显著影响弱验证器的性能。此外，他们提出了一种在线算法，能够有效管理接受和拒绝错误，而无需依赖于特定的查询流或所涉及的模型的假设。</div>
</details>
</div>
<div class="card">
<div class="title">SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</div>
<div class="meta-line">Authors: Nathan S. de Lara, Florian Shkurti</div>
<div class="meta-line">First: 2026-02-19T18:47:31+00:00 · Latest: 2026-02-19T18:47:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17632v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMAC：用于稳健离线到在线转移的得分匹配演员-评论家</div>
<div class="mono" style="margin-top:8px">现代离线强化学习（RL）方法找到高效的演员-评论家，然而，使用基于价值的RL算法在线微调这些演员-评论家通常会导致性能的立即下降。我们提供的证据与以下假设一致：在损失景观中，先前算法的离线极大值和在线极大值之间被低性能的谷底分隔，梯度基础的微调会穿越这些谷底。随后，我们提出了得分匹配演员-评论家（SMAC），这是一种离线RL方法，旨在学习能够无性能下降地过渡到在线基于价值的RL算法的演员-评论家。SMAC通过在离线阶段对Q函数进行正则化，以尊重策略得分与Q函数的动作梯度之间的一阶导数等式，从而避免离线和在线极大值之间的谷底。我们通过实验表明，SMAC收敛到与更好的在线极大值通过单调递增奖励的路径相连的离线极大值，这些路径是通过一阶优化找到的。SMAC在6/6个D4RL任务中实现了平滑转移到软演员-评论家和TD3。在4/6个环境中，它将后悔减少了34-58%，优于最佳基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance drops that occur when fine-tuning actor-critics from offline reinforcement learning (RL) to online value-based RL algorithms. The authors propose a novel method called Score Matched Actor-Critic (SMAC), which regularizes the Q-function during the offline phase to maintain a first-order derivative equality between the policy score and the action-gradient of the Q-function, thereby avoiding low-performance valleys in the loss landscape. Experimental results show that SMAC successfully converges to offline maxima that are connected to superior online maxima, achieving smooth transitions to Soft Actor-Critic and TD3 in all six D4RL tasks tested, and reducing regret by 34-58% in four of those environments compared to the best baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从离线强化学习（RL）到在线基于价值的RL算法微调时出现的性能下降问题。作者提出了得分匹配演员-评论家（SMAC）方法，该方法在离线阶段对Q函数进行正则化，以保持策略得分与Q函数的动作梯度之间的一阶导数相等，从而避免损失景观中的低性能谷。实验结果表明，SMAC成功收敛到与更优在线极大值相连的离线极大值，在所有六个D4RL任务中实现了平滑过渡到Soft Actor-Critic和TD3，并在六个环境中的四个中将遗憾减少了34-58%，相较于最佳基线。</div>
</details>
</div>
<div class="card">
<div class="title">Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning</div>
<div class="meta-line">Authors: Obaidullah Zaland, Zulfiqar Ahmad Khan, Monowar Bhuyan</div>
<div class="meta-line">First: 2026-02-19T18:44:23+00:00 · Latest: 2026-02-19T18:44:23+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the IEEE International Conference on Big Data (IEEE BigData) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17625v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client&#x27;s data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>抗灾难性遗忘的单次增量联邦学习</div>
<div class="mono" style="margin-top:8px">现代大数据系统生成大量异构且地理分散的流，这些流规模庞大且敏感于隐私，使得集中化变得具有挑战性。虽然联邦学习（FL）提供了一种增强隐私的训练机制，但它假设数据流是静态的，并在多个回合中学习协作模型，这使得在有限通信场景下使用增量数据进行学习变得困难。本文提出了单次增量联邦学习（OSI-FL），这是第一个解决通信开销和灾难性遗忘双重挑战的FL框架。OSI-FL在单次通信回合中传递由每个客户端的冻结视觉-语言模型（VLM）设计的类别特定嵌入，服务器上的预训练扩散模型使用这些嵌入合成与客户端数据分布相似的新数据。合成的样本在服务器上用于训练。然而，仍然存在两个挑战：i）增量到达的任务需要重新训练全局模型，ii）随着未来任务的到来，重新训练模型会引入灾难性遗忘。为此，我们通过选择性样本保留（SSR）增强训练，该方法根据样本损失识别并保留每个类别和任务对中最具信息性的前p个样本。SSR通过确保在进一步迭代中将代表性保留样本纳入训练来限制遗忘。实验结果表明，OSI-FL在三个基准数据集的类别增量和领域增量场景中均优于传统和单次FL方法的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of catastrophic forgetting and communication overhead in federated learning (FL) when dealing with incremental data in privacy-sensitive environments. The authors propose a novel framework called One-Shot Incremental Federated Learning (OSI-FL), which utilizes category-specific embeddings generated by a frozen vision-language model from clients in a single communication round, allowing a pre-trained diffusion model at the server to synthesize new data that aligns with the clients&#x27; data distributions. Experimental results demonstrate that OSI-FL significantly outperforms traditional and one-shot FL methods in both class-incremental and domain-incremental scenarios across three benchmark datasets, effectively mitigating the issues of forgetting and communication constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在隐私敏感环境中处理增量数据时，联邦学习（FL）中的灾难性遗忘和通信开销问题。作者提出了一种名为一次性增量联邦学习（OSI-FL）的框架，该框架利用来自每个客户端的冻结视觉-语言模型生成的类别特定嵌入，在单次通信轮次中允许服务器上的预训练扩散模型合成与客户端数据分布相似的新数据。实验结果表明，OSI-FL在三个基准数据集的类别增量和领域增量场景中显著优于传统和一次性FL方法，有效缓解了遗忘和通信限制的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</div>
<div class="meta-line">Authors: Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han</div>
<div class="meta-line">First: 2026-02-19T18:40:51+00:00 · Latest: 2026-02-19T18:40:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定异步性：针对大型语言模型的方差控制离线策略强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）广泛用于提高大型语言模型在推理任务上的表现，而异步RL训练因其提高了端到端吞吐量而受到青睐。然而，对于广泛采用的无评论者策略梯度方法，如REINFORCE和GRPO，高异步性使得策略梯度估计器的方差显著增加：在过时的回放上进行训练会产生重尾重要性比率，导致少量样本主导更新。这种放大使得梯度噪声增大，学习相对于匹配的在线训练变得不稳定。在数学和一般推理基准测试中，我们发现崩溃可以通过有效样本大小（ESS）和不稳定的梯度范数可靠预测。基于这一诊断，我们提出了方差控制策略优化（VCPO），这是一种针对REINFORCE/GRPO风格算法的通用稳定化方法，(i) 根据有效样本大小调整学习率，以抑制不可靠的更新，(ii) 在离线设置中应用闭式最小方差基线，避免辅助价值模型并增加最小开销。实证结果表明，VCPO显著提高了异步训练在数学、一般推理和工具使用任务中的鲁棒性，超越了广泛的基线，包括掩蔽/剪切稳定器和算法变体。这将长上下文、多轮训练时间减少了2.5倍，同时匹配同步性能，证明了显式控制策略梯度方差是大规模可靠异步RL的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of high variance in policy-gradient methods used in reinforcement learning (RL) for large language models, particularly under asynchronous training conditions. The authors propose a new method called Variance Controlled Policy Optimization (VCPO), which stabilizes learning by adjusting the learning rate based on effective sample size and implementing a minimum-variance baseline without requiring an auxiliary value model. Experimental results show that VCPO significantly enhances the robustness of asynchronous training across various reasoning tasks, achieving a 2.5 times reduction in training time while maintaining performance comparable to synchronous methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决异步强化学习（RL）训练中政策梯度估计器的高方差问题，这可能导致大型语言模型学习不稳定。作者提出了一种名为方差控制政策优化（VCPO）的方法，通过根据有效样本大小调整学习率并使用闭式最小方差基线来稳定训练，而无需辅助价值模型。实验结果表明，VCPO显著增强了异步训练在各种推理任务中的鲁棒性，实现了长上下文、多轮训练时间减少2.5倍，同时保持与同步方法相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning</div>
<div class="meta-line">Authors: Obaidullah Zaland, Sajib Mistry, Monowar Bhuyan</div>
<div class="meta-line">First: 2026-02-19T18:40:12+00:00 · Latest: 2026-02-19T18:40:12+00:00</div>
<div class="meta-line">Comments: Accepted for Publication in IEEE International Conference on Big Data (IEEE BigData) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17614v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients&#x27; side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients&#x27; private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>守护中间：保护联邦分割学习中的中间表示</div>
<div class="mono" style="margin-top:8px">大数据场景中，海量异构数据集分布在客户端，需求可扩展的、保护隐私的学习方法。联邦学习（FL）使得在客户端之间进行去中心化的机器学习（ML）模型训练，而无需数据集中。然而，去中心化训练给客户端设备带来了计算负担。U型联邦分割学习（UFSL）将部分客户端计算卸载到服务器，同时将数据和标签保留在客户端。然而，客户端与服务器共享的中间表示（即压缩数据）容易暴露客户端的私有数据。为了减少通过中间数据表示暴露客户端数据的风险，本研究提出了k-匿名差分隐私UFSL（KD-UFSL），利用微聚合和差分隐私等隐私增强技术，最小化传输到服务器的压缩数据泄露。我们首先证明了对手可以通过数据重构攻击从中间表示中访问私有客户端数据，然后提出了隐私增强解决方案KD-UFSL以减轻这一风险。我们的实验表明，在某些情况下，KD-UFSL将实际图像与重构图像之间的均方误差增加了多达50%，同时在四个基准数据集上将它们之间的结构相似性降低了多达40%。更重要的是，KD-UFSL在提高隐私的同时保持了全局模型的效用。这突显了其在隐私和效用必须平衡的大规模大数据应用中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the privacy concerns associated with intermediate representations in federated split learning (UFSL), which can expose client data during decentralized training. The authors propose a k-anonymous differentially private UFSL (KD-UFSL) method that employs privacy-enhancing techniques like microaggregation and differential privacy to protect client data. Experimental results show that KD-UFSL increases the mean squared error between actual and reconstructed images by up to 50% and decreases structural similarity by up to 40% across four benchmarking datasets, effectively improving privacy while maintaining the utility of the global model, making it suitable for large-scale big data applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决联邦分割学习（UFSL）中间表示的隐私问题，这些表示在去中心化训练过程中可能暴露客户端数据。作者提出了一种k-匿名差分隐私UFSL（KD-UFSL）方法，该方法结合了微聚合和差分隐私等隐私增强技术，以减少数据泄露。实验结果表明，KD-UFSL可以在四个基准数据集上将实际图像与重建图像之间的均方误差提高最多50%，并将它们之间的结构相似度降低最多40%，有效提高隐私保护，同时保持全局模型的效用，使其适用于大规模大数据应用。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</div>
<div class="meta-line">Authors: Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick</div>
<div class="meta-line">First: 2025-10-16T00:48:05+00:00 · Latest: 2026-02-19T18:33:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14190v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14190v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models excel at generation, but their latent spaces are high dimensional and not explicitly organized for interpretation or control. We introduce ConDA (Contrastive Diffusion Alignment), a plug-and-play geometry layer that applies contrastive learning to pretrained diffusion latents using auxiliary variables (e.g., time, stimulation parameters, facial action units). ConDA learns a low-dimensional embedding whose directions align with underlying dynamical factors, consistent with recent contrastive learning results on structured and disentangled representations. In this embedding, simple nonlinear trajectories support smooth interpolation, extrapolation, and counterfactual editing while rendering remains in the original diffusion space. ConDA separates editing and rendering by lifting embedding trajectories back to diffusion latents with a neighborhood-preserving kNN decoder and is robust across inversion solvers. Across fluid dynamics, neural calcium imaging, therapeutic neurostimulation, facial expression dynamics, and monkey motor cortex activity, ConDA yields more interpretable and controllable latent structure than linear traversals and conditioning-based baselines, indicating that diffusion latents encode dynamics-relevant structure that can be exploited by an explicit contrastive geometry layer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比扩散对齐：学习可控生成的结构化潜变量</div>
<div class="mono" style="margin-top:8px">扩散模型在生成方面表现出色，但其潜在空间维度高，且未明确组织以便于解释或控制。我们引入了ConDA（对比扩散对齐），这是一种即插即用的几何层，通过使用辅助变量（例如时间、刺激参数、面部动作单元）对预训练的扩散潜变量应用对比学习。ConDA学习一个低维嵌入，其方向与潜在动态因素对齐，这与最近在结构化和解耦表示上的对比学习结果一致。在这个嵌入中，简单的非线性轨迹支持平滑的插值、外推和反事实编辑，同时渲染仍保持在原始扩散空间。ConDA通过将嵌入轨迹提升回扩散潜变量，使用保持邻域的kNN解码器来分离编辑和渲染，并在反演求解器中表现稳健。在流体动力学、神经钙成像、治疗性神经刺激、面部表情动态和猴子运动皮层活动中，ConDA提供了比线性遍历和基于条件的基线更具可解释性和可控性的潜在结构，表明扩散潜变量编码了与动态相关的结构，可以通过显式的对比几何层进行利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the interpretability and control of high-dimensional latent spaces in diffusion models, which are typically not organized for these purposes. The authors introduce a method called Contrastive Diffusion Alignment (ConDA), which employs contrastive learning on pretrained diffusion latents using auxiliary variables to create a low-dimensional embedding that aligns with underlying dynamical factors. Experimental results demonstrate that ConDA provides a more interpretable and controllable latent structure compared to traditional linear traversals and conditioning-based methods across various applications, including fluid dynamics and facial expression dynamics, suggesting that diffusion latents contain relevant structural information that can be effectively utilized through this contrastive approach.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高扩散模型中高维潜在空间的可解释性和可控性，这些潜在空间通常没有为此目的进行组织。作者提出了ConDA（对比扩散对齐），一种几何层，利用对比学习对预训练的扩散潜在变量进行处理，并结合辅助变量，创建与动态因素对齐的低维嵌入。实验结果表明，ConDA在流体动力学和面部表情动态等多个应用中提供了比传统线性遍历和基于条件的方法更具可解释性和可控性的潜在结构。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Testing and Estimation by Comparisons</div>
<div class="meta-line">Authors: Xiwen Tao, Chenyi Zhang, Helin Wang, Yexin Zhang, Tongyang Li</div>
<div class="meta-line">First: 2024-05-19T05:39:46+00:00 · Latest: 2026-02-19T18:33:13+00:00</div>
<div class="meta-line">Comments: v2: Significant changes compared to v1. v2 focuses on the gradient testing and gradient estimation problems, with an improved bound on classical gradient estimation, a new result on classical gradient testing, as well as a new quantum algorithm and lower bound on gradient estimation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.11454v2">Abs</a> · <a href="https://arxiv.org/pdf/2405.11454v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study gradient testing and gradient estimation of smooth functions using only a comparison oracle that, given two points, indicates which one has the larger function value. For any smooth $f\colon\mathbb R^n\to\mathbb R$, $\mathbf{x}\in\mathbb R^n$, and $\varepsilon&gt;0$, we design a gradient testing algorithm that determines whether the normalized gradient $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ is $\varepsilon$-close or $2\varepsilon$-far from a given unit vector $\mathbf{v}$ using $O(1)$ queries, as well as a gradient estimation algorithm that outputs an $\varepsilon$-estimate of $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ using $O(n\log(1/\varepsilon))$ queries which we prove to be optimal. Furthermore, we study gradient estimation in the quantum comparison oracle model where queries can be made in superpositions, and develop a quantum algorithm using $O(\log (n/\varepsilon))$ queries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过比较进行梯度测试和估计</div>
<div class="mono" style="margin-top:8px">我们研究使用仅比较oracle的平滑函数的梯度测试和梯度估计，该oracle在给定两个点时指示哪个点具有更大的函数值。对于任何平滑函数 $f\colon\mathbb R^n\to\mathbb R$、点 $\mathbf{x}\in\mathbb R^n$ 和 $\varepsilon&gt;0$，我们设计了一种梯度测试算法，该算法确定归一化梯度 $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ 是否与给定单位向量 $\mathbf{v}$ 近似 $\varepsilon$ 或远离 $2\varepsilon$，使用 $O(1)$ 次查询，以及一种梯度估计算法，该算法输出 $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ 的 $\varepsilon$ 估计，使用 $O(n\log(1/\varepsilon))$ 次查询，我们证明这是最优的。此外，我们研究了量子比较oracle模型中的梯度估计，其中查询可以以叠加态进行，并开发了一种使用 $O(\log (n/\varepsilon))$ 次查询的量子算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates gradient testing and estimation of smooth functions using a comparison oracle that identifies which of two points has a larger function value. The authors propose a gradient testing algorithm that can determine if the normalized gradient is close to or far from a specified unit vector with a constant number of queries, and a gradient estimation algorithm that provides an epsilon-accurate estimate of the normalized gradient using a logarithmic number of queries, which is shown to be optimal. Additionally, they explore gradient estimation in a quantum context, developing a quantum algorithm that requires fewer queries than classical methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用比较oracle进行平滑函数的梯度测试和估计的问题，该oracle仅指示两个点中哪个具有更大的函数值。作者提出了一种梯度测试算法，可以在常数查询次数内确定归一化梯度是否接近或远离指定的单位向量，以及一种梯度估计算法，能够使用对数数量的查询提供归一化梯度的epsilon精确估计，并证明这是最优的。此外，他们还探讨了在量子比较oracle框架下的梯度估计，提出了一种需要对数数量查询的量子算法。</div>
</details>
</div>
<div class="card">
<div class="title">ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization</div>
<div class="meta-line">Authors: Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-05T17:47:42+00:00 · Latest: 2026-02-19T18:32:53+00:00</div>
<div class="meta-line">Comments: This work was accepted and presented at NeurIPS 2025. Code is available at https://github.com/mts-ai/replaceme Reviews at OpenReview: https://openreview.net/forum?id=zEj1FSYCRn NeurIPS 2025 Proceedings: https://openreview.net/pdf?id=zEj1FSYCRn</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02819v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.02819v4">PDF</a> · <a href="https://github.com/mts-ai/replaceme">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\% pruning while retaining approximately 90\% of the original model&#x27;s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead. We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReplaceMe：通过深度剪枝和变换器块线性化实现网络简化</div>
<div class="mono" style="margin-top:8px">我们介绍了ReplaceMe，一种通用的无训练深度剪枝方法，能够有效地用线性操作替代变换器块，同时在低压缩比下保持高性能。与传统的剪枝方法需要额外的训练或微调不同，我们的方法只需一个小的校准数据集，用于估计线性变换，近似剪枝块。估计的线性映射可以与剩余的变换器块无缝合并，消除对任何额外网络参数的需求。我们的实验表明，ReplaceMe始终优于其他无训练方法，并在与涉及广泛重训练/微调和架构修改的最先进剪枝方法的竞争中保持高度竞争力。应用于多个大型语言模型（LLMs），ReplaceMe在保留原始模型约90%性能的同时，实现了高达25%的剪枝——无需任何训练或修复步骤，导致最小的计算开销。我们提供了一个开源库，实施ReplaceMe以及几种最先进的深度剪枝技术，网址为https://github.com/mts-ai/ReplaceMe。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a method for network simplification that avoids the extensive training typically required by conventional pruning techniques. The authors introduce ReplaceMe, a training-free depth pruning method that replaces transformer blocks with a linear operation using a small calibration dataset to estimate a linear transformation. Experimental results demonstrate that ReplaceMe achieves up to 25% pruning while maintaining approximately 90% of the original model&#x27;s performance on open benchmarks, outperforming other training-free methods and remaining competitive with state-of-the-art pruning techniques that require retraining or architectural changes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种高效的网络简化方法，避免传统剪枝方法中通常需要的广泛再训练。作者提出了ReplaceMe，这是一种无训练的深度剪枝技术，通过使用小型校准数据集来估计线性变换，从而将变压器块替换为线性操作。实验结果表明，ReplaceMe在保持原始模型约90%性能的同时，实现了高达25%的剪枝，超越了其他无训练方法，并在与需要广泛再训练和架构更改的最先进剪枝技术的竞争中保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Anytime-Valid Statistical Watermarking</div>
<div class="meta-line">Authors: Baihe Huang, Eric Xu, Kannan Ramchandran, Jiantao Jiao, Michael I. Jordan</div>
<div class="meta-line">First: 2026-02-19T18:32:26+00:00 · Latest: 2026-02-19T18:32:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向随时有效的统计水印技术</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的普及需要有效的机制来区分机器生成的内容和人类文本。尽管统计水印已成为一种有前景的解决方案，但现有方法存在两个关键限制：缺乏选择采样分布的原则性方法，以及依赖固定时间假设检验，这排除了有效的提前停止。在本文中，我们通过开发第一个基于e值的水印框架——锚定E水印，填补了这一空白，该框架将最优采样与随时有效的推断统一起来。与传统方法中可选停止使得第一类错误保证失效不同，我们的框架通过构建检测过程的测试超鞅，实现了有效的随时推断。通过利用锚定分布来近似目标模型，我们描述了相对于最坏情况对数增长率的最优e值，并推导出最优期望停止时间。我们的理论主张通过模拟和在已建立基准上的评估得到了证实，显示我们的框架可以显著提高样本效率，相对于最先进的基线减少13-15%的检测所需平均令牌预算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of Large Language Models (LLMs) highlights the need for effective methods to differentiate between machine-generated and human-written text. This study introduces Anchored E-Watermarking, an e-value-based watermarking framework that integrates optimal sampling with anytime-valid inference, addressing the limitations of existing statistical watermarking techniques. Experimental results demonstrate that this framework improves sample efficiency, achieving a 13-15% reduction in the average token budget required for detection compared to current state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的广泛应用，区分机器生成文本和人类文本的有效方法变得愈发重要。本研究提出了锚定E水印（Anchored E-Watermarking），这是一种基于e值的水印框架，结合了最优采样和随时有效的推断，解决了现有统计水印技术的局限性。实验结果表明，该框架提高了样本效率，与当前最先进的方法相比，检测所需的平均令牌预算减少了13-15%。</div>
</details>
</div>
<div class="card">
<div class="title">AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing</div>
<div class="meta-line">Authors: Jianda Du, Youran Sun, Haizhao Yang</div>
<div class="meta-line">First: 2026-02-19T18:31:52+00:00 · Latest: 2026-02-19T18:31:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17607v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoNumerics：一个自主的、与偏微分方程无关的多智能体科学计算管道</div>
<div class="mono" style="margin-top:8px">偏微分方程在科学和工程建模中至关重要，但设计准确的数值求解器通常需要大量的数学专业知识和手动调优。最近的基于神经网络的方法提高了灵活性，但通常需要高计算成本，并且可解释性有限。我们介绍了\texttt{AutoNumerics}，这是一个多智能体框架，能够自主设计、实现、调试和验证通用偏微分方程的数值求解器，直接基于自然语言描述。与黑箱神经求解器不同，我们的框架生成基于经典数值分析的透明求解器。我们引入了一种粗到细的执行策略和基于残差的自我验证机制。在24个经典和真实世界的偏微分方程问题上的实验表明，\texttt{AutoNumerics}在准确性上与现有的神经和基于大语言模型的基线相比具有竞争力或更优的表现，并且能够根据偏微分方程的结构特性正确选择数值方案，表明其作为自动偏微分方程求解的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in designing accurate numerical solvers for partial differential equations (PDEs), which typically require significant mathematical expertise and manual tuning. The authors present AutoNumerics, a multi-agent framework that autonomously creates, implements, debugs, and verifies numerical solvers from natural language descriptions, utilizing a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experimental results on 24 canonical and real-world PDE problems indicate that AutoNumerics achieves competitive or superior accuracy compared to existing neural and LLM-based approaches and effectively selects numerical schemes based on the structural properties of the PDEs, highlighting its potential as an accessible solution for automated PDE solving.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决设计准确的偏微分方程（PDE）数值求解器所面临的挑战，这通常需要大量的数学专业知识和手动调优。作者提出了AutoNumerics，这是一个多智能体框架，可以根据自然语言描述自主创建、实现、调试和验证数值求解器，采用粗到细的执行策略和基于残差的自我验证机制。在对24个经典和真实世界的PDE问题进行的实验中，AutoNumerics的准确性与现有的神经网络和大型语言模型（LLM）方法相比具有竞争力或更优，能够有效地根据PDE的结构特性选择数值方案。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</div>
<div class="meta-line">Authors: Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly</div>
<div class="meta-line">First: 2026-02-19T18:30:18+00:00 · Latest: 2026-02-19T18:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17605v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method&#x27;s reliability at uncovering targets with limited data and a varying environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态适应：基于相关性的在线元学习与潜在概念的地理空间发现</div>
<div class="mono" style="margin-top:8px">在许多现实世界场景中，如环境监测、灾害响应或公共卫生，数据收集成本高且困难，环境动态变化，从未观察区域进行战略性采样对于在资源紧张的情况下有效发现隐藏目标至关重要。然而，稀疏和偏倚的地理空间真实数据限制了现有基于学习的方法的适用性，如强化学习。为了解决这个问题，我们提出了一个统一的地理空间发现框架，整合了主动学习、在线元学习和概念引导推理。我们的方法引入了两个基于*概念相关性*的关键创新，该概念捕捉了领域特定因素如何影响目标存在：*概念加权不确定性采样策略*，其中不确定性由基于现成的领域特定概念（例如，土地覆盖、源接近度）学习的相关性调节；以及*关注相关性的元批次形成策略*，在在线元更新期间促进语义多样性，提高动态环境中的泛化能力。我们的实验包括在一个真实世界的致癌PFAS（全氟和多氟烷基物质）污染数据集上进行测试，展示了我们的方法在有限数据和变化环境下发现目标的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently uncovering hidden targets in dynamic environments with limited and biased geospatial data, which is critical in fields like environmental monitoring and public health. The authors propose a unified framework that combines active learning, online meta-learning, and concept-guided reasoning, introducing innovations such as a concept-weighted uncertainty sampling strategy and a relevance-aware meta-batch formation strategy. Experimental results demonstrate the framework&#x27;s effectiveness in identifying targets in a real-world dataset related to PFAS contamination, highlighting its reliability in scenarios with constrained data and changing conditions.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于在动态环境中有效发现隐藏目标的挑战，尤其是在环境监测和公共健康等领域，面临有限和偏见的地理空间数据。作者提出了一种统一的地理空间发现框架，结合了主动学习、在线元学习和概念引导推理，创新性地引入了概念加权不确定性采样策略和相关性意识的元批次形成策略。实验结果表明，该方法在PFAS污染相关的真实数据集中，即使在数据可用性受限和条件变化的情况下，也能有效可靠地识别目标。</div>
</details>
</div>
<div class="card">
<div class="title">pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</div>
<div class="meta-line">Authors: Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-16T17:59:51+00:00 · Latest: 2026-02-19T18:30:05+00:00</div>
<div class="meta-line">Comments: ICLR 2026. Code: https://github.com/Lakonik/piFlow Demos: https://huggingface.co/spaces/Lakonik/pi-Qwen | https://huggingface.co/spaces/Lakonik/pi-FLUX.1 | https://huggingface.co/spaces/Lakonik/pi-FLUX.2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14974v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.14974v3">PDF</a> · <a href="https://github.com/Lakonik/piFlow">Code1</a> · <a href="https://huggingface.co/spaces/Lakonik/pi-Qwen">Code2</a> · <a href="https://huggingface.co/spaces/Lakonik/pi-FLUX.1">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($π$-Flow). $π$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy&#x27;s ODE trajectory to the teacher&#x27;s, we introduce a novel imitation distillation approach, which matches the policy&#x27;s velocity to the teacher&#x27;s along the policy&#x27;s trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher&#x27;s behavior, $π$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $π$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>π-Flow：基于策略的少步生成通过模仿蒸馏</div>
<div class="mono" style="margin-top:8px">少步扩散或基于流的生成模型通常将预测速度的教师蒸馏为预测去噪数据捷径的学生。这种格式不匹配导致了复杂的蒸馏过程，通常面临质量与多样性的权衡。为了解决这个问题，我们提出了基于策略的流模型（$π$-Flow）。$π$-Flow 修改学生流模型的输出层，以在一个时间步预测无网络策略。该策略随后在未来的子步骤中产生动态流速度，几乎没有额外开销，使得在这些子步骤上快速准确地进行常微分方程（ODE）积分，而无需额外的网络评估。为了将策略的ODE轨迹与教师的匹配，我们引入了一种新颖的模仿蒸馏方法，该方法使用标准的 $\ell_2$ 流匹配损失将策略的速度与教师的速度沿着策略的轨迹进行匹配。通过简单地模仿教师的行为，$π$-Flow 实现了稳定和可扩展的训练，避免了质量与多样性的权衡。在 ImageNet 256$^2$ 上，它达到了 1-NFE FID 为 2.85，超越了同一 DiT 架构的先前 1-NFE 模型。在 FLUX.1-12B 和 Qwen-Image-20B 上，$π$-Flow 在 4 NFEs 下实现了显著更好的多样性，同时保持了教师级别的质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and quality of few-step diffusion or flow-based generative models, which often face challenges due to format mismatches in distillation processes. The authors propose a novel approach called policy-based flow models ($π$-Flow), which modifies the output layer of a student flow model to predict a network-free policy at a single timestep, allowing for the generation of dynamic flow velocities in subsequent substeps with minimal computational overhead. Experimental results demonstrate that $π$-Flow achieves a 1-NFE FID of 2.85 on ImageNet 256$^2$, surpassing previous models of the same architecture, and shows significantly better diversity than leading DMD models on FLUX.1-12B and Qwen-Image-20B at 4 NFEs, all while maintaining high quality comparable to the teacher model.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于解决少步扩散或基于流的生成模型中的挑战，特别是复杂蒸馏过程导致的质量与多样性之间的权衡。作者提出了一种新方法，称为基于策略的流模型（π-Flow），该方法修改了学生流模型的输出层，以在一个时间步预测无网络策略，从而允许在未来的子步骤中以最小的开销产生动态流速。实验结果表明，π-Flow在ImageNet 256^2上实现了2.85的1-NFE FID，超越了相同架构的先前模型，并在FLUX.1-12B和Qwen-Image-20B上以4 NFEs相比于最先进的DMD模型显示出显著更好的多样性，同时保持高质量。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting Medical Visual Understanding From Multi-Granular Language Learning</div>
<div class="meta-line">Authors: Zihan Li, Yiqing Wang, Sina Farsiu, Paul Kinahan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-20T00:24:26+00:00 · Latest: 2026-02-19T18:27:29+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026. 40 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15943v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.15943v2">PDF</a> · <a href="https://github.com/HUANGLIZI/MGLL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多粒度语言学习提升医学视觉理解</div>
<div class="mono" style="margin-top:8px">最近在图像-文本预训练方面的进展显著增强了视觉理解，通过对齐视觉和文本表示。对比语言-图像预训练（CLIP）在多模态学习中发挥了关键作用。然而，它对单标签、单粒度对齐的关注限制了其在复杂领域（如医学影像）中的有效性，因为图像通常对应于多个高层标签（例如，疾病类别），并且在不同的注释粒度（例如，诊断描述、临床解释）之间存在差异。为了解决这个问题，我们提出了多粒度语言学习（MGLL），这是一种对比学习框架，旨在改善多标签和跨粒度对齐。MGLL利用结构化的多标签监督，整合不同粒度的文本描述，并引入具有点约束的软标签监督以增强对齐。MGLL采用平滑的Kullback-Leibler（KL）散度，以确保跨粒度一致性，同时保持计算效率，作为视觉-语言模型的即插即用模块。在我们构建的大规模多粒度数据集上进行预训练，并在多个数据集上进行评估，MGLL在下游任务中超越了其他最先进的方法。代码可在https://github.com/HUANGLIZI/MGLL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance visual understanding in medical imaging, which often involves complex, multi-label scenarios that traditional single-label models struggle to address. The authors propose a novel framework called Multi-Granular Language Learning (MGLL), which utilizes contrastive learning to improve alignment across multiple labels and granularities by incorporating structured multi-label supervision and soft-label supervision with point-wise constraints. Experimental results demonstrate that MGLL significantly outperforms existing state-of-the-art methods in various downstream tasks when evaluated on large-scale multi-granular datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升医学影像中的视觉理解能力，因为传统方法如CLIP由于专注于单标签对齐，难以应对复杂的多标签场景。作者提出了一种新的框架，称为多粒度语言学习（MGLL），该框架利用对比学习，通过整合结构化多标签监督和带有逐点约束的软标签监督来改善多标签和多粒度之间的对齐。实验结果表明，MGLL在多个下游任务中显著优于现有的最先进方法，验证了其在医学视觉理解中改善多标签和多粒度对齐的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models</div>
<div class="meta-line">Authors: Hojung Jung, Rodrigo Hormazabal, Jaehyeong Jo, Youngrok Park, Kyunggeun Roh, Se-Young Yun, Sehui Han, Dae-Woong Jeong</div>
<div class="meta-line">First: 2026-02-19T18:27:11+00:00 · Latest: 2026-02-19T18:27:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17602v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolHIT：利用层次离散扩散模型推进分子图生成</div>
<div class="mono" style="margin-top:8px">利用扩散模型进行分子生成已成为人工智能驱动的药物发现和材料科学的一个有前景的方向。尽管由于二维分子图的离散特性，图扩散模型已被广泛采用，但现有模型在化学有效性方面表现不佳，并且在满足所需属性方面相较于一维建模存在困难。在这项工作中，我们介绍了MolHIT，一个强大的分子图生成框架，克服了现有方法中的长期性能限制。MolHIT基于层次离散扩散模型，该模型将离散扩散推广到编码化学先验的其他类别，并采用解耦原子编码，根据化学角色划分原子类型。总体而言，MolHIT在MOSES数据集上首次实现了图扩散的新最先进性能，具有近乎完美的有效性，超越了多个指标上的强一维基线。我们进一步展示了在下游任务中的强大表现，包括多属性引导生成和骨架扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve molecular generation for AI-driven drug discovery and materials science, addressing the limitations of existing graph diffusion models that exhibit low chemical validity. The authors introduce MolHIT, a molecular graph generation framework that utilizes a Hierarchical Discrete Diffusion Model to incorporate chemical priors and employs decoupled atom encoding to differentiate atom types based on their chemical roles. Experimental results show that MolHIT achieves state-of-the-art performance on the MOSES dataset, demonstrating near-perfect validity in graph diffusion and outperforming strong 1D baselines across various metrics, while also excelling in downstream tasks such as multi-property guided generation and scaffold extension.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高用于人工智能驱动的药物发现和材料科学的分子生成，解决现有图扩散模型在化学有效性方面的局限性。作者提出了MolHIT，这是一种分子图生成框架，利用层次离散扩散模型，结合化学先验，并采用解耦原子编码以提高性能。实验结果表明，MolHIT在MOSES数据集上实现了最新的性能，展现出近乎完美的有效性，并在多个指标上超越了强大的1D基线，同时在多属性引导生成和骨架扩展等下游任务中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment</div>
<div class="meta-line">Authors: Ivan Rinaldi, Matteo Mendula, Nicola Fanelli, Florence Levé, Matteo Testi, Giovanna Castellano, Gennaro Vessio</div>
<div class="meta-line">First: 2026-02-19T18:23:58+00:00 · Latest: 2026-02-19T18:23:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Art2Mus：通过视觉条件和大规模跨模态对齐生成艺术作品音乐</div>
<div class="mono" style="margin-top:8px">音乐生成通过多模态深度学习显著进步，使模型能够从文本合成音频，最近也能从图像合成。然而，现有的图像条件系统存在两个基本限制：（i）它们通常在自然照片上训练，限制了捕捉艺术作品更丰富的语义、风格和文化内容的能力；（ii）大多数依赖于图像到文本的转换阶段，使用语言作为简化条件的语义捷径，但阻碍了直接的视觉到音频学习。基于这些缺口，我们引入了ArtSound，这是一个包含105,884对艺术作品-音乐的多模态大规模数据集，配有双模态标题，通过扩展ArtGraph和自由音乐档案获得。我们进一步提出ArtToMus，这是第一个专门设计用于直接艺术作品到音乐生成的框架，它将数字化艺术作品映射到音乐，而无需图像到文本的翻译或基于语言的语义监督。该框架将视觉嵌入投影到潜在扩散模型的条件空间，使音乐合成仅由视觉信息引导。实验结果表明，ArtToMus生成的输出在音乐上连贯且风格一致，反映了源艺术作品的显著视觉线索。尽管绝对对齐分数仍低于文本条件系统的分数——考虑到去除语言监督的难度显著增加——ArtToMus在感知质量和有意义的跨模态对应方面表现出竞争力。本研究确立了直接视觉到音乐生成作为一个独特且具有挑战性的研究方向，并提供了支持多媒体艺术、文化遗产和AI辅助创作实践应用的资源。代码和数据集将在接受后公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing image-conditioned music generation systems, which primarily rely on natural photographs and often use language as an intermediary for conditioning. The authors introduce ArtSound, a large-scale dataset of 105,884 artwork-music pairs with dual-modality captions, and propose ArtToMus, a novel framework for direct artwork-to-music generation that bypasses image-to-text translation. Experimental results demonstrate that ArtToMus produces musically coherent and stylistically consistent outputs that align with the visual characteristics of the artworks, achieving competitive perceptual quality despite lower absolute alignment scores compared to text-conditioned systems, thus highlighting the potential of direct visual-to-music generation as a new research avenue.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有的图像条件音乐生成系统的局限性，这些系统主要使用自然照片并依赖图像到文本的转换，从而妨碍了直接的视觉到音频学习。作者引入了ArtSound，这是一个包含105,884对艺术作品与音乐的双模态标题的大规模数据集，并提出了ArtToMus，一个用于直接艺术作品到音乐生成的框架，避免了基于语言的语义监督。实验结果表明，ArtToMus生成的音乐在风格上与艺术作品的视觉线索一致，尽管与文本条件系统相比，绝对对齐分数较低，但其感知质量具有竞争力，从而突显了直接视觉到音乐生成作为新研究方向的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?</div>
<div class="meta-line">Authors: Jayadev Billa</div>
<div class="meta-line">First: 2026-02-19T18:22:39+00:00 · Latest: 2026-02-19T18:22:39+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17598v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17598v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>级联等价假设：何时语音LLM表现得像ASR$\rightarrow$LLM管道？</div>
<div class="mono" style="margin-top:8px">当前的语音LLM在很大程度上执行隐式ASR：在可通过转录解决的任务中，它们在行为和机制上等同于简单的Whisper$\to$LLM级联。我们通过对四个语音LLM和六个任务进行匹配骨干测试，首次控制LLM骨干，展示了这一点。Ultravox在统计上与其匹配级联无显著区别（$κ{=}0.93$）；logit镜头揭示了隐藏状态中出现的字面文本；LEACE概念消除确认文本表示在测试的两种架构中都是因果必要的，准确率降至接近零。Qwen2-Audio确实出现了分歧，揭示了级联等价是依赖于架构的，而非普遍的。对于大多数已部署的用例，当前的语音LLM是昂贵的级联，并且在噪声下表现更差，在干净条件下的优势在0 dB时最多反转7.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the performance of speech large language models (LLMs) in comparison to traditional automatic speech recognition (ASR) followed by LLM pipelines, motivated by the need to understand their behavioral and mechanistic equivalence. The study employs matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone to assess their performance. The findings indicate that Ultravox behaves similarly to its matched cascade, with a high statistical correlation, while logit lens analysis shows that text representations are essential for both architectures; however, Qwen2-Audio demonstrates divergence, suggesting that the equivalence is architecture-dependent, and highlights that current speech LLMs may be inefficient under noisy conditions, with performance dropping significantly compared to clean conditions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了语音LLM与传统的ASR到LLM管道之间的行为，旨在理解它们的功能等价性。作者通过在四个语音LLM和六个任务中进行匹配骨干测试，控制LLM骨干以评估性能。研究结果表明，Ultravox的表现与其匹配的级联相似，具有较高的统计相关性，而logit lens分析显示文本表示对两种架构至关重要；然而，Qwen2-Audio表现出分歧，表明等价性依赖于所使用的架构。总体而言，研究得出结论，当前的语音LLM作为昂贵的级联存在，尤其在嘈杂条件下，其性能可能显著下降。</div>
</details>
</div>
<div class="card">
<div class="title">Asymptotic Smoothing of the Lipschitz Loss Landscape in Overparameterized One-Hidden-Layer ReLU Networks</div>
<div class="meta-line">Authors: Saveliy Baturin</div>
<div class="meta-line">First: 2026-02-19T18:20:21+00:00 · Latest: 2026-02-19T18:20:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17596v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17596v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the topology of the loss landscape of one-hidden-layer ReLU networks under overparameterization. On the theory side, we (i) prove that for convex $L$-Lipschitz losses with an $\ell_1$-regularized second layer, every pair of models at the same loss level can be connected by a continuous path within an arbitrarily small loss increase $ε$ (extending a known result for the quadratic loss); (ii) obtain an asymptotic upper bound on the energy gap $ε$ between local and global minima that vanishes as the width $m$ grows, implying that the landscape flattens and sublevel sets become connected in the limit. Empirically, on a synthetic Moons dataset and on the Wisconsin Breast Cancer dataset, we measure pairwise energy gaps via Dynamic String Sampling (DSS) and find that wider networks exhibit smaller gaps; in particular, a permutation test on the maximum gap yields $p_{perm}=0$, indicating a clear reduction in the barrier height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过参数化单隐层ReLU网络的Lipschitz损失景观的渐近平滑</div>
<div class="mono" style="margin-top:8px">我们研究了过参数化下单隐层ReLU网络的损失景观的拓扑结构。在理论方面，我们(i)证明了对于具有$L$-Lipschitz损失的凸函数，带有$\ell_1$正则化的第二层，任何一对在相同损失水平的模型可以通过一个连续路径连接，损失增加量可以任意小$ε$（扩展了已知的二次损失结果）；(ii)获得了局部和全局最小值之间的能量间隙$ε$的渐近上界，该间隙随着宽度$m$的增加而消失，意味着景观在极限情况下变得平坦，子水平集变得连通。在经验上，我们在合成的Moons数据集和威斯康星乳腺癌数据集上，通过动态字符串采样（DSS）测量成对的能量间隙，发现更宽的网络表现出更小的间隙；特别是，对最大间隙的置换检验得出$p_{perm}=0$，表明障碍高度明显降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the loss landscape topology of overparameterized one-hidden-layer ReLU networks, motivated by the need to understand model connectivity and optimization in such architectures. The authors prove that for convex L-Lipschitz losses with an ℓ1-regularized second layer, models at the same loss level can be connected by a continuous path with a minimal loss increase, and they derive an asymptotic upper bound on the energy gap between local and global minima that decreases as the network width increases. Experimental results on synthetic and real datasets demonstrate that wider networks have smaller pairwise energy gaps, with a permutation test confirming a significant reduction in barrier height between minima.</div>
<div class="mono" style="margin-top:8px">本研究探讨了过参数化的一层隐藏ReLU网络的损失景观拓扑，旨在理解高维环境中模型行为。作者证明，对于具有ℓ1正则化第二层的凸L-Lipschitz损失，相同损失水平的模型可以通过连续路径连接，且随着网络宽度的增加，局部和全局最小值之间的能量间隙的渐近上界逐渐减小。对合成数据集和真实数据集的实验结果表明，宽网络具有较小的能量间隙，排列检验确认了最小值之间障碍高度的显著降低，p值为零。</div>
</details>
</div>
<div class="card">
<div class="title">CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography</div>
<div class="meta-line">Authors: Qingqing Zhu, Qiao Jin, Tejas S. Mathai, Yin Fang, Zhizheng Wang, Yifan Yang, Maame Sarfo-Gyamfi, Benjamin Hou, Ran Gu, Praveen T. S. Balamuralikrishna, Kenneth C. Wang, Ronald M. Summers, Zhiyong Lu</div>
<div class="meta-line">First: 2026-02-16T16:10:19+00:00 · Latest: 2026-02-19T18:19:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14879v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.14879v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CT-Bench：计算机断层扫描中多模态病变理解的基准</div>
<div class="mono" style="margin-top:8px">人工智能（AI）可以自动描绘计算机断层扫描（CT）上的病变并生成放射学报告内容，但由于缺乏公开的带有病变级别注释的CT数据集，进展受到限制。为填补这一空白，我们推出了CT-Bench，这是首个此类基准数据集，包含两个部分：一个包含20,335个病变的病变图像和元数据集，来自7,795个CT研究，带有边界框、描述和大小信息，以及一个包含2,850个问答对的多任务视觉问答基准，涵盖病变定位、描述、大小估计和属性分类。包含困难的负例以反映现实世界的诊断挑战。我们评估了多种最先进的多模态模型，包括视觉-语言和医学CLIP变体，通过将其性能与放射科医生的评估进行比较，展示了CT-Bench作为病变分析综合基准的价值。此外，在病变图像和元数据集上微调模型在两个部分都带来了显著的性能提升，强调了CT-Bench的临床实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limited availability of publicly accessible CT datasets with detailed lesion annotations, which hinders the advancement of AI in medical imaging. The authors introduce CT-Bench, a benchmark dataset that includes a Lesion Image and Metadata Set with 20,335 lesions from 7,795 CT studies, along with a multitask visual question answering benchmark featuring 2,850 QA pairs related to lesion analysis. Experimental results show that state-of-the-art multimodal models, when evaluated against radiologist assessments, demonstrate improved performance after fine-tuning on the Lesion Image and Metadata Set, highlighting the dataset&#x27;s potential to enhance lesion understanding in clinical settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决缺乏公开的具有病灶级注释的CT数据集的问题，这限制了人工智能在自动化病灶描绘和放射学报告生成方面的进展。作者提出了CT-Bench，一个基准数据集，包括包含20,335个病灶的病灶图像和元数据集，来自7,795个CT研究，以及一个包含2,850个与病灶分析相关的问答对的多任务视觉问答基准。实验结果表明，将各种最先进的多模态模型与放射科医生的评估进行比较，显示了CT-Bench的有效性，并且在该数据集上对模型进行微调显著提高了性能，突显了其临床相关性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260222_0334.html">20260222_0334</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0348.html">20260220_0348</a>
<a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
