<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 03:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0339</div>
    <div class="row"><div class="card">
<div class="title">PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning</div>
<div class="meta-line">Authors: Jiaying Wu, Can Gao, Jinglu Hu, Hui Li, Xiaofeng Cao, Jingcai Guo</div>
<div class="meta-line">First: 2026-01-20T16:06:23+00:00 · Latest: 2026-01-20T16:06:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMCE：基于概率的多粒度语义与标题引导增强的少样本学习</div>
<div class="mono" style="margin-top:8px">少样本学习旨在仅从少量标记样本中识别新类别，其中从稀缺数据估计的原型往往存在偏差且泛化能力差。基于语义的方法通过引入粗略的类别级信息来缓解这一问题，但它们主要应用于支持侧，查询表示保持不变。本文提出了PMCE，一种利用多粒度语义与标题引导增强的概率少样本框架。PMCE构建了一个非参数知识库，存储每个类别的视觉统计信息以及基础类别的CLIP编码类名嵌入。在元测试时，根据每个新类别的类名嵌入的相似性检索最相关的基础类别。这些统计信息随后被聚合为类别特定的先验信息，并通过简单的MAP更新与支持集原型融合。同时，一个冻结的BLIP标题生成器提供无标签的实例级图像描述，而一个在基础类别上训练的轻量级增强器在归纳协议下优化支持原型和查询特征，并通过一致性正则化来稳定噪声标题。在四个基准上的实验表明，PMCE在强基线之上持续改进，在1-shot设置下在MiniImageNet上实现了相对于最强语义竞争者高达7.71%的绝对增益。我们的代码可在https://anonymous.4open.science/r/PMCE-275D获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot learning, which struggles with biased prototypes and poor generalization due to limited labeled samples. The authors propose PMCE, a probabilistic framework that utilizes multi-granularity semantics and caption-guided enhancement, constructing a nonparametric knowledge bank that stores visual statistics and class name embeddings. Experimental results demonstrate that PMCE significantly outperforms strong baselines, achieving an absolute gain of up to 7.71% over the best semantic competitor on the MiniImageNet dataset in the 1-shot setting.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善少样本学习，该领域因标记样本有限而面临偏倚原型和较差泛化能力的问题。作者提出了PMCE，这是一种利用多粒度语义和基于标题增强的概率框架，构建了一个结合视觉统计和类别名称嵌入的非参数知识库。实验结果表明，PMCE在MiniImageNet数据集的1-shot设置中，相较于最强的语义竞争者，取得了高达7.71%的绝对增益，超越了强基线。</div>
</details>
</div>
<div class="card">
<div class="title">Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model</div>
<div class="meta-line">Authors: Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu, Yuyang Zhang, Longxiang Gao, Shuaiguang Li, Xingyu Li, Yanran Xu, Changwei Wang</div>
<div class="meta-line">First: 2026-01-20T15:06:10+00:00 · Latest: 2026-01-20T15:06:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你也需要视觉：利用多模态大语言模型进行分布外检测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测是一项关键任务，受到了广泛关注。CLIP的出现推动了零样本OOD检测的广泛研究，通常采用无训练的方法。目前的方法利用大型语言模型（LLMs）的专家知识来识别潜在的异常值。然而，这些方法往往过于依赖文本空间的知识，忽视了在图像空间中检测分布外样本所面临的固有挑战。本文提出了一种新颖的管道MM-OOD，利用多模态大语言模型（MLLMs）的多模态推理能力及其进行多轮对话的能力来增强异常值检测。我们的方法旨在提高近OOD和远OOD任务的性能。具体而言，(1) 对于近OOD任务，我们直接将ID图像和相应的文本提示输入MLLMs以识别潜在的异常值；(2) 对于远OOD任务，我们引入了草图-生成-详细框架：首先，我们使用文本提示草拟异常值曝光，然后生成相应的视觉OOD样本，最后通过多模态提示进行详细说明。实验表明，我们的方法在广泛使用的多模态数据集（如Food-101）上取得了显著的改进，同时验证了其在ImageNet-1K上的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of out-of-distribution (OOD) detection, particularly in the context of multimodal data where existing methods often rely heavily on textual information. The authors propose a novel pipeline called MM-OOD that utilizes the multimodal reasoning capabilities of large language models (LLMs) to enhance outlier detection through multi-round conversations. Experimental results show that MM-OOD significantly improves performance on near OOD and far OOD tasks, achieving notable advancements on datasets like Food-101 and demonstrating scalability on ImageNet-1K.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决分布外（OOD）检测的挑战，特别是在多模态数据的背景下，现有方法往往过于依赖文本信息。作者提出了一种名为MM-OOD的新颖管道，利用大型语言模型（LLM）的多模态推理能力，通过多轮对话方法增强异常值检测。实验结果表明，MM-OOD在近OOD和远OOD任务上显著提高了性能，在Food-101等数据集上取得了显著进展，并在ImageNet-1K上展示了可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3</div>
<div class="meta-line">Authors: Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li</div>
<div class="meta-line">First: 2026-01-20T12:25:41+00:00 · Latest: 2026-01-20T12:25:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniOVCD：利用SAM 3 简化开放词汇变化检测</div>
<div class="mono" style="margin-top:8px">变化检测（CD）是遥感中的一项基本任务，监测土地覆盖的演变。基于此，开放词汇变化检测（OVCD）引入了新的要求，旨在减少对预定义类别的依赖。现有的无训练OVCD方法主要使用CLIP来识别类别，这些方法还需要额外的模型如DINO来提取特征。然而，结合不同模型往往会导致特征匹配问题，使系统不稳定。最近，推出了Segment Anything Model 3（SAM 3），它将分割和识别能力集成在一个可提示模型中，为OVCD任务提供了新的可能性。本文提出了OmniOVCD，一个专为OVCD设计的独立框架。通过利用SAM 3的解耦输出头，我们提出了一种协同融合到实例解耦（SFID）策略。SFID首先融合SAM 3的语义、实例和存在输出以构建土地覆盖掩膜，然后将其分解为单个实例掩膜以进行变化比较。该设计在类别识别中保持高准确性，并在图像间保持实例级一致性。因此，模型能够生成准确的变化掩膜。在四个公共基准（LEVIR-CD、WHU-CD、S2Looking和SECOND）上的实验表明，模型达到了SOTA性能，分别实现了67.2、66.5、24.5和27.1（类别平均）的IoU分数，超越了所有先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Open-Vocabulary Change Detection (OVCD) in remote sensing by reducing reliance on predefined categories. The authors propose OmniOVCD, a standalone framework that utilizes the Segment Anything Model 3 (SAM 3) to streamline the change detection process. By implementing a Synergistic Fusion to Instance Decoupling (SFID) strategy, the framework effectively fuses and decouples outputs from SAM 3 to create accurate land-cover masks and individual instance masks for change comparison. Experimental results on four public benchmarks demonstrate state-of-the-art performance, achieving intersection over union (IoU) scores of 67.2, 66.5, 24.5, and 27.1, respectively, outperforming all previous methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过减少对预定义类别的依赖来增强开放词汇变化检测（OVCD）在遥感中的应用。作者提出了OmniOVCD，一个独立的框架，利用了Segment Anything Model 3（SAM 3），并提出了一种协同融合到实例解耦（SFID）策略，该策略首先结合SAM 3的输出以创建土地覆盖掩膜，然后将其分解为单个实例掩膜进行变化检测。在四个公共基准上的实验结果表明，OmniOVCD实现了最先进的性能，交并比（IoU）得分分别为67.2、66.5、24.5和27.1，超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍然存在分歧：视觉-语言模型（如CLIP）在全局语义对齐方面表现出色，但缺乏空间精度，而自监督方法（如MAE、DINO）捕捉复杂的局部结构，但在高层语义上下文中表现不佳。我们认为这些范式在根本上是互补的，可以整合到一个有原则的多任务框架中，并通过密集的空间监督进一步增强。我们介绍了MTV，一个多任务视觉预训练框架，联合优化视觉-语言对比、自监督和密集空间目标的共享主干。为了减少对手动标注的需求，我们利用高容量的“专家”模型——如Depth Anything V2和OWLv2——在大规模上合成密集的、结构化的伪标签。除了框架外，我们还系统地研究了多任务视觉学习的机制，分析：（i）每个目标的边际收益，（ii）任务协同与干扰，以及（iii）在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了“兼具两全其美”的性能，显著增强了细粒度空间推理，而不妨碍全局语义理解。我们的发现表明，多任务学习在高质量伪监督的推动下，是通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current visual representation learning methods, which are divided between vision-language models that excel in semantic alignment but lack spatial precision, and self-supervised methods that capture local structures but struggle with high-level semantics. The authors propose a multi-task visual pretraining framework called MTV, which integrates vision-language contrastive, self-supervised, and dense spatial objectives while utilizing high-capacity expert models to generate structured pseudo-labels without manual annotations. Experimental results indicate that MTV achieves superior performance in fine-grained spatial reasoning and maintains strong global semantic understanding, demonstrating that multi-task learning with high-quality pseudo-supervision can lead to more effective visual encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前视觉表征学习方法的局限性，这些方法通常分为在语义对齐方面表现出色但缺乏空间精度的视觉-语言模型，以及捕捉局部结构但在高层语义上表现不佳的自监督方法。作者提出了一种名为MTV的多任务视觉预训练框架，该框架整合了视觉-语言对比、自监督和密集空间目标，同时利用高容量专家模型生成结构化伪标签，避免了手动标注。实验结果表明，MTV显著提高了细粒度空间推理能力，并保持了强大的全局语义理解，证明了高质量伪监督的多任务学习能够增强视觉编码器的可扩展性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive Generative Augmentation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Wei Liu, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-20T08:09:38+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v3">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应生成增强与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两项协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样但语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它使MoCov2在ImageNet线性分类上提高了2.5%。在视觉-语言学习中，它在十个数据集上使平均零-shot分类准确率提高了12.31%（相较于CLIP）和5.31%（相较于SLIP），并进一步提高了Flickr30k文本检索R@5的表现，提升幅度为3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for high-quality positive pairs in contrastive learning, which current methods struggle to construct due to limited diversity and semantic corruption. To address these issues, the authors propose GenView++, a unified framework that employs a multi-source adaptive view generation mechanism for creating diverse and semantically coherent views, alongside a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ enhances MoCov2 by 2.5% on ImageNet linear classification and improves zero-shot classification accuracy by 12.31% over CLIP and 5.31% over SLIP across ten datasets, while also increasing Flickr30k text retrieval R@5 by 3.2%.</div>
<div class="mono" style="margin-top:8px">本研究解决了当前对比学习方法在构建高质量正样本对方面的局限性，并缺乏评估样本对质量的机制。作者提出了GenView++，一个统一框架，通过多源自适应视图生成机制来增强样本对构建，创造出多样且语义一致的视图。此外，提出了一种质量驱动的对比学习机制，用于评估每个样本对的语义对齐和多样性，并动态调整其训练贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的性能2.5%，在十个数据集上使零-shot分类准确率比CLIP提高了12.31%，比SLIP提高了5.31%，同时在Flickr30k文本检索R@5上提升了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</div>
<div class="meta-line">Authors: Donghee Lee, Rui Cai, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-20T05:44:33+00:00 · Latest: 2026-01-20T05:44:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13622v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARPE：通过集成实现的大型视觉语言模型的上下文感知图像表示优先级</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）的最新进展使其更接近成为通用助手。尽管表现强劲，LVLMs在图像分类等以视觉为中心的任务上仍然存在困难，表现不及其基础视觉编码器，后者通常是基于CLIP的模型。为了解决这一限制，我们提出了上下文感知图像表示优先级通过集成（CARPE），这是一种新颖的模型无关框架，引入了视觉集成层和上下文感知集成策略，以识别何时优先考虑图像表示或依赖语言模型的推理能力。该设计增强了模型自适应加权视觉和文本模态的能力，使模型能够捕捉图像表示的各个方面，从而在分类和视觉语言基准测试中实现一致的泛化改进。大量实验表明，CARPE不仅提高了图像分类基准的性能，还增强了各种视觉语言基准的结果。最后，CARPE旨在与大多数由视觉编码器和语言模型组成的开源LVLMs有效集成，确保其在多种架构中的适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Large Vision-Language Models (LVLMs) in vision-centric tasks, where they currently lag behind traditional vision encoders like CLIP. The authors introduce CARPE, a model-agnostic framework that employs vision-integration layers and a context-aware ensemble strategy to determine when to prioritize image representations over language reasoning. Experimental results show that CARPE significantly improves performance on image classification benchmarks and enhances results across various vision-language tasks, demonstrating its adaptability with existing LVLM architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型视觉语言模型（LVLMs）在视觉中心任务中的表现，而这些任务目前仍落后于传统的视觉编码器，如CLIP。作者提出了一种名为上下文感知图像表示优先级排序的集成框架（CARPE），该框架结合了视觉集成层和上下文感知的集成策略，以优化图像表示的使用和语言模型的推理能力。实验结果表明，CARPE显著提高了图像分类基准的性能，并改善了各种视觉语言任务的泛化能力，证明了其有效性和与现有LVLM架构的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</div>
<div class="meta-line">Authors: Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour</div>
<div class="meta-line">First: 2025-09-03T17:56:46+00:00 · Latest: 2026-01-19T23:50:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03515v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.03515v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Waymo开放运动数据集能否支持现实行为建模？基于自然轨迹的验证研究</div>
<div class="mono" style="margin-top:8px">Waymo开放运动数据集（WOMD）已成为自主车辆（AV）行为数据驱动建模的热门资源。然而，由于专有后处理、缺乏误差量化以及将轨迹分割为20秒片段，其在行为分析中的有效性仍不确定。本研究考察WOMD是否准确捕捉到现实世界AV操作中的动态和交互。利用在亚利桑那州凤凰城（PHX）进行的4级AV操作独立收集的自然数据集，我们在三个典型城市驾驶场景中进行比较分析：在信号交叉口的卸载、跟车和变道行为。在卸载分析中，手动从航拍视频中提取车距，以确保测量误差微乎其微。对于跟车和变道情况，我们应用模拟外推（SIMEX）方法来考虑PHX数据中经验估计的误差，并使用动态时间规整（DTW）距离量化行为差异。所有场景的结果一致表明，PHX中的行为超出了WOMD的行为范围。值得注意的是，WOMD低估了短车距和突然减速。这些发现表明，仅基于WOMD校准的行为模型可能系统性地低估自然驾驶的变异性、风险和复杂性。因此，在没有与独立收集的数据进行适当验证的情况下，使用WOMD进行行为建模时应谨慎。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to assess the validity of the Waymo Open Motion Dataset (WOMD) for behavioral modeling of autonomous vehicles, given concerns about its proprietary processing and lack of error quantification. The researchers conducted comparative analyses using an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona, focusing on three urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. The results indicate that the behaviors observed in Phoenix significantly differ from those in WOMD, with WOMD notably underrepresenting short headways and abrupt decelerations, suggesting that models based solely on WOMD may underestimate the complexities of real-world driving behavior.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Waymo开放运动数据集（WOMD）在建模自动驾驶汽车行为方面的有效性，动机是对其专有处理和缺乏误差量化的担忧。研究人员使用来自亚利桑那州凤凰城的独立收集的自然驾驶数据集，比较了城市驾驶场景，包括在交叉口放客、跟车和变道的行为。结果显示，凤凰城观察到的行为与WOMD中的行为显著不同，WOMD明显低估了短车距和突然减速的情况，这表明仅基于WOMD的模型可能低估现实驾驶场景的复杂性和风险。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</div>
<div class="meta-line">Authors: Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</div>
<div class="meta-line">First: 2026-01-19T22:55:30+00:00 · Latest: 2026-01-19T22:55:30+00:00</div>
<div class="meta-line">Comments: 10 pages,4 images</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13440v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的异常分类与分割方法分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs），特别是CLIP，通过实现零样本和少样本缺陷识别，彻底改变了异常检测，无需大量标注数据集。通过学习图像和文本的对齐表示，VLMs通过自然语言描述正常和异常状态，促进异常分类和分割，消除了对特定任务训练或缺陷示例的传统要求。本项目对基于VLM的方法进行全面分析，涵盖异常分类（AC）和异常分割（AS）。我们系统地研究了关键架构范式，包括基于滑动窗口的密集特征提取（WinCLIP）、具有可学习投影的多阶段特征对齐（AprilLab框架）和组合提示集成策略。我们的分析在关键维度上评估这些方法：特征提取机制、文本-视觉对齐策略、提示工程技术、零样本与少样本的权衡、计算效率和跨领域泛化。通过在MVTec AD和VisA等基准上的严格实验，我们比较了分类准确性、分割精度和推理效率。主要贡献是对VLMs在异常检测中成功的基础理解，综合了方法选择的实用见解并识别当前的局限性。本研究旨在促进VLM方法在工业质量控制中的知情采用，并指导未来的研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance anomaly detection using Vision-Language Models (VLMs), particularly CLIP, which allow for zero-shot and few-shot defect identification without the need for extensive labeled datasets. The study employs a comprehensive analysis of various VLM-based approaches for anomaly classification and segmentation, systematically investigating architectural paradigms such as sliding window-based dense feature extraction, multi-stage feature alignment, and compositional prompt ensemble strategies. Key experimental findings reveal insights into feature extraction mechanisms, text-visual alignment strategies, and the trade-offs between zero-shot and few-shot methods, demonstrating improvements in classification accuracy, segmentation precision, and inference efficiency on benchmarks like MVTec AD and VisA, ultimately contributing to a foundational understanding of VLMs in anomaly detection and guiding future research and industrial applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索视觉语言模型（VLMs），特别是CLIP，在增强异常检测方面的能力，而无需大量标注数据集。该研究对多种基于VLM的方法进行全面分析，考察了如WinCLIP和AprilLab框架等架构范式，以及提示工程策略。关键实验结果表明，这些方法在MVTec AD和VisA等基准上有效提高了分类准确性和分割精度，同时突出了零样本学习与少样本学习之间的权衡、计算效率和跨域泛化，最终为工业质量控制的实际应用和未来研究提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations</div>
<div class="meta-line">Authors: Tim Lachmann, Alexandra Israelsson, Christina Tornberg, Teimuraz Saghinadze, Michal Balazia, Philipp Müller, Petri Laukka</div>
<div class="meta-line">First: 2026-01-19T16:59:45+00:00 · Latest: 2026-01-19T16:59:45+00:00</div>
<div class="meta-line">Comments: Accepted for publication at IEEE Face &amp; Gesture 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有混合情感都是相等的：带有相对显著性注释的混合情感表达BLEMORE数据集</div>
<div class="mono" style="margin-top:8px">人类常常同时体验多种情感的混合，而不仅仅是单一的基本情感，这些情感的显著性各不相同。尽管混合情感的重要性不言而喻，但大多数基于视频的情感识别方法仅设计用于识别单一情感。少数尝试识别混合情感的方法通常无法评估混合情感中各情感的相对显著性。这一局限性主要源于缺乏包含大量带有相对显著性注释的混合情感样本的数据集。为了解决这一问题，我们推出了BLEMORE，这是一个用于多模态（视频、音频）混合情感识别的新数据集，包含每种情感在混合中的相对显著性信息。BLEMORE包含来自58位演员的3000多个片段，表演6种基本情感和10种不同的混合情感，每种混合情感有3种不同的显著性配置（50/50、70/30和30/70）。利用该数据集，我们对两项混合情感预测任务进行了广泛评估： (1) 预测给定样本中情感的存在， (2) 预测混合情感中情感的相对显著性。我们的结果表明，单模态分类器在验证集上实现了最高29%的存在准确率和13%的显著性准确率，而多模态方法则明显改善，ImageBind + WavLM达到了35%的存在准确率，HiCMAE达到了18%的显著性准确率。在保留的测试集上，最佳模型实现了33%的存在准确率（VideoMAEv2 + HuBERT）和18%的显著性准确率（HiCMAE）。总之，BLEMORE数据集为推进考虑混合情感表达复杂性和重要性的情感识别系统研究提供了宝贵资源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve emotion recognition systems by addressing the complexity of blended emotions, which are often experienced in real life but inadequately represented in existing datasets. The authors introduce the BLEMORE dataset, which includes over 3,000 video and audio clips featuring 58 actors expressing six basic emotions and ten distinct blends, each annotated with relative salience configurations. Experimental results indicate that while unimodal classifiers achieve up to 29% accuracy in detecting the presence of emotions, multimodal methods significantly enhance performance, with the best models reaching 35% presence accuracy and 18% salience accuracy on the test set.</div>
<div class="mono" style="margin-top:8px">本研究解决了情感识别系统通常只关注单一情感的不足，而人类常常体验到具有不同显著性的混合情感。为了解决这一问题，作者引入了BLEMORE数据集，该数据集包含3000多个视频和音频片段，展示了58名演员表达6种基本情感和10种不同的混合情感，每种混合情感都带有相对显著性配置的注释。实验结果表明，单模态分类器在情感存在性预测中最高可达29%的准确率，而在显著性预测中为13%；而多模态方法表现更佳，最佳模型在验证集上达到35%的情感存在性准确率和18%的显著性准确率，突显了该数据集在提升情感识别研究中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</div>
<div class="meta-line">Authors: Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan</div>
<div class="meta-line">First: 2026-01-19T15:19:28+00:00 · Latest: 2026-01-19T15:19:28+00:00</div>
<div class="meta-line">Comments: Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的可适应自监督学习用于以人为中心的视觉任务</div>
<div class="mono" style="margin-top:8px">以人为中心的视觉分析在监控、医疗保健和人机交互等多种应用中发挥着关键作用。随着大规模无标签人类图像数据集的出现，对能够支持多样化以人为中心的下游任务的一般无监督预训练模型的需求日益增加。为实现这一目标，我们提出了CLASP（基于CLIP的可适应自监督学习），这是一个旨在进行以人为中心的视觉任务无监督预训练的新框架。CLASP利用强大的视觉-语言模型CLIP生成低级（例如，身体部位）和高级（例如，属性）语义伪标签。这些多层次的语义线索被整合到学习的视觉表示中，丰富了其表现力和泛化能力。考虑到不同的下游任务对语义粒度的需求不同，CLASP结合了一个提示控制的专家混合（MoE）模块。MoE根据特定任务的提示动态调整特征提取，减轻潜在的特征冲突并增强可迁移性。此外，CLASP采用多任务预训练策略，其中来自CLIP的部分和属性级伪标签指导表示学习过程。在多个基准上的广泛实验表明，CLASP始终优于现有的无监督预训练方法，推动了以人为中心的视觉分析领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the need for a general unsupervised pre-training model that can support various human-centric visual tasks, given the rise of large-scale unlabeled human image datasets. The authors propose CLASP, a framework that utilizes the CLIP vision-language model to generate multi-level semantic pseudo-labels, which are integrated into visual representations to enhance their expressiveness. Experimental results show that CLASP, through its Prompt-Controlled Mixture-of-Experts module and multi-task pre-training strategy, consistently outperforms existing unsupervised pre-training methods across multiple benchmarks, thereby advancing human-centric visual analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是针对随着大规模无标签人类图像数据集的出现，对能够有效支持各种以人为中心的视觉任务的一般无监督预训练模型的需求。作者提出了CLASP框架，该框架利用CLIP视觉-语言模型生成多层次的语义伪标签，并将其整合到视觉表示中，以增强其表现力。实验结果表明，CLASP在多个基准测试中优于现有的无监督预训练方法，证明了其在改善以人为中心的视觉分析任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</div>
<div class="meta-line">Authors: Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</div>
<div class="meta-line">First: 2025-10-11T10:17:38+00:00 · Latest: 2026-01-19T15:00:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10150v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process carries a critical risk: entropy collapse. This phenomenon is a rapid decrease in policy entropy, which severely limits exploration and diminishes learning effectiveness. Recent methods attempt to mitigate this collapse via heuristic entropy interventions, yet the underlying mechanisms governing entropy remain unclear. In this work, we conduct a theoretical and quantitative analysis of GRPO&#x27;s entropy dynamics, revealing that token-level entropy change in each update step is jointly governed by four key factors: clipping strategy, advantage, token probability, and token entropy. These findings not only explain the mechanisms of existing methods, but also reveal their limitations: they rely on heuristic adjustments to only one or two factors, leaving other relevant factors unconsidered and reducing their effectiveness. This motivates us to propose a new method, STEER, which adaptively reweights tokens based on their estimated entropy change to regulate entropy in a principled manner. Experiments on both math and coding benchmarks demonstrate that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考RLVR中的熵干预：熵变化视角</div>
<div class="mono" style="margin-top:8px">尽管可验证奖励的强化学习（RLVR）可以增强大型语言模型（LLM）的推理能力，但其训练过程存在一个关键风险：熵崩溃。这一现象是策略熵的快速下降，严重限制了探索并降低了学习效果。最近的方法试图通过启发式熵干预来减轻这种崩溃，但控制熵的基本机制仍不清楚。在本研究中，我们对GRPO的熵动态进行了理论和定量分析，揭示了每个更新步骤中令牌级熵变化由四个关键因素共同控制：裁剪策略、优势、令牌概率和令牌熵。这些发现不仅解释了现有方法的机制，还揭示了它们的局限性：它们依赖于对一两个因素的启发式调整，忽略了其他相关因素，从而降低了有效性。这促使我们提出了一种新方法STEER，该方法根据估计的熵变化自适应地重新加权令牌，以原则性地调节熵。在数学和编码基准上的实验表明，STEER有效减轻了熵崩溃，并始终优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the critical issue of entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR), which hampers exploration and learning effectiveness. The authors conduct a theoretical and quantitative analysis of the entropy dynamics in GRPO, identifying four key factors that influence token-level entropy change during updates. Their findings highlight the limitations of existing heuristic methods that only consider a subset of these factors. In response, they propose a new method called STEER, which adaptively reweights tokens based on their estimated entropy change, and experimental results on math and coding benchmarks show that STEER effectively mitigates entropy collapse and outperforms current state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决可验证奖励的强化学习（RLVR）中熵崩溃的关键问题，这一问题妨碍了探索和学习的有效性。作者对GRPO中的熵动态进行了理论和定量分析，识别出在更新过程中影响令牌级熵变化的四个关键因素。研究结果突显了现有启发式方法的局限性，这些方法仅调整一两个因素，因此促使开发了一种新的方法STEER，该方法根据估计的熵变化自适应地重新加权令牌。数学和编码基准上的实验结果表明，STEER有效地缓解了熵崩溃，并超越了当前的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Proxy Robustness in Vision Language Models is Effortlessly Transferable</div>
<div class="meta-line">Authors: Xiaowei Fu, Fuxiang Huang, Lei Zhang</div>
<div class="meta-line">First: 2026-01-19T09:23:11+00:00 · Latest: 2026-01-19T09:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12865v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12865v1">PDF</a> · <a href="http://github.com/fxw13/HPT-GPD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的代理鲁棒性易于转移</div>
<div class="mono" style="margin-top:8px">作为提高深度模型防御能力的关键技术，通过蒸馏实现对抗鲁棒性转移在传统图像分类任务中取得了显著成功。然而，当应用于视觉语言模型（VLM）（例如CLIP）时，这一范式面临重大挑战：为大规模多模态模型构建对抗鲁棒教师需要极高的计算资源。我们通过揭示一个有趣的现象来弥补这一差距：普通CLIP（未经过对抗训练）对由不同架构的另一个CLIP生成的对抗样本表现出内在的防御能力。我们正式将其定义为代理对抗鲁棒性，并自然提出了一个异构代理转移（HPT）框架，该框架在CLIP变体之间建立跨架构鲁棒性蒸馏通道，轻松实现从代理到目标模型的VLM鲁棒性转移。然而，这种代理转移范式容易导致严重的过拟合，导致零-shot自然泛化的急剧下降。为了解决这个问题，我们通过利用学习率调度的差异设计了泛化-支点解耦（GPD）。这将代理转移过程解耦为一个保持泛化的泛化锚定预热和一个促进对抗鲁棒性的泛化拉动HPT，以实现自然泛化与对抗鲁棒性之间的平衡。在15个零-shot数据集上的大量实验证明了我们HPT-GPD方法的有效性。代码可在github.com/fxw13/HPT-GPD网站上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enhancing adversarial robustness in vision-language models (VLMs) like CLIP, which traditionally require significant computational resources for constructing robust teachers. The authors introduce a Heterogeneous Proxy Transfer (HPT) framework that leverages the intrinsic defensive capabilities of vanilla CLIP against adversarial examples from differently architected CLIPs, facilitating robustness transfer without extensive resources. To mitigate overfitting and maintain zero-shot natural generalization, they propose Generalization-Pivot Decoupling (GPD), which balances the warm-up phase for generalization with the HPT process for adversarial robustness. Experimental results across 15 zero-shot datasets validate the effectiveness of the HPT-GPD method.</div>
<div class="mono" style="margin-top:8px">本研究解决了增强视觉语言模型（VLM）对抗鲁棒性所面临的挑战，这通常需要大量计算资源来构建鲁棒教师。作者提出了一种异构代理转移（HPT）框架，利用普通CLIP模型对不同CLIP架构生成的对抗样本的内在防御能力。为了减轻过拟合并改善零-shot自然泛化，他们提出了泛化-支点解耦（GPD），将代理转移过程分为一个用于泛化的热身阶段和一个用于对抗鲁棒性的后续阶段。对15个零-shot数据集的实验结果验证了HPT-GPD方法在自然泛化与对抗鲁棒性之间实现平衡的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</div>
<div class="meta-line">Authors: Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</div>
<div class="meta-line">First: 2026-01-19T08:16:11+00:00 · Latest: 2026-01-19T08:16:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成空间关系数据训练的CLIP风格视觉-语言模型中的左右对称破缺</div>
<div class="mono" style="margin-top:8px">空间理解仍然是视觉-语言模型中的一个关键挑战。然而，目前尚不清楚这种理解是否真正获得，以及如果获得，是通过什么机制。我们提出了一个可控的1D图像-文本测试平台，以探究在使用CLIP风格对比目标训练的基于Transformer的视觉和文本编码器中，左右关系理解是如何出现的。我们在一对一和二物体场景的描述上端到端训练轻量级的基于Transformer的视觉和文本编码器，并在系统性变化标签和布局多样性的同时评估对未见物体对的泛化能力。我们发现对比训练学习了左右关系，并且在这种情况下，标签多样性比布局多样性更是泛化的主要驱动因素。为了获得机制理解，我们进行了注意力分解，显示位置和标记嵌入之间的交互引发了一个水平注意力梯度，破坏了编码器中的左右对称性；消除这一贡献会显著降低左右区分能力。我们的结果提供了CLIP风格模型何时以及如何获得关系能力的机制性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the mechanisms behind spatial understanding in vision-language models, particularly focusing on left-right relational understanding. The authors developed a controllable 1D image-text testbed and trained lightweight Transformer-based encoders on paired descriptions of one- and two-object scenes using a CLIP-style contrastive objective. The findings reveal that contrastive training effectively learns left-right relations, with label diversity being the key factor driving generalization, while an attention decomposition analysis indicates that interactions between positional and token embeddings create a horizontal attention gradient that disrupts left-right symmetry, highlighting the conditions under which these models develop relational competence.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型中空间理解的机制，特别关注左右关系理解。作者开发了一个可控的1D图像-文本测试平台，并使用CLIP风格的对比目标，训练了轻量级的Transformer编码器，以配对的单物体和双物体场景描述为基础。研究结果表明，对比训练有效地学习了左右关系，其中标签多样性是推动泛化的关键因素，而注意力分解表明，位置嵌入和标记嵌入之间的相互作用产生了破坏左右对称的水平注意力梯度，显著影响了区分能力。</div>
</details>
</div>
<div class="card">
<div class="title">Open Vocabulary Panoptic Segmentation With Retrieval Augmentation</div>
<div class="meta-line">Authors: Nafis Sadeq, Qingfeng Liu, Mostafa El-Khamy</div>
<div class="meta-line">First: 2026-01-19T07:16:45+00:00 · Latest: 2026-01-19T07:16:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带检索增强的开放词汇全景分割</div>
<div class="mono" style="margin-top:8px">给定输入图像和类别名称集合，全景分割旨在为图像中的每个像素标注类别标签和实例标签。相比之下，开放词汇全景分割旨在根据用户输入促进任意类别的分割。挑战在于，训练于特定数据集的全景分割系统通常无法很好地推广到训练数据之外的未见类别。在这项工作中，我们提出了RetCLIP，一种检索增强的全景分割方法，旨在提高未见类别的性能。具体而言，我们使用配对的图像-文本数据构建了一个掩码段特征数据库。在推理时，我们使用输入图像的掩码段特征作为查询键，从数据库中检索相似特征和相关类别标签。掩码段的分类分数基于查询特征和检索特征之间的相似性进行分配。基于检索的分类分数与基于CLIP的分数结合，以生成最终输出。我们将我们的解决方案与之前的SOTA方法（FC-CLIP）结合。当在COCO上训练时，所提方法在ADE20k数据集上表现出30.9 PQ，19.3 mAP，44.0 mIoU，相较于基线实现了+4.5 PQ，+2.5 mAP，+10.0 mIoU的绝对提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance open vocabulary panoptic segmentation, which struggles to generalize to unseen classes beyond the training dataset. The authors propose a method called RetCLIP, which utilizes a retrieval-augmented approach by constructing a masked segment feature database from paired image-text data. Experimental results show that when trained on the COCO dataset, RetCLIP achieves significant improvements on the ADE20k dataset, with scores of 30.9 PQ, 19.3 mAP, and 44.0 mIoU, resulting in absolute gains of +4.5 PQ, +2.5 mAP, and +10.0 mIoU over the baseline method FC-CLIP.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使基于用户输入的任意类别分割成为可能，来增强全景分割，解决现有系统在处理未见类别时的局限性。作者提出了一种名为RetCLIP的方法，该方法通过从配对的图像-文本数据构建掩码段特征数据库，利用检索增强的方法。在COCO数据集上训练后，RetCLIP在ADE20k数据集上取得了显著的改进，得分为30.9 PQ、19.3 mAP和44.0 mIoU，相较于基线方法FC-CLIP，分别实现了+4.5 PQ、+2.5 mAP和+10.0 mIoU的绝对提升。</div>
</details>
</div>
<div class="card">
<div class="title">Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval</div>
<div class="meta-line">Authors: Zequn Xie, Boyun Zhang, Yuxiao Lin, Tao Jin</div>
<div class="meta-line">First: 2026-01-19T06:55:33+00:00 · Latest: 2026-01-19T06:55:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12768v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12768v1">PDF</a> · <a href="https://github.com/boyun-zhang/HVP-Net">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&#x27;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入探讨：用于鲁棒视频-文本检索的层次视觉感知</div>
<div class="mono" style="margin-top:8px">视频-文本检索（VTR）旨在使用自然语言查询定位相关视频。当前的方法通常基于预训练模型如CLIP，但受到视频固有冗余和对粗糙最终层特征的依赖，限制了匹配精度。为了解决这个问题，我们引入了HVP-Net（层次视觉感知网络），这是一个通过从视觉编码器的多个中间层提取和精炼特征来挖掘更丰富视频语义的框架。我们的方法逐步从不同语义层次的原始补丁标记中提炼显著的视觉概念，减轻冗余，同时保留对齐所需的关键细节。这导致了更鲁棒的视频表示，在包括MSRVTT、DiDeMo和ActivityNet在内的挑战性基准上实现了新的最先进性能。我们的工作验证了利用层次特征推进视频-文本检索的有效性。我们的代码可在https://github.com/boyun-zhang/HVP-Net获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of video-text retrieval (VTR), which is often limited by the redundancy in videos and the reliance on coarse features from pre-trained models like CLIP. The authors propose the HVP-Net (Hierarchical Visual Perception Network), which extracts and refines features from multiple intermediate layers of a vision encoder to capture richer video semantics. Experimental results demonstrate that this method significantly enhances video representation, achieving state-of-the-art performance on benchmarks such as MSRVTT, DiDeMo, and ActivityNet, thereby validating the effectiveness of hierarchical feature exploitation in VTR tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视频文本检索（VTR）的准确性，目前的技术受到视频冗余和对预训练模型（如CLIP）粗糙特征依赖的限制。作者提出了HVP-Net（分层视觉感知网络），该网络从视觉编码器的多个中间层提取和精炼特征，以捕捉更丰富的视频语义。实验结果表明，该方法显著增强了视频表示，并在MSRVTT、DiDeMo和ActivityNet等基准测试中达到了最先进的性能，证实了在VTR任务中利用分层特征的优势。</div>
</details>
</div>
<div class="card">
<div class="title">DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition</div>
<div class="meta-line">Authors: Hanyu Zhu, Zhihao Zhan, Yuhang Ming, Liang Li, Dibo Hou, Javier Civera, Wanzeng Kong</div>
<div class="meta-line">First: 2026-01-19T05:19:56+00:00 · Latest: 2026-01-19T05:19:56+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DC-VLAQ：用于鲁棒视觉位置识别的查询残差聚合</div>
<div class="mono" style="margin-top:8px">视觉位置识别（VPR）中的一个核心挑战是学习一个在大视角变化、光照变化和严重领域转移下仍然具有区分性的鲁棒全局表示。虽然视觉基础模型（VFM）提供了强大的局部特征，但大多数现有方法依赖于单一模型，忽视了不同VFM提供的互补线索。然而，利用这些互补信息不可避免地改变了标记分布，这对现有基于查询的全局聚合方案的稳定性构成了挑战。为了解决这些挑战，我们提出了DC-VLAQ，一个以表示为中心的框架，集成了互补VFM的融合和鲁棒的全局聚合。具体而言，我们首先引入了一种轻量级的残差引导互补融合，将表示锚定在DINOv2特征空间，同时通过学习的残差校正注入来自CLIP的互补语义。此外，我们提出了局部聚合查询向量（VLAQ），这是一种查询-残差全局聚合方案，通过其对可学习查询的残差响应对局部标记进行编码，从而提高了稳定性并保留了细粒度的区分线索。在标准VPR基准测试（包括Pitts30k、Tokyo24/7、MSLS、Nordland、SPED和AmsterTime）上的大量实验表明，DC-VLAQ始终优于强基线，并在具有挑战性的领域转移和长期外观变化下实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance visual place recognition (VPR) by developing a robust global representation that can withstand significant changes in viewpoint, illumination, and domain shifts. The authors propose a novel framework called DC-VLAQ, which integrates complementary visual foundation models (VFMs) and a robust global aggregation method. Key experimental findings reveal that DC-VLAQ, through its innovative residual-guided complementary fusion and the Vector of Local Aggregated Queries (VLAQ) approach, consistently surpasses existing baselines and achieves state-of-the-art results across various VPR benchmarks, particularly in challenging scenarios involving domain shifts and long-term appearance variations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过开发一种能够抵御视角、光照和领域变化显著影响的强大全局表示，来增强视觉地点识别（VPR）。作者提出了一种名为DC-VLAQ的新框架，该框架整合了互补的视觉基础模型（VFM）和稳健的全局聚合方法。关键实验结果表明，DC-VLAQ在多个VPR基准测试中显著优于现有方法，尤其是在面临挑战性领域变化和长期外观变化的情况下，达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Source-Free Domain Adaptation</div>
<div class="meta-line">Authors: Song Tang, Wenxin Su, Mao Ye, Boyu Wang, Xiatian Zhu</div>
<div class="meta-line">First: 2024-03-12T12:40:08+00:00 · Latest: 2026-01-19T03:29:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.07601v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.07601v4">PDF</a> · <a href="https://github.com/tntek/CausalDA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including Closed-set, Open-set, Partial-set, and Generalized settings. Existing methods, focusing on specific scenarios, not only address a limited subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. In this paper, we propose a novel approach latent Causal factors discovery for unified SFDA (CausalDA). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate CausalDA from a causality perspective. The objective is to uncover potential causality between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that CausalDA can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization. Our code and data are available at https://github.com/tntek/CausalDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一无源领域适应</div>
<div class="mono" style="margin-top:8px">在没有访问源训练数据的情况下，将源模型转移到目标领域的过程中，无源领域适应（SFDA）在包括闭集、开集、部分集和广义设置等各种场景中得到了广泛探索。现有方法专注于特定场景，不仅解决了有限的挑战子集，还需要对目标领域的先验知识，这显著限制了它们的实际效用和可部署性。考虑到这些因素，我们引入了一个更实用但具有挑战性的问题，称为统一SFDA，它以统一的方式全面整合所有特定场景。在本文中，我们提出了一种新颖的方法，称为统一SFDA的潜在因果因素发现（CausalDA）。与之前强调学习现实统计描述的替代方法不同，我们从因果性角度构建CausalDA。其目标是揭示潜在变量与模型决策之间的潜在因果关系，提高所学模型对领域变化的可靠性和鲁棒性。为了整合广泛的世界知识，我们利用了预训练的视觉-语言模型，如CLIP。这有助于在缺乏监督的情况下形成和发现潜在因果因素，结合新设计的信息瓶颈及其理论保证。大量实验表明，CausalDA能够在不同的SFDA设置中实现新的最先进结果，以及无源的分布外泛化。我们的代码和数据可在https://github.com/tntek/CausalDA获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing Source-Free Domain Adaptation (SFDA) methods, which often require prior knowledge of the target domain and only tackle specific scenarios. To overcome these challenges, the authors propose a novel approach called CausalDA, which integrates all SFDA scenarios into a unified framework and focuses on discovering latent causal factors from a causality perspective. Experimental results indicate that CausalDA achieves state-of-the-art performance across various SFDA settings and demonstrates effective source-free out-of-distribution generalization.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有源无关领域适应（SFDA）方法的局限性，这些方法通常需要目标领域的先验知识，并且仅处理特定场景。作者提出了一种新方法CausalDA，从因果关系的角度出发，发现影响模型决策的潜在因果因素。实验结果表明，CausalDA在各种SFDA设置中实现了最新的性能，并展示了有效的源无关分布外泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-19T02:37:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0%的图像-AUROC和92.2%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that struggle with balancing global semantic generalization and fine-grained structural discriminability. The authors propose a novel method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through a Hierarchical Semantic-Visual Synergy (HSVS) mechanism that combines multi-scale structural priors from DINOv3 with the CLIP semantic space. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with scores of 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临局限，无法有效平衡语义泛化和结构可区分性。作者提出了一种名为协同语义-视觉提示（SSVP）的新方法，通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，将DINOv3的多尺度结构先验与CLIP语义空间结合。实验结果表明，SSVP显著提升了性能，在MVTec-AD基准上实现了93.0%的图像AUROC和92.2%的像素AUROC，超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">SiLVR: A Simple Language-based Video Reasoning Framework</div>
<div class="meta-line">Authors: Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius</div>
<div class="meta-line">First: 2025-05-30T17:59:19+00:00 · Latest: 2026-01-18T22:02:25+00:00</div>
<div class="meta-line">Comments: Accepted by TMLR (01/2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24869v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.24869v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/cs.unc.edu/silvr">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SILVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SILVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an Adaptive Context Reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. More details can be found at https://sites.google.com/cs.unc.edu/silvr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SiLVR：一个简单的基于语言的视频推理框架</div>
<div class="mono" style="margin-top:8px">最近的测试时优化进展使大型语言模型（LLMs）在推理能力上取得了显著进展，使其能够解决数学和编码中的高度复杂问题。然而，多模态LLMs（MLLMs）的推理能力仍显著滞后，尤其是在复杂的视频语言任务中。为了解决这个问题，我们提出了SILVR，一个简单的基于语言的视频推理框架，将复杂的视频理解分解为两个阶段。在第一阶段，SILVR使用多感官输入（如短片字幕和音频/语音字幕）将原始视频转换为基于语言的表示。在第二阶段，语言描述被输入到一个强大的推理LLM中，以解决复杂的视频语言理解任务。为了处理长上下文的多感官输入，我们使用了一种自适应上下文缩减方案，动态确定采样令牌的时间粒度。我们的简单、模块化且无需训练的视频推理框架在Video-MME（长）、Video-MMMU（理解）、Video-MMLU、CGBench和EgoLife上取得了最佳报告结果。此外，我们的实证研究集中于视频推理能力，表明尽管没有明确针对视频进行训练，强大的推理LLM仍能有效聚合来自视频、语音和音频的多感官输入信息，以应对视频中的复杂时间、因果、长上下文和知识获取推理任务。更多细节请访问https://sites.google.com/cs.unc.edu/silvr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of multimodal large language models (MLLMs) for complex video-language tasks, which currently lag behind their unimodal counterparts. The authors propose SiLVR, a two-stage framework that first converts raw video into language-based representations using multisensory inputs, and then utilizes a reasoning LLM to tackle video understanding tasks. Experimental results demonstrate that SiLVR achieves state-of-the-art performance on various benchmarks, indicating that strong reasoning LLMs can effectively process and integrate multisensory information from video and audio for complex reasoning tasks, even without explicit training on video data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高多模态大型语言模型（MLLMs）在复杂视频语言任务中的推理能力，而目前这一领域的表现仍落后于其他领域。作者提出了SILVR框架，将视频理解分为两个阶段：首先，利用多感官输入将原始视频转换为基于语言的表示；其次，利用推理LLM处理这些表示以完成视频语言理解任务。实验结果表明，SILVR在多个基准测试中实现了最先进的性能，表明强大的推理LLM能够有效整合多感官信息以应对复杂的视频推理任务，尽管它们并未在视频数据上进行显式训练。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Defense in Vision-Language Models: An Overview</div>
<div class="meta-line">Authors: Xiaowei Fu, Lei Zhang</div>
<div class="meta-line">First: 2026-01-18T14:57:51+00:00 · Latest: 2026-01-18T14:57:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12443v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的对抗防御：概述</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs，例如CLIP）的广泛使用引发了对其易受复杂且不可察觉的对抗攻击的担忧。这些攻击可能会影响模型性能和跨模态任务的系统安全。为应对这一挑战，提出了三种主要的防御范式：训练时防御、测试时适应防御和无训练防御。训练时防御涉及修改训练过程，通常通过对抗微调来提高对抗样本的鲁棒性。尽管有效，但这种方法需要大量计算资源，并且可能无法在所有对抗攻击中泛化。测试时适应防御专注于在推理时适应模型，通过更新其参数来处理未标记的对抗样本，提供灵活性，但通常以增加复杂性和计算开销为代价。无训练防御避免修改模型本身，而是专注于改变对抗输入或其特征嵌入，通过强制输入扰动来减轻攻击的影响，而无需额外训练。本调查回顾了VLMs对抗防御策略的最新进展，强调了这些方法的优缺点，并讨论了增强VLMs鲁棒性所面临的持续挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing deployment of Vision Language Models (VLMs) has highlighted their susceptibility to adversarial attacks, which can undermine their performance and security in cross-modal applications. This paper reviews three primary defense strategies: Training-time Defense, which enhances robustness through adversarial fine-tuning during training; Test-time Adaptation Defense, which modifies model parameters at inference to address adversarial examples; and Training-free Defense, which alters inputs or feature embeddings to reduce attack impact without retraining. The findings indicate that while each method has its advantages, they also present challenges such as computational demands and generalization issues, underscoring the need for ongoing improvements in VLM robustness.</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）的广泛应用，其对复杂和不可察觉的对抗攻击的脆弱性引发了关注，这可能会影响模型性能和跨模态任务的系统安全性。本文回顾了三种主要的防御策略：训练时防御通过对抗微调增强鲁棒性，但需要大量计算资源；测试时适应防御在推理过程中修改模型参数以处理对抗样本，尽管增加了复杂性；以及无训练防御，通过改变对抗输入或其特征嵌入来减少攻击影响，而无需重新训练。研究结果强调了每种方法的优缺点，同时讨论了提高VLM鲁棒性面临的持续挑战。</div>
</details>
</div>
<div class="card">
<div class="title">TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</div>
<div class="meta-line">Authors: Zhongyuan Bao, Lejun Zhang</div>
<div class="meta-line">First: 2025-09-19T05:08:05+00:00 · Latest: 2026-01-18T08:47:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15602v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.15602v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks from the stroke level to the rally level and includes 2527 human-verified questions. Evaluating 17 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TennisTV：多模态大型语言模型是否理解网球对打？</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）在一般视频理解方面表现出色，但在快速、高频的运动如网球中却面临挑战，因为对打片段短小却信息密集。为了系统性地评估MLLMs在这一具有挑战性的领域，我们提出了TennisTV，这是第一个也是最全面的网球视频理解基准。TennisTV将每个对打建模为一系列时间顺序的连续击球事件，使用自动化流程进行过滤和问题生成。它涵盖了从击球层面到对打层面的8个任务，并包括2527个经过人工验证的问题。通过评估17个代表性的MLLMs，我们提供了网球视频理解的首次系统评估。结果得出两个关键见解：（i）帧采样密度应根据任务进行调整和平衡，以及（ii）改善时间定位对增强推理至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the capabilities of multimodal large language models (MLLMs) in understanding tennis rallies, a domain characterized by fast-paced and information-dense video content. The authors introduce TennisTV, a comprehensive benchmark that models tennis rallies as sequences of stroke events and employs automated pipelines for filtering and question generation, covering eight tasks and including 2527 human-verified questions. The evaluation of 17 MLLMs reveals that tailored frame-sampling density across tasks and enhanced temporal grounding are crucial for improving reasoning in tennis video understanding.</div>
<div class="mono" style="margin-top:8px">本研究探讨了多模态大型语言模型（MLLMs）理解网球比赛的能力，该领域以快速和信息密集的视频内容为特征。为此，作者提出了TennisTV，这是一个全面的基准，将网球比赛建模为击球事件的序列，并采用自动化流程进行过滤和问题生成。对17个MLLM的评估表明，优化任务间的帧采样密度和增强时间基础对于提高网球视频理解中的推理能力至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations</div>
<div class="meta-line">Authors: Shizhan Gong, Xiaofan Zhang, Qi Dou</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-18T08:01:44+00:00 · Latest: 2026-01-18T08:01:44+00:00</div>
<div class="meta-line">Comments: AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12303v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12303v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP&#x27;s visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>来自表示的概念：通过视觉表示的稀疏分解实现的后验概念瓶颈模型</div>
<div class="mono" style="margin-top:8px">深度学习在图像识别中取得了显著成功，但其固有的不透明性在关键领域的应用中带来了挑战。基于概念的解释旨在通过人类可理解的概念解释模型推理。然而，现有的后验方法和先验概念瓶颈模型（CBMs）存在一些局限性，如概念相关性不可靠、非视觉或劳动密集型的概念定义，以及模型或数据无关的假设。本文介绍了一种通过表示分解实现的后验概念瓶颈模型（PCBM-ReD），这是一个将可解释性重新适配到预训练不透明模型的新管道。PCBM-ReD自动从预训练编码器中提取视觉概念，利用多模态大型语言模型（MLLMs）根据视觉可识别性和任务相关性对概念进行标记和过滤，并通过重构引导优化选择独立子集。借助CLIP的视觉-文本对齐，它将图像表示分解为概念嵌入的线性组合，以适应CBMs的抽象。针对11个图像分类任务的广泛实验表明，PCBM-ReD实现了最先进的准确性，缩小了与端到端模型的性能差距，并展现了更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of interpretability in deep learning models used for image recognition, which are often opaque and difficult to understand in critical applications. The authors propose a new method called Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), which retrofits interpretability onto pretrained models by extracting visual concepts and utilizing multimodal large language models to label and filter these concepts. Experimental results across 11 image classification tasks demonstrate that PCBM-ReD achieves state-of-the-art accuracy, reduces the performance gap with end-to-end models, and provides improved interpretability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高深度学习模型在图像识别中的可解释性，以应对其在关键应用中固有的不透明性带来的挑战。作者提出了一种新颖的后置概念瓶颈模型，通过表示分解（PCBM-ReD），该模型通过自动提取视觉概念并利用多模态大型语言模型对这些概念进行标记和过滤，从而为预训练模型增添可解释性。针对11个图像分类任务的实验结果表明，PCBM-ReD达到了最先进的准确性，缩小了与端到端模型的性能差距，并提供了比现有方法更好的可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training</div>
<div class="meta-line">Authors: Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam</div>
<div class="meta-line">First: 2026-01-18T06:42:24+00:00 · Latest: 2026-01-18T06:42:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12282v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model&#x27;s understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CytoCLIP：利用对比语言图像预训练学习发育中人脑的细胞结构特征</div>
<div class="mono" style="margin-top:8px">人脑不同区域的功能与其独特的细胞结构密切相关，细胞结构由细胞的空间排列和形态定义。通过细胞结构识别脑区可以进行各种科学分析。然而，在脑组织切片中手动划分这些区域耗时且需要专业知识。需要一种自动化的方法来减少人类专家的工作量。为此，我们提出了CytoCLIP，这是一个基于预训练的对比语言-图像预训练（CLIP）框架的视觉-语言模型套件，用于学习脑细胞结构的联合视觉-文本表示。CytoCLIP包括两种模型变体：一种使用低分辨率的整体区域图像进行训练，以理解区域的整体细胞结构模式，另一种则在高分辨率图像块上进行训练，以获得详细的细胞级表示。训练数据集由不同妊娠周数的发育胎儿脑的NISSL染色组织切片创建。它包括86个低分辨率图像的不同区域和384个高分辨率图像块的脑区。我们通过区域分类和跨模态检索任务评估模型对细胞结构的理解和泛化能力。在不同数据设置下进行多次实验，包括来自不同年龄样本和切片平面的数据。实验结果表明，CytoCLIP优于现有方法。它在整体区域分类中获得了0.87的F1分数，在高分辨率图像块分类中获得了0.91的F1分数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of identifying distinct brain regions based on their cytoarchitecture, which is crucial for various scientific analyses but traditionally requires extensive manual effort and expertise. To automate this process, the authors developed CytoCLIP, a suite of vision-language models leveraging Contrastive Language-Image Pre-Training (CLIP) to learn joint visual-text representations of brain cytoarchitecture. The model was trained on histological sections of developing fetal brains, achieving an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification, thus demonstrating superior performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要自动化识别基于其独特细胞结构的脑区，这对各种科学分析至关重要，但目前的手动过程耗时。作者提出了CytoCLIP，这是一个基于对比语言-图像预训练（CLIP）的视觉-语言模型套件，用于学习脑细胞结构的视觉-文本联合表示，利用低分辨率整体区域图像和高分辨率图像块。实验结果表明，CytoCLIP显著优于现有方法，在整体区域分类中获得了0.87的F1分数，在高分辨率图像块分类中获得了0.91的F1分数。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Joint Learning for Domain and Class Generalization</div>
<div class="meta-line">Authors: Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-18T04:24:11+00:00 · Latest: 2026-01-18T04:24:11+00:00</div>
<div class="meta-line">Comments: ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12253v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>领域和类别泛化的联邦联合学习</div>
<div class="mono" style="margin-top:8px">由于视觉语言模型如CLIP的参数规模大和预训练需求广泛，效率高的微调变得至关重要。现有方法通常单独解决未见类别或未见领域的问题，而未考虑两者的联合框架。本文提出了\textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization，称为\textbf{FedDCG}，这是一种新颖的方法，旨在在联邦学习环境中同时解决类别和领域泛化。我们的方法引入了一种领域分组策略，在每个组内训练类别泛化网络，以防止决策边界混淆。在推理过程中，我们根据领域相似性聚合类别泛化结果，有效整合类别和领域泛化的知识。具体而言，采用可学习网络增强类别泛化能力，并通过解耦机制分离一般知识和领域特定知识，提高对未见领域的泛化能力。大量实验表明，\textbf{FedDCG}在准确性和鲁棒性方面优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for efficient fine-tuning of large-scale visual-language models like CLIP, particularly in addressing the challenges of unseen classes and domains. The authors propose a novel method called Federated Joint Learning for Domain and Class Generalization (FedDCG), which employs a domain grouping strategy to train class-generalized networks and aggregates results based on domain similarity during inference. Experimental results demonstrate that FedDCG significantly outperforms existing state-of-the-art methods in accuracy and robustness across various datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于高效微调视觉语言模型（如CLIP）的需求，特别是在处理未见类别和领域的挑战方面。作者提出了一种新方法，称为联合学习的联邦学习（FedDCG），该方法采用领域分组策略训练类别泛化网络，并在推理过程中根据领域相似性聚合结果。实验结果表明，FedDCG在多个数据集上显著优于现有的最先进方法，具有更高的准确性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking and Red-Teaming Protective Perturbation in Personalized Diffusion Models</div>
<div class="meta-line">Authors: Yixin Liu, Ruoxi Chen, Xun Chen, Lichao Sun</div>
<div class="meta-line">First: 2024-06-27T07:14:14+00:00 · Latest: 2026-01-17T23:23:17+00:00</div>
<div class="meta-line">Comments: Our code is available at https://github.com/liuyixin-louis/DiffShortcut</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.18944v5">Abs</a> · <a href="https://arxiv.org/pdf/2406.18944v5">PDF</a> · <a href="https://github.com/liuyixin-louis/DiffShortcut">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Personalized diffusion models (PDMs) have become prominent for adapting pre-trained text-to-image models to generate images of specific subjects using minimal training data. However, PDMs are susceptible to minor adversarial perturbations, leading to significant degradation when fine-tuned on corrupted datasets. These vulnerabilities are exploited to create protective perturbations that prevent unauthorized image generation. Existing purification methods attempt to red-team the protective perturbation to break the protection but often over-purify images, resulting in information loss. In this work, we conduct an in-depth analysis of the fine-tuning process of PDMs through the lens of shortcut learning. We hypothesize and empirically demonstrate that adversarial perturbations induce a latent-space misalignment between images and their text prompts in the CLIP embedding space. This misalignment causes the model to erroneously associate noisy patterns with unique identifiers during fine-tuning, resulting in poor generalization. Based on these insights, we propose a systematic red-teaming framework that includes data purification and contrastive decoupling learning. We first employ off-the-shelf image restoration techniques to realign images with their original semantic content in latent space. Then, we introduce contrastive decoupling learning with noise tokens to decouple the learning of personalized concepts from spurious noise patterns. Our study not only uncovers shortcut learning vulnerabilities in PDMs but also provides a thorough evaluation framework for developing stronger protection. Our extensive evaluation demonstrates its advantages over existing purification methods and its robustness against adaptive perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考个性化扩散模型中的保护性扰动与红队测试</div>
<div class="mono" style="margin-top:8px">个性化扩散模型（PDMs）因其能够使用最少的训练数据将预训练的文本到图像模型适应于生成特定主题的图像而变得突出。然而，PDMs容易受到微小的对抗性扰动的影响，在对受损数据集进行微调时会导致显著的性能下降。这些脆弱性被利用来创建保护性扰动，以防止未经授权的图像生成。现有的净化方法试图对保护性扰动进行红队测试以打破保护，但往往会过度净化图像，导致信息丢失。在本研究中，我们通过捷径学习的视角对PDMs的微调过程进行了深入分析。我们假设并实证证明，对抗性扰动在CLIP嵌入空间中导致图像与其文本提示之间的潜在空间不对齐。这种不对齐使得模型在微调过程中错误地将噪声模式与唯一标识符关联，从而导致泛化能力差。基于这些见解，我们提出了一个系统的红队测试框架，包括数据净化和对比解耦学习。我们首先采用现成的图像恢复技术，使图像在潜在空间中与其原始语义内容重新对齐。然后，我们引入带有噪声标记的对比解耦学习，将个性化概念的学习与虚假噪声模式解耦。我们的研究不仅揭示了PDMs中的捷径学习脆弱性，还提供了一个全面的评估框架，以开发更强的保护。我们的广泛评估展示了其相对于现有净化方法的优势及其对自适应扰动的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerabilities of personalized diffusion models (PDMs) to adversarial perturbations, which can significantly degrade performance when fine-tuned on corrupted datasets. The authors conduct an in-depth analysis of the fine-tuning process through the lens of shortcut learning and demonstrate that adversarial perturbations cause latent-space misalignment in the CLIP embedding space, leading to poor generalization. They propose a systematic red-teaming framework that includes data purification and contrastive decoupling learning, showing that their approach effectively realigns images with their semantic content and improves robustness against adaptive perturbations compared to existing purification methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决个性化扩散模型（PDMs）对对抗扰动的脆弱性，这些扰动在对受损数据集进行微调时会显著降低性能。作者通过快捷学习的视角对微调过程进行了深入分析，并提出了一种系统的红队框架，包括数据净化和对比解耦学习。实验结果表明，对抗扰动导致CLIP嵌入空间中的潜在空间错位，从而导致泛化能力差，并且证明了他们提出的方法在性能上优于现有的净化技术，同时对自适应扰动保持了鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization</div>
<div class="meta-line">Authors: Jingren Liu, Hanzhang Qin, Junyi Liu, Mabel C. Chou, Jong-Shi Pang</div>
<div class="meta-line">First: 2026-01-17T17:35:00+00:00 · Latest: 2026-01-17T17:35:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12117v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline policy learning aims to use historical data to learn an optimal personalized decision rule. In the standard estimate-then-optimize framework, reweighting-based methods (e.g., inverse propensity weighting or doubly robust estimators) are widely used to produce unbiased estimates of policy values. However, when the propensity scores of some treatments are small, these reweighting-based methods suffer from high variance in policy value estimation, which may mislead the downstream policy optimization and yield a learned policy with inferior value. In this paper, we systematically develop an offline policy learning algorithm based on a weight-clipping estimator that truncates small propensity scores via a clipping threshold chosen to minimize the mean squared error (MSE) in policy value estimation. Focusing on linear policies, we address the bilevel and discontinuous objective induced by weight-clipping-based policy optimization by reformulating the problem as a Heaviside composite optimization problem, which provides a rigorous computational framework. The reformulated policy optimization problem is then solved efficiently using the progressive integer programming method, making practical policy learning tractable. We establish an upper bound for the suboptimality of the proposed algorithm, which reveals how the reduction in MSE of policy value estimation, enabled by our proposed weight-clipping estimator, leads to improved policy learning performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带权重裁剪和海维赛德复合优化的离线策略学习</div>
<div class="mono" style="margin-top:8px">离线策略学习旨在利用历史数据学习最佳个性化决策规则。在标准的估计-再优化框架中，基于重加权的方法（例如，逆倾向加权或双重稳健估计器）被广泛用于生成无偏的策略价值估计。然而，当某些处理的倾向分数较小时，这些基于重加权的方法在策略价值估计中会遭受高方差，这可能误导下游策略优化并产生价值较低的学习策略。本文系统地开发了一种基于权重裁剪估计器的离线策略学习算法，该估计器通过选择裁剪阈值来截断小的倾向分数，以最小化策略价值估计中的均方误差（MSE）。我们专注于线性策略，通过将问题重新表述为海维赛德复合优化问题，解决了由基于权重裁剪的策略优化引起的双层和不连续目标，从而提供了一个严格的计算框架。然后，使用渐进整数规划方法高效地解决重新表述的策略优化问题，使实际的策略学习变得可行。我们为所提算法的次优性建立了上界，揭示了我们提出的权重裁剪估计器所带来的策略价值估计MSE的降低如何改善策略学习性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve offline policy learning by addressing the high variance in policy value estimation caused by small propensity scores in reweighting-based methods. The authors propose a novel algorithm that utilizes a weight-clipping estimator to truncate small propensity scores, minimizing the mean squared error in policy value estimation. The key experimental findings demonstrate that this approach, when applied to linear policies and solved through a Heaviside composite optimization framework, leads to a significant reduction in suboptimality and enhances overall policy learning performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决重加权方法中小倾向得分导致的政策价值估计高方差问题来改善离线政策学习。作者提出了一种新颖的离线政策学习算法，该算法利用权重裁剪估计器来截断小倾向得分，从而最小化政策价值估计的均方误差。关键实验结果表明，当将该方法应用于线性政策并通过海维赛德复合优化框架求解时，能够显著降低次优性并增强整体政策学习性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification</div>
<div class="meta-line">Authors: Xiaomei Yang, Xizhan Gao, Antai Liu, Kang Wei, Fa Zhu, Guang Feng, Xiaofeng Qu, Sijie Niu</div>
<div class="meta-line">First: 2026-01-17T14:18:23+00:00 · Latest: 2026-01-17T14:18:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12062v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features&#x27; discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视频的可见-红外人重识别的语言驱动序列级模态不变表示学习</div>
<div class="mono" style="margin-top:8px">基于视频的可见-红外人重识别（VVI-ReID）的核心在于学习跨不同模态的序列级模态不变表示。近期研究倾向于使用由CLIP生成的模态共享语言提示来指导模态不变表示的学习。尽管取得了最佳性能，这些方法在高效的时空建模、充分的跨模态交互和明确的模态级损失指导方面仍面临限制。为了解决这些问题，我们提出了语言驱动的序列级模态不变表示学习（LSMRL）方法，其中包括时空特征学习（STFL）模块、语义扩散（SD）模块和跨模态交互（CMI）模块。为了实现参数和计算高效的时空建模，STFL模块在CLIP的基础上进行了最小修改。为了实现充分的跨模态交互并增强模态不变特征的学习，SD模块被提出以将模态共享语言提示扩散到可见和红外特征中，以建立初步的模态一致性。CMI模块进一步开发，利用双向跨模态自注意力消除残余模态差距并细化模态不变表示。为了明确增强模态不变表示的学习，引入了两个模态级损失，以提高特征的区分能力及其对未见类别的泛化能力。在大规模VVI-ReID数据集上的广泛实验表明，LSMRL优于AOTA方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video-based visible-infrared person re-identification (VVI-ReID) by developing effective sequence-level modal-invariant representations. The authors propose a novel method called language-driven sequence-level modal-invariant representation learning (LSMRL), which incorporates a spatial-temporal feature learning module, a semantic diffusion module, and a cross-modal interaction module to enhance the learning process. Experimental results on large-scale VVI-ReID datasets show that LSMRL outperforms existing methods, addressing issues related to spatial-temporal modeling, cross-modal interaction, and modality-level loss guidance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在空间-时间建模和跨模态交互方面的局限性，来改善基于视频的可见-红外人员重识别（VVI-ReID）。作者提出了一种新颖的语言驱动序列级模态不变表示学习（LSMRL）方法，该方法结合了空间-时间特征学习模块、语义扩散模块和跨模态交互模块。对大规模VVI-ReID数据集的实验结果表明，LSMRL优于现有方法，通过引入模态级损失和有效的跨模态自注意机制，增强了特征的区分能力和对未见类别的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</div>
<div class="meta-line">Authors: Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu</div>
<div class="meta-line">First: 2024-11-07T18:59:16+00:00 · Latest: 2026-01-17T14:15:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.04997v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.04997v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs&#x27; superior text understanding and extensive open-world knowledge can enhance CLIP&#x27;s capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM2CLIP：强大的语言模型解锁更丰富的视觉表示</div>
<div class="mono" style="margin-top:8px">CLIP是一个基础的多模态模型，通过对大规模图像-文本对进行对比学习，将图像和文本特征对齐到共享表示空间。其有效性主要源于使用自然语言作为丰富的监督。受到大型语言模型（LLMs）显著进展的启发，本研究探讨了LLMs卓越的文本理解和广泛的开放世界知识如何增强CLIP的能力，特别是在处理更长和更复杂的图像标题方面。我们提出了一种高效的后训练策略，将LLMs集成到预训练的CLIP中。为了解决LLMs自回归特性带来的挑战，我们引入了一种标题到标题的对比微调框架，显著提高了LLM输出的区分质量。大量实验表明，我们的方法优于基于LoRA的方法，实现了近四倍的训练速度提升和更优的性能。此外，我们在各种零-shot多模态检索任务、跨语言检索任务和多模态语言模型预训练中验证了相较于最先进模型如CLIP、EVA02和SigLip2的显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the potential of large language models (LLMs) to enhance the capabilities of the CLIP model, which aligns image and text features through contrastive learning. The authors propose a post-training strategy that integrates LLMs into the pretrained CLIP model, utilizing a caption-to-caption contrastive fine-tuning framework to improve the quality of LLM outputs. Experimental results indicate that this method significantly outperforms LoRA-based approaches, achieving nearly fourfold faster training and demonstrating substantial improvements over state-of-the-art models like CLIP, EVA02, and SigLip2 in various multimodal retrieval tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大型语言模型（LLMs）在增强CLIP模型能力方面的潜力，CLIP模型通过对比学习将图像和文本特征对齐。作者提出了一种后训练策略，将LLMs整合到预训练的CLIP中，利用标题到标题的对比微调框架来提高输出的判别质量。实验结果表明，该方法优于基于LoRA的方法，实现了近四倍的训练速度提升，并在各种多模态检索任务中显著超越了CLIP、EVA02和SigLip2等最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs</div>
<div class="meta-line">Authors: Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya</div>
<div class="meta-line">First: 2026-01-17T10:13:07+00:00 · Latest: 2026-01-17T10:13:07+00:00</div>
<div class="meta-line">Comments: 16 pages, 5 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11995v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11995v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled &quot;train&quot; might also contain motorcycle audio and visual, because &quot;motorcycle&quot; is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., &quot;Train (visual)&quot; -&gt; &quot;Motorcycle (audio)&quot;) that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过推断潜在交互图学习音视频嵌入</div>
<div class="mono" style="margin-top:8px">学习稳健的音视频嵌入需要将真正相关的音频和视觉信号结合在一起，同时过滤掉偶然的共现——背景噪声、无关元素或未标注事件。大多数对比和三元组损失方法使用每个片段的稀疏标注标签，并将任何共现视为语义相似性。例如，一个标记为“火车”的视频可能还包含摩托车的音频和视觉，因为“摩托车”不是选择的标注；标准方法将这些共现视为与其他地方真实摩托车锚点的负例，造成假阴性并错过真实的跨模态依赖关系。我们提出了一个框架，利用软标签预测和推断的潜在交互来解决这些问题：(1) 音视频语义对齐损失（AV-SAL）训练教师网络在各模态间生成对齐的软标签分布，为共现但未标注的事件分配非零概率，丰富监督信号。(2) 推断潜在交互图（ILI）应用GRaSP算法于教师软标签，推断类之间的稀疏有向依赖图。该图突出方向性依赖（例如，“火车（视觉）”-&gt;“摩托车（音频）”），揭示类之间可能的语义或条件关系；这些被解释为估计的依赖模式。(3) 潜在交互正则化器（LIR）：学生网络在度量损失和由ILI图指导的正则化器下进行训练，将依赖关联但未标注的对的嵌入按其软标签概率的比例拉在一起。在AVE和VEGAS基准上的实验显示平均精度（mAP）的一致提升，证明将推断的潜在交互整合到嵌入学习中增强了稳健性和语义一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the robustness of audio-visual embeddings by effectively aligning genuinely related audio and visual signals while filtering out incidental co-occurrences. The authors propose a framework that includes an Audio-Visual Semantic Alignment Loss (AV-SAL) to produce aligned soft-label distributions, an Inferred Latent Interaction Graph (ILI) to identify directional dependencies among classes, and a Latent Interaction Regularizer (LIR) to enhance the training of a student network. Experimental results on the AVE and VEGAS benchmarks demonstrate significant improvements in mean average precision (mAP), indicating that the integration of inferred latent interactions enhances the robustness and semantic coherence of the embeddings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效对齐真正相关的音频和视觉信号，同时过滤掉偶然的共现，来提高音视频嵌入的鲁棒性。作者提出了一个框架，包括音视频语义对齐损失（AV-SAL）以生成对齐的软标签分布、推断潜在交互图（ILI）以识别类之间的方向依赖关系，以及潜在交互正则化器（LIR）以增强学生网络的训练。在AVE和VEGAS基准上的实验结果表明，平均精度（mAP）显著提高，表明推断潜在交互的整合增强了嵌入的鲁棒性和语义一致性。</div>
</details>
</div>
<div class="card">
<div class="title">DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset</div>
<div class="meta-line">Authors: Yiming Li, Chen Cai, Tianyi Liu, Dan Lin, Wenqian Wang, Wenfei Liang, Bingbing Li, Kim-Hui Yap</div>
<div class="meta-line">First: 2026-01-17T09:53:48+00:00 · Latest: 2026-01-17T09:53:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11990v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11990v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DAOS：一种多模态车内行为监测与驾驶员动作-物体协同数据集</div>
<div class="mono" style="margin-top:8px">在驾驶员活动监测中，动作主要限于上半身，这使得许多动作看起来相似。为了区分这些动作，人类通常依赖于驾驶员使用的物体，例如握手机与握方向盘的比较。然而，大多数现有的驾驶员监测数据集缺乏准确的物体位置注释，或未将物体与其相关动作关联起来，留下了可靠动作识别的关键空白。为了解决这个问题，我们引入了驾驶员动作与物体协同（DAOS）数据集，包含9,787个视频片段，注释了36种细粒度驾驶员动作和15个物体类别，总计超过250万个相应的物体实例。DAOS提供来自前方、面部、左侧和右侧视角的多模态、多视图数据（RGB、IR和深度）。尽管DAOS捕捉了广泛的车内物体，但只有少数与每个动作的预测直接相关，因此专注于任务特定的人-物体关系至关重要。为应对这一挑战，我们提出了动作-物体-关系网络（AOR-Net）。AOR-Net通过多层次推理和链式动作提示机制理解复杂的驾驶员动作，建模动作、物体及其关系之间的逻辑关系。此外，引入了思维混合模块，以动态选择每个阶段的关键知识，提高在物体丰富和稀缺条件下的鲁棒性。大量实验表明，我们的模型在各种数据集上优于其他最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve driver activity monitoring by accurately distinguishing between similar upper body movements using object interactions, as existing datasets often lack precise object-location annotations. The authors introduce the Driver Action with Object Synergy (DAOS) dataset, which includes 9,787 video clips annotated with 36 driver actions and 15 object classes, providing multi-modal data from various perspectives. The key experimental findings indicate that the proposed Action-Object-Relation Network (AOR-Net) effectively enhances action recognition by utilizing multi-level reasoning and a chain-of-action prompting mechanism, outperforming state-of-the-art methods across multiple datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有数据集的局限性来改善驾驶员活动监测，这些数据集通常缺乏准确的物体位置注释，并未将物体与相关动作关联起来。作者介绍了驾驶员动作与物体协同（DAOS）数据集，包含9,787个视频片段，标注了36种驾驶员动作和15种物体类别，并提供来自不同视角的多模态数据。他们提出了动作-物体-关系网络（AOR-Net），通过多层次推理和动作链提示机制来增强动作识别，建模动作与物体之间的关系。实验结果表明，AOR-Net在多个数据集上显著优于最先进的方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
