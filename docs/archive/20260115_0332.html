<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-15 03:32</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260115_0332</div>
    <div class="row"><div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型快速发展，但数据在影响运动方面的作用仍不清楚。我们提出了Motive（视频生成的运动归因），这是一个以运动为中心的基于梯度的数据归因框架，能够扩展到现代大型高质量视频数据集和模型。我们利用此框架研究哪些微调片段能改善或恶化时间动态。Motive通过运动加权损失掩码将时间动态与静态外观隔离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型中，Motive识别出强烈影响运动的片段，并指导数据策划，以改善时间一致性和物理合理性。使用Motive选择的高影响数据，我们的方法在VBench上提高了运动平滑度和动态程度，与预训练基础模型相比，获得了74.1%的人工偏好胜率。据我们所知，这是第一个在视频生成模型中归因于运动而非视觉外观的框架，并利用它来策划微调数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to better understand how data influences motion in video generation models, an area that has not been thoroughly explored. The authors introduce Motive, a motion-centric, gradient-based data attribution framework designed to analyze large, high-quality video datasets and models. Through the use of motion-weighted loss masks, Motive effectively isolates temporal dynamics from static appearance, allowing for the identification of fine-tuning clips that enhance or detract from motion quality. Experimental results demonstrate that using data selected by Motive leads to improvements in motion smoothness and dynamic degree, achieving a 74.1% human preference win rate compared to the pretrained base model, marking a significant advancement in motion attribution in video generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是更好地理解数据如何影响视频生成模型中的运动，尽管该领域已经取得了进展，但这一点仍然不够清楚。作者提出了Motive，这是一个以运动为中心的基于梯度的数据归因框架，旨在分析大型高质量视频数据集和模型。实验结果表明，Motive能够有效识别对运动动态有显著影响的剪辑，从而提高文本到视频模型的时间一致性和物理合理性，使用Motive选择的数据相比于预训练基础模型，在运动平滑度和动态程度上获得了74.1%的人工偏好胜率。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：用于行人重识别的视频超分辨率</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹质量通常被视为事后考虑，绝大多数研究集中于对基础模型的架构修改。这些方法忽视了一个重要的限制，在现实世界的困难场景中部署ReID系统时面临挑战。本文介绍了S3-CLIP，一种基于视频超分辨率的CLIP-ReID框架，旨在2026年WACV的VReID-XFD挑战中开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，适应视频基础的行人重识别设置。据我们所知，这项工作代表了首次系统性研究视频超分辨率作为提高行人ReID轨迹质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法在基线性能上具有竞争力，在空中到地面场景中实现了37.52%的mAP，在地面到空中场景中实现了29.16%的mAP。在地面到空中设置中，S3-CLIP在排名准确性上取得了显著提升，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the often-overlooked quality of tracklets in person re-identification (ReID) systems, which can hinder their effectiveness in real-world scenarios. The authors introduce S3-CLIP, a novel framework that combines video super-resolution techniques with CLIP-ReID to enhance tracklet quality specifically for the VReID-XFD challenge. Experimental results indicate that S3-CLIP achieves competitive performance, with a mean Average Precision (mAP) of 37.52% in aerial-to-ground and 29.16% in ground-to-aerial scenarios, along with significant improvements in ranking accuracy in the ground-to-aerial setting, enhancing Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人重识别（ReID）方法中常被忽视的轨迹质量问题，这可能会影响这些系统在现实场景中的有效性。作者提出了S3-CLIP，这是一种基于视频超分辨率的框架，结合了超分辨率网络的最新进展和专门针对视频ReID的任务驱动管道，标志着首次系统性探索视频超分辨率以提高轨迹质量。实验结果表明，S3-CLIP在空中到地面场景中达到37.52%的mAP，在地面到空中场景中达到29.16%的mAP，并在地面到空中的设置中显著提高了排名准确性，Rank-1、Rank-5和Rank-10的表现分别提高了11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Visual World with Artificial Intelligence: A Roadmap</div>
<div class="meta-line">Authors: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu</div>
<div class="meta-line">First: 2025-11-11T18:59:50+00:00 · Latest: 2026-01-13T15:42:01+00:00</div>
<div class="meta-line">Comments: Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08585v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08585v2">PDF</a> · <a href="https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://world-model-roadmap.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a &quot;window&quot; into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用人工智能模拟视觉世界：路线图</div>
<div class="mono" style="margin-top:8px">视频生成的格局正在发生变化，从专注于生成视觉吸引力的剪辑转向构建支持交互并保持物理合理性的虚拟环境。这些发展指向视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐式世界模型，模拟支配真实或想象世界的物理动态、代理-环境交互和任务规划。本调查提供了这一演变的系统概述，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型和视频渲染器。世界模型编码了关于世界的结构化知识，包括物理法则、交互动态和代理行为。它作为潜在的模拟引擎，使得连贯的视觉推理、长期时间一致性和目标驱动的规划成为可能。视频渲染器将这种潜在模拟转化为现实的视觉观察，有效地将视频作为“窗口”展示模拟世界。我们通过四个世代追踪视频生成的进展，其中核心能力逐步提升，最终形成一个建立在视频生成模型基础上的世界模型，体现内在的物理合理性、实时多模态交互和跨多个时空尺度的规划能力。对于每一代，我们定义其核心特征，突出代表性作品，并考察其应用领域，如机器人技术、自动驾驶和互动游戏。最后，我们讨论下一代世界模型的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作的最新列表可在此链接中查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this paper is to explore the transition in video generation from creating visually appealing clips to developing interactive virtual environments that maintain physical plausibility. The authors systematically review the evolution of video foundation models, which consist of an implicit world model that encodes structured knowledge about physical dynamics and agent interactions, and a video renderer that produces realistic visual outputs. Key findings indicate that the progression of video generation has advanced through four generations, culminating in a model that supports intrinsic physical plausibility, real-time interaction, and multi-scale planning, with applications in fields such as robotics and autonomous driving, while also identifying challenges and design principles for future developments.</div>
<div class="mono" style="margin-top:8px">本文的动机在于探讨视频生成从创建视觉吸引的剪辑到开发保持物理合理性的互动虚拟环境的演变。作者系统回顾了视频基础模型的出现，这些模型将编码物理法则和代理交互的结构化知识的隐式世界模型与生成现实视觉输出的视频渲染器结合在一起。主要发现表明，这些模型经历了四个代际的发展，最终形成了一个复杂的世界模型，能够实现实时多模态交互和跨多个时空尺度的规划，应用于机器人和自动驾驶等领域，同时识别出未来进展的持续挑战和设计原则。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts</div>
<div class="meta-line">Authors: Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Loppi, Antti Honkela</div>
<div class="meta-line">First: 2024-06-25T06:04:58+00:00 · Latest: 2026-01-13T15:13:42+00:00</div>
<div class="meta-line">Comments: 19 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.17298v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.17298v3">PDF</a> · <a href="https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无捷径的差分隐私深度学习的高效可扩展实现</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DP-SGD）是基于差分隐私（DP）训练机器学习模型的标准算法。最常见的DP-SGD隐私会计依赖于泊松子采样以确保理论上的DP保证。使用泊松子采样实现计算高效的DP-SGD并非易事，这导致许多实现通过使用计算更快的子采样走捷径。我们通过实现和基准测试正确的泊松子采样的高效方法，量化了在DP下训练深度学习模型的计算成本。我们发现，使用PyTorch中Opacus的DP-SGD的简单实现，其吞吐量比SGD低2.6到8倍。然而，像Ghost Clipping这样的高效梯度裁剪实现可以大致将这一成本减半。我们提出了一种使用JAX的DP-SGD替代计算高效实现，采用泊松子采样，并与基于PyTorch的高效裁剪优化表现相当。我们研究了使用多达80个GPU的扩展行为，发现DP-SGD的扩展性优于SGD。我们在https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL分享我们的库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of differentially private stochastic gradient descent (DP-SGD) for training machine learning models while adhering to theoretical privacy guarantees. The authors implemented and benchmarked various methods, focusing on the computational costs associated with DP-SGD using Poisson subsampling. Their findings reveal that the naive implementation of DP-SGD in PyTorch has a throughput significantly lower than standard SGD, but by employing efficient gradient clipping techniques like Ghost Clipping, the computational cost can be reduced by approximately 50%. Additionally, they introduced a new implementation of DP-SGD using JAX that maintains comparable performance with efficient clipping optimizations and demonstrated that DP-SGD scales better than SGD when utilizing up to 80 GPUs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在遵循理论隐私保证的同时高效实现差分隐私随机梯度下降（DP-SGD）所面临的挑战。作者对在差分隐私下训练深度学习模型的各种方法进行了基准测试，重点关注与泊松子采样相关的计算成本。他们发现，使用PyTorch中Opacus的DP-SGD简单实现的速度显著低于标准SGD，但通过采用Ghost Clipping等高效梯度裁剪技术，可以降低这一成本。此外，他们提出了一种使用JAX的新实现，该实现保持了泊松子采样的优势，并在对多达80个GPU进行测试时显示出更好的可扩展性，优于SGD。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</div>
<div class="meta-line">Authors: Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</div>
<div class="meta-line">First: 2026-01-13T13:42:05+00:00 · Latest: 2026-01-13T13:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08557v1">PDF</a> · <a href="https://github.com/Simula/HEDGE#videohedge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHEDGE：基于熵的视觉语言模型视频幻觉检测框架，通过语义聚类和时空扰动</div>
<div class="mono" style="margin-top:8px">在视频能力的视觉语言模型（Video-VLMs）中，幻觉现象仍然频繁且置信度高，而现有的不确定性度量往往无法与正确性对齐。我们提出了VideoHEDGE，这是一个用于视频问答中幻觉检测的模块化框架，将基于熵的可靠性估计从图像扩展到时间结构化输入。给定一个视频-问题对，VideoHEDGE从干净片段和光度及时空扰动变体中提取基线答案和多个高温生成，然后使用基于自然语言推理（NLI）或嵌入的方法将结果文本输出聚类为语义假设。聚类级别的概率质量产生三个可靠性评分：语义熵（SE）、RadFlag和视觉增强语义熵（VASE）。我们在SoccerChat基准上评估VideoHEDGE，使用LLM作为评判者获得二元幻觉标签。在三个7B Video-VLM（Qwen2-VL、Qwen2.5-VL和一个SoccerChat微调模型）中，VASE在较大的失真预算下始终实现最高的ROC-AUC，而SE和RadFlag的表现往往接近随机。我们进一步表明，基于嵌入的聚类在检测性能上与基于NLI的聚类相匹配，但计算成本显著较低，并且领域微调减少了幻觉频率，但在校准方面仅带来了适度的改善。hedge-bench PyPI库支持可重复和可扩展的基准测试，完整代码和实验资源可在https://github.com/Simula/HEDGE#videohedge获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the frequent and high-confidence hallucinations in video-capable vision-language models (Video-VLMs), which existing uncertainty metrics fail to accurately assess. The authors introduce VideoHEDGE, a modular framework that extends entropy-based reliability estimation to video question answering by generating baseline answers and high-temperature outputs from both original and perturbed video clips, followed by semantic clustering of the textual outputs. Experimental results on the SoccerChat benchmark demonstrate that the Vision-Amplified Semantic Entropy (VASE) score outperforms other reliability metrics in detecting hallucinations, particularly under larger distortion budgets, while also showing that embedding-based clustering can achieve similar performance to Natural Language Inference-based methods at a lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频能力视觉语言模型（Video-VLMs）中频繁且高置信度的幻觉问题，而现有的不确定性度量无法准确评估。作者提出了VideoHEDGE，这是一个用于视频问答中检测幻觉的模块化框架，通过将基于熵的可靠性估计扩展到时间结构输入。对SoccerChat基准的实验结果表明，视觉增强语义熵（VASE）分数在三个7B Video-VLMs中始终实现了最高的ROC-AUC，尤其是在较大的失真预算下，而其他分数如语义熵和RadFlag的表现接近随机水平；此外，基于嵌入的聚类在计算成本较低的情况下显示出与基于NLI的聚类相当的性能，而领域微调略微减少了幻觉频率，但校准改进有限。</div>
</details>
</div>
<div class="card">
<div class="title">Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning</div>
<div class="meta-line">Authors: Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano</div>
<div class="meta-line">First: 2025-07-18T17:59:55+00:00 · Latest: 2026-01-13T13:22:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14137v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14137v3">PDF</a> · <a href="https://github.com/valeoai/Franca">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Franca：用于可扩展视觉表示学习的嵌套俄罗斯套娃聚类</div>
<div class="mono" style="margin-top:8px">我们介绍Franca（发音为Fran-ka）：一个免费的开源视觉基础模型，数据、代码和权重完全开放，性能与许多最先进的专有模型（如DINOv2、CLIP、SigLIPv2等）相匹配，并在许多情况下超越它们。我们的方法基于受Web-SSL启发的透明训练流程，使用公开可用的数据：ImageNet-21K和ReLAION-2B的一个子集。除了模型发布外，我们还解决了SSL聚类方法中的关键限制。现代模型依赖于通过像Sinkhorn-Knopp这样的聚类算法将图像特征分配给大型代码本，但未能考虑聚类语义中的固有模糊性。为了解决这个问题，我们引入了一种基于嵌套俄罗斯套娃表示的参数高效多头聚类投影器。该设计逐步将特征细化为越来越细粒度的聚类，而不增加模型大小，从而实现性能和内存效率。此外，我们提出了一种新颖的位置解耦策略，明确消除密集表示中的位置偏差，从而改善语义内容的编码。这在多个下游基准测试中带来了持续的提升，证明了更清晰特征空间的实用性。我们的贡献为透明、高性能的视觉模型建立了新的标准，并为更可重复和可推广的基础模型在更广泛的AI社区中开辟了道路。代码和模型检查点可在https://github.com/valeoai/Franca获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to create a fully open-source vision foundation model that can match or exceed the performance of proprietary models while addressing limitations in self-supervised learning (SSL) clustering methods. The authors developed Franca, which employs a transparent training pipeline using publicly available datasets like ImageNet-21K and a subset of ReLAION-2B, and introduces a multi-head clustering projector based on nested Matryoshka representations to refine features into fine-grained clusters efficiently. Experimental results show that this approach, along with a novel positional disentanglement strategy, leads to consistent improvements on various downstream benchmarks, highlighting the effectiveness of cleaner feature spaces in enhancing model performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个完全开源的视觉基础模型，不仅能够匹配而且通常超越领先的专有模型的性能，同时解决自监督学习（SSL）聚类方法中的局限性。作者提出了Franca，该模型采用透明的训练流程，利用ImageNet-21K和ReLAION-2B的子集等公开可用的数据集，并具有基于嵌套马特里奥什卡表示的多头聚类投影器。关键实验结果表明，这种方法提高了性能和内存效率，并在多个下游基准测试中取得了一致的提升，突显了更清晰特征空间的有效性，并为AI社区建立了高性能视觉模型的新标准。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Autoregressive Generation</div>
<div class="meta-line">Authors: Stepan Maschan, Haoxuan Qu, Jun Liu</div>
<div class="meta-line">First: 2026-01-06T17:07:27+00:00 · Latest: 2026-01-13T11:19:48+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03184v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去中心化自回归生成</div>
<div class="mono" style="margin-top:8px">我们对自回归生成的去中心化进行了理论分析。我们通过将概率生成速度表示为专家流的线性组合，定义了去中心化离散流匹配目标。我们还进行了实验，展示了在多模态语言模型的不同基准测试中，去中心化和中心化训练设置之间的等价性。具体而言，我们比较了两种不同的范式：LLaVA和InternVL 2.5-1B，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调（ViT+MLP+LLM）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to analyze the decentralization of autoregressive generation in multimodal language models. The authors introduce the Decentralized Discrete Flow Matching objective, which represents probability generating velocity as a linear combination of expert flows. Experimental results show that decentralized training settings can achieve equivalence with centralized training across various benchmarks, particularly when comparing the LLaVA and InternVL 2.5-1B models, which utilize a fixed CLIP vision encoder and undergo full-parameter fine-tuning during instruction tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是分析多模态语言模型中自回归生成的去中心化。作者提出了去中心化离散流匹配目标，该目标将概率生成速度表示为专家流的线性组合。实验结果表明，在比较LLaVA和InternVL 2.5-1B模型时，去中心化和中心化训练设置在各种基准测试中表现出等效的性能，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调。</div>
</details>
</div>
<div class="card">
<div class="title">MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</div>
<div class="meta-line">Authors: Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra, Biplab Banerjee</div>
<div class="meta-line">First: 2026-01-13T10:44:37+00:00 · Latest: 2026-01-13T10:44:37+00:00</div>
<div class="meta-line">Comments: Accepted at InGARSS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08420v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08420v1">PDF</a> · <a href="https://github.com/AdityaChaudhary2913/CLIP_HSI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMLGNet：使用CLIP进行遥感数据的跨模态对齐</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的多模态框架——多模态语言引导网络（MMLGNet），旨在使用视觉-语言模型（如CLIP）将异构遥感模态（如高光谱成像（HSI）和激光雷达）与自然语言语义对齐。随着多模态地球观测数据的日益丰富，迫切需要有效融合光谱、空间和几何信息的方法，同时实现语义级理解。MMLGNet采用特定模态的编码器，通过双向对比学习在共享潜在空间中将视觉特征与手工制作的文本嵌入对齐。受到CLIP训练范式的启发，我们的方法弥合了高维遥感数据与语言引导解释之间的差距。值得注意的是，MMLGNet在简单的基于CNN的编码器上表现出色，在两个基准数据集上超越了几种已建立的仅视觉多模态方法，展示了语言监督的显著优势。代码可在https://github.com/AdityaChaudhary2913/CLIP_HSI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to effectively fuse heterogeneous remote sensing modalities, such as Hyperspectral Imaging and LiDAR, with natural language semantics to enhance semantic-level understanding of Earth observation data. The authors propose a novel framework called Multimodal Language-Guided Network (MMLGNet), which utilizes modality-specific encoders and bi-directional contrastive learning to align visual features with textual embeddings in a shared latent space. Experimental results show that MMLGNet, using simple CNN-based encoders, outperforms several established multimodal visual-only methods on two benchmark datasets, highlighting the advantages of incorporating language supervision in remote sensing data analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是由于多模态地球观测数据的日益增加，需要有效融合异构遥感模态，如高光谱成像和激光雷达与自然语言语义。作者提出了一种名为多模态语言引导网络（MMLGNet）的新框架，该框架利用特定模态的编码器和双向对比学习，将视觉特征与共享潜在空间中的文本嵌入对齐。实验结果表明，MMLGNet使用简单的CNN编码器在两个基准数据集上超越了几种已建立的多模态视觉方法，突显了在遥感数据解释中引入语言监督的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care</div>
<div class="meta-line">Authors: Jorge García-Torres, Øyvind Meinich-Bache, Sara Brunner, Siren Rettedal, Vilde Kolstad, Kjersti Engan</div>
<div class="meta-line">First: 2025-03-05T07:52:52+00:00 · Latest: 2026-01-13T10:18:52+00:00</div>
<div class="meta-line">Comments: This work has been accepted at IEEE 25th International Conference on Digital Signal Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03244v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03244v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Around 10% of newborns require some help to initiate breathing, and 5\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双流热成像融合技术用于新生儿护理中出生时间检测的增强</div>
<div class="mono" style="margin-top:8px">约10%的新生儿需要帮助以启动呼吸，5%需要通气支持。准确的出生时间（ToB）记录对于优化新生儿护理至关重要，因为及时干预对适当的复苏至关重要。然而，目前记录ToB的临床方法往往依赖手动过程，容易出现不准确。在本研究中，我们提出了一种新颖的双流融合系统，结合图像和视频分析的优势，从分娩室和手术室的热成像记录中准确检测ToB。通过整合静态和动态流，我们的方法捕捉到更丰富的与出生相关的时空特征，从而实现更强大和精确的ToB估计。我们证明了数据模态之间的协同作用在性能上优于单流方法。我们的系统在短视频片段中检测出生的精确度达到95.7%，召回率为84.8%。此外，借助评分聚合模块，它成功识别了100%的测试案例，较手动标注的中位绝对误差为2秒，绝对平均偏差为4.5秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for accurate Time of Birth (ToB) documentation in neonatal care, as timely interventions are crucial for newborns requiring respiratory assistance. The study introduces a two-stream fusion system that integrates image and video analysis to enhance ToB detection from thermal recordings in delivery rooms and operating theaters. Experimental results indicate that this method achieves 95.7% precision and 84.8% recall in detecting birth events within short video clips, and it successfully identifies ToB in 100% of test cases, with a median absolute error of just 2 seconds compared to manual annotations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于新生儿护理中对准确出生时间（ToB）记录的需求，因为当前的手动方法可能导致不准确，从而影响及时干预。作者开发了一种双流融合系统，结合图像和视频分析，以增强从分娩室和手术室的热成像记录中检测ToB的能力。实验结果表明，该方法在短视频片段中检测出生事件的精确度达到95.7%，召回率为84.8%，并且在所有测试案例中成功识别ToB，较手动标注的中位绝对误差仅为2秒。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment</div>
<div class="meta-line">Authors: Hua Ye, Hang Ding, Siyuan Chen, Yiyang Jiang, Changyuan Zhang, Xuan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-11T16:15:15+00:00 · Latest: 2026-01-13T06:56:09+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08399v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过不对齐进行对齐：面向边界的多模态对齐课程学习</div>
<div class="mono" style="margin-top:8px">大多数多模态模型将每个负样本视为相同，忽略了与正样本仅在细节上有所不同的模糊负样本。我们提出了边界感知课程学习与局部注意力（BACL），这是一个轻量级的附加模块，将这些边界案例转化为课程信号。边界感知负样本采样器逐步提高难度，而对比局部注意力损失则突出不匹配发生的地方。这两个模块都是完全可微的，并且可以与任何现成的双编码器配合使用。理论预测错误率快速下降至O(1/n)；实践表明在CLIP上提高了高达32%的R@1，并在四个大规模基准上达到了新的SOTA，且无需额外标签。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in multimodal models where negative pairs are treated uniformly, overlooking ambiguous negatives that are similar to positives. The authors introduce Boundary-Aware Curriculum with Local Attention (BACL), which incorporates a Boundary-aware Negative Sampler to incrementally increase difficulty and a Contrastive Local Attention loss to identify mismatch areas. Experimental results demonstrate that this approach achieves up to a 32% improvement in R@1 over CLIP and sets new state-of-the-art performance on four large-scale benchmarks, all without requiring additional labels.</div>
<div class="mono" style="margin-top:8px">本研究解决了多模态模型中模糊负样本的处理问题，这些样本常常被统一对待，尽管它们与正样本之间的差异微小。作者提出了边界感知课程学习与局部注意力（BACL）的方法，该方法结合了边界感知负样本选择器，逐步提高训练样本的难度，以及对比局部注意力损失，以准确识别不匹配之处。实验结果表明，BACL在R@1上比CLIP提高了32%，并在四个大规模基准上创造了新的最先进性能，且无需额外标签。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向预测的深度强化学习在奶牛场高效电力负荷调度中的应用</div>
<div class="mono" style="margin-top:8px">奶牛养殖是一个能源密集型行业，严重依赖电网电力。随着可再生能源的不断整合，可持续能源管理已成为减少对电网依赖和支持联合国可持续发展目标7（可负担和清洁能源）的关键。然而，可再生能源的间歇性特征在实时平衡供需方面带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面显示出良好前景。然而，大多数基于RL的调度方法假设对未来价格或发电有完全的了解，这在动态环境中是不现实的。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，通常导致在可变电价下训练不稳定。为了解决这些挑战，本研究提出了一种深度强化学习框架，用于在奶牛场高效调度负荷，重点关注电池储存和水加热，并考虑现实的操作约束。所提出的面向预测的PPO结合了基于日时和月份的残差校准的短期需求和可再生发电预测，而PID KL PPO变体则采用比例-积分-微分控制器自适应调节KL散度，以实现稳定的策略更新。基于真实奶牛场数据进行训练，该方法的电力成本比PPO低1%，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO将电网进口减少了13.1%，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance energy management in dairy farming, an energy-intensive sector that is increasingly integrating renewable energy sources. The study introduces a Deep Reinforcement Learning framework for efficient load scheduling, specifically addressing the challenges posed by the intermittent nature of renewable energy and the unrealistic assumptions of complete future knowledge in traditional methods. Key experimental findings indicate that the proposed Forecast Aware PPO method achieves up to 1% lower electricity costs compared to standard PPO, 4.8% lower than DQN, and 1.5% lower than SAC, while also reducing grid imports by 13.1% for battery scheduling, demonstrating its effectiveness in sustainable energy management.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决可再生能源间歇性带来的挑战，提升奶牛养殖业这一能源密集型行业的能源管理。研究提出了一种深度强化学习框架用于高效负载调度，该框架结合了需求和可再生发电的短期预测，并利用PID KL PPO变体实现稳定的策略更新。实验结果表明，所提出的方法在电力成本上比标准PPO低1%，比DQN低4.8%，比SAC低1.5%，同时在电池调度方面减少了13.1%的电网进口，显示出其在奶牛养殖可持续能源管理中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-12T21:57:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：用于科学复合图的视觉条件面板检测与标题生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个标记面板合并为单个图像，但在实际流程中，标题往往缺失或仅提供图形级摘要，使得面板级理解变得困难。本文提出了FigEx2，一种视觉条件框架，能够从复合图中定位面板并直接生成面板级标题。为了减轻开放式标题生成中多样化措辞的影响，我们引入了一种噪声感知门控融合模块，能够自适应过滤标记级特征，以稳定检测查询空间。此外，我们采用了一种分阶段优化策略，将监督学习与强化学习（RL）相结合，利用基于CLIP的对齐和基于BERTScore的语义奖励来强制执行严格的多模态一致性。为了支持高质量的监督，我们策划了BioSci-Fig-Cap，这是一个经过精炼的面板级基础数据集，并配有物理和化学领域的跨学科测试套件。实验结果表明，FigEx2在检测方面达到了优越的0.726 mAP@0.5:0.95，并在METEOR上比Qwen3-VL-8B高出0.51，在BERTScore上高出0.24。值得注意的是，FigEx2在没有任何微调的情况下，展现出显著的零-shot迁移能力，能够适应分布外的科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the understanding of scientific compound figures, which often lack detailed panel-level captions. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates specific captions for each panel using a noise-aware gated fusion module to enhance token-level feature stability. Experimental results show that FigEx2 achieves a mean Average Precision of 0.726 for detection and outperforms the Qwen3-VL-8B model by 0.51 in METEOR and 0.24 in BERTScore, demonstrating strong zero-shot transferability to different scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善科学复合图的理解，这些图通常缺乏详细的面板级标题。作者提出了FigEx2，这是一种视觉条件框架，能够定位面板并为每个面板生成特定标题，采用噪声感知门控融合模块以增强标记级特征的稳定性。实验结果表明，FigEx2在检测中实现了0.726的平均精度（mAP），并在METEOR上比Qwen3-VL-8B模型高出0.51，在BERTScore上高出0.24，显示出在不同科学领域的强大零样本迁移能力，无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</div>
<div class="meta-line">Authors: Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi</div>
<div class="meta-line">First: 2025-12-28T18:24:19+00:00 · Latest: 2026-01-12T08:31:50+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23035v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23035v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xavierjiezou.github.io/Co2S/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过共同引导和共同融合实现稳定的半监督遥感分割</div>
<div class="mono" style="margin-top:8px">半监督遥感（RS）图像语义分割为减轻全面标注的负担提供了有前景的解决方案，但它在根本上面临伪标签漂移的问题，即确认偏差导致训练过程中错误的积累。在这项工作中，我们提出了Co2S，一个稳定的半监督RS分割框架，协同融合来自视觉-语言模型和自监督模型的先验信息。具体而言，我们构建了一个异构双学生架构，由两个不同的基于ViT的视觉基础模型组成，这些模型以预训练的CLIP和DINOv3初始化，以减轻错误积累和伪标签漂移。为了有效地结合这些不同的先验，提出了一种显式-隐式语义共同引导机制，利用文本嵌入和可学习查询分别提供显式和隐式的类别级引导，从而共同增强语义一致性。此外，开发了一种全局-局部特征协同融合策略，有效地将CLIP捕获的全局上下文信息与DINOv3生成的局部细节融合，使模型能够生成高度精确的分割结果。在六个流行数据集上的广泛实验表明，所提方法的优越性，在各种划分协议和不同场景中始终实现领先性能。项目页面可访问 https://xavierjiezou.github.io/Co2S/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of pseudo-label drift in semi-supervised remote sensing image semantic segmentation, which can lead to errors during training due to confirmation bias. The authors propose a framework called Co2S that integrates priors from vision-language models and self-supervised models through a heterogeneous dual-student architecture utilizing ViT-based models initialized with CLIP and DINOv3. Experimental results on six datasets show that Co2S effectively mitigates error accumulation and achieves superior segmentation performance across various protocols and scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决半监督遥感图像语义分割中的伪标签漂移问题，该问题由于确认偏差可能导致训练过程中的错误。作者提出了一种名为Co2S的框架，采用异构双学生架构，使用两个基于ViT的模型，分别初始化为预训练的CLIP和DINOv3，以减少错误积累。对六个数据集的实验结果表明，Co2S显著优于现有方法，在各种协议和场景中实现了更高的分割精度。</div>
</details>
</div>
<div class="card">
<div class="title">Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</div>
<div class="meta-line">Authors: Yuanyang Yin, Yufan Deng, Shenghai Yuan, Kaipeng Zhang, Xiao Yang, Feng Zhao</div>
<div class="meta-line">First: 2026-01-12T07:48:26+00:00 · Latest: 2026-01-12T07:48:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07287v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model&#x27;s learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦引导：从视频扩散模型中的语义弱层解锁可控性</div>
<div class="mono" style="margin-top:8px">图像到视频（I2V）生成任务旨在从参考图像和文本提示合成视频。这要求扩散模型在去噪过程中调和高频视觉约束和低频文本引导。然而，现有的I2V模型优先考虑视觉一致性，如何有效结合这两种引导以确保强烈遵循文本提示仍然未被充分探索。在本研究中，我们观察到在基于扩散变换器（DiT）的I2V模型中，某些中间层表现出弱语义响应（称为语义弱层），这通过文本-视觉相似度的可测量下降得以体现。我们将此归因于一种称为条件隔离的现象，其中对视觉特征的注意力部分脱离了文本引导，过度依赖学习到的视觉先验。为了解决这个问题，我们提出了聚焦引导（FG），它增强了来自语义弱层的可控性。FG包括两个机制：（1）细粒度语义引导（FSG）利用CLIP识别参考帧中的关键区域，并将其作为锚点来引导语义弱层。（2）注意力缓存将语义响应层的注意力图转移到语义弱层，注入显式语义信号，减轻它们对模型学习到的视觉先验的过度依赖，从而增强对文本指令的遵循。为了进一步验证我们的方法并解决这一方向缺乏评估的问题，我们引入了一个基准，用于评估I2V模型中的指令遵循。在这个基准上，聚焦引导证明了其有效性和通用性，将Wan2.1-I2V的总分提高到0.7250（+3.97%），并将基于MMDiT的HunyuanVideo-I2V提升到0.5571（+7.44%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adherence of Image-to-Video (I2V) generation models to text prompts, as existing models struggle to effectively couple visual and textual guidance. The authors propose a method called Focal Guidance (FG), which enhances controllability from Semantic-Weak Layers in Diffusion Transformer-based I2V models by implementing Fine-grained Semantic Guidance and Attention Cache mechanisms. Experimental results demonstrate that FG significantly improves instruction following, achieving a score of 0.7250 on the Wan2.1-I2V benchmark and a score of 0.5571 on the MMDiT-based HunyuanVideo-I2V benchmark, representing increases of 3.97% and 7.44%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像到视频（I2V）生成模型对文本提示的遵循程度，因为现有模型在有效结合视觉和文本指导方面存在困难。作者提出了一种名为焦点引导的方法，通过实施细粒度语义引导和注意力缓存机制，增强了扩散变换器基础的I2V模型中语义弱层的可控性。实验结果表明，焦点引导显著改善了指令遵循，在Wan2.1-I2V基准上获得了0.7250的分数，在MMDiT基础的HunyuanVideo-I2V基准上获得了0.5571的分数，分别提高了3.97%和7.44%。</div>
</details>
</div>
<div class="card">
<div class="title">VENUS: Visual Editing with Noise Inversion Using Scene Graphs</div>
<div class="meta-line">Authors: Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</div>
<div class="meta-line">First: 2026-01-12T05:24:58+00:00 · Latest: 2026-01-12T05:24:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07219v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VENUS：基于场景图的噪声反转视觉编辑</div>
<div class="mono" style="margin-top:8px">最先进的基于文本的图像编辑模型常常难以平衡背景保留与语义一致性，导致合成全新图像或输出未能实现预期编辑。相比之下，基于场景图的图像编辑通过提供语义实体及其关系的结构化表示来解决这一限制，从而提供更好的可控性。然而，现有的场景图编辑方法通常依赖于模型微调，这会产生高计算成本并限制可扩展性。为此，我们提出了VENUS（基于场景图的噪声反转视觉编辑），这是一个无训练的场景图引导图像编辑框架。具体而言，VENUS采用分离提示条件策略，将编辑的目标对象与其背景上下文分离，同时利用噪声反转在未编辑区域保持保真度。此外，我们提出的方法将从多模态大型语言模型中提取的场景图与扩散骨干网络集成，无需任何额外训练。实证结果表明，VENUS在PIE-Bench上显著提高了背景保留和语义对齐，将PSNR从22.45提高到24.80，SSIM从0.79提高到0.84，并将LPIPS从0.100降低到0.070，相较于最先进的场景图编辑模型（SGEdit）。此外，VENUS在CLIP相似性测量下增强了语义一致性（24.97对比24.19）。在EditVal上，VENUS以0.87的DINO分数实现了最高的保真度，并且显著将每张图像的运行时间从6-10分钟缩短至仅20-30秒。除了基于场景图的编辑，VENUS还超越了强大的基于文本的编辑基线，如LEDIT++和P2P+DirInv，从而在两种范式中均展示了一致的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing text-based image editing models, which often struggle with background preservation and semantic consistency. The authors introduce VENUS, a training-free framework for scene graph-guided image editing that utilizes a split prompt conditioning strategy and noise inversion to enhance controllability and fidelity. Experimental results show that VENUS significantly improves background preservation and semantic alignment, achieving a PSNR increase from 22.45 to 24.80 and enhancing CLIP similarity scores, while also reducing per-image runtime from 6-10 minutes to just 20-30 seconds compared to state-of-the-art models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有基于文本的图像编辑模型在背景保留和语义一致性方面的局限性。作者提出了VENUS，这是一种无训练的场景图引导图像编辑框架，利用分离提示条件策略和噪声反转来增强可控性和保真度。实验结果表明，VENUS在PIE-Bench上显著改善了背景保留和语义对齐，PSNR从22.45提高到24.80，SSIM从0.79提高到0.84，LPIPS从0.100降低到0.070，相较于最先进的模型SGEdit，同时在EditVal上以0.87的DINO得分实现了最高的保真度，并将每张图像的运行时间从6-10分钟减少到20-30秒。</div>
</details>
</div>
<div class="card">
<div class="title">CLIMP: Contrastive Language-Image Mamba Pretraining</div>
<div class="meta-line">Authors: Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes</div>
<div class="meta-line">First: 2026-01-11T12:31:55+00:00 · Latest: 2026-01-11T12:31:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06891v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI&#x27;s CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP&#x27;s fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIMP：对比语言-图像Mamba预训练</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）依赖于视觉变换器，其注意机制容易受到虚假相关性的影响，并且与分辨率呈二次方关系。为了解决这些限制，我们提出了CLIMP，这是第一个完全基于Mamba的对比视觉-语言模型，替换了视觉和文本编码器。新架构在视觉和语言中编码序列结构，VMamba捕捉视觉空间归纳偏差，减少对虚假相关性的依赖，并生成有利于跨模态检索和分布外鲁棒性的嵌入空间，在ImageNet-O上超越OpenAI的CLIP-ViT-B 7.5%。CLIMP自然支持可变输入分辨率，无需位置编码插值或专门训练，在16倍训练分辨率下实现高达6.6%的检索准确率，同时使用5倍更少的内存和1.8倍更少的FLOPs。自回归文本编码器进一步克服了CLIP的固定上下文限制，使得密集标注检索成为可能。我们的研究结果表明，Mamba在视觉-语言学习中表现出有利特性，使其成为基于变换器的CLIP的有力替代品。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of the CLIP model, particularly its susceptibility to spurious correlations and its quadratic scaling with resolution due to the Vision Transformers&#x27; attention mechanism. The authors introduce CLIMP, a novel contrastive vision-language model that utilizes Mamba for both vision and text encoders, allowing for the encoding of sequential structures and reducing reliance on spurious correlations. Experimental results demonstrate that CLIMP surpasses OpenAI&#x27;s CLIP-ViT-B by 7.5% on ImageNet-O, achieves up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs, and supports dense captioning retrieval through its autoregressive text encoder, indicating Mamba&#x27;s potential advantages in vision-language learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有对比语言-图像模型的局限性，特别是视觉变换器对虚假相关性的敏感性以及其在分辨率上的平方级扩展。作者提出了CLIMP，这是一种新颖的模型，利用Mamba基础的编码器来处理视觉和文本，从而能够编码序列结构并减少对虚假相关性的依赖。实验结果表明，CLIMP在ImageNet-O上超越了OpenAI的CLIP-ViT-B 7.5%，在16倍训练分辨率下实现了高达6.6%的检索准确率提升，同时使用了5倍更少的内存和1.8倍更少的FLOP，并通过自回归文本编码器支持密集的标题检索，表明Mamba在视觉-语言学习中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Continual Learning for Privacy-Preserving Hospital Imaging Classification</div>
<div class="meta-line">Authors: Anay Sinhal, Arpana Sinhal, Amit Sinhal</div>
<div class="meta-line">First: 2026-01-11T01:28:34+00:00 · Latest: 2026-01-11T01:28:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06742v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06742v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning models for radiology interpretation increasingly rely on multi-institutional data, yet privacy regulations and distribution shift across hospitals limit central data pooling. Federated learning (FL) allows hospitals to collaboratively train models without sharing raw images, but current FL algorithms typically assume a static data distribution. In practice, hospitals experience continual evolution in case mix, annotation protocols, and imaging devices, which leads to catastrophic forgetting when models are updated sequentially. Federated continual learning (FCL) aims to reconcile these challenges but existing methods either ignore the stringent privacy constraints of healthcare or rely on replay buffers and public surrogate datasets that are difficult to justify in clinical settings. We study FCL for chest radiography classification in a setting where hospitals are clients that receive temporally evolving streams of cases and labels. We introduce DP-FedEPC (Differentially Private Federated Elastic Prototype Consolidation), a method that combines elastic weight consolidation (EWC), prototype-based rehearsal, and client-side differential privacy within a standard FedAvg framework. EWC constrains updates along parameters deemed important for previous tasks, while a memory of latent prototypes preserves class structure without storing raw images. Differentially private stochastic gradient descent (DP-SGD) at each client adds calibrated Gaussian noise to clipped gradients, providing formal privacy guarantees for individual radiographs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>隐私保护医院影像分类的联邦持续学习</div>
<div class="mono" style="margin-top:8px">放射学解读的深度学习模型越来越依赖于多机构数据，但隐私法规和医院间的分布变化限制了中央数据汇聚。联邦学习（FL）允许医院在不共享原始图像的情况下协作训练模型，但当前的FL算法通常假设数据分布是静态的。在实践中，医院的病例组合、注释协议和影像设备持续演变，这导致模型在顺序更新时出现灾难性遗忘。联邦持续学习（FCL）旨在解决这些挑战，但现有方法要么忽视医疗保健的严格隐私约束，要么依赖于重放缓冲区和难以在临床环境中证明的公共替代数据集。我们研究了在医院作为客户接收时间演变的病例和标签流的环境中进行胸部X光分类的FCL。我们引入了DP-FedEPC（差分隐私联邦弹性原型整合），这是一种结合了弹性权重整合（EWC）、基于原型的排练和客户端差分隐私的标准FedAvg框架的方法。EWC限制了对被认为对先前任务重要的参数的更新，而潜在原型的记忆在不存储原始图像的情况下保留了类别结构。每个客户端的差分隐私随机梯度下降（DP-SGD）向裁剪的梯度添加了经过校准的高斯噪声，为单个X光片提供了正式的隐私保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of privacy regulations and data distribution shifts in multi-institutional hospital settings, which hinder the effective use of deep learning models for radiology interpretation. The authors propose a novel method called DP-FedEPC, which integrates differential privacy with federated continual learning to enable hospitals to collaboratively train models on evolving data streams without sharing raw images. Key experimental findings demonstrate that DP-FedEPC effectively mitigates catastrophic forgetting while maintaining privacy, allowing for improved chest radiography classification across different hospitals over time.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多机构医院环境中隐私法规和数据分布变化带来的挑战，这些因素阻碍了深度学习模型在放射学解读中的有效应用。作者提出了一种新方法DP-FedEPC，该方法在联邦学习框架内结合了差分隐私、弹性权重巩固和基于原型的重演，能够在不妨碍患者隐私的情况下实现持续学习。实验结果表明，DP-FedEPC有效减轻了灾难性遗忘，同时保持隐私保证，使医院能够在病例组合和注释协议不断变化的情况下，共同提高胸部放射影像分类的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</div>
<div class="meta-line">Authors: Renren Jin, Pengzhi Gao, Yuqi Ren, Zhuowen Han, Tongxuan Zhang, Wuwei Huang, Wei Liu, Jian Luan, Deyi Xiong</div>
<div class="meta-line">First: 2025-11-08T12:50:41+00:00 · Latest: 2026-01-10T08:58:33+00:00</div>
<div class="meta-line">Comments: 22 pages, 25 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05993v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05993v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has emerged as a prominent paradigm for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, leading to premature convergence to suboptimal local minima and hindering further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To bridge this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our results identify three key factors that influence entropy: the clipping thresholds in the optimization objective, the number of off-policy updates, and the diversity of the training data. Furthermore, through both theoretical analysis and empirical validation, we demonstrate that tokens with positive advantages are the primary drivers of entropy collapse. Motivated by this insight, we propose Positive-Advantage Reweighting, a simple yet effective approach that regulates model entropy by adjusting the loss weights assigned to tokens with positive advantages during RLVR training, while maintaining competitive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视大推理模型中的强化学习熵</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习（RLVR）已成为增强大型语言模型（LLMs）推理能力的一个重要范式。然而，LLMs的熵在RLVR训练过程中通常会崩溃，导致过早收敛到次优局部最小值，阻碍进一步的性能提升。尽管已经提出了各种方法来减轻熵崩溃，但对RLVR中熵的全面研究仍然缺乏。为填补这一空白，我们进行广泛实验，调查使用RLVR训练的LLMs的熵动态，并分析模型熵与响应多样性、校准和在各种基准上的性能之间的关系。我们的结果识别出影响熵的三个关键因素：优化目标中的裁剪阈值、离策略更新的数量以及训练数据的多样性。此外，通过理论分析和实证验证，我们证明了具有正优势的标记是熵崩溃的主要驱动因素。基于这一见解，我们提出了正优势重加权，这是一种简单而有效的方法，通过调整在RLVR训练过程中分配给具有正优势的标记的损失权重来调节模型熵，同时保持竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of entropy collapse in large language models (LLMs) during reinforcement learning with verifiable rewards (RLVR), which can lead to suboptimal performance. The authors conducted extensive experiments to analyze the dynamics of entropy in LLMs trained with RLVR, focusing on its correlation with response diversity, calibration, and overall performance across various benchmarks. They identified three main factors affecting entropy: clipping thresholds in the optimization objective, the number of off-policy updates, and the diversity of training data, and proposed a method called Positive-Advantage Reweighting to regulate model entropy by adjusting loss weights for tokens with positive advantages, ultimately maintaining competitive performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）在可验证奖励的强化学习（RLVR）训练中出现的熵崩溃问题，这可能导致性能不佳。作者进行了广泛的实验，分析了使用RLVR训练的LLMs的熵动态，并探讨了模型熵、响应多样性、校准和在各种基准上的性能之间的相关性。主要发现表明，熵受剪切阈值、离线更新次数和训练数据多样性的影响，而具有正优势的标记是熵崩溃的主要因素。为了缓解这一问题，他们提出了正优势重加权方法，该方法通过调整这些标记的损失权重来调节模型熵，同时保持竞争性能。</div>
</details>
</div>
<div class="card">
<div class="title">FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching</div>
<div class="meta-line">Authors: Hongyaoxing Gul, Lijuan Hu, Shuzi Niu, Fangfang Liu</div>
<div class="meta-line">First: 2026-01-09T10:06:45+00:00 · Latest: 2026-01-09T10:06:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLRQ：基于灵活低秩矩阵草图的快速LLM量化</div>
<div class="mono" style="margin-top:8px">传统的后训练量化（PTQ）被认为是减少模型大小和加速大规模语言模型（LLM）推理的有效方法。然而，现有的低秩PTQ方法需要昂贵的微调来确定适用于大型模型中多样数据和层的折中秩，未能充分发挥其潜力。此外，当前基于SVD的低秩近似增加了计算开销。在本研究中，我们全面分析了在代表性模型中不同层次的低秩近似的有效性变化。因此，我们引入了灵活低秩量化（FLRQ），这是一种新颖的解决方案，旨在快速识别准确性最优的秩并将其聚合以实现最小存储组合。FLRQ包含两个强大的组件，基于Rank1-Sketch的灵活秩选择（R1-FLR）和剪切下的最佳低秩近似（BLC）。R1-FLR应用带有高斯投影的R1-Sketch进行快速低秩近似，使每层的异常值感知秩提取成为可能。同时，BLC旨在通过迭代方法在缩放和剪切策略下最小化低秩量化误差。FLRQ在全面实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面均实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of post-training quantization (PTQ) for large-scale language models (LLMs), as existing methods often require expensive fine-tuning and do not fully leverage the potential of low-rank approximations. The authors propose a novel approach called Flexible Low-Rank Quantization (FLRQ), which includes two main components: Rank1-Sketch-based Flexible Rank Selection (R1-FLR) for rapid low-rank approximation and Best Low-rank Approximation under Clipping (BLC) to minimize quantization error. Experimental results show that FLRQ achieves state-of-the-art performance in both quantization quality and algorithm efficiency, demonstrating its effectiveness and robustness across various model layers.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大规模语言模型（LLMs）后训练量化（PTQ）的效率，因为传统方法通常需要昂贵的微调，并且未能充分利用模型的能力。作者提出了一种新方法，称为灵活低秩量化（FLRQ），该方法包括基于Rank1-Sketch的灵活秩选择（R1-FLR），用于快速低秩近似，以及在剪辑下的最佳低秩近似（BLC），以最小化量化误差。实验结果表明，FLRQ在量化质量和算法效率方面均达到了最先进的性能，展示了其在各种模型层上的有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Neural-Driven Image Editing</div>
<div class="meta-line">Authors: Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, Kai Wang, Qian Zheng, Hao Jin, Xiaojun Chang, Gang Pan, Shurong Dong, Kaipeng Zhang, Yang You</div>
<div class="meta-line">First: 2025-07-07T18:31:50+00:00 · Latest: 2026-01-09T10:06:25+00:00</div>
<div class="meta-line">Comments: 22 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05397v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05397v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://loongx1.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. The code and dataset are released on the project website: https://loongx1.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经驱动的图像编辑</div>
<div class="mono" style="margin-top:8px">传统的图像编辑通常依赖手动提示，劳动强度大且对运动控制或语言能力有限的个体不够友好。利用脑-计算机接口（BCI）和生成模型的最新进展，我们提出了LoongX，一种基于多模态神经生理信号的免手动图像编辑方法。LoongX利用在23,928对图像编辑配对上训练的最先进的扩散模型，每对配对都与同步的脑电图（EEG）、功能近红外光谱（fNIRS）、光电容积描记（PPG）和捕捉用户意图的头部运动信号相结合。为有效应对这些信号的异质性，LoongX集成了两个关键模块。跨尺度状态空间（CS3）模块编码信息丰富的模态特征。动态门控融合（DGF）模块进一步将这些特征聚合到统一的潜在空间中，然后通过在扩散变换器（DiT）上的微调与编辑语义对齐。此外，我们使用对比学习预训练编码器，以将认知状态与嵌入自然语言的语义意图对齐。大量实验表明，LoongX的性能与文本驱动的方法相当（CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636），并且在神经信号与语音结合时表现更佳（CLIP-T: 0.2588 vs. 0.2549）。这些结果突显了神经驱动生成模型在实现可访问、直观的图像编辑方面的潜力，并为认知驱动的创意技术开辟了新方向。代码和数据集已在项目网站发布：https://loongx1.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a more accessible image editing tool for individuals with limited motor control or language abilities, addressing the labor-intensive nature of traditional methods. The authors propose LoongX, a hands-free image editing system that utilizes multimodal neurophysiological signals, integrating a cross-scale state space module and a dynamic gated fusion module to process and unify these signals. Experimental results show that LoongX achieves performance comparable to existing text-driven methods and outperforms them when combining neural signals with speech, indicating its potential for intuitive and accessible image editing.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为运动能力或语言能力有限的个体创造一种更易于访问的图像编辑方法，以解决传统编辑的劳动密集型特性。作者提出了LoongX，这是一种利用多模态神经生理信号和最先进的扩散模型的免手动图像编辑方法，训练数据集包含23,928对图像编辑配对及同步的脑电图（EEG）、功能近红外光谱（fNIRS）、光电容积描记（PPG）和头部运动信号。实验结果表明，LoongX的性能与文本驱动的方法相当，并且在结合神经信号与语音时表现更佳，显示出其在实现直观图像编辑和推动认知驱动创意技术方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR</div>
<div class="meta-line">Authors: Zijun Min, Bingshuai Liu, Ante Wang, Long Zhang, Anxiang Zeng, Haibo Zhang, Jinsong Su</div>
<div class="meta-line">First: 2026-01-09T07:57:40+00:00 · Latest: 2026-01-09T07:57:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05607v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>令牌与序列的协调：用于RLVR的动态混合策略优化</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）为优化推理任务中的大型语言模型提供了一个有前景的框架。然而，现有的RLVR算法关注不同的粒度，各自具有互补的优缺点。组相对策略优化（GRPO）通过令牌级重要性比率更新策略，保留了细粒度的信用分配，但通常面临高方差和不稳定性。相比之下，组序列策略优化（GSPO）在响应中的所有令牌上应用单一序列级重要性比率，更好地匹配序列级奖励，但牺牲了令牌级信用分配。本文提出动态混合策略优化（DHPO），在单一剪切代理目标中桥接GRPO和GSPO。DHPO使用加权机制结合令牌级和序列级重要性比率。我们探索了两种混合机制的变体，包括平均混合和熵引导混合。为了进一步稳定训练，我们采用了特定于分支的剪切策略，在混合之前将令牌级和序列级比率限制在单独的信任区域内，防止任一分支中的异常值主导更新。在七个具有挑战性的数学推理基准上，对Qwen3系列的稠密模型和MoE模型的实验表明，DHPO始终优于GRPO和GSPO。我们将在论文接受后发布我们的代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the optimization of large language models in reasoning tasks using Reinforcement Learning with Verifiable Rewards (RLVR), addressing the limitations of existing algorithms that operate at different granularities. The authors propose Dynamic Hybrid Policy Optimization (DHPO), which integrates token-level and sequence-level importance ratios through a single clipped surrogate objective, employing two mixing mechanisms and a branch-specific clipping strategy to stabilize training. Experimental results demonstrate that DHPO outperforms both Group Relative Policy Optimization (GRPO) and Group Sequence Policy Optimization (GSPO) across seven challenging mathematical reasoning benchmarks, indicating its effectiveness in improving model performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过可验证奖励的强化学习（RLVR）提升大型语言模型在推理任务中的优化，解决现有算法在不同粒度下的局限性。作者提出了动态混合策略优化（DHPO），通过单一的剪切代理目标整合了基于令牌和序列的重要性比率，采用两种混合机制变体和特定分支的剪切策略以稳定训练。实验结果表明，DHPO在七个具有挑战性的数学推理基准测试中始终优于基于令牌的相对策略优化（GRPO）和基于序列的策略优化（GSPO），显示出其在提升模型性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization</div>
<div class="meta-line">Authors: Xinghao Wang, Changtao Miao, Dianmo Sheng, Tao Gong, Qi Chu, Nenghai Yu, Quanchen Zou, Deyue Zhang, Xiangzheng Zhang</div>
<div class="meta-line">First: 2026-01-09T07:25:55+00:00 · Latest: 2026-01-09T07:25:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06222v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06222v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that feature variations at manipulated boundaries are substantially larger than in interior regions. To address this gap, we propose Semantic-Agnostic Prompt Learning (SAPL) in CLIP, which learns text prompts that intentionally encode non-semantic, boundary-centric cues so that CLIPs multimodal similarity highlights manipulation edges rather than high-level object semantics. SAPL combines two complementary modules Edge-aware Contextual Prompt Learning (ECPL) and Hierarchical Edge Contrastive Learning (HECL) to exploit edge information in both textual and visual spaces. The proposed ECPL leverages edge-enhanced image features to generate learnable textual prompts via an attention mechanism, embedding semantic-irrelevant information into text features, to guide CLIP focusing on manipulation edges. The proposed HECL extract genuine and manipulated edge patches, and utilize contrastive learning to boost the discrimination between genuine edge patches and manipulated edge patches. Finally, we predict the manipulated regions from the similarity map after processing. Extensive experiments on multiple public benchmarks demonstrate that SAPL significantly outperforms existing approaches, achieving state-of-the-art localization performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAPL：CLIP中语义无关的提示学习用于弱监督图像操控定位</div>
<div class="mono" style="margin-top:8px">恶意图像操控威胁公共安全，需要高效的定位方法。现有方法依赖于昂贵的像素级注释，使训练成本高昂。现有的弱监督方法仅依赖于图像级二元标签，专注于全局分类，常常忽视对精确定位至关重要的局部边缘线索。我们观察到，操控边界的特征变化显著大于内部区域。为了解决这一问题，我们提出了CLIP中的语义无关提示学习（SAPL），该方法学习有意编码非语义、以边界为中心的文本提示，使CLIP的多模态相似性突出操控边缘而非高层次对象语义。SAPL结合了两个互补模块：边缘感知上下文提示学习（ECPL）和层次边缘对比学习（HECL），以利用文本和视觉空间中的边缘信息。所提出的ECPL利用边缘增强的图像特征，通过注意机制生成可学习的文本提示，将与语义无关的信息嵌入文本特征中，以引导CLIP关注操控边缘。所提出的HECL提取真实和操控的边缘补丁，并利用对比学习增强真实边缘补丁与操控边缘补丁之间的区分。最后，我们从处理后的相似性图中预测操控区域。在多个公共基准上的广泛实验表明，SAPL显著优于现有方法，实现了最先进的定位性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of malicious image manipulation, which poses risks to public safety and necessitates effective localization methods. The authors introduce Semantic-Agnostic Prompt Learning (SAPL) in CLIP, a method that generates text prompts focused on boundary-centric cues rather than high-level semantics, enhancing the detection of manipulation edges. Experimental results show that SAPL, through its Edge-aware Contextual Prompt Learning and Hierarchical Edge Contrastive Learning modules, significantly improves localization performance on various public benchmarks, outperforming existing techniques that rely on pixel-level annotations or image-level labels.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决恶意图像篡改带来的公共安全风险，并寻找不依赖昂贵像素级注释的有效定位方法。作者提出了一种名为语义无关提示学习（SAPL）的CLIP方法，重点学习强调边界中心线索而非高层语义的文本提示，利用两个模块：边缘感知上下文提示学习（ECPL）和分层边缘对比学习（HECL）。实验结果表明，SAPL在多个公共基准测试中显著提高了定位性能，有效突出篡改边缘，超越了现有的弱监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances</div>
<div class="meta-line">Authors: Qiwei Yang, Pingping Zhang, Yuhao Wang, Zijing Gong</div>
<div class="meta-line">First: 2026-01-09T05:22:58+00:00 · Latest: 2026-01-09T05:22:58+00:00</div>
<div class="meta-line">Comments: Accepted by WACV2026 VReID-XFD Workshop. Our final framework ranks the first on the VReID-XFD challenge leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05535v1">PDF</a> · <a href="https://github.com/YangQiWei3/SAS-VPReID">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAS-VPReID：一种具有形状先验的尺度自适应框架，用于极远距离视频基础的人体重识别</div>
<div class="mono" style="margin-top:8px">视频基础的人体重识别（VPReID）旨在从非重叠摄像头捕获的视频中检索同一人。在极远距离下，由于严重的分辨率降级、剧烈的视角变化和不可避免的外观噪声，VPReID面临着巨大的挑战。为了解决这些问题，我们提出了一种具有形状先验的尺度自适应框架，命名为SAS-VPReID。该框架建立在三个互补模块之上。首先，我们部署了一个增强记忆的视觉主干（MEVB）来提取区分性特征表示，利用CLIP视觉编码器和多代理记忆。其次，我们提出了多粒度时间建模（MGTM），以在多个时间粒度上构建序列，并自适应地强调跨尺度的运动线索。第三，我们结合了先验正则化的形状动态（PRSD）来捕捉身体结构动态。通过这些模块，我们的框架能够获得更具区分性的特征表示。在VReID-XFD基准上的实验证明了每个模块的有效性，我们的最终框架在VReID-XFD挑战排行榜上排名第一。源代码可在https://github.com/YangQiWei3/SAS-VPReID获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video-based person re-identification (VPReID) at extreme distances, where challenges such as resolution degradation and viewpoint variation hinder performance. The authors propose a Scale-Adaptive framework with Shape Priors, named SAS-VPReID, which consists of three modules: a Memory-Enhanced Visual Backbone for feature extraction, a Multi-Granularity Temporal Modeling for emphasizing motion cues, and Prior-Regularized Shape Dynamics for capturing body structure dynamics. Experimental results on the VReID-XFD benchmark show that each module contributes effectively, leading to the framework achieving the top rank on the challenge leaderboard.</div>
<div class="mono" style="margin-top:8px">本研究解决了在极远距离下视频行人重识别（VPReID）面临的挑战，如分辨率降低和视角变化等问题。作者提出了一种名为SAS-VPReID的尺度自适应框架与形状先验，该框架由三个模块组成：用于特征提取的记忆增强视觉主干、多粒度时间建模以强调运动线索，以及用于捕捉身体结构动态的先验正则化形状动态。对VReID-XFD基准的实验结果表明，该框架有效增强了特征表示，并在挑战排行榜上获得了第一名。</div>
</details>
</div>
<div class="card">
<div class="title">Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation</div>
<div class="meta-line">Authors: Mingxia Zhan, Li Zhang, Beibei Wang, Yingjie Wang, Zenglin Shi</div>
<div class="meta-line">First: 2026-01-04T09:59:43+00:00 · Latest: 2026-01-09T02:54:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01457v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01457v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言作为先验，视觉作为校准：单目深度估计的度量尺度恢复</div>
<div class="mono" style="margin-top:8px">相对深度基础模型转移良好，但单目度量深度由于无法识别的全局尺度和增强的领域转移敏感性而仍然是病态的。在冻结骨干校准设置下，我们通过图像特定的仿射变换在逆深度中恢复度量深度，并仅训练轻量级校准头，同时保持相对深度骨干和CLIP文本编码器固定。由于字幕提供了粗略但嘈杂的尺度线索，这些线索随着措辞和缺失对象而变化，我们使用语言预测一个不确定性感知的包络，界定在无约束空间中可行的校准参数，而不是仅仅承诺于文本的点估计。然后，我们使用汇聚的多尺度冻结视觉特征在这个包络内选择图像特定的校准。在训练过程中，逆深度中的封闭形式最小二乘oracle为学习包络和选定校准提供每图像监督。在NYUv2和KITTI上的实验提高了领域内的准确性，而对SUN-RGBD和DDAD的零-shot转移则展示了相较于强语言仅基线的改进鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges of monocular metric depth estimation, which suffers from issues related to unidentifiable global scale and sensitivity to domain shifts. The authors propose a method that utilizes a frozen-backbone calibration approach, where metric depth is recovered through an image-specific affine transform in inverse depth while training lightweight calibration heads. Key experimental findings indicate that this method improves in-domain accuracy on datasets like NYUv2 and KITTI, and demonstrates enhanced robustness in zero-shot transfer scenarios to SUN-RGBD and DDAD compared to strong language-only baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决单目度量深度估计面临的全球尺度不可识别和对领域转移敏感的问题。作者提出了一种方法，采用冻结骨干校准方法，利用逆深度中的图像特定仿射变换，同时训练轻量级校准头，保持相对深度骨干和CLIP文本编码器不变。关键实验结果表明，他们的方法在NYUv2和KITTI等数据集上提高了领域内的准确性，并在零样本转移到SUN-RGBD和DDAD的场景中，相较于强语言基线显示出更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots</div>
<div class="meta-line">Authors: Shubham Agarwal, Ofek Nourian, Michael Sidorov, Sharon Chemweno, Ofer Hadar, Naftali Lazarovitch, Jhonathan E. Ephrath</div>
<div class="meta-line">First: 2026-01-09T02:30:48+00:00 · Latest: 2026-01-09T02:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05482v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于植物根系检测与分析的多图像超分辨率框架</div>
<div class="mono" style="margin-top:8px">理解植物根系对于推进土壤-植物相互作用、养分吸收和整体植物健康的研究至关重要。然而，由于遮挡、土壤湿度变化和固有的低对比度等不利条件，准确成像地下环境中的根系仍然是一个持续的挑战，这限制了传统基于视觉的方法的有效性。在本研究中，我们提出了一种新型地下成像系统，该系统捕捉植物根系的多个重叠视图，并集成了基于深度学习的多图像超分辨率（MISR）框架，旨在增强根系的可见性和细节。为了训练和评估我们的方法，我们构建了一个合成数据集，模拟现实的地下成像场景，结合影响图像质量的关键环境因素。我们提出的MISR算法利用视图之间的空间冗余重建高分辨率图像，提高了结构保真度和视觉清晰度。定量评估表明，我们的方法优于最先进的超分辨率基线，BRISQUE降低了2.3%，表明在相同的CLIP-IQA评分下图像质量得到了改善，从而增强了根系的表型分析。这反过来又促进了对关键根系特征的准确估计，包括根毛数量和根毛密度。所提出的框架为农业和生态研究中的自动地下植物根系成像和特征量化提供了一个有前景的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of accurately imaging plant root systems in subterranean environments, which is essential for understanding soil-plant interactions and plant health. The authors developed a novel underground imaging system that captures multiple overlapping views of roots and employs a deep learning-based Multi-Image Super Resolution (MISR) framework to enhance image quality. Experimental results demonstrate that the MISR algorithm significantly improves image clarity and structural fidelity, achieving a 2.3 percent reduction in BRISQUE scores compared to state-of-the-art methods, thus facilitating better phenotypic analysis and estimation of critical root traits such as root hair count and density.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善植物根系的成像，这对于理解土壤-植物相互作用和植物健康至关重要，尤其是在地下环境中面临的挑战。作者开发了一种新型地下成像系统，捕捉植物根系的多个重叠视图，并集成了基于深度学习的多图像超分辨率（MISR）框架，以增强根系的可见性。实验结果表明，MISR算法显著提高了图像质量，与最先进的方法相比，BRISQUE评分降低了2.3%，这有助于更准确地分析根系特征，如根毛数量和密度。</div>
</details>
</div>
<div class="card">
<div class="title">e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings</div>
<div class="meta-line">Authors: Haonan Chen, Sicheng Gao, Radu Timofte, Tetsuya Sakai, Zhicheng Dou</div>
<div class="meta-line">First: 2026-01-07T07:39:40+00:00 · Latest: 2026-01-09T02:24:32+00:00</div>
<div class="meta-line">Comments: https://huggingface.co/Haon-Chen/e5-omni-7B</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03666v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03666v2">PDF</a> · <a href="https://huggingface.co/Haon-Chen/e5-omni-7B">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>e5-omni：用于全模态嵌入的显式跨模态对齐</div>
<div class="mono" style="margin-top:8px">现代信息系统通常涉及不同类型的项目，例如文本查询、图像、视频片段或音频段。这促使了全模态嵌入模型的出现，这些模型将异构模态映射到共享空间以进行直接比较。然而，最近的大多数全模态嵌入仍然严重依赖于从预训练的视觉-语言模型（VLM）骨干网络继承的隐式对齐。在实践中，这导致了三个常见问题：（i）相似性对数具有模态依赖的尖锐度，因此得分不在一致的尺度上；（ii）批次中的负样本随着时间的推移变得不那么有效，因为混合模态批次创建了不平衡的困难分布；因此，许多负样本迅速变得微不足道，贡献的梯度很少；（iii）跨模态的嵌入显示出不匹配的一级和二级统计，这使得排名不够稳定。为了解决这些问题，我们提出了e5-omni，这是一种轻量级的显式对齐方案，将现成的VLM适配为稳健的全模态嵌入模型。e5-omni结合了三个简单的组件：（1）模态感知的温度校准以对齐相似性尺度，（2）可控的负样本课程与去偏见，以关注混淆的负样本，同时减少假负样本的影响，以及（3）带有协方差正则化的批次白化，以更好地匹配共享嵌入空间中的跨模态几何。MMEB-V2和AudioCaps上的实验显示出相对于强大的双模态和全模态基线的一致增益，并且相同的方案也很好地迁移到其他VLM骨干网络。我们在https://huggingface.co/Haon-Chen/e5-omni-7B发布了我们的模型检查点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve omni-modal embedding models that map various types of items, such as text, images, videos, and audio, into a shared space for better comparison. The authors propose e5-omni, which employs a lightweight explicit alignment method to enhance the performance of existing vision-language models (VLMs). Key experimental results demonstrate that e5-omni consistently outperforms strong bi-modal and omni-modal baselines on datasets like MMEB-V2 and AudioCaps, while also showing effective transferability to other VLM backbones.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善将文本、图像、视频和音频等不同类型项目映射到共享空间的全模态嵌入模型，以便进行更好的比较。作者提出了e5-omni，这是一种通过三种组件实现显式跨模态对齐的方法：模态感知温度校准、可控负样本课程和带协方差正则化的批量白化。MMEB-V2和AudioCaps上的实验结果表明，e5-omni在强双模态和全模态基线之上始终表现出一致的提升，表明其在解决相似性对数、负样本采样和嵌入稳定性等问题上的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-task Cross-modal Learning for Chest X-ray Image Retrieval</div>
<div class="meta-line">Authors: Zhaohui Liang, Sivaramakrishnan Rajaraman, Niccolo Marini, Zhiyun Xue, Sameer Antani</div>
<div class="meta-line">First: 2026-01-08T21:44:00+00:00 · Latest: 2026-01-08T21:44:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05399v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05399v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model&#x27;s enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多任务跨模态学习用于胸部X光图像检索</div>
<div class="mono" style="margin-top:8px">CLIP和BiomedCLIP是视觉-语言基础模型的例子，提供强大的跨模态嵌入；然而，它们并未针对细粒度医学检索任务进行优化，例如使用胸部X光（CXR）图像查询检索临床相关的放射学报告。为了解决这一不足，我们提出了一种多任务学习框架，以微调BiomedCLIP并评估CXR图像-文本检索的改进。以BiomedCLIP为基础，我们结合了一个轻量级的MLP投影头，该头使用多任务复合损失函数进行训练，包括：（1）用于区分正常与异常CXR研究的二元交叉熵损失，（2）用于增强类内一致性的监督对比损失，以及（3）用于保持跨模态对齐的CLIP损失。实验结果表明，微调后的模型在图像到文本和文本到图像检索任务中相比于预训练的BiomedCLIP和通用CLIP模型，表现出更平衡和临床相关的性能。此外，t-SNE可视化显示正常和异常案例的语义聚类更清晰，展示了模型增强的诊断敏感性。这些发现突显了领域自适应的多任务学习在推进生物医学应用中的跨模态检索的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the retrieval of clinically relevant radiology reports using chest X-ray images, as existing vision-language models like CLIP and BiomedCLIP are not optimized for such fine-grained medical tasks. The authors propose a multi-task learning framework that fine-tunes BiomedCLIP by integrating a lightweight MLP projector head trained with a composite loss function, which includes binary cross-entropy, supervised contrastive loss, and CLIP loss. Experimental results indicate that the fine-tuned model outperforms both the pretrained BiomedCLIP and general-purpose CLIP models in image-to-text and text-to-image retrieval tasks, with t-SNE visualizations showing improved semantic clustering of normal and abnormal cases, thus enhancing diagnostic sensitivity in biomedical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善使用胸部X光图像检索临床相关放射学报告的能力，因为现有的视觉-语言模型如CLIP和BiomedCLIP并未针对细粒度医学任务进行优化。作者提出了一种多任务学习框架，通过结合轻量级的MLP投影头和复合损失函数来微调BiomedCLIP，该损失函数包括二元交叉熵损失、监督对比损失和CLIP损失。实验结果表明，微调后的模型在图像到文本和文本到图像的检索任务中优于预训练的BiomedCLIP和通用CLIP模型，并且t-SNE可视化显示正常和异常病例的语义聚类得到了改善，从而增强了生物医学应用中的诊断敏感性。</div>
</details>
</div>
<div class="card">
<div class="title">From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</div>
<div class="meta-line">Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar</div>
<div class="meta-line">First: 2026-01-08T16:02:56+00:00 · Latest: 2026-01-08T16:02:56+00:00</div>
<div class="meta-line">Comments: Contributed original research to top tier conference in VLM; currently undergoing peer review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05059v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05059v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut &amp; Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从理解到参与：通过视觉语言模型（VLMs）个性化药学视频剪辑</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）有望通过实现智能、可扩展和自动化的多模态内容处理，彻底改变制药行业的数字化转型。传统的异构数据模态（文本、图像、视频、音频和网页链接）的手动标注容易导致不一致、质量下降和内容利用效率低下。大量的长视频和音频数据进一步加剧了这些挑战（例如，长时间的临床试验访谈和教育研讨会）。在这里，我们介绍了一种领域适应的视频到视频剪辑生成框架，该框架集成了音频语言模型（ALMs）和视觉语言模型（VLMs）以生成精彩剪辑。我们的贡献有三方面：（i）可重复的剪切与合并算法，具有淡入/淡出和时间戳归一化，确保平滑过渡和音视频对齐；（ii）基于角色定义和提示注入的个性化机制，以实现定制输出（市场营销、培训、监管）；（iii）一种成本高效的端到端管道策略，平衡ALM/VLM增强处理。在视频MME基准（900）和我们在14个疾病领域的16,159个药学视频的专有数据集上的评估表明，速度提升3到4倍，成本降低4倍，剪辑质量具有竞争力。除了效率提升，我们还报告了我们的方法在剪辑连贯性得分（0.348）和信息量得分（0.721）上优于最先进的VLM基线（例如，Gemini 2.5 Pro），突显了透明、定制提取和合规支持的视频摘要在生命科学中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies and inconsistencies in the traditional manual annotation of diverse data modalities in the pharmaceutical industry, particularly in processing long video and audio content. The authors propose a Video to Video Clip Generation framework that combines Audio Language Models and Vision Language Models to create highlight clips, utilizing a reproducible Cut &amp; Merge algorithm for smooth transitions, a personalization mechanism for tailored outputs, and a cost-efficient end-to-end pipeline. Experimental results on a benchmark dataset and a proprietary collection of pharmacy videos show a 3 to 4 times speedup, a 4 times reduction in costs, and improved clip coherence and informativeness scores compared to existing state-of-the-art models, indicating significant advancements in video summarization for the life sciences sector.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决制药行业中传统手动注释多种数据模态的低效和不一致性，特别是针对长视频和音频数据。作者提出了一种视频到视频剪辑生成框架，结合音频语言模型和视觉语言模型来创建高亮剪辑，利用可重复的剪切与合并算法实现平滑过渡，个性化机制提供定制输出，以及成本高效的端到端管道。实验结果显示，在基准数据集和一个专有的药房视频集合上，处理速度提高了3到4倍，成本降低了4倍，剪辑的连贯性和信息量评分也优于现有的最先进模型，表明在生命科学领域的视频摘要方面取得了显著进展。</div>
</details>
</div>
<div class="card">
<div class="title">On the Hidden Objective Biases of Group-based Reinforcement Learning</div>
<div class="meta-line">Authors: Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</div>
<div class="meta-line">First: 2026-01-08T15:00:35+00:00 · Latest: 2026-01-08T15:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05002v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于基于群体的强化学习的隐含目标偏差</div>
<div class="mono" style="margin-top:8px">基于群体的强化学习方法，如群体相对策略优化（GRPO），如今被广泛用于对大型语言模型进行后训练。尽管它们在经验上取得了成功，但在奖励优化与基础训练目标之间存在结构性不匹配。本文通过在统一的替代公式中研究GRPO风格的方法，提出了理论分析。这一视角揭示了影响所有分析方法的反复出现的特性：（i）非均匀的群体加权在共享前缀标记上引发系统性梯度偏差；（ii）与AdamW优化器的交互使得训练动态对奖励缩放几乎不敏感；（iii）优化器动量可能在重复优化步骤中将策略更新推送到超出预期裁剪区域。我们认为，这些发现突显了当前方法的基本局限性，并为未来公式的设计提供了原则性指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the structural mismatches between reward optimization and the training objectives in group-based reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), which are commonly used for post-training large language models. The authors employ a theoretical analysis within a unified surrogate formulation to investigate these methods. The key findings indicate that non-uniform group weighting leads to systematic gradient biases, interactions with the AdamW optimizer render training dynamics insensitive to reward scaling, and optimizer momentum can cause policy updates to exceed the intended clipping region during repeated optimization steps, highlighting significant limitations in current approaches and offering guidance for future developments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于组的强化学习方法（特别是组相对策略优化（GRPO））中奖励优化与训练目标之间的结构不匹配，这些方法通常用于大型语言模型的后训练。作者采用统一的替代公式进行理论分析，以研究GRPO风格方法的特性。主要发现表明，非均匀组加权导致系统性的梯度偏差，与AdamW优化器的交互使得对奖励缩放的敏感性降低，优化器动量可能导致策略更新在重复优化步骤中超出预期的裁剪区域，这突显了当前方法的重大局限性，并为未来方法设计提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">GAPO: Robust Advantage Estimation for Real-World Code LLMs</div>
<div class="meta-line">Authors: Jianqing Zhang, Zhezheng Hao, Wei Xia, Hande Dong, Hong Wang, Chenxing Wei, Yuyan Zhou, Yubin Qi, Qiang Lin, Jian Cao</div>
<div class="meta-line">First: 2025-10-22T03:37:49+00:00 · Latest: 2026-01-08T08:42:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21830v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.21830v4">PDF</a> · <a href="https://github.com/TsingZ0/verl-GAPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods, such as GRPO, are popular due to their critic-free and normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable noise, leading to distorted advantage computation and increased rollout outliers. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an interval with the highest SNR (Signal to Noise Ratio) per prompt and uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation to reduce noise further. This adaptive Q robustly handles rollout noise while remaining plug-and-play and efficient. We evaluate GAPO on nine instruction-tuned LLMs (3B-14B) using a collected large dataset of 51,844 real-world, history-aware code-editing tasks spanning 10 programming languages. GAPO yields up to 4.35 in-domain (ID) and 5.30 out-of-domain (OOD) exact-match improvements over GRPO and its variant DAPO, while achieving lower clipping ratios and higher GPU throughput. Code: https://github.com/TsingZ0/verl-GAPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAPO：针对现实世界代码LLMs的鲁棒优势估计</div>
<div class="mono" style="margin-top:8px">强化学习（RL）广泛用于后训练大型语言模型（LLMs）在代码编辑中的应用，其中群体相对方法，如GRPO，由于其无评论者和标准化优势估计而受到欢迎。然而，在现实世界的代码编辑场景中，奖励分布往往偏斜且噪声不可预测，导致优势计算扭曲和回滚异常增多。为了解决这个问题，我们提出了群体自适应策略优化（GAPO），它自适应地为每个提示找到具有最高信噪比（SNR）的区间，并使用该区间的中位数作为自适应Q，以替代优势计算中的群体均值，从而进一步减少噪声。这个自适应Q能够鲁棒地处理回滚噪声，同时保持即插即用和高效。我们在九个经过指令调优的LLMs（3B-14B）上评估GAPO，使用收集的51844个现实世界、历史感知的代码编辑任务的大型数据集，涵盖10种编程语言。GAPO在与GRPO及其变体DAPO的比较中，域内（ID）和域外（OOD）精确匹配提升分别达到4.35和5.30，同时实现了更低的剪切比和更高的GPU吞吐量。代码：https://github.com/TsingZ0/verl-GAPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the robustness of advantage estimation in reinforcement learning for real-world code editing tasks, where traditional methods struggle due to skewed reward distributions and noise. The authors propose Group Adaptive Policy Optimization (GAPO), which adaptively identifies the interval with the highest signal-to-noise ratio for each prompt and uses the median of that interval as an adaptive Q value, enhancing the accuracy of advantage calculations. Experimental results demonstrate that GAPO outperforms existing methods, achieving improvements of up to 4.35 in-domain and 5.30 out-of-domain exact matches over GRPO and DAPO, while also reducing clipping ratios and increasing GPU throughput.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善强化学习在代码编辑任务中的优势估计的鲁棒性，这些任务常常受到奖励分布偏斜和噪声的影响。作者提出了群体自适应策略优化（GAPO）方法，该方法自适应地为每个提示识别具有最高信噪比的区间，并使用该区间的中位数作为自适应Q值，替代优势计算中的组均值，以减轻噪声。实验结果表明，GAPO显著提高了性能，相比现有方法在领域内和领域外的精确匹配上分别提高了4.35和5.30，同时降低了剪切比率并提高了GPU吞吐量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
