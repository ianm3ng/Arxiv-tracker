<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-17 03:41</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260217_0341</div>
    <div class="row"><div class="card">
<div class="title">Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos</div>
<div class="meta-line">Authors: Albert J. Zhai, Kuo-Hao Zeng, Jiasen Lu, Ali Farhadi, Shenlong Wang, Wei-Chiu Ma</div>
<div class="meta-line">First: 2026-02-13T18:59:10+00:00 · Latest: 2026-02-13T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot&#x27;s ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模仿有效方法：从人类视频中进行模拟过滤的模块化策略学习</div>
<div class="mono" style="margin-top:8px">通过观看人类视频学习操作技能的能力有潜力为机器人学习解锁一种新的高度可扩展的数据源。在这里，我们处理抓取操作，其中任务涉及在执行各种抓取后动作之前抓取物体。人类视频为学习抓取后动作提供了强有力的信号，但对于学习先决的抓取行为则不太有用，尤其是对于没有类人手的机器人。一个有前景的解决方案是使用模块化策略设计，利用专用的抓取生成器来产生稳定的抓取。然而，任意稳定的抓取往往与任务不兼容，阻碍了机器人执行所需下游动作的能力。为了解决这个挑战，我们提出了感知-模拟-模仿（PSI）框架，用于使用经过配对抓取轨迹过滤的模拟处理的人类视频运动数据训练模块化操作策略。这个模拟步骤扩展了轨迹数据，并附加了抓取适用性标签，从而允许对面向任务的抓取能力进行监督学习。我们通过现实世界实验表明，我们的框架可以在没有任何机器人数据的情况下高效地学习精确的操作技能，结果表现出比简单使用抓取生成器更强的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the potential of learning manipulation skills from human videos to enhance robot learning. The authors propose a framework called Perceive-Simulate-Imitate (PSI), which utilizes a modular policy design and simulation-filtered grasp-trajectory data to train robots in prehensile manipulation tasks. Experimental results demonstrate that this approach enables robots to learn precise manipulation skills efficiently without requiring any robot data, leading to significantly improved performance compared to traditional methods using a grasp generator alone.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过观看人类视频学习操作技能，以增强机器人学习的可扩展数据潜力。作者提出了一种名为感知-模拟-模仿（PSI）的框架，该框架采用模块化策略设计，包括一个抓取生成器，并利用模拟对人类视频数据进行过滤，以获得与任务兼容的抓取行为。实验结果表明，该方法使机器人能够高效地学习精确的操作技能，而无需任何机器人数据，从而显著提高了性能，相较于简单使用抓取生成器的效果更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision</div>
<div class="meta-line">Authors: Aadarsh Sahoo, Georgia Gkioxari</div>
<div class="meta-line">First: 2026-02-13T18:58:30+00:00 · Latest: 2026-02-13T18:58:30+00:00</div>
<div class="meta-line">Comments: Project webpage: https://glab-caltech.github.io/converseg/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13195v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/converseg/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., &quot;left-most apple&quot;) and overlooks functional and physical reasoning (e.g., &quot;where can I safely store the knife?&quot;). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对话式图像分割：用可扩展监督基础抽象概念</div>
<div class="mono" style="margin-top:8px">对话式图像分割将抽象的、意图驱动的概念基础到像素精确的掩码上。之前的图像基础工作主要集中在类别和空间查询（例如，“最左边的苹果”）上，忽视了功能和物理推理（例如，“我可以安全存放刀子在哪里？”）。我们解决了这一空白，介绍了对话式图像分割（CIS）和ConverSeg，一个涵盖实体、空间关系、意图、可用性、功能、安全性和物理推理的基准。我们还提出了ConverSeg-Net，它将强大的分割先验与语言理解相结合，以及一个无需人工监督生成提示-掩码对的AI驱动数据引擎。我们展示了当前的语言引导分割模型对CIS来说是不够的，而在我们的数据引擎上训练的ConverSeg-Net在ConverSeg上取得了显著的提升，并在现有的语言引导分割基准上保持了强劲的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance conversational image segmentation by grounding abstract concepts into precise pixel masks, addressing limitations in prior work that primarily focused on categorical and spatial queries. The authors introduce a new framework called Conversational Image Segmentation (CIS) and a benchmark named ConverSeg, which encompasses a variety of elements including spatial relations and functional reasoning. They also develop ConverSeg-Net, which integrates advanced segmentation techniques with language comprehension, alongside an AI-driven data engine for generating prompt-mask pairs autonomously. Experimental results indicate that existing language-guided segmentation models fall short for CIS tasks, while ConverSeg-Net demonstrates substantial improvements on the ConverSeg benchmark and retains strong performance on traditional language-guided segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过将抽象概念与精确的像素掩码结合，提升对话式图像分割的能力，解决了以往方法主要关注类别和空间查询的局限性。作者提出了一种新的框架，称为对话式图像分割（CIS）以及一个名为ConverSeg的基准，涵盖了包括功能和物理推理在内的更广泛查询。他们还提出了ConverSeg-Net，该网络将先进的分割技术与语言理解相结合，并且开发了一个AI驱动的数据引擎，能够自主生成提示-掩码对。实验结果表明，现有的语言引导分割模型在CIS任务中表现不佳，而使用新数据引擎训练的ConverSeg-Net在ConverSeg基准上显示出显著的改进，并在已建立的语言引导分割任务中保持强劲表现。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Chunking and the Entropy of Natural Language</div>
<div class="meta-line">Authors: Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks</div>
<div class="meta-line">First: 2026-02-13T18:58:10+00:00 · Latest: 2026-02-13T18:58:10+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13194v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义分块与自然语言的熵</div>
<div class="mono" style="margin-top:8px">印刷英语的熵率被著名地估计为每个字符约一比特，这是现代大型语言模型（LLMs）最近才接近的基准。这一熵率意味着英语相对于随机文本预期的每个字符五比特，几乎包含80%的冗余。我们引入了一种统计模型，试图捕捉自然语言复杂的多尺度结构，为这一冗余水平提供了从第一原理出发的解释。我们的模型描述了一种自相似地将文本分割成语义连贯块的过程，直到单词级别。文本的语义结构可以层次分解，从而允许进行分析处理。与现代LLMs和开放数据集的数值实验表明，我们的模型定量捕捉了真实文本在不同语义层次上的结构。我们模型预测的熵率与印刷英语的估计熵率一致。此外，我们的理论进一步揭示，自然语言的熵率并非固定，而应随着语料库的语义复杂性系统性增加，这一复杂性由我们模型中唯一的自由参数捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the redundancy in natural language, specifically the estimated entropy rate of printed English, which suggests a significant level of redundancy. The authors introduce a statistical model that segments text into semantically coherent chunks, allowing for a hierarchical decomposition of its semantic structure. Experimental results indicate that this model accurately captures the structure of real texts at various semantic levels and predicts an entropy rate that aligns with existing estimates, while also suggesting that the entropy rate increases with the semantic complexity of the text corpus.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自然语言中的冗余，特别关注印刷英语的熵率，约为每个字符一比特。为此，作者提出了一种统计模型，将文本分割成语义上连贯的块，从而允许对其结构进行分层分解。实验结果表明，该模型有效捕捉了真实文本的语义层次，并与印刷英语的估计熵率一致，同时还表明熵率随着文本语料的语义复杂性而增加。</div>
</details>
</div>
<div class="card">
<div class="title">CoPE-VideoLM: Codec Primitives For Efficient Video Language Models</div>
<div class="meta-line">Authors: Sayan Deb Sarkar, Rémi Pautrat, Ondrej Miksik, Marc Pollefeys, Iro Armeni, Mahdi Rad, Mihai Dusmanu</div>
<div class="meta-line">First: 2026-02-13T18:57:31+00:00 · Latest: 2026-02-13T18:57:31+00:00</div>
<div class="meta-line">Comments: Project Page: https://sayands.github.io/cope/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13191v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13191v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sayands.github.io/cope/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoPE-VideoLM：高效视频语言模型的编解码原语</div>
<div class="mono" style="margin-top:8px">视频语言模型（VideoLMs）使AI系统能够理解视频中的时间动态。为了适应最大上下文窗口限制，当前方法使用关键帧采样，这可能会因稀疏的时间覆盖而错过宏观事件和微观细节。此外，处理每帧的完整图像及其标记会产生大量计算开销。为了解决这些限制，我们建议利用视频编解码原语（特别是运动矢量和残差），这些原语本质上编码了视频的冗余和稀疏性，而无需对大多数帧进行昂贵的全图像编码。为此，我们引入了轻量级的基于变换器的编码器，这些编码器聚合编解码原语，并通过预训练策略将其表示与图像编码器嵌入对齐，从而加速端到端微调过程中的收敛。与标准VideoLMs相比，我们的方法将首次标记的时间缩短了高达$86\%$，标记使用量减少了高达$93\%$。此外，通过改变关键帧和编解码原语的密度，我们能够在$14$个多样的视频理解基准上保持或超过性能，这些基准涵盖了一般问答、时间推理、长篇理解和空间场景理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of Video Language Models (VideoLMs) by addressing the limitations of keyframe sampling, which can overlook important events and details in videos while also incurring high computational costs. The authors propose a method that utilizes video codec primitives, specifically motion vectors and residuals, to capture video redundancy and sparsity without the need for full-image encoding. Experimental results demonstrate that this approach significantly reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to traditional VideoLMs, while maintaining or exceeding performance across 14 diverse video understanding benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视频语言模型（VideoLMs）在理解视频时间动态方面的效率，解决关键帧采样可能忽视重要细节的局限性。作者提出了一种利用视频编码原语（特别是运动矢量和残差）的方法，以在不需要全图像编码的情况下编码视频冗余和稀疏性，并结合轻量级的基于变换器的编码器，通过预训练策略将编码原语的表示与图像嵌入对齐。实验结果表明，该方法显著减少了首次生成令牌的时间，最多可减少86%，令牌使用量最多可减少93%，同时在14个不同的视频理解基准上保持或超过了标准VideoLMs的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares</div>
<div class="meta-line">Authors: Po-Heng Chou, Chiapin Wang, Kuan-Hao Chen, Wei-Chen Hsiao</div>
<div class="meta-line">First: 2025-11-12T00:14:10+00:00 · Latest: 2026-02-13T18:56:52+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, 1 table, and submitted to 2026 IEEE ICC Workshops</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08852v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations. Unlike conventional geometry or CSI-dependent approaches, the policy learns directly from uplink pilot responses and geometry features, enabling robust localization without explicit CSI estimation. An augmented WLS jointly estimates position and receiver clock bias, improving numerical stability under dynamic beam geometry. Across representative scenarios, the proposed method reduces the mean positioning error by 99.3% compared with the geometry-based baseline, achieving 0.395 m RMSE with near real-time inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的低轨卫星星座波束定位方法</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于强化学习的波束加权框架，将策略网络与增强加权最小二乘（WLS）估计器结合，以实现多波束低轨卫星星座中的准确且低复杂度的定位。与传统的几何或CSI依赖方法不同，该策略直接从上行导频响应和几何特征中学习，实现了无需显式CSI估计的鲁棒定位。增强WLS联合估计位置和接收机时钟偏差，在动态波束几何下提高了数值稳定性。在代表性场景中，所提方法相比于基于几何的基线减少了99.3%的平均定位误差，实现了0.395米的均方根误差（RMSE），并接近实时推断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurate positioning in multi-beam low Earth orbit (LEO) satellite constellations by proposing a reinforcement learning-based beam weighting framework. The method integrates a policy network with an augmented weighted least squares estimator, allowing the system to learn from uplink pilot responses and geometry features without relying on conventional channel state information (CSI) estimation. Experimental results demonstrate that the proposed approach significantly reduces the mean positioning error by 99.3% compared to traditional geometry-based methods, achieving a root mean square error of 0.395 m with near real-time inference capabilities.</div>
<div class="mono" style="margin-top:8px">本研究解决了多波束低地球轨道（LEO）卫星星座中精确定位的挑战，动机在于传统几何或信道状态信息（CSI）依赖方法的局限性。作者提出了一种基于强化学习的波束加权框架，将策略网络与增强加权最小二乘（WLS）估计器相结合，允许直接从上行导频响应和几何特征进行稳健定位，而无需显式的CSI估计。实验结果表明，该方法相比几何基线显著降低了99.3%的平均定位误差，实现了0.395米的均方根误差，并具备近实时推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments</div>
<div class="meta-line">Authors: Po-Heng Chou, Da-Chih Lin, Hung-Yu Wei, Walid Saad, Yu Tsao</div>
<div class="meta-line">First: 2025-11-12T00:13:37+00:00 · Latest: 2026-02-13T18:53:36+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, 2 tables, and submitted to 2026 IEEE ICC Workshops</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08851v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.08851v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a measurement-driven case study on early radio link failure (RLF) warning as device-side network sensing and analytics for proactive mobility management in 5G non-standalone (NSA) railway environments. Using 10~Hz metro-train measurement traces with serving- and neighbor-cell indicators, we benchmark six representative learning models, including CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under multiple observation windows and prediction horizons. Rather than proposing a new prediction architecture, this study focuses on quantifying the feasibility of early warning and the trade-offs among observation context, prediction horizon, and alarm reliability under real railway mobility. Experimental results show that learning models can anticipate RLF-related reliability degradation seconds in advance using lightweight features available on commercial devices. The presented benchmark provides practical insights for sensing-assisted communication control, such as proactive redundancy activation and adaptive handover strategies, aligning with the 6G vision of integrating sensing and analytics into mobility control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于测量数据集的铁路环境学习型无线链路故障预测</div>
<div class="mono" style="margin-top:8px">本文提出了一项基于测量的案例研究，旨在实现早期无线链路故障（RLF）警告，作为设备端网络感知和分析，以便在5G非独立（NSA）铁路环境中进行主动移动管理。利用10~Hz的地铁列车测量轨迹及服务和邻近小区指标，我们基准测试了六种代表性的学习模型，包括CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST和TimesNet，涵盖多个观察窗口和预测视野。该研究并未提出新的预测架构，而是专注于量化早期警告的可行性以及在真实铁路移动环境中观察上下文、预测视野和警报可靠性之间的权衡。实验结果表明，学习模型可以利用商业设备上可用的轻量特征提前数秒预测与RLF相关的可靠性下降。所呈现的基准为感知辅助通信控制提供了实用见解，如主动冗余激活和自适应切换策略，符合将感知与分析整合到移动控制中的6G愿景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance proactive mobility management in 5G non-standalone railway environments by providing early warnings for radio link failures (RLF). The authors utilize a measurement-driven approach, benchmarking six learning models, including CNN, LSTM, and XGBoost, using 10 Hz metro-train measurement traces with serving- and neighbor-cell indicators across various observation windows and prediction horizons. The key findings indicate that these models can predict RLF-related reliability degradation several seconds in advance by leveraging lightweight features from commercial devices, offering valuable insights for improving communication control strategies in line with the 6G vision.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提供无线链路故障（RLF）的早期警告来增强5G非独立铁路环境中的主动移动管理。作者采用基于测量的方法，利用10 Hz地铁列车测量轨迹，并在多个观察窗口和预测时间范围内对六种学习模型（包括CNN、LSTM和XGBoost）进行基准测试。关键发现表明，这些模型能够利用商业设备中的轻量特征提前几秒预测与RLF相关的可靠性下降，为改善与6G愿景相一致的通信控制策略提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">R-Zero: Self-Evolving Reasoning LLM from Zero Data</div>
<div class="meta-line">Authors: Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-08-07T03:38:16+00:00 · Latest: 2026-02-13T18:53:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05004v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.05004v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R-Zero：从零数据自我进化的推理大语言模型</div>
<div class="mono" style="margin-top:8px">自我进化的大语言模型（LLMs）通过自主生成、完善和学习自身经验，为超智能提供了一条可扩展的路径。然而，现有的训练此类模型的方法仍然严重依赖于大量人工策划的任务和标签，通常通过微调或强化学习，这对推动人工智能系统向超越人类智能的能力发展构成了根本瓶颈。为克服这一限制，我们引入了R-Zero，一个完全自主的框架，从零开始生成自己的训练数据。从单一基础LLM开始，R-Zero初始化两个具有不同角色的独立模型，一个是挑战者（Challenger），另一个是解决者（Solver）。这些模型分别进行优化，并通过互动共同进化：挑战者因提出接近解决者能力边缘的任务而获得奖励，而解决者因解决挑战者提出的日益具有挑战性的任务而获得奖励。这个过程产生了一个有针对性的、自我改善的课程，而无需任何预先存在的任务和标签。实证结果表明，R-Zero显著提高了不同基础LLM的推理能力，例如，在数学推理基准上提升Qwen3-4B-Base的表现+6.49，在通用领域推理基准上提升+7.54。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop self-evolving Large Language Models (LLMs) that can achieve super-intelligence by autonomously generating and learning from their own experiences, thus overcoming the limitations of existing methods that rely on human-curated tasks. The authors introduce R-Zero, an autonomous framework that creates its own training data from scratch by initializing two models, a Challenger and a Solver, which co-evolve through interaction where the Challenger proposes tasks and the Solver attempts to solve them. The experimental results demonstrate that R-Zero significantly enhances reasoning capabilities across various backbone LLMs, achieving improvements of +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks for the Qwen3-4B-Base model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发能够自主生成和学习自身经验的自我进化大型语言模型（LLM），从而克服当前依赖人工策划任务和标签的方法的局限性。作者提出了R-Zero，这是一种自主框架，初始化两个模型，一个是挑战者，另一个是解决者，这两个模型通过互动共同进化，而无需依赖现有任务。实验结果表明，R-Zero显著提高了各种基础LLM的推理能力，使Qwen3-4B-Base模型在数学推理基准上提高了+6.49，在一般领域推理基准上提高了+7.54。</div>
</details>
</div>
<div class="card">
<div class="title">Selection of CMIP6 Models for Regional Precipitation Projection and Climate Change Assessment in the Jhelum and Chenab River Basins</div>
<div class="meta-line">Authors: Saad Ahmed Jamal, Ammara Nusrat, Muhammad Azmat, Muhammad Osama Nusrat</div>
<div class="meta-line">First: 2026-02-13T18:41:40+00:00 · Latest: 2026-02-13T18:41:40+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective water resource management depends on accurate projections of flows in water channels. For projected climate data, use of different General Circulation Models (GCM) simulates contrasting results. This study shows selection of GCM for the latest generation CMIP6 for hydroclimate change impact studies. Envelope based method was used for the selection, which includes components based on machine learning techniques, allowing the selection of GCMs without the need for in-situ reference data. According to our knowledge, for the first time, such a comparison was performed for the CMIP6 Shared Socioeconomic Pathway (SSP) scenarios data. In addition, the effect of climate change under SSP scenarios was studied, along with the calculation of extreme indices. Finally, GCMs were compared to quantify spatiotemporal differences between CMIP5 and CMIP6 data. Results provide NorESM2 LM, FGOALS g3 as selected models for the Jhelum and Chenab River. Highly vulnerable regions under the effect of climate change were highlighted through spatial maps, which included parts of Punjab, Jammu, and Kashmir. Upon comparison of CMIP5 and CMIP6, no discernible difference was found between the RCP and SSP scenarios precipitation projections. In the future, more detailed statistical comparisons could further reinforce the proposition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>选择CMIP6模型用于杰赫勒姆和陈纳布河流域的区域降水预测和气候变化评估</div>
<div class="mono" style="margin-top:8px">有效的水资源管理依赖于对水道流量的准确预测。对于气候数据的预测，不同的全球气候模型（GCM）模拟出截然不同的结果。本研究展示了为水文气候变化影响研究选择最新一代CMIP6的GCM。采用基于信封的方法进行选择，该方法包括基于机器学习技术的组件，允许在没有现场参考数据的情况下选择GCM。据我们所知，这是首次针对CMIP6共享社会经济路径（SSP）情景数据进行此类比较。此外，还研究了SSP情景下气候变化的影响，并计算了极端指数。最后，对GCM进行了比较，以量化CMIP5和CMIP6数据之间的时空差异。结果提供了NorESM2 LM和FGOALS g3作为杰赫勒姆和陈纳布河的选定模型。通过空间地图突出显示了气候变化影响下的高度脆弱区域，包括旁遮普、查谟和克什米尔的部分地区。在比较CMIP5和CMIP6时，RCP和SSP情景的降水预测之间没有明显差异。未来，更详细的统计比较可能进一步加强这一提议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for accurate water resource management through effective projections of river flows, which are influenced by varying results from different General Circulation Models (GCMs). The researchers employed an envelope-based method incorporating machine learning techniques to select suitable CMIP6 models for hydroclimate change impact studies without relying on in-situ reference data. The findings identified NorESM2 LM and FGOALS g3 as the optimal models for the Jhelum and Chenab River basins, highlighted vulnerable regions affected by climate change, and revealed no significant differences in precipitation projections between CMIP5 and CMIP6 under RCP and SSP scenarios.</div>
<div class="mono" style="margin-top:8px">本研究解决了对杰赫伦和陈纳布河流域水流准确预测的需求，这对有效的水资源管理至关重要。研究人员采用了一种基于信封的方法，结合机器学习技术，从最新的CMIP6数据集中选择气候模型（GCM），使得在没有现场参考数据的情况下进行模型选择成为可能。研究结果表明，NorESM2 LM和FGOALS g3是这些流域最合适的模型，同时空间地图揭示了受气候变化影响的高脆弱区域，特别是在旁遮普、查谟和克什米尔地区；然而，在RCP和SSP情景下，CMIP5和CMIP6之间的降水预测没有显著差异。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps</div>
<div class="meta-line">Authors: Swati Gupta, Jai Moondra, Mohit Singh</div>
<div class="meta-line">First: 2026-02-13T18:37:26+00:00 · Latest: 2026-02-13T18:37:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13177v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).
  Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用镜像映射组合改进在线镜像下降的遗憾保证</div>
<div class="mono" style="margin-top:8px">OMD及其变体为在线凸优化提供了灵活的框架，其性能在很大程度上依赖于镜像映射的选择。虽然OPGD和OEG这两种OMD的特例所基于的几何结构已被很好理解，但如何为任何给定的约束集和一般的损失函数族（例如稀疏损失）构造最优镜像映射仍然是一个具有挑战性的开放问题。我们考虑一个更简单的问题：通过使用在$L_1$和$L_2$之间插值的几何体的镜像映射，是否有可能获得多项式的遗憾增益，这在仅限于OEG（$L_1$）或OPGD（$L_2$）的情况下可能是不可能的。我们的主要结果对此问题给出了肯定的答案。我们表明，基于块范数的镜像映射比以前的$L_p$（对于$p \in [1, 2]$）插值更好地适应损失函数的稀疏性。特别地，我们构造了一系列在$\mathbb{R}^d$中的在线凸优化实例，其中基于块范数的镜像映射在稀疏损失函数上相较于OEG和OPGD实现了可证明的多项式（关于$d$）的遗憾改进。然后，我们转向损失函数的稀疏性水平未知的情况。在这种情况下，几何体的选择本身成为一个在线决策问题。我们首先表明，天真地在OEG和OPGD之间切换可能会导致线性遗憾，突显了几何选择的内在困难。为了解决这个问题，我们提出了一种基于乘法权重的元算法，动态选择一组均匀块范数。我们表明，这种方法有效地调整OMD以适应损失的稀疏性，从而产生自适应的遗憾保证。总体而言，我们的结果表明，在线镜像映射选择可以显著增强OMD在在线凸优化中利用稀疏性的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of constructing optimal mirror maps for online convex optimization (OCO) that can adapt to various loss functions, particularly sparse losses. The authors investigate the potential for polynomial gains in regret by employing mirror maps that interpolate between L1 and L2 geometries, rather than being limited to OEG or OPGD. Their key findings reveal that block norm-based mirror maps provide a significant improvement in regret performance for sparse loss functions, achieving polynomial improvements over traditional methods. Additionally, they propose a meta-algorithm that dynamically selects among uniform block norms to address the unknown sparsity level of loss functions, demonstrating that this adaptive approach enhances the effectiveness of online mirror descent.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于构建能够有效处理各种损失函数，特别是稀疏损失的在线凸优化（OCO）最优镜像映射的挑战。作者探讨了通过使用在L1和L2几何之间插值的镜像映射来获得多项式的遗憾增益的潜力，而不是仅限于OEG或OPGD。研究的主要发现表明，基于块范数的镜像映射在稀疏损失函数下相比传统方法提供了显著的多项式遗憾改善，并提出了一种动态选择均匀块范数的元算法，以自适应地调整OMD以适应损失的稀疏性，从而增强遗憾保证。</div>
</details>
</div>
<div class="card">
<div class="title">Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace</div>
<div class="meta-line">Authors: Seth Donahue, J. D. Peiffer, R. Tyler Richardson, Yishan Zhong, Shaun Q. Y. Tan, Benoit Marteau, Stephanie R. Russo, May D. Wang, R. James Cotton, Ross Chafetz</div>
<div class="meta-line">First: 2026-02-13T18:36:27+00:00 · Latest: 2026-02-13T18:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13176v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13176v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单目无标记运动捕捉实现上肢可达工作空间的定量评估</div>
<div class="mono" style="margin-top:8px">验证一种临床可及的方法，通过单个（单目）相机和人工智能（AI）驱动的无标记运动捕捉（MMC）进行上肢可达工作空间（UERW）的定量分析。对这些技术在特定临床任务中的客观评估和验证对于其在临床运动分析中的采用至关重要。AI驱动的单目MMC降低了在临床中采用的障碍，并有潜力减少对这种常见临床评估的分析开销。九名没有障碍的成年参与者执行标准化的UERW任务，该任务涉及在以躯干为中心的虚拟球体中达到分布的目标，目标通过VR头显显示。使用基于标记的运动捕捉系统和一组八个FLIR相机同时捕捉运动。我们对这两个视频相机视角进行了单目视频分析，以比较正面和偏移相机配置。正面相机方向与基于标记的参考显示出强一致性，平均偏差为每个八分之一空间达到的$0.61 \pm 0.12$ \%。相比之下，偏移相机视角低估了达到的工作空间百分比（$-5.66 \pm 0.45$ \%）。结论：研究结果支持正面单目相机配置用于UERW评估的可行性，特别是在前方工作空间评估中，与基于标记的运动捕捉的一致性最高。整体表现展示了单相机评估的临床潜力。本研究首次验证了单目MMC系统用于UERW任务评估。通过降低技术复杂性，该方法使定量上肢活动评估的更广泛实施成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to validate a clinically accessible method for quantifying the Upper Extremity Reachable Workspace (UERW) using a single monocular camera and AI-driven Markerless Motion Capture (MMC). The study involved nine adult participants performing a standardized UERW task, with movements captured by both a marker-based system and a monocular camera. Results indicated that the frontal camera configuration showed strong agreement with the marker-based reference, with a minimal mean bias of 0.61 ± 0.12% reachspace reached per octant, while the offset camera view underestimated the workspace reached by -5.66 ± 0.45%. These findings support the use of a frontal monocular camera for UERW assessment, highlighting its clinical potential for practical evaluations of upper extremity mobility.</div>
<div class="mono" style="margin-top:8px">本研究旨在验证一种临床可及的方法，通过单个摄像头和人工智能驱动的无标记运动捕捉（MMC）来量化上肢可达工作空间（UERW）。研究人员让九名健康成人参与标准化的UERW任务，同时使用标记基础系统和单目摄像头捕捉运动。结果显示，正面摄像头配置与标记基础参考具有较强的一致性，平均偏差为0.61 ± 0.12%可达空间，而偏移摄像头视图低估了可达空间的百分比，达到-5.66 ± 0.45%。这些发现表明，使用正面单目摄像头进行UERW评估是可行的，特别是在前方工作空间评估方面，暗示其在临床应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees</div>
<div class="meta-line">Authors: Mohammed Himayath Ali, Mohammed Aqib Abdullah, Syed Muneer Hussain, Mohammed Mudassir Uddin, Shahnawaz Alam</div>
<div class="meta-line">First: 2026-01-18T15:06:30+00:00 · Latest: 2026-02-13T18:35:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12447v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12447v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \log n) while maintaining (\dparam, \deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可验证公平性保证的隐私保护联邦学习</div>
<div class="mono" style="margin-top:8px">联邦学习使得在分布式机构之间进行协作模型训练成为可能，而无需集中敏感数据；然而，在保持隐私的同时，确保异构数据分布下的算法公平性仍然是一个根本未解决的问题。本文介绍了CryptoFair-FL，这是一种新颖的密码学框架，为联邦学习系统提供了在正式安全定义下的首个可验证公平性保证。所提出的方法结合了加性同态加密和安全多方计算，以实现对人口平衡和均等机会指标的隐私保护验证，而无需揭示受保护属性分布或个体预测。一个新颖的批量验证协议将计算复杂度从BigO(n^2)降低到BigO(n \log n)，同时保持(\dparam, \deltap)-差分隐私，其中dparam = 0.5和deltap = 10^{-6}。理论分析建立了公平性验证的隐私成本的信息论下界，证明所提出的协议实现了近乎最优的隐私与公平性权衡。在四个基准数据集（MIMIC-IV医疗记录、成人收入、CelebA和一个新颖的FedFair-100基准）上的全面实验表明，CryptoFair-FL将公平性违规从0.231降低到0.031的人口平衡差异，同时与标准联邦平均相比，仅增加了2.3倍的计算开销。该框架成功抵御属性推断攻击，在所有测试配置中保持对抗成功概率低于0.05。这些结果为在需要隐私保护和算法问责的受监管行业中部署关注公平性的联邦学习建立了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of ensuring algorithmic fairness in federated learning while preserving privacy, particularly across heterogeneous data distributions. The authors introduce CryptoFair-FL, a cryptographic framework that combines additively homomorphic encryption with secure multi-party computation to provide verifiable fairness guarantees without disclosing sensitive information. Experimental results show that CryptoFair-FL significantly reduces fairness violations, achieving a demographic parity difference reduction from 0.231 to 0.031, with only 2.3 times the computational overhead compared to standard federated averaging, while also effectively defending against attribute inference attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在保护隐私的同时确保联邦学习系统算法公平性的问题，特别是在数据分布异质的情况下。作者提出了CryptoFair-FL，这是一种结合加法同态加密和安全多方计算的密码学框架，能够在不泄露敏感数据的情况下提供可验证的公平性保证。实验结果表明，CryptoFair-FL显著降低了人口平衡中的公平性违规，从0.231降至0.031，相比标准联邦平均方法仅增加了2.3倍的计算开销，同时在对抗性推断攻击中保持了低于0.05的成功概率。</div>
</details>
</div>
<div class="card">
<div class="title">tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models</div>
<div class="meta-line">Authors: Kevin Li, Dibyadeep Saha, Avni Kanodia, Fan Lai</div>
<div class="meta-line">First: 2026-02-06T23:26:02+00:00 · Latest: 2026-02-13T18:35:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07263v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07263v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>tLoRA：高效的多LoRA训练与弹性共享超模型</div>
<div class="mono" style="margin-top:8px">随着低秩适应（LoRA）成为高效微调大型语言模型（LLMs）的标准方法，共享集群越来越多地在同一冻结主干上执行多个并发的LoRA训练任务。尽管最近的进展使得在服务期间批处理（共置）多个适配器成为可能，但异构LoRA适配器在训练时间的高效共置仍然面临独特挑战。任务通常在适配器秩、批量大小和资源分配上有所不同，简单的批处理可能会引入同步停滞、通信开销和每个任务的减速，甚至比独立执行更糟。我们提出了tLoRA，一个能够高效批量训练多个LoRA任务的框架。tLoRA将共享同一基础模型的适配器融合为一个弹性共享超模型，利用现有的分布式训练框架推导有效共享资源的并行计划。在内核层面，tLoRA采用融合的LoRA内核，适应性地重构低秩计算块，并调度秩感知的纳米批次，以最大化适配器之间计算和通信的重叠。在调度层面，tLoRA结合了一个在线的、残余容量感知的调度器，适应性地对任务进行分组，以最大化集体吞吐量。使用真实集群跟踪的评估表明，tLoRA提高了训练吞吐量1.2--1.8倍，任务训练完成时间提高2.3--5.4倍，GPU利用率提高37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies in training multiple Low-Rank Adaptation (LoRA) jobs concurrently on large language models, which can lead to synchronization stalls and resource allocation issues. The authors propose tLoRA, a framework that facilitates efficient batch training of multiple LoRA jobs by creating an elastic shared super-model that optimally utilizes existing distributed training frameworks. Experimental results indicate that tLoRA significantly enhances training throughput by 1.2 to 1.8 times, reduces job training completion time by 2.3 to 5.4 times, and increases GPU utilization by 37%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在同一冻结主干上并行训练多个低秩适配器（LoRA）作业时的低效率问题，这可能导致同步停滞和资源分配问题。作者提出了tLoRA框架，通过创建一个弹性共享超级模型来促进多个LoRA作业的高效批量训练，从而优化分布式训练框架的利用。实验结果表明，tLoRA提高了训练吞吐量1.2到1.8倍，减少了作业训练完成时间2.3到5.4倍，并将GPU利用率提高了37%。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Conic Programs over Sparse Graphs using a Variational Quantum Approach: The Case of the Optimal Power Flow</div>
<div class="meta-line">Authors: Thinh Viet Le, Mark M. Wilde, Vassilis Kekatos</div>
<div class="meta-line">First: 2025-08-30T03:47:52+00:00 · Latest: 2026-02-13T18:33:19+00:00</div>
<div class="meta-line">Comments: 21 pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00341v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.00341v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conic programs arise broadly in physics, quantum information, machine learning, and engineering, many of which are defined over sparse graphs. Although such problems can be solved in polynomial time using classical interior-point solvers, the computational complexity scales unfavorably with graph size. In this context, this work proposes a variational quantum paradigm for solving conic programs, including quadratically constrained quadratic programs (QCQPs) and semidefinite programs (SDPs). We encode primal variables via the state of a parameterized quantum circuit (PQC), and dual variables via the probability mass function of a second PQC. The Lagrangian function can thus be expressed as scaled expectations of quantum observables. A primal-dual solution can be found by minimizing/maximizing the Lagrangian over the parameters of the first/second PQC. We pursue saddle points of the Lagrangian in a hybrid fashion. Gradients of the Lagrangian are estimated using the two PQCs, while PQC parameters are updated classically using a primal-dual method. We propose permuting the primal variables so that related observables are expressed in a banded form, enabling efficient measurement. The proposed framework is applied to the OPF problem, a large-scale optimization problem central to the operation of electric power systems. Numerical tests on the IEEE 57-node power system using Pennylane&#x27;s simulator corroborate that the proposed doubly variational quantum framework can find high-quality OPF solutions. Although showcased for the OPF, this framework features a broader scope, including conic programs with numerous variables and constraints, problems defined over sparse graphs, and training quantum machine learning models to satisfy constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用变分量子方法解决稀疏图上的锥形规划：最优潮流的案例</div>
<div class="mono" style="margin-top:8px">锥形规划广泛出现在物理学、量子信息、机器学习和工程中，许多问题是在稀疏图上定义的。尽管这些问题可以使用经典内点求解器在多项式时间内解决，但计算复杂度随着图的大小不利地增长。在这种背景下，本研究提出了一种变分量子范式来解决锥形规划，包括二次约束二次规划（QCQP）和半正定规划（SDP）。我们通过参数化量子电路（PQC）的状态编码原始变量，通过第二个PQC的概率质量函数编码对偶变量。因此，拉格朗日函数可以表示为量子可观测量的缩放期望。通过最小化/最大化第一个/第二个PQC的参数上的拉格朗日，可以找到原始-对偶解。我们以混合方式追求拉格朗日的鞍点。拉格朗日的梯度使用两个PQC进行估计，而PQC参数则通过原始-对偶方法进行经典更新。我们建议对原始变量进行置换，以便相关可观测量以带状形式表达，从而实现高效测量。所提出的框架应用于最优潮流问题，这是电力系统运行中的一个大规模优化问题。对使用Pennylane模拟器的IEEE 57节点电力系统的数值测试证实，所提出的双重变分量子框架能够找到高质量的最优潮流解。尽管展示了最优潮流，但该框架具有更广泛的适用范围，包括具有众多变量和约束的锥形规划、在稀疏图上定义的问题，以及训练量子机器学习模型以满足约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of solving conic programs over sparse graphs, which are prevalent in various fields but become computationally intensive as graph size increases when using classical methods. The authors propose a variational quantum approach that encodes primal and dual variables through parameterized quantum circuits, allowing the Lagrangian function to be expressed as scaled expectations of quantum observables. Experimental results demonstrate that this framework effectively identifies high-quality solutions for the optimal power flow problem in a numerical test on the IEEE 57-node power system, indicating its potential applicability to a wider range of conic programs and quantum machine learning models.</div>
<div class="mono" style="margin-top:8px">本研究解决了在稀疏图上定义的锥形规划问题的挑战，这些问题在多个领域中普遍存在，但在使用经典方法时计算复杂度较高。作者提出了一种变分量子方法，利用参数化量子电路编码原始变量和对偶变量，从而使拉格朗日函数能够表示为量子可观测量的期望。实验结果表明，该框架在对IEEE 57节点电力系统的数值测试中有效地找到高质量的最优潮流解决方案，表明其在更广泛的锥形规划和量子机器学习模型中的潜在应用。</div>
</details>
</div>
<div class="card">
<div class="title">Learning functional components of PDEs from data using neural networks</div>
<div class="meta-line">Authors: Torkel E. Loman, Yurij Salmaniw, Antonio Leon Villares, Jose A. Carrillo, Ruth E. Baker</div>
<div class="meta-line">First: 2026-02-13T18:32:33+00:00 · Latest: 2026-02-13T18:32:33+00:00</div>
<div class="meta-line">Comments: 16 pages with 6 figures. Additional 24 pages and 19 figures supplementary information</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用神经网络从数据中学习偏微分方程的函数组件</div>
<div class="mono" style="margin-top:8px">偏微分方程通常包含难以或不可能直接测量的未知函数，这妨碍了我们从模型中推导预测的能力。从数据中恢复标量偏微分方程参数的工作流程已被广泛研究：在这里，我们展示了如何使用类似的工作流程从数据中恢复函数。具体而言，我们将神经网络嵌入到偏微分方程中，并展示它们在数据训练过程中如何以任意精度逼近未知函数。以非局部聚合-扩散方程为案例研究，我们从稳态数据中恢复交互核和外部势。具体来说，我们研究了可用解的数量、它们的属性、采样密度和测量噪声等多种因素如何影响我们成功恢复函数的能力。我们的方法具有优势，因为它可以利用标准的参数拟合工作流程，并且训练后的偏微分方程可以作为普通偏微分方程用于生成系统预测等目的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of recovering unknown functions in partial differential equations (PDEs) that are difficult to measure directly, which limits predictive capabilities. The authors propose a method that integrates neural networks into the PDE framework, allowing for the approximation of these unknown functions with high accuracy as the networks are trained on data. Through the case study of nonlocal aggregation-diffusion equations, the study demonstrates that various factors, including the number of solutions, their characteristics, sampling density, and measurement noise, significantly influence the successful recovery of functions, highlighting the method&#x27;s effectiveness in utilizing standard parameter-fitting workflows for generating predictions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在偏微分方程（PDE）中恢复难以直接测量的未知函数的挑战，这限制了预测能力。作者提出了一种将神经网络嵌入PDE框架的方法，使这些网络能够在数据上进行训练，从而高精度地近似未知函数。通过对非局部聚合扩散方程的案例研究，研究表明，解决方案的数量、特性、采样密度和测量噪声等多种因素显著影响从稳态数据中恢复交互核和外部势的能力。</div>
</details>
</div>
<div class="card">
<div class="title">MissionHD: Hyperdimensional Refinement of Distribution-Deficient Reasoning Graphs for Video Anomaly Detection</div>
<div class="meta-line">Authors: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Nathaniel D. Bastian, Mohsen Imani</div>
<div class="meta-line">First: 2025-08-20T14:43:04+00:00 · Latest: 2026-02-13T18:30:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14746v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.14746v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). However, they are typically treated as fixed despite being generic and distribution-deficient. Conventional graph structure refinement (GSR) methods are ill-suited to this setting, as they rely on learning structural distributions that are absent in LLM-generated graphs. We propose HDC-constrained Graph Structure Refinement (HDC-GSR), a new paradigm that directly optimizes a decodable, task-aligned graph representation in a single hyperdimensional space without distribution modeling. Leveraging Hyperdimensional Computing (HDC), our framework encodes graphs via binding and bundling operations, aligns the resulting graph code with downstream loss, and decodes edge contributions to refine the structure. We instantiate this approach as MissionHD for weakly supervised VAD/VAR and demonstrate consistent performance gains on benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MissionHD：用于视频异常检测的分布不足推理图的超维精炼</div>
<div class="mono" style="margin-top:8px">LLM生成的推理图，称为任务特定图（MSGs），在视频异常检测（VAD）和识别（VAR）中越来越多地被使用。然而，它们通常被视为固定的，尽管它们是通用的且分布不足。传统的图结构精炼（GSR）方法不适合这种情况，因为它们依赖于在LLM生成的图中缺失的结构分布。我们提出了HDC约束的图结构精炼（HDC-GSR），这是一种新范式，直接在单一超维空间中优化可解码的、任务对齐的图表示，而无需分布建模。利用超维计算（HDC），我们的框架通过绑定和捆绑操作对图进行编码，将生成的图代码与下游损失对齐，并解码边缘贡献以精炼结构。我们将这种方法实例化为MissionHD，用于弱监督的VAD/VAR，并在基准数据集上展示了一致的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video anomaly detection (VAD) and recognition (VAR) using reasoning graphs generated by large language models, which are often generic and lack sufficient distribution. The authors introduce a novel method called HDC-constrained Graph Structure Refinement (HDC-GSR), which optimizes graph representations in a hyperdimensional space without relying on traditional distribution modeling. Experimental results show that the MissionHD framework achieves consistent performance improvements on benchmark datasets for weakly supervised VAD and VAR tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过使用大型语言模型生成的推理图来改善视频异常检测（VAD）和识别（VAR），这些推理图通常是通用的且缺乏足够的分布。作者提出了一种新方法，称为HDC约束图结构优化（HDC-GSR），该方法在超维空间中优化与任务对齐的图表示，而无需依赖分布建模。实验结果表明，MissionHD这一方法的实现，在弱监督的VAD和VAR任务上，在基准数据集上取得了一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Realistic Face Reconstruction from Facial Embeddings via Diffusion Models</div>
<div class="meta-line">Authors: Dong Han, Yong Li, Joachim Denzler</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-02-13T18:28:24+00:00 · Latest: 2026-02-13T18:28:24+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13168v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过扩散模型从面部嵌入重建真实面孔</div>
<div class="mono" style="margin-top:8px">随着人脸识别（FR）系统的发展，隐私保护人脸识别（PPFR）系统因其准确的识别、增强的面部隐私保护和对各种攻击的鲁棒性而受到欢迎。然而，目前对通过重建这些系统的嵌入生成真实高分辨率面孔图像的隐私风险的研究仍然有限，尤其是对于PPFR。在本研究中，我们提出了面部嵌入映射（FEM），这是一个通用框架，探索了Kolmogorov-Arnold网络（KAN），通过利用预训练的身份保护扩散模型对最先进（SOTA）FR和PPFR系统进行嵌入到面孔的攻击。基于广泛的实验，我们验证了重建的面孔可以用于访问其他真实世界的FR系统。此外，所提出的方法在从部分和受保护的面部嵌入重建面孔方面表现出鲁棒性。此外，FEM可以作为评估FR和PPFR系统在隐私泄露方面安全性的工具。本研究中使用的所有图像均来自公共数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the privacy risks associated with privacy-preserving face recognition (PPFR) systems by reconstructing realistic high-resolution face images from their embeddings. The authors propose a face embedding mapping (FEM) framework that utilizes a Kolmogorov-Arnold Network (KAN) and a pre-trained Identity-Preserving diffusion model to conduct embedding-to-face attacks on state-of-the-art face recognition systems. Experimental results demonstrate that the reconstructed faces can successfully access real-world face recognition systems, and the method shows robustness in reconstructing faces from partial and protected embeddings, highlighting its potential as a tool for evaluating the privacy leakage of FR and PPFR systems.</div>
<div class="mono" style="margin-top:8px">本研究针对隐私保护人脸识别（PPFR）系统中日益严重的隐私风险问题，探讨了从人脸嵌入中重建逼真的高分辨率人脸图像的可能性。作者提出了一种人脸嵌入映射（FEM）框架，该框架结合了Kolmogorov-Arnold网络（KAN）和预训练的身份保护扩散模型，以对最先进的人脸识别（FR）和PPFR系统进行嵌入到人脸的攻击。实验结果表明，重建的人脸能够有效访问现实世界的FR系统，即使在使用部分和受保护的嵌入时也表现出鲁棒性，从而提供了一种评估这些系统隐私泄露风险的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Take-off under Fuzzy Clearances</div>
<div class="meta-line">Authors: Hugo Henry, Arthur Tsai, Kelly Cohen</div>
<div class="meta-line">First: 2026-02-13T18:25:24+00:00 · Latest: 2026-02-13T18:25:24+00:00</div>
<div class="meta-line">Comments: 12 pages, 12 figures, conference paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13166v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模糊清除下的最佳起飞</div>
<div class="mono" style="margin-top:8px">本文提出了一种混合障碍物规避架构，结合了清除下的最优控制与模糊规则基础系统（FRBS），以实现无人机的自适应约束处理。受到经典最优控制在不确定性下的局限性以及安全关键航空系统中可解释决策需求的启发，我们设计了一个三阶段的Takagi Sugeno Kang模糊层，根据FAA和EASA的监管分离最小值和适航指南调节约束半径、紧急程度和激活决策。这些模糊派生的清除随后作为软约束纳入使用FALCON工具箱和IPOPT求解的最优控制问题中。该框架旨在通过选择性激活障碍物规避更新来减少不必要的重新计算，同时保持对航空程序的合规性。使用简化飞机模型的概念验证实现表明，该方法可以在单线程MATLAB环境中以每次迭代2.3秒的计算时间生成最优轨迹，表明其在近实时应用中的可行性。然而，我们的实验揭示了FALCON和IPOPT最新版本中的一个关键软件不兼容性，其中拉格朗日惩罚项始终为零，阻止了适当的约束执行。这种行为在不同场景中是一致的，表明这是求解器工具箱的回归，而不是建模缺陷。未来的工作包括通过回退到早期软件版本来验证这一影响，使用进化方法优化模糊隶属函数，并将系统扩展到更高保真度的飞机模型和随机障碍环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of classical optimal control methods in uncertain environments and the need for interpretable decision-making in safety-critical aviation systems. The authors propose a hybrid architecture that combines Optimal Control with a Fuzzy Rule Based System to manage adaptive constraints for unmanned aircraft. Experimental results using a simplified aircraft model indicate that the framework can generate optimal trajectories with computation times of 2.3 seconds per iteration, although a critical software incompatibility was identified that hindered proper constraint enforcement, suggesting a need for further validation and optimization in future work.</div>
<div class="mono" style="margin-top:8px">本文解决了经典最优控制在不确定环境中的局限性以及航空安全中可解释决策的需求。作者提出了一种混合架构，将最优控制与模糊规则系统（FRBS）相结合，以处理无人机的自适应约束。使用简化的飞机模型进行的实验结果表明，该方法能够以每次迭代2.3秒的计算时间生成最优轨迹，但在FALCON和IPOPT工具箱中发现了关键的软件不兼容性，阻碍了约束的正确执行，这表明是求解器的回归问题，而非建模问题。</div>
</details>
</div>
<div class="card">
<div class="title">Asynchronous Verified Semantic Caching for Tiered LLM Architectures</div>
<div class="meta-line">Authors: Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, Tak Chiam, Weihua Zhu</div>
<div class="meta-line">First: 2026-02-13T18:25:00+00:00 · Latest: 2026-02-13T18:25:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分层LLM架构的异步验证语义缓存</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）现在处于搜索、辅助和代理工作流的关键路径上，使得语义缓存对于降低推理成本和延迟至关重要。生产部署通常使用分层静态-动态设计：一个由日志中挖掘的经过策划、离线审核的响应组成的静态缓存，后面是一个在线填充的动态缓存。在实践中，这两个层次通常由单一的嵌入相似性阈值控制，这导致了一个硬性权衡：保守的阈值错过安全重用机会，而激进的阈值则有风险提供语义上不正确的响应。我们引入了\textbf{Krites}，一种异步的、LLM判断的缓存策略，它在不改变服务决策的情况下扩展静态覆盖。在关键路径上，Krites的行为与标准静态阈值策略完全相同。当提示的最近静态邻居刚好低于静态阈值时，Krites异步调用LLM判断者来验证静态响应是否适合新的提示。被批准的匹配项被提升到动态缓存中，允许未来的重复和改述重用策划的静态答案，并随着时间的推移扩展静态覆盖。在对话和搜索工作负载的追踪驱动模拟中，Krites将使用策划静态答案（直接静态命中加上验证提升）服务的请求比例提高了高达$\textbf{3.9}$倍，相对于调优基线，且关键路径延迟保持不变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of large language models (LLMs) in search and assistance workflows by improving semantic caching to reduce inference costs and latency. The authors propose Krites, an asynchronous caching policy that allows for the verification of static responses without altering the serving decisions, thereby expanding the coverage of static caches. Experimental results from trace-driven simulations indicate that Krites can increase the fraction of requests served with curated static answers by up to 3.9 times for conversational and search workloads compared to tuned baselines, while maintaining the same critical path latency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改进语义缓存来提高大型语言模型（LLMs）在搜索和辅助工作流程中的效率，从而降低推理成本和延迟。作者提出了Krites，一种异步缓存策略，利用LLM验证静态响应而不改变服务决策，从而扩展静态缓存的覆盖范围。基于追踪驱动的模拟实验结果表明，Krites能够将服务于策划静态答案的请求比例提高最多3.9倍，适用于对话和搜索风格的查询，同时保持相同的关键路径延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Learnable Chernoff Baselines for Inference-Time Alignment</div>
<div class="meta-line">Authors: Sunil Madhow, Yuchen Liang, Ness Shroff, Yingbin Liang, Yu-Xiang Wang</div>
<div class="meta-line">First: 2026-02-08T00:09:40+00:00 · Latest: 2026-02-13T18:15:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07738v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07738v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可学习的切尔诺夫基线用于推理时对齐</div>
<div class="mono" style="margin-top:8px">我们研究了生成模型的推理时奖励引导对齐。现有方法通常依赖于特定架构的适应或计算成本高昂的推理过程。我们引入可学习的切尔诺夫基线（LCBs），作为一种有效且近似地从因KL正则化奖励对齐而产生的指数倾斜核中采样的方法。仅使用对预训练模型的黑箱采样访问，LCBs实现了一种拒绝采样形式，具有自适应选择的接受概率，从而允许对推理计算规模进行细粒度控制。我们建立了与理想对齐模型的全变差保证，并在连续和离散扩散设置中展示了LCB采样与理想拒绝采样的紧密匹配，同时对预训练模型的查询次数显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for efficient inference-time reward-guided alignment in generative models, which often face challenges due to architecture-specific adaptations or high computational costs. The authors propose Learnable Chernoff Baselines (LCBs) as a novel method that utilizes black-box sampling to perform rejection sampling with adaptively selected acceptance probabilities, allowing for better control over inference-compute scaling. Experimental results show that LCB sampling closely approximates ideal rejection sampling while significantly reducing the number of queries needed to the pretrained model in both continuous and discrete diffusion settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高生成模型中推理时的奖励引导对齐效率，因为现有方法通常依赖于特定架构的适应或高计算成本。作者提出了可学习的切尔诺夫基准（LCBs），该方法利用从预训练模型的黑箱采样进行拒绝采样，并采用自适应选择的接受概率，从而更好地控制推理计算的比例。实验结果表明，LCB采样在连续和离散扩散设置中与理想的拒绝采样非常接近，同时所需的预训练模型查询次数显著减少。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach</div>
<div class="meta-line">Authors: Yiran Gao, Kim Hammar, Tao Li</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-02-13T18:09:30+00:00 · Latest: 2026-02-13T18:09:30+00:00</div>
<div class="meta-line">Comments: 2026 AAAI Summer Symposium on Human-Aware AI Agents for the Cyber Battlefield</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13156v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models&#x27; (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文自主网络事件响应：一种端到端的大型语言模型代理方法</div>
<div class="mono" style="margin-top:8px">快速演变的网络攻击需要能够自主学习和适应变化威胁的事件响应系统。之前的工作广泛探讨了强化学习方法，该方法通过对事件的广泛模拟学习响应策略。虽然这种方法有效，但需要手工建模模拟器，并抑制来自原始系统日志和警报的有用语义。为了解决这些局限性，我们提议利用大型语言模型（LLM）预训练的安全知识和上下文学习，创建一个端到端的事件响应规划代理解决方案。具体而言，我们的代理将感知、推理、规划和行动四种功能集成到一个轻量级的LLM（14b模型）中。通过微调和思维链推理，我们的LLM代理能够处理系统日志并推断潜在的网络状态（感知），更新其攻击模型的推测（推理），在不同响应策略下模拟后果（规划），并生成有效响应（行动）。通过将LLM模拟的结果与实际观察进行比较，LLM代理不断完善其攻击推测和相应响应，从而展示上下文适应性。我们的方法不需要建模，可以在普通硬件上运行。在对文献中报告的事件日志进行评估时，我们的代理的恢复速度比前沿LLM快23%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for incident response systems that can autonomously adapt to rapidly evolving cyberattacks. The authors propose a novel approach that utilizes large language models (LLMs) to create an end-to-end agent capable of incident response planning, integrating perception, reasoning, planning, and action functionalities into a single lightweight model. Experimental results show that this LLM agent can process system logs, update attack models, simulate response strategies, and generate effective responses, achieving recovery rates up to 23% faster than existing leading LLMs when evaluated on incident logs from the literature.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要能够自主适应快速演变的网络威胁的事件响应系统。作者提出了一种新方法，利用大型语言模型（LLM）创建一个能够进行事件响应规划的端到端代理，将感知、推理、规划和行动整合到一个轻量级模型中。实验结果表明，该LLM代理能够处理系统日志以推断网络状态、更新攻击模型、模拟响应策略并生成有效响应，在对文献中的事件日志进行评估时，其恢复速度比现有LLM快高达23%。</div>
</details>
</div>
<div class="card">
<div class="title">Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation</div>
<div class="meta-line">Authors: Kehang Zhu, Nithum Thain, Vivian Tsai, James Wexler, Crystal Qian</div>
<div class="meta-line">First: 2026-02-12T15:41:57+00:00 · Latest: 2026-02-13T18:08:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12089v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants access to a single LLM assistance modality: proactive recommendations from an Advisor, reactive feedback from a Coach, or autonomous execution by a Delegate; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the Advisor modality, participants achieve the highest mean individual gains with the Delegate, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in access-to-delegate treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the Delegate agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>选择你的代理人：在多方谈判中采用AI顾问、教练和代表的权衡</div>
<div class="mono" style="margin-top:8px">随着AI在社会环境中的使用越来越普遍，理解代理人与用户之间的互动对于设计能够改善个人和群体结果的系统至关重要。我们展示了一项在线行为实验（N = 243），参与者在三人小组中进行三轮多回合的讨价还价游戏。每个游戏以随机顺序呈现，提供一种LLM辅助模式：来自顾问的主动建议、来自教练的反应反馈或由代表的自主执行；所有模式均由一个在全代理环境中实现超人类表现的LLM驱动。在每一轮中，参与者私下决定是手动操作还是使用该游戏中可用的AI模式。尽管参与者偏好顾问模式，但他们在代表模式下获得的平均个人收益最高，显示出偏好与表现之间的不一致。此外，委托产生了积极的外部效应；即使在接入代表的处理组中未采用的用户也通过获得更高质量的报价而受益。机制分析表明，代表代理充当市场制造者，注入理性、帕累托改进的提案，重构交易环境。我们的研究揭示了代理能力与实现的群体福利之间的差距。尽管自主代理可以表现出超人类的战略表现，但它们对实现的福利增益的影响可能受到界面、用户感知和采用障碍的限制。辅助模式应设计为具有内生参与机制；与自动化辅助相兼容的互动规则是改善人类福利的前提。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the interaction between users and AI agents in multi-party negotiations, motivated by the increasing prevalence of AI in social contexts. An online behavioral experiment with 243 participants was conducted, where they engaged in three bargaining games using different AI modalities: an Advisor providing proactive recommendations, a Coach offering reactive feedback, and a Delegate executing actions autonomously. The findings reveal that while participants preferred the Advisor modality, they achieved the highest individual gains with the Delegate, indicating a misalignment between preference and performance. Additionally, the Delegate&#x27;s role as a market maker led to improved outcomes for both users and non-users, highlighting the need for better-designed AI systems that align user perceptions with agent capabilities to enhance overall welfare.</div>
<div class="mono" style="margin-top:8px">本研究探讨了用户与AI代理在多方谈判中的互动，动机是随着AI在社会环境中的普及，理解这种互动变得至关重要。研究通过一项在线行为实验，招募了243名参与者，他们参与了三种不同AI模式的讨价还价游戏：提供主动建议的顾问、提供反应性反馈的教练和自主执行的代理。研究结果显示，尽管参与者偏好顾问模式，但使用代理时他们获得的个人收益最高，表明偏好与表现之间存在不一致。此外，代理作为市场制造者的角色产生了积极的外部效应，甚至使未采用代理的用户也受益，强调了设计更好互动规则以提高自动化辅助系统的福利结果的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Approximate Uniform Facility Location via Graph Neural Networks</div>
<div class="meta-line">Authors: Chendi Qian, Christopher Morris, Stefanie Jegelka, Christian Sohler</div>
<div class="meta-line">First: 2026-02-13T18:08:23+00:00 · Latest: 2026-02-13T18:08:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13155v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图神经网络学习近似均匀设施位置</div>
<div class="mono" style="margin-top:8px">近年来，使用神经网络，特别是消息传递神经网络（MPNN），以启发式方式解决困难的组合优化问题的兴趣日益增长。然而，现有的基于学习的困难组合优化任务方法通常依赖于监督训练数据、强化学习或梯度估计器，导致显著的计算开销、不稳定的训练或缺乏可证明的性能保证。相比之下，经典的近似算法在最坏情况下提供这样的性能保证，但它们是不可微分的，无法自适应地利用自然输入分布中的结构规律。我们通过均匀设施位置（UniFL）的基本示例来解决这一二分法，UniFL是组合设施位置问题的一种变体，应用于聚类、数据摘要、物流和供应链设计。我们开发了一个完全可微的MPNN模型，该模型嵌入了近似算法原则，同时避免了求解器监督或离散松弛的需求。我们的方法承认可证明的近似和规模泛化保证，适用于比训练期间看到的更大实例。实证结果表明，我们的方法在解决方案质量方面优于标准的非学习近似算法，缩小了与计算密集型整数线性规划方法之间的差距。总体而言，这项工作为弥合基于学习的方法和离散优化的近似算法迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the application of neural networks, particularly message-passing neural networks (MPNNs), in solving hard combinatorial optimization problems like the Uniform Facility Location (UniFL) problem, which is relevant in various fields such as clustering and logistics. The authors propose a fully differentiable MPNN model that integrates approximation-algorithmic principles without relying on supervised training data or discrete relaxations. Experimental results demonstrate that this model outperforms traditional non-learned approximation algorithms in solution quality and approaches the performance of more computationally intensive methods like integer linear programming, while also providing provable approximation guarantees for larger instances than those used in training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于使用神经网络解决组合优化问题的挑战，这些问题通常由于依赖监督数据或强化学习而面临高计算成本和不稳定训练等问题。作者提出了一种完全可微的消息传递神经网络（MPNN）模型，该模型整合了近似算法原理，无需求解器监督或离散松弛。实验结果表明，该方法在解的质量上超越了传统的非学习近似算法，并接近于更高计算需求的整数线性规划方法的性能，从而推动了学习方法与近似算法在离散优化中的结合。</div>
</details>
</div>
<div class="card">
<div class="title">Quantization-Robust LLM Unlearning via Low-Rank Adaptation</div>
<div class="meta-line">Authors: João Vitor Boer Abitante, Joana Meneguzzo Pasquali, Luan Fonseca Garcia, Ewerton de Oliveira, Thomas da Silva Paula, Rodrigo C. Barros, Lucas S. Kupssinskü</div>
<div class="meta-line">First: 2026-02-13T18:01:40+00:00 · Latest: 2026-02-13T18:01:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于低秩适应的量化鲁棒大语言模型遗忘</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）遗忘旨在从训练好的模型中移除特定知识，但实际部署通常需要后训练量化（PTQ）以实现高效推理。然而，激进的低位PTQ可能会掩盖或抹去遗忘更新，导致量化模型恢复到遗忘前的行为。我们表明，标准的全参数微调通常会引入过小的参数变化，无法在4位量化中存活。我们提出通过低秩适应（LoRA）实现量化鲁棒遗忘：我们冻结基础模型，将遗忘集中到可训练的适配器中，以便在量化后保留有效更新。在使用MUSE数据集（BOOKS和NEWS）评估的Llama-2-7B上，LoRA将4位效用提高了最多7.93分（NPO+GDR在BOOKS上：50.17到58.10），并在NEWS上对GA+GDR的4位效用也有所提高（40.06到44.82，增加4.76）。LoRA还显著减少了在4位PTQ下的隐私泄露，例如，对于GA+KLR在BOOKS上的PrivLeak从-25.68移动到-5.86（更接近理想的0），同时保持强遗忘（VerMem和KnowMem接近0）。因此，在模型部署需要量化的场景中，使用LoRA进行机器遗忘是有益的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of unlearning targeted knowledge from large language models (LLMs) while maintaining efficiency through post-training quantization (PTQ). The authors propose a method called low-rank adaptation (LoRA), which involves freezing the base model and concentrating unlearning updates into trainable adapters to ensure that these updates survive aggressive quantization. Experimental results demonstrate that LoRA significantly enhances the utility of quantized models, achieving improvements of up to 7.93 points on the MUSE dataset for BOOKS and 4.76 points for NEWS, while also reducing privacy leakage and maintaining effective forgetting of the targeted knowledge.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在保持高效的后训练量化（PTQ）情况下，从大型语言模型（LLM）中去除特定知识的挑战。作者提出了一种称为低秩适应（LoRA）的方法，该方法通过冻结基础模型并将去学习更新集中到可训练的适配器中，以确保这些更新在激进量化后得以保留。实验结果表明，LoRA显著提高了量化模型的效用，在MUSE数据集的BOOKS上提高了最多7.93分，在NEWS上提高了4.76分，同时减少了隐私泄露并保持了有效的遗忘能力。</div>
</details>
</div>
<div class="card">
<div class="title">Rule-Based Spatial Mixture-of-Experts U-Net for Explainable Edge Detection</div>
<div class="meta-line">Authors: Bharadwaj Dogga, Kaaustaaub Shankar, Gibin Raju, Wilhelm Louw, Kelly Cohen</div>
<div class="meta-line">First: 2026-02-04T22:33:18+00:00 · Latest: 2026-02-13T17:56:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05100v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05100v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning models like U-Net and its variants, have established state-of-the-art performance in edge detection tasks and are used by Generative AI services world-wide for their image generation models. However, their decision-making processes remain opaque, operating as &quot;black boxes&quot; that obscure the rationale behind specific boundary predictions. This lack of transparency is a critical barrier in safety-critical applications where verification is mandatory. To bridge the gap between high-performance deep learning and interpretable logic, we propose the Rule-Based Spatial Mixture-of-Experts U-Net (sMoE U-Net). Our architecture introduces two key innovations: (1) Spatially-Adaptive Mixture-of-Experts (sMoE) blocks integrated into the decoder skip connections, which dynamically gate between &quot;Context&quot; (smooth) and &quot;Boundary&quot; (sharp) experts based on local feature statistics; and (2) a Takagi-Sugeno-Kang (TSK) Fuzzy Head that replaces the standard classification layer. This fuzzy head fuses deep semantic features with heuristic edge signals using explicit IF-THEN rules. We evaluate our method on the BSDS500 benchmark, achieving an Optimal Dataset Scale (ODS) F-score of 0.7628, effectively matching purely deep baselines like HED (0.7688) while outperforming the standard U-Net (0.7437). Crucially, our model provides pixel-level explainability through &quot;Rule Firing Maps&quot; and &quot;Strategy Maps,&quot; allowing users to visualize whether an edge was detected due to strong gradients, high semantic confidence, or specific logical rule combinations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于规则的空间专家混合U-Net用于可解释的边缘检测</div>
<div class="mono" style="margin-top:8px">深度学习模型如U-Net及其变体在边缘检测任务中已建立了最先进的性能，并被全球生成AI服务用于图像生成模型。然而，它们的决策过程仍然不透明，作为“黑箱”操作，掩盖了特定边界预测背后的理由。这种缺乏透明度是安全关键应用中的一个重要障碍，在这些应用中，验证是强制性的。为了弥合高性能深度学习与可解释逻辑之间的差距，我们提出了基于规则的空间专家混合U-Net（sMoE U-Net）。我们的架构引入了两个关键创新：（1）集成到解码器跳跃连接中的空间自适应专家混合（sMoE）模块，根据局部特征统计动态选择“上下文”（平滑）和“边界”（锐利）专家；（2）替代标准分类层的高田-菅野-康（TSK）模糊头。该模糊头使用显式的IF-THEN规则融合深层语义特征与启发式边缘信号。我们在BSDS500基准上评估了我们的方法，获得了0.7628的最佳数据集规模（ODS）F-score，有效匹配了纯深度基线如HED（0.7688），同时超越了标准U-Net（0.7437）。重要的是，我们的模型通过“规则触发图”和“策略图”提供像素级可解释性，使用户能够可视化边缘是由于强梯度、高语义置信度还是特定逻辑规则组合而检测到的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the interpretability of deep learning models used in edge detection, which are often criticized for their opaque decision-making processes. The authors propose a novel architecture called Rule-Based Spatial Mixture-of-Experts U-Net (sMoE U-Net), which incorporates Spatially-Adaptive Mixture-of-Experts blocks and a Takagi-Sugeno-Kang Fuzzy Head to improve both performance and explainability. Experimental results on the BSDS500 benchmark demonstrate that the sMoE U-Net achieves an Optimal Dataset Scale F-score of 0.7628, closely matching the performance of existing deep learning models while providing pixel-level explainability through visual tools that clarify the reasoning behind edge detection decisions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高用于边缘检测的深度学习模型的可解释性，这些模型常因决策过程不透明而受到批评。作者提出了一种新颖的架构，称为基于规则的空间专家混合U-Net（sMoE U-Net），该架构结合了空间自适应专家混合块和Takagi-Sugeno-Kang模糊头，以提高性能和可解释性。在BSDS500基准测试中的实验结果显示，sMoE U-Net达到了0.7628的最佳数据集规模F-score，接近现有深度学习模型的性能，同时通过规则触发图和策略图等可视化工具提供像素级解释。</div>
</details>
</div>
<div class="card">
<div class="title">Generating Physical Dynamics under Priors</div>
<div class="meta-line">Authors: Zihan Zhou, Xiaoxue Wang, Tianshu Yu</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2024-09-01T14:43:47+00:00 · Latest: 2026-02-13T17:52:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.00730v4">Abs</a> · <a href="https://arxiv.org/pdf/2409.00730v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of physical priors, resulting in violation of basic physical laws and suboptimal performance. In this paper, we introduce a novel framework that seamlessly incorporates physical priors into diffusion-based generative models to address this limitation. Our approach leverages two categories of priors: 1) distributional priors, such as roto-translational invariance, and 2) physical feasibility priors, including energy and momentum conservation laws and PDE constraints. By embedding these priors into the generative process, our method can efficiently generate physically realistic dynamics, encompassing trajectories and flows. Empirical evaluations demonstrate that our method produces high-quality dynamics across a diverse array of physical phenomena with remarkable robustness, underscoring its potential to advance data-driven studies in AI4Physics. Our contributions signify a substantial advancement in the field of generative modeling, offering a robust solution to generate accurate and physically consistent dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在先验下生成物理动态</div>
<div class="mono" style="margin-top:8px">在数据驱动的背景下生成物理可行的动态是具有挑战性的，尤其是在遵循以特定方程或公式表达的物理先验时。现有的方法往往忽视物理先验的整合，导致违反基本物理定律和性能不佳。本文提出了一种新颖的框架，能够无缝地将物理先验融入基于扩散的生成模型，以解决这一局限。我们的方法利用两类先验：1）分布先验，如旋转-平移不变性；2）物理可行性先验，包括能量和动量守恒定律及偏微分方程约束。通过将这些先验嵌入生成过程中，我们的方法能够高效生成物理上真实的动态，包括轨迹和流动。实证评估表明，我们的方法在多种物理现象中产生高质量的动态，具有显著的鲁棒性，突显了其在AI4Physics数据驱动研究中的潜力。我们的贡献标志着生成建模领域的重大进展，提供了一种生成准确且物理一致的动态的强大解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of generating physically feasible dynamics in a data-driven context while adhering to established physical laws. The authors propose a novel framework that integrates physical priors into diffusion-based generative models, utilizing distributional priors like roto-translational invariance and physical feasibility priors such as conservation laws and PDE constraints. Experimental results indicate that this method successfully generates high-quality, physically realistic dynamics across various physical phenomena, demonstrating its robustness and potential to enhance data-driven studies in AI4Physics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在数据驱动背景下生成符合物理规律的动态的挑战。作者提出了一种新框架，将物理先验融入扩散基础的生成模型中，利用分布先验和物理可行性先验，如守恒定律和偏微分方程约束。实验结果表明，该方法能够有效生成高质量、符合物理规律的动态，涵盖多种物理现象，展示了其稳健性和在AI4Physics数据驱动研究中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics</div>
<div class="meta-line">Authors: Pingzhi Li, Hongxuan Li, Zirui Liu, Xingcheng Lin, Tianlong Chen</div>
<div class="meta-line">First: 2026-02-13T17:49:12+00:00 · Latest: 2026-02-13T17:49:12+00:00</div>
<div class="meta-line">Comments: Code is at https://github.com/UNITES-Lab/flash-molecular-dynamics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13140v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13140v1">PDF</a> · <a href="https://github.com/UNITES-Lab/flash-molecular-dynamics">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashSchNet：快速准确的粗粒度神经网络分子动力学</div>
<div class="mono" style="margin-top:8px">图神经网络（GNN）势能如SchNet通过学习多体相互作用提高了分子动力学（MD）模拟的准确性和可转移性，但由于碎片化的内核和内存受限的管道，仍然比经典力场慢。我们展示了一个缺失的原则是使GNN-MD具备IO感知，仔细考虑GPU高带宽内存（HBM）和片上SRAM之间的读写。我们提出了FlashSchNet，这是一个高效且准确的IO感知SchNet风格GNN-MD框架，基于四项技术构建：(1) 快速径向基，融合了成对距离计算、高斯基扩展和余弦包络为单个平铺传递，仅计算每个距离一次并在所有基函数中重用；(2) 快速消息传递，融合了截止、邻居聚集、过滤乘法和归约，以避免在HBM中物化边缘张量；(3) 快速聚合，通过CSR段归约重新构造散加，减少原子写入的数量，按特征维度降低，并在前向和反向传递中实现无争用累加；(4) 通道级16位量化，利用SchNet MLP权重中每通道的低动态范围，进一步提高吞吐量而几乎不损失准确性。在单个NVIDIA RTX PRO 6000上，FlashSchNet在包含269个珠子的粗粒度（CG）蛋白质上实现了1000 ns/天的总模拟吞吐量，超过64个并行副本（比CGSchNet基线快6.5倍，峰值内存减少80%），超越经典力场（如MARTINI），同时保持SchNet级别的准确性和可转移性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the speed and efficiency of molecular dynamics simulations using graph neural network potentials, which are typically slower than classical force fields due to inefficiencies in GPU utilization. The authors developed FlashSchNet, an IO-aware SchNet-style GNN-MD framework that employs four innovative techniques to optimize performance: flash radial basis for efficient distance computation, flash message passing to streamline data handling, flash aggregation to minimize atomic writes, and channel-wise 16-bit quantization to improve throughput with minimal accuracy loss. Experimental results demonstrate that FlashSchNet achieves an aggregate simulation throughput of 1000 ns/day on a single NVIDIA RTX PRO 6000 across 64 parallel replicas of a coarse-grained protein, outperforming the CGSchNet baseline by 6.5 times and classical force fields while maintaining high accuracy and transferability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高分子动力学模拟中图神经网络势能的速度和效率，传统上由于GPU利用效率低下而落后于经典力场。作者提出了FlashSchNet，这是一种IO感知的SchNet风格框架，采用了四种创新技术：闪存径向基函数优化距离计算，闪存消息传递简化数据处理，闪存聚合最小化原子写入，以及通道级16位量化在保持精度损失最小的情况下提高吞吐量。实验结果表明，FlashSchNet在单个NVIDIA RTX PRO 6000上实现了64个并行副本的总模拟吞吐量为1000 ns/天，超越了CGSchNet基线6.5倍，并在保持相似精度和可转移性的同时超过了经典力场。</div>
</details>
</div>
<div class="card">
<div class="title">Highlight &amp; Summarize: RAG without the jailbreaks</div>
<div class="meta-line">Authors: Giovanni Cherubin, Andrew Paverd</div>
<div class="meta-line">First: 2025-08-04T20:01:00+00:00 · Latest: 2026-02-13T17:48:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02872v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.02872v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is an important yet challenging task. When interacting with a chatbot, malicious users can input specially crafted prompts that cause the LLM to generate undesirable content or perform a different task from its intended purpose. Existing systems attempt to mitigate this by hardening the LLM&#x27;s system prompt or using additional classifiers to detect undesirable content or off-topic conversations. However, these probabilistic approaches are relatively easy to bypass due to the very large space of possible inputs and undesirable outputs. We present and evaluate Highlight &amp; Summarize (H&amp;S), a new design pattern for retrieval-augmented generation (RAG) systems that prevents these attacks by design. The core idea is to perform the same task as a standard RAG pipeline (i.e., to provide natural language answers to questions, based on relevant sources) without ever revealing the user&#x27;s question to the generative LLM. This is achieved by splitting the pipeline into two components: a highlighter, which takes the user&#x27;s question and extracts (&quot;highlights&quot;) relevant passages from the retrieved documents, and a summarizer, which takes the highlighted passages and summarizes them into a cohesive answer. We describe and implement several possible instantiations of H&amp;S and evaluate their responses in terms of correctness, relevance, and quality. For certain question-answering (QA) tasks, the responses produced by H&amp;S are judged to be as good, if not better, than those of a standard RAG pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高亮与总结：无越狱的检索增强生成</div>
<div class="mono" style="margin-top:8px">防止大型语言模型（LLMs）的越狱和模型劫持是一项重要但具有挑战性的任务。在与聊天机器人互动时，恶意用户可以输入特制的提示，导致LLM生成不良内容或执行与其预期目的不同的任务。现有系统试图通过强化LLM的系统提示或使用额外的分类器来检测不良内容或离题对话来缓解这一问题。然而，由于可能输入和不良输出的空间非常大，这些概率方法相对容易被绕过。我们提出并评估了高亮与总结（H&amp;S），这是一种新的检索增强生成（RAG）系统设计模式，通过设计防止这些攻击。核心思想是执行与标准RAG管道相同的任务（即根据相关来源提供自然语言答案），而不向生成LLM透露用户的问题。这是通过将管道分为两个组件实现的：高亮器，它接受用户的问题并从检索到的文档中提取（“高亮”）相关段落；总结器，它接受高亮的段落并将其总结成一个连贯的答案。我们描述并实现了几种可能的H&amp;S实例，并根据正确性、相关性和质量评估其响应。对于某些问答（QA）任务，H&amp;S生成的响应被认为与标准RAG管道的响应一样好，甚至更好。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of preventing jailbreaking and model hijacking in Large Language Models (LLMs), which can lead to the generation of undesirable content. The authors propose a new design pattern called Highlight &amp; Summarize (H&amp;S) for retrieval-augmented generation (RAG) systems, which separates the process into two components: a highlighter that extracts relevant passages from documents based on the user&#x27;s question, and a summarizer that compiles these passages into a coherent answer without exposing the original question to the LLM. Experimental results indicate that the responses generated by H&amp;S for certain question-answering tasks are comparable to, or even exceed, the quality of those produced by traditional RAG pipelines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型（LLM）中防止越狱和模型劫持的挑战，这可能导致生成不良内容。作者提出了一种名为高亮与总结（H&amp;S）的新设计模式，用于检索增强生成（RAG）系统，该模式将过程分为两个组件：高亮器根据用户的问题从文档中提取相关段落，摘要器生成连贯的答案，而不暴露原始问题给LLM。实验评估表明，在某些问答任务中，H&amp;S生成的响应在正确性、相关性和质量方面与标准RAG管道的响应相当或更好。</div>
</details>
</div>
<div class="card">
<div class="title">Weight Decay may matter more than muP for Learning Rate Transfer in Practice</div>
<div class="meta-line">Authors: Atli Kosson, Jeremy Welborn, Yang Liu, Martin Jaggi, Xi Chen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-21T21:36:14+00:00 · Latest: 2026-02-13T17:48:03+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19093v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19093v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transferring the optimal learning rate from small to large neural networks can enable efficient training at scales where hyperparameter tuning is otherwise prohibitively expensive. To this end, the Maximal Update Parameterization (muP) proposes a learning rate scaling designed to keep the update dynamics of internal representations stable across different model widths. However, the scaling rules of muP rely on strong assumptions, particularly about the geometric alignment of a layer&#x27;s inputs with both its weights and gradient updates. In this large-scale empirical investigation, we show that these assumptions hold only briefly at the start of training in the practical setups where learning rate transfer is most valuable, such as LLM training. For the remainder of training it is weight decay rather than muP that correctly stabilizes the update dynamics of internal representations across widths, facilitating learning rate transfer. This suggests muP&#x27;s scaling primarily acts as a form of implicit learning rate warmup, allowing us to largely replace it with modified warmup schedules. Together these findings fundamentally challenge prevailing beliefs about learning rate transfer and can explain empirical observations such as why muP requires the independent weight decay variant for good transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>权重衰减可能比 muP 在实践中更重要于学习率转移</div>
<div class="mono" style="margin-top:8px">将最佳学习率从小型神经网络转移到大型神经网络可以在超参数调优成本过高的情况下实现高效训练。为此，最大更新参数化（muP）提出了一种学习率缩放方法，旨在保持不同模型宽度下内部表示的更新动态稳定。然而，muP 的缩放规则依赖于强假设，特别是关于层输入与其权重和梯度更新的几何对齐。在这项大规模实证研究中，我们表明这些假设仅在训练开始时短暂成立，而在学习率转移最有价值的实际设置中，如 LLM 训练。在训练的其余时间里，正确稳定内部表示更新动态的是权重衰减而非 muP，从而促进学习率转移。这表明 muP 的缩放主要充当隐式学习率预热的形式，使我们能够在很大程度上用修改后的预热计划替代它。这些发现从根本上挑战了关于学习率转移的普遍信念，并可以解释一些实证观察，例如为什么 muP 需要独立的权重衰减变体以实现良好的转移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of training large neural networks by effectively transferring optimal learning rates from smaller networks, which is often hindered by the high cost of hyperparameter tuning. The study employs a large-scale empirical investigation to analyze the Maximal Update Parameterization (muP) and its assumptions regarding the stability of update dynamics across different model widths. The key findings reveal that while muP&#x27;s assumptions hold briefly at the beginning of training, it is actually weight decay that stabilizes update dynamics throughout the training process, suggesting that muP primarily serves as an implicit learning rate warmup and can be largely replaced with modified warmup schedules, challenging existing beliefs about learning rate transfer.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过从小型神经网络转移最佳学习率来提高大型神经网络的训练效率，但这通常受到高昂的超参数调优成本的限制。该研究采用大规模实证调查分析最大更新参数化（muP）在不同模型宽度之间维持稳定更新动态的有效性。研究结果表明，尽管muP关于几何对齐的假设在训练开始时仅短暂有效，但实际上是权重衰减在整个训练过程中起着关键作用，稳定更新动态，从而促进有效的学习率转移，并挑战了关于muP在此背景下必要性的现有信念。</div>
</details>
</div>
<div class="card">
<div class="title">Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching</div>
<div class="meta-line">Authors: Chenguang Wang, Zihan Zhou, Lei Bai, Tianshu Yu</div>
<div class="meta-line">First: 2026-02-13T17:39:21+00:00 · Latest: 2026-02-13T17:39:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13136v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逆合成中的顺序重要性：通过反应中心引导的离散流匹配进行结构感知生成</div>
<div class="mono" style="margin-top:8px">无模板逆合成方法将任务视为黑箱序列生成，限制了学习效率，而半模板方法依赖于刚性反应库，限制了泛化能力。我们通过一个关键见解来解决这一差距：神经表示中的原子顺序很重要。基于这一见解，我们提出了一种结构感知的无模板框架，将化学反应的两阶段特性编码为位置归纳偏置。通过将反应中心原子放置在序列头部，我们的方法将隐式化学知识转化为模型可以轻松捕捉的显式位置模式。所提出的RetroDiT主干是一个带有旋转位置嵌入的图变换器，利用这种顺序优先考虑化学关键区域。结合离散流匹配，我们的方法将训练与采样解耦，使生成步骤从之前的500步减少到20-50步。我们的方法在USPTO-50k（61.2% top-1）和大规模USPTO-Full（51.3% top-1）上实现了最先进的性能，并预测反应中心。使用oracle中心时，性能分别达到71.1%和63.4%，超越了在100亿反应上训练的基础模型，同时使用的数据显示出数量级的减少。消融研究进一步表明，结构先验优于暴力扩展：一个具有适当顺序的280K参数模型与一个没有顺序的65M参数模型相匹配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing retrosynthesis methods, which either treat the task as a black-box sequence generation or rely on rigid reaction libraries, by emphasizing the importance of atom ordering in neural representations. The authors propose a structure-aware, template-free framework that incorporates positional inductive bias to encode the two-stage nature of chemical reactions, utilizing a graph transformer with rotary position embeddings to prioritize reaction center atoms. The method, named RetroDiT, significantly improves generation efficiency, achieving state-of-the-art performance on USPTO-50k and USPTO-Full datasets, with top-1 accuracies of 61.2% and 51.3% respectively, and demonstrating that proper atom ordering can outperform larger models with less data.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有逆合成方法的局限性，这些方法要么将任务视为黑箱，要么依赖于僵化的模板，强调了原子排序在神经表示中的重要性。作者提出了一种结构感知的无模板框架，结合位置归纳偏差来编码化学反应的两阶段特性，利用带有旋转位置嵌入的图变换器。其方法RetroDiT显著提高了生成效率，在USPTO-50k和USPTO-Full数据集上实现了最先进的性能，top-1准确率分别为61.2%和51.3%，并证明了适当的原子排序可以超越在大量数据集上训练的更大模型。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Assumption-Based Argumentation Frameworks</div>
<div class="meta-line">Authors: Emanuele De Angelis, Fabio Fioravanti, Maria Chiara Meo, Alberto Pettorossi, Maurizio Proietti, Francesca Toni</div>
<div class="meta-line">First: 2026-02-13T17:36:15+00:00 · Latest: 2026-02-13T17:36:15+00:00</div>
<div class="meta-line">Comments: Extended version with proofs and additional results of the full paper accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026). DOI: https://doi.org/10.65109/KRAP9309</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13135v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13135v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束假设基础论证框架</div>
<div class="mono" style="margin-top:8px">假设基础论证（ABA）是一种成熟的结构化论证形式。具有基础原子语言的ABA框架被广泛研究，但其适用性受到仅限于基于命题原子的无变量论证和攻击的表示限制。本文解除这一限制，提出了一种新颖的约束ABA（CABA）概念，其组件及由其构建的论证可以包含约束变量，范围可能涵盖无限域。我们定义了CABA的非基础语义，涉及各种非基础攻击的概念。我们展示了新的语义保守地推广了标准ABA语义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional Assumption-based Argumentation (ABA) frameworks, which are restricted to ground arguments and attacks. The authors propose a new framework called constrained ABA (CABA), allowing for the inclusion of constrained variables that can range over infinite domains. The key experimental findings demonstrate that the non-ground semantics defined for CABA conservatively generalize the standard ABA semantics, thereby expanding the applicability of argumentation frameworks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统假设基础论证（ABA）框架的局限性，这些框架仅限于地面论证和命题原子。作者提出了一种新的框架，称为约束ABA（CABA），允许包含可能在无限域上变化的约束变量。主要实验结果表明，为CABA定义的非地面语义保守性地推广了标准ABA语义，从而扩展了论证框架在复杂场景中的适用性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
