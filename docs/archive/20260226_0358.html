<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-26 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260226_0358</div>
    <div class="row"><div class="card">
<div class="title">Test-Time Training with KV Binding Is Secretly Linear Attention</div>
<div class="meta-line">Authors: Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li</div>
<div class="meta-line">First: 2026-02-24T18:59:30+00:00 · Latest: 2026-02-24T18:59:30+00:00</div>
<div class="meta-line">Comments: Webpage: https://research.nvidia.com/labs/sil/projects/tttla/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21204v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21204v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/tttla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>测试时训练与 KV 绑定是隐式线性注意力</div>
<div class="mono" style="margin-top:8px">将 KV 绑定作为序列建模层的测试时训练（TTT）通常被解释为一种在线元学习形式，在测试时记忆键值映射。然而，我们的分析揭示了多种现象，与这种基于记忆的解释相矛盾。基于这些发现，我们重新审视了 TTT 的公式，并展示了一类广泛的 TTT 架构可以被表达为一种学习的线性注意力算子。除了阐明之前令人困惑的模型行为外，这种视角还带来了多种实际好处：它使得原则性的架构简化成为可能，允许完全并行的公式，同时保持性能并提高效率，并提供了将多种 TTT 变体系统性地简化为标准线性注意力形式的途径。总体而言，我们的结果将 TTT 重新框定为学习的线性注意力，而非测试时的记忆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the interpretation of test-time training (TTT) with KV binding, which is often viewed as online meta-learning for memorizing key-value mappings during testing. The authors analyze this approach and propose that many TTT architectures can actually be understood as learned linear attention operators. Their findings not only clarify previously confusing behaviors of these models but also offer practical advantages, such as enabling architectural simplifications, improving efficiency through fully parallel formulations, and standardizing various TTT variants into a linear attention framework. This reframing suggests that TTT should be seen as a method of learned linear attention rather than mere memorization at test time.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于挑战对测试时训练（TTT）与键值（KV）绑定的常见解释，即将其视为在测试过程中仅仅是对键值映射的记忆。作者分析了TTT，并证明它可以重新表述为一种学习的线性注意力操作符，这为理解其基本机制提供了更准确的视角。主要发现表明，这一新视角不仅阐明了先前令人困惑的模型行为，还允许架构简化，通过完全并行的形式提高效率，并将各种TTT变体系统地简化为标准线性注意力格式。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models</div>
<div class="meta-line">Authors: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</div>
<div class="meta-line">First: 2025-09-30T17:58:03+00:00 · Latest: 2026-02-24T18:58:30+00:00</div>
<div class="meta-line">Comments: 23 pages, 10 figures. Project page: https://rsa-llm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26626v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26626v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rsa-llm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA with Gemini 3 Flash attains performance near the top of the ARC-AGI-2 public leaderboard. RSA also enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further propose a novel aggregation-aware reinforcement learning approach that yields significant performance gains by training the model to combine solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归自聚合解锁大型语言模型的深度思考</div>
<div class="mono" style="margin-top:8px">测试时扩展方法通过增加推理期间使用的计算量来提高大型语言模型（LLMs）的能力。推理时的计算可以通过选择多个独立解决方案并行扩展，或通过自我精炼顺序扩展。我们提出了递归自聚合（RSA），这是一种受进化方法启发的测试时扩展方法，结合了并行和顺序扩展的优点。RSA的每一步通过聚合子集来精炼候选推理链的种群，从而产生改进解决方案的种群，这些解决方案随后用作下一次迭代的候选池。实证表明，RSA在不同任务、模型系列和规模中，随着计算预算的增加，提供了显著的性能提升。值得注意的是，使用Gemini 3 Flash的RSA在ARC-AGI-2公共排行榜上达到了接近顶尖的性能。RSA还使Qwen3-4B-Instruct-2507能够与更大的推理模型（包括DeepSeek-R1和o3-mini（高））实现竞争性能，在AIME-25、HMMT-25、Reasoning Gym、LiveCodeBench-v6和SuperGPQA中超越了纯粹的并行和顺序扩展策略。我们进一步提出了一种新颖的聚合感知强化学习方法，通过训练模型组合解决方案，获得显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the capabilities of large language models (LLMs) during inference through improved test-time scaling methods. The authors introduce Recursive Self-Aggregation (RSA), a novel approach that integrates both parallel and sequential scaling by refining a population of candidate reasoning chains through iterative aggregation. Experimental results demonstrate that RSA significantly improves performance across various tasks and model sizes, achieving competitive results on the ARC-AGI-2 leaderboard and enabling smaller models like Qwen3-4B-Instruct-2507 to outperform larger reasoning models, while also introducing an aggregation-aware reinforcement learning method that further boosts performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改进测试时间缩放方法来增强大型语言模型（LLMs）在推理过程中的能力。作者提出了递归自聚合（RSA），这是一种新颖的方法，结合了并行和顺序缩放，通过迭代聚合来优化候选推理链的人群。实验结果表明，RSA在各种任务和模型规模上显著提高了性能，在ARC-AGI-2排行榜上接近顶尖结果，并使得像Qwen3-4B-Instruct-2507这样的小模型能够有效与更大模型竞争，同时在多个基准测试中超越传统的缩放方法。</div>
</details>
</div>
<div class="card">
<div class="title">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</div>
<div class="meta-line">Authors: Abdulaziz Almuzairee, Henrik I. Christensen</div>
<div class="meta-line">First: 2026-02-24T18:58:11+00:00 · Latest: 2026-02-24T18:58:11+00:00</div>
<div class="meta-line">Comments: For website and code, see https://aalmuzairee.github.io/squint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aalmuzairee.github.io/squint">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Squint：用于仿真到现实机器人快速视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习对机器人具有吸引力，但成本高昂——离线策略方法样本效率高但速度慢；在线策略方法并行化良好但浪费样本。最近的研究表明，离线策略方法在基于状态的控制中可以比在线策略方法更快地训练。将这一点扩展到视觉仍然具有挑战性，因为高维输入图像使训练动态复杂，并引入了大量存储和编码开销。为了解决这些挑战，我们引入了Squint，一种视觉软演员评论家方法，能够比以前的视觉离线和在线方法实现更快的墙钟训练。Squint通过并行仿真、分布式评论家、分辨率缩放、层归一化、调整的数据更新比率和优化的实现来实现这一点。我们在SO-101任务集上进行评估，这是ManiSkill3中八个操作任务的新套件，具有大量领域随机化，并展示了仿真到现实的转移到真实的SO-101机器人。我们在单个RTX 3090 GPU上训练策略15分钟，大多数任务在6分钟内收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of visual reinforcement learning in robotics, which is often hindered by the high costs associated with training methods. The authors introduce Squint, a visual Soft Actor Critic method that improves training speed by employing techniques such as parallel simulation, a distributional critic, resolution squinting, and optimized implementation. Experimental results show that Squint enables faster wall-clock training compared to existing visual off-policy and on-policy methods, achieving policy convergence for most tasks in under 6 minutes when trained for 15 minutes on a single RTX 3090 GPU, and successfully demonstrating sim-to-real transfer to a physical robot in the SO-101 Task Set.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高机器人视觉强化学习的效率，但训练方法的高成本常常成为障碍。作者提出了Squint，这是一种视觉Soft Actor Critic方法，通过并行仿真、分布式评论家、分辨率缩小、层归一化和优化的数据更新比率，实现了比现有视觉离线和在线方法更快的墙钟训练时间。在SO-101任务集上的实验结果表明，Squint能够在单个RTX 3090 GPU上仅用15分钟训练策略，大多数任务在6分钟内收敛，有效促进了仿真到现实的转移。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Vector Index Compression in Any Modality</div>
<div class="meta-line">Authors: Hanxiang Qin, Alexander Martin, Rohan Jha, Chunsheng Zuo, Reno Kriz, Benjamin Van Durme</div>
<div class="meta-line">First: 2026-02-24T18:57:33+00:00 · Latest: 2026-02-24T18:57:33+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21202v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21202v1">PDF</a> · <a href="http://github.com/hanxiangqin/omni-col-press">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任何模态下的多向量索引压缩</div>
<div class="mono" style="margin-top:8px">我们研究了在任何模态下进行高效的多向量检索以实现后期交互。后期交互已成为文本、图像、视觉文档和视频信息检索的主导范式，但其计算和存储成本随着文档长度线性增长，使得在图像、视频和音频丰富的语料库中成本高昂。为了解决这一限制，我们探索了在固定向量预算下压缩多向量文档表示的无查询方法。我们提出了四种索引压缩方法：序列调整、内存令牌、层次池化和一种新颖的注意力引导聚类（AGC）。AGC使用注意力引导机制识别文档中最语义显著的区域作为聚类中心，并对令牌聚合进行加权。在涵盖文本（BEIR）、视觉文档（ViDoRe）和视频（MSR-VTT，MultiVENT 2.0）的检索任务中评估这些方法，我们表明注意力引导聚类始终优于其他参数化压缩方法（序列调整和内存令牌），在索引大小上提供比非参数层次聚类更大的灵活性，并且与完整的未压缩索引相比，性能具有竞争力或有所提升。源代码可在：github.com/hanxiangqin/omni-col-press获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computation and storage costs associated with late interaction multi-vector retrieval across various modalities, which become prohibitive for rich media corpora. To mitigate these costs, the authors propose four query-agnostic index compression methods: sequence resizing, memory tokens, hierarchical pooling, and attention-guided clustering (AGC). Experimental results demonstrate that AGC significantly outperforms other compression techniques, offering greater flexibility in index size while maintaining or enhancing retrieval performance across diverse datasets, including text, visual documents, and videos.</div>
<div class="mono" style="margin-top:8px">本研究解决了在各种模态下进行高效多向量检索的挑战，尤其是在文档长度增加时，计算和存储成本的增长对图像和视频等丰富媒体的影响。为了减轻这些成本，作者提出了四种无查询的索引压缩方法：序列调整、内存令牌、层次池化和注意力引导聚类（AGC），该方法识别文档中语义重要的区域以有效聚合令牌。实验结果表明，AGC在不同的检索任务中始终优于其他压缩技术，提供了更大的索引大小灵活性，并在与未压缩索引的比较中保持了竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Aletheia tackles FirstProof autonomously</div>
<div class="meta-line">Authors: Tony Feng, Junehyuk Jung, Sang-hyun Kim, Carlo Pagano, Sergei Gukov, Chiang-Chiang Tsai, David Woodruff, Adel Javanmard, Aryan Mokhtari, Dawsen Hwang, Yuri Chervonyi, Jonathan N. Lee, Garrett Bingham, Trieu H. Trinh, Vahab Mirrokni, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-02-24T18:56:10+00:00 · Latest: 2026-02-24T18:56:10+00:00</div>
<div class="meta-line">Comments: 34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21201v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21201v1">PDF</a> · <a href="https://github.com/google-deepmind/superhuman/tree/main/aletheia">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Aletheia自主解决FirstProof挑战</div>
<div class="mono" style="margin-top:8px">我们报告了Aletheia（Feng等，2026b）的表现，这是一种由Gemini 3 Deep Think驱动的数学研究代理，在首届FirstProof挑战中。在挑战允许的时间内，Aletheia根据多数专家评估自主解决了10个问题中的6个（2, 5, 7, 8, 9, 10）；我们注意到专家在问题8上并未达成一致。为了完全透明，我们解释了我们对FirstProof的理解，并披露了我们的实验细节以及评估。原始提示和输出可在https://github.com/google-deepmind/superhuman/tree/main/aletheia获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the capabilities of Aletheia, a mathematics research agent, in solving problems autonomously during the FirstProof challenge. The main method employed involves using the Gemini 3 Deep Think framework to tackle the challenge&#x27;s problems. Aletheia successfully solved 6 out of 10 problems, with expert assessments indicating a majority agreement on its performance, although there was some disagreement on one specific problem (Problem 8).</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估数学研究代理Aletheia在FirstProof挑战中的表现。主要方法是使用Gemini 3 Deep Think框架自主解决数学问题。关键实验结果表明，Aletheia成功解决了10个问题中的6个，专家评估确认了其表现，但在一个特定问题上专家之间存在一些分歧。</div>
</details>
</div>
<div class="card">
<div class="title">Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs</div>
<div class="meta-line">Authors: Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu, Yejin Choi</div>
<div class="meta-line">First: 2026-02-24T18:55:18+00:00 · Latest: 2026-02-24T18:55:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21198v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21198v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从试错中学习：具身大语言模型的反思性测试时间规划</div>
<div class="mono" style="margin-top:8px">具身大语言模型赋予机器人高级任务推理能力，但它们无法反思错误发生的原因，导致部署变成一系列独立的试验，错误重复而不是积累经验。借鉴人类反思性实践者，我们引入了反思性测试时间规划，整合了两种反思模式：\textit{行动中的反思}，代理在执行前利用测试时间缩放生成和评分多个候选动作；以及\textit{行动后的反思}，在执行后基于外部反思更新其内部反思模型和行动策略。我们还包括回顾性反思，允许代理重新评估早期决策，并在事后进行模型更新，以便进行适当的长期信用分配。在我们新设计的长期家庭基准和MuJoCo橱柜拟合基准上的实验显示，相较于基线模型有显著提升，消融研究验证了行动中的反思和行动后的反思的互补作用。定性分析，包括真实机器人试验，强调了通过反思进行行为修正。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of embodied LLMs in learning from past mistakes during task execution, which often leads to repeated errors instead of accumulated experience. The authors propose a method called Reflective Test-Time Planning that incorporates two types of reflection: reflection-in-action, where the agent generates and evaluates multiple actions at test time, and reflection-on-action, which updates the agent&#x27;s model and policy based on outcomes after execution. Experimental results on the Long-Horizon Household and MuJoCo Cupboard Fitting benchmarks demonstrate significant performance improvements over baseline models, with additional studies confirming the effectiveness of both reflection modes in enhancing decision-making and behavioral correction in real-robot trials.</div>
<div class="mono" style="margin-top:8px">本研究解决了具身大语言模型在任务执行中无法从过去错误中学习的局限性，这常常导致重复错误而不是积累经验。作者提出了一种名为反思测试时间规划的方法，该方法结合了两种反思模式：行动中的反思，代理在测试时生成和评估多个动作，以及行动后的反思，根据执行后的评估更新代理的模型和策略。对长时间家庭基准和MuJoCo橱柜拟合基准的实验结果显示，与基线模型相比，性能显著提升，额外研究确认了两种反思模式在提高性能和纠正真实机器人试验中的行为方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</div>
<div class="meta-line">Authors: Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin</div>
<div class="meta-line">First: 2026-02-24T18:54:39+00:00 · Latest: 2026-02-24T18:54:39+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解开乌利西斯：通过头级分块实现内存高效的上下文并行</div>
<div class="mono" style="margin-top:8px">使用Transformer模型高效处理长序列通常需要通过上下文并行将计算分散到加速器上。这类方法中的主流方法，如环形注意力或DeepSpeed Ulysses，能够在上下文维度上进行扩展，但并未关注内存效率，这限制了它们所能支持的序列长度。更先进的技术，如完全流水线分布式Transformer或激活卸载，虽然可以进一步扩展可能的上下文长度，但会牺牲训练吞吐量。在本文中，我们提出了UPipe，这是一种简单而有效的上下文并行技术，在注意力头级别进行细粒度分块。该技术显著减少了自注意力的激活内存使用，打破了激活内存的限制，解锁了更长的上下文长度。我们的方法在注意力层中将中间张量内存使用减少了多达87.5%（对于32B的Transformer），同时在训练速度上与之前的上下文并行技术相匹配。UPipe在单个8×H100节点上训练Llama3-8B时可以支持5M tokens的上下文长度，较之前的方法提高了超过25%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the memory efficiency of context parallelism in Transformer models, which is crucial for processing long sequences. The authors introduce UPipe, a novel method that employs fine-grained chunking at the attention head level to reduce activation memory usage in self-attention. Experimental results demonstrate that UPipe can decrease intermediate tensor memory usage by up to 87.5% for 32B Transformers while maintaining training speed comparable to existing techniques, enabling support for context lengths of 5M tokens when training Llama3-8B on a single 8×H100 node, surpassing previous methods by over 25%.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用Transformer模型高效处理长序列的挑战，这些模型在使用上下文并行方法时通常面临内存效率的限制。作者提出了UPipe，这是一种新颖的技术，采用注意力头级别的细粒度分块，显著减少自注意力机制中的激活内存使用。实验结果表明，UPipe可以将32B Transformer的中间张量内存使用减少多达87.5%，同时保持可比的训练速度，使得在单个8×H100节点上训练Llama3-8B时支持5M标记的上下文长度，超过之前方法25%以上的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Transfer Learning in Infinite Width Feature Learning Networks</div>
<div class="meta-line">Authors: Clarissa Lauditi, Blake Bordelon, Cengiz Pehlevan</div>
<div class="meta-line">First: 2025-07-06T16:14:43+00:00 · Latest: 2026-02-24T18:49:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04448v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.04448v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a theory of transfer learning in infinitely wide neural networks under gradient flow that quantifies when pretraining on a source task improves generalization on a target task. We analyze both (i) fine-tuning, when the downstream predictor is trained on top of source-induced features and (ii) a jointly rich setting, where both pretraining and downstream tasks can operate in a feature learning regime, but the downstream model is initialized with the features obtained after pre-training. In this setup, the summary statistics of randomly initialized networks after a rich pre-training are adaptive kernels which depend on both source data and labels. For (i), we analyze the performance of a readout for different pretraining data regimes. For (ii), the summary statistics after learning the target task are still adaptive kernels with features from both source and target tasks. We test our theory on linear and polynomial regression tasks as well as real datasets. Our theory allows interpretable conclusions on performance, which depend on the amount of data on both tasks, the alignment between tasks, and the feature learning strength.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无限宽特征学习网络中的迁移学习</div>
<div class="mono" style="margin-top:8px">我们在梯度流下发展了无限宽神经网络中迁移学习的理论，量化了在源任务上预训练何时能改善目标任务的泛化能力。我们分析了两种情况：(i) 微调，即下游预测器在源诱导特征上进行训练；(ii) 联合丰富设置，其中预训练和下游任务都可以在特征学习模式下运行，但下游模型是用预训练后获得的特征初始化的。在这种设置中，经过丰富预训练后随机初始化网络的摘要统计量是依赖于源数据和标签的自适应核。对于(i)，我们分析了不同预训练数据模式下的读出性能。对于(ii)，学习目标任务后的摘要统计量仍然是自适应核，具有来自源任务和目标任务的特征。我们在线性和多项式回归任务以及真实数据集上测试了我们的理论。我们的理论允许对性能得出可解释的结论，这取决于两个任务的数据量、任务之间的对齐程度和特征学习的强度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the effectiveness of transfer learning in infinitely wide neural networks, motivated by the need to understand how pretraining on a source task can enhance generalization on a target task. The authors develop a theoretical framework that distinguishes between fine-tuning and a jointly rich setting for feature learning, analyzing the performance of models under different pretraining conditions. Key findings indicate that the summary statistics of networks after pretraining serve as adaptive kernels influenced by both source and target data, with performance outcomes being contingent on data volume, task alignment, and feature learning strength, validated through experiments on regression tasks and real datasets.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无限宽神经网络中迁移学习的有效性，旨在理解在源任务上进行预训练如何增强目标任务的泛化能力。作者开发了一个理论框架，考察了两种情境：使用源诱导特征的微调和一个共同丰富的设置，其中预训练和下游任务都采用特征学习方法。关键发现表明，模型的性能受限于两个任务的数据量、源任务与目标任务之间的对齐程度以及特征学习的强度，并在线性和多项式回归任务以及真实数据集上进行了实验验证。</div>
</details>
</div>
<div class="card">
<div class="title">Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning</div>
<div class="meta-line">Authors: Seyed Hossein Alavi, Zining Wang, Shruthi Chockkalingam, Raymond T. Ng, Vered Shwartz</div>
<div class="meta-line">First: 2026-02-20T00:07:18+00:00 · Latest: 2026-02-24T18:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17905v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17905v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>教学游戏与说服聊天：比较互动与静态格式在说服学习中的应用</div>
<div class="mono" style="margin-top:8px">互动系统如聊天机器人和游戏越来越多地用于在可持续性相关主题上进行说服和教育，但不同的传递格式如何在内容保持不变的情况下影响学习和说服结果仍不清楚。基于相同的论点和事实内容，我们进行了一项对照用户研究，比较三种信息传递模式：静态论文、对话聊天机器人和叙事文本游戏。在主观测量中，聊天机器人条件始终优于其他模式，并提高了对主题的重要性感知。然而，感知学习与客观结果并不总是一致：文本游戏条件的参与者报告的学习少于阅读论文的参与者，但在延迟（24小时）知识测验中获得了更高的分数。额外的探索性分析进一步表明，常见的参与度代理，如冗长性和互动时长，与主观体验的关系更密切，而非实际学习。这些发现突显了说服体验的感受与参与者所保留的内容之间的脱节，并指出了在说服系统和严肃游戏中互动性、现实性与学习之间的重要设计权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how different formats of information delivery affect learning and persuasion regarding sustainability topics, motivated by the increasing use of interactive systems like chatbots and games. A controlled user study was conducted comparing static essays, conversational chatbots, and narrative text-based games while keeping the content constant. The results showed that the chatbot format led to a higher perceived importance of the topic, although perceived learning did not correlate with objective knowledge retention, as participants in the game condition reported less learning but scored higher on a delayed quiz compared to those reading essays. This indicates a disconnect between subjective experiences of engagement and actual learning outcomes, emphasizing the need to balance interactivity, realism, and educational effectiveness in persuasive systems.</div>
<div class="mono" style="margin-top:8px">本研究探讨了不同的信息传递格式——静态文章、对话式聊天机器人和叙事文本游戏——如何影响可持续性教育中的学习和说服效果。通过使用相同的论点和事实内容进行的控制用户研究显示，聊天机器人格式提高了主题的重要性感知，而文本游戏条件下的参与者报告的学习感知较低，但在延迟知识测验中得分高于阅读文章的参与者。这些发现揭示了说服体验的主观感受与实际知识保留之间的脱节，强调了在教育系统中平衡互动性、真实性和学习的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Query Lower Bounds for Smoothed Agnostic Learning</div>
<div class="meta-line">Authors: Ilias Diakonikolas, Daniel M. Kane</div>
<div class="meta-line">First: 2026-02-24T18:46:46+00:00 · Latest: 2026-02-24T18:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21191v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21191v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\tilde{O}(1/σ^2) \log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑无知学习的统计查询下界</div>
<div class="mono" style="margin-top:8px">我们研究了最近由~\cite{CKKMS24}提出的平滑无知学习的复杂性，在该学习中，学习者在输入的轻微高斯扰动下与目标类中的最佳分类器竞争。具体而言，我们关注在平滑模型下，在次高斯分布下无知学习半空间的典型任务。该问题已知的最佳上界依赖于$L_1$-多项式回归，复杂度为$d^{\tilde{O}(1/σ^2) \log(1/ε)}$，其中$σ$是平滑参数，$ε$是过量误差。我们的主要结果是一个统计查询（SQ）下界，提供了该上界接近最佳可能性的正式证据。更详细地说，我们表明（即使对于高斯边际）任何用于平滑无知学习半空间的SQ算法都需要复杂度$d^{Ω(1/σ^{2}+\log(1/ε))}$。这是该任务复杂性的第一个非平凡下界，并且几乎与已知的上界相匹配。大致而言，我们表明将$L_1$-多项式回归应用于函数的平滑版本在本质上是最佳的。我们的技术涉及通过线性规划对偶性找到一个矩匹配困难分布。该对偶程序恰好对应于找到一个低度逼近多项式，以平滑版本的目标函数（这恰好是$L_1$-多项式回归工作的条件）。我们的显式SQ下界来自于证明该半空间类的逼近度的下界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to understand the complexity of smoothed agnostic learning, particularly in the context of learning halfspaces under subgaussian distributions. The authors focus on establishing a Statistical Query (SQ) lower bound for this learning task, demonstrating that any SQ algorithm requires a complexity of d^{Ω(1/σ^{2}+log(1/ε))}, which closely matches the previously known upper bound of d^{\tilde{O}(1/σ^{2}) log(1/ε)}. This finding indicates that the upper bound is nearly optimal and highlights the effectiveness of using L_1-polynomial regression in this smoothed learning scenario, with the techniques involving linear programming duality to find a moment-matching hard distribution.</div>
<div class="mono" style="margin-top:8px">本研究探讨了平滑不可知学习的复杂性，该学习涉及从带有轻微高斯扰动的数据中学习。作者专注于在子高斯分布下学习半空间的特定情况，并建立了一个统计查询下界，表明在此背景下任何算法的复杂性。他们的研究结果显示，任何用于该学习任务的统计查询算法的复杂性至少为d^{Ω(1/σ^{2}+log(1/ε))}，这与已知的最佳上界非常接近，从而为该问题提供了第一个重要的下界，并证明了L_1多项式回归的应用在此场景中几乎是最优的。</div>
</details>
</div>
<div class="card">
<div class="title">Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training</div>
<div class="meta-line">Authors: Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-02-24T18:43:08+00:00 · Latest: 2026-02-24T18:43:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21189v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21189v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么 Pass@k 优化会降低 Pass@1：LLM 后训练中的提示干扰</div>
<div class="mono" style="margin-top:8px">Pass@k 是一个广泛使用的可验证大型语言模型任务的性能指标，包括数学推理、代码生成和简答推理。如果 $k$ 个独立采样的解决方案中有任何一个通过验证器，则定义为成功。这个多样本推理指标激励了直接优化 pass@$k$ 的推理感知微调方法。然而，先前的研究报告了一个反复出现的权衡：在这种方法下，pass@k 改善而 pass@1 降低。这个权衡在实践中很重要，因为 pass@1 通常由于延迟和成本预算、不完美的验证器覆盖以及对可靠单次回退的需求而成为一个严格的操作约束。我们研究了这个权衡的起源，并提供了一个理论表征，说明何时 pass@k 策略优化会通过提示干扰引起的梯度冲突降低 pass@1。我们表明，pass@$k$ 策略梯度可能与 pass@1 梯度冲突，因为 pass@$k$ 优化隐式地将提示重新加权为低成功提示；当这些提示被称为负干扰时，它们的加权可能会使 pass@k 更新方向偏离 pass@1 方向。我们通过在可验证的数学推理任务上进行的大型语言模型实验来说明我们的理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand the trade-off between Pass@k and Pass@1 performance metrics in large language models, particularly in contexts where Pass@1 is critical due to operational constraints. The authors investigate this issue through theoretical analysis and large language model experiments focused on verifiable mathematical reasoning tasks. They find that optimizing for Pass@k can lead to a degradation of Pass@1 due to gradient conflicts caused by prompt interference, where the reweighting of prompts toward low-success options negatively impacts the Pass@1 performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型中Pass@k与Pass@1性能指标之间的权衡，动机在于保持Pass@1的重要性，因为它受到操作限制的影响。作者分析了提示干扰现象，并提供了理论框架来解释优化Pass@k如何导致Pass@1性能下降。实验结果表明，优化过程可能无意中偏向低成功率的提示，导致梯度方向冲突，从而在数学推理任务的大型语言模型评估中对Pass@1结果产生负面影响。</div>
</details>
</div>
<div class="card">
<div class="title">The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum</div>
<div class="meta-line">Authors: Justin Deschenaux, Caglar Gulcehre, Subham Sekhar Sahoo</div>
<div class="meta-line">First: 2026-02-24T18:35:22+00:00 · Latest: 2026-02-24T18:35:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散二重性，第二章：$Ψ$-采样器与高效课程</div>
<div class="mono" style="margin-top:8px">均匀状态离散扩散模型因其自我修正能力，在少步生成和引导方面表现优异，因此在这些设置中优于自回归或掩蔽扩散模型。然而，随着步骤数量的增加，其采样质量在祖先采样器上达到饱和。我们引入了一类用于离散扩散的预测-修正（PC）采样器，推广了先前的方法，并适用于任意噪声过程。当与均匀状态扩散配对时，我们的采样器在语言和图像建模上均优于祖先采样，在OpenWebText上实现了匹配单元熵下的更低生成困惑度，并在CIFAR10上获得了更好的FID/IS分数。至关重要的是，与传统采样器不同，我们的PC方法在更多采样步骤下继续改进。综合来看，这些发现质疑了掩蔽扩散是基于扩散的语言建模不可避免的未来这一假设。除了采样外，我们还开发了一种内存高效的课程，用于高斯松弛训练阶段，与Duo相比，训练时间减少了25%，内存减少了33%，同时在OpenWebText和LM1B上保持了可比的困惑度和强大的下游性能。我们发布了代码、检查点和视频教程，网址为：https://s-sahoo.com/duo-ch2</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of uniform-state discrete diffusion models, which excel in few-step generation but experience a plateau in sampling quality with increasing steps. The authors introduce Predictor-Corrector (PC) samplers that generalize previous methods and can be applied to various noise processes. Experimental results demonstrate that these PC samplers outperform ancestral sampling in both language and image modeling, achieving lower generative perplexity and better FID/IS scores, while also improving with more sampling steps. Additionally, a memory-efficient curriculum for Gaussian relaxation training is developed, reducing training time by 25% and memory usage by 33% compared to previous methods, while maintaining comparable perplexity and strong downstream performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于统一状态离散扩散模型的局限性，这些模型在少步生成中表现出色，但随着步骤增加，祖先采样器的采样质量停滞不前。作者引入了预测-校正（PC）采样器，这些采样器推广了之前的方法，并适用于任意噪声过程，证明这些采样器在语言和图像建模中优于祖先采样，获得了更低的生成困惑度和更好的FID/IS分数。此外，他们开发了一种内存高效的课程，用于高斯松弛训练阶段，减少了25%的训练时间和33%的内存使用，同时保持了相似的困惑度和强大的下游性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision</div>
<div class="meta-line">Authors: Nicolás Gaggion, Maria J. Ledesma-Carbayo, Stergios Christodoulidis, Maria Vakalopoulou, Enzo Ferrante</div>
<div class="meta-line">First: 2026-02-24T18:29:13+00:00 · Latest: 2026-02-24T18:29:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21179v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21179v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mask-HybridGNet：基于图的分割与来自像素级监督的突现解剖对应</div>
<div class="mono" style="margin-top:8px">基于图的医学图像分割使用边界图表示解剖结构，提供固定拓扑的标志和固有的人群级对应。然而，它们在临床应用中的推广受到一个主要要求的阻碍：在实践中，具有手动标注标志且能在患者之间保持点对点对应的训练数据集几乎不存在。我们引入了Mask-HybridGNet，一个直接使用标准像素级掩膜训练基于图的模型的框架，消除了对手动标注的需求。我们的方法通过结合Chamfer距离监督和基于边缘的正则化，将可变长度的真实边界与固定长度的标志预测对齐，以确保局部平滑性和规则的标志分布，进一步通过可微光栅化进行细化。该框架的一个显著突现特性是，预测的标志位置在没有显式对应监督的情况下，能够与患者之间的特定解剖位置一致关联。这种隐式图谱学习使得时间跟踪、跨切片重建和形态人群分析成为可能。除了直接分割，Mask-HybridGNet还可以从现有的分割掩膜中提取对应，使其能够从任何高质量的基于像素的模型生成稳定的解剖图谱。在胸部X光、心脏超声、心脏MRI和胎儿成像的实验中，我们的模型在与最先进的基于像素的方法相比时，取得了具有竞争力的结果，同时通过强制边界连通性确保了解剖学的合理性，使用固定的图邻接矩阵。该框架利用标准分割掩膜的广泛可用性构建保持拓扑完整性并提供隐式对应的结构化模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of training graph-based medical image segmentation models, which typically require manually annotated landmarks that are often unavailable. The authors propose Mask-HybridGNet, a framework that utilizes standard pixel-wise masks for training, thus eliminating the need for manual annotations. Key experimental findings indicate that the model not only achieves competitive segmentation results across various imaging modalities, such as chest radiography and cardiac MRI, but also establishes consistent anatomical correspondences across patients, enabling applications like temporal tracking and morphological analysis without explicit correspondence supervision.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决训练图基医学图像分割模型所需的手动标注地标通常不可用的问题。作者提出了Mask-HybridGNet框架，该框架利用标准的像素级掩膜进行训练，从而消除了对手动地标注释的需求。关键实验结果表明，该模型在胸部放射学和心脏MRI等多种成像模式下，不仅在性能上与最先进的基于像素的方法竞争，而且还促进了隐式图谱学习，使得解剖对应关系在没有显式监督的情况下出现，从而支持时间跟踪和形态分析等应用。</div>
</details>
</div>
<div class="card">
<div class="title">XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence</div>
<div class="meta-line">Authors: Sepehr Salem Ghahfarokhi, M. Moein Esfahani, Raj Sunderraman, Vince Calhoun, Mohammed Alser</div>
<div class="meta-line">First: 2026-02-24T18:28:08+00:00 · Latest: 2026-02-24T18:28:08+00:00</div>
<div class="meta-line">Comments: Accepted in ICCABS 2026: The 14th International Conference on Computational Advances in Bio and Medical Sciences</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21178v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21178v1">PDF</a> · <a href="https://github.com/ALSER-Lab/XMorph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque &#x27;&#x27;black boxes&#x27;&#x27; and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XMorph：通过LLM辅助的混合深度智能进行可解释的脑肿瘤分析</div>
<div class="mono" style="margin-top:8px">深度学习显著推动了自动化脑肿瘤诊断的发展，但临床应用仍受限于可解释性和计算约束。传统模型往往作为不透明的“黑箱”运作，无法量化恶性生长特征的复杂、不规则肿瘤边界。为了解决这些挑战，我们提出了XMorph，一个可解释且计算高效的框架，用于对三种主要脑肿瘤类型进行细粒度分类：胶质瘤、脑膜瘤和垂体肿瘤。我们提出了一种信息加权边界归一化（IWBN）机制，强调诊断相关的边界区域以及非线性混沌和临床验证特征，从而实现肿瘤生长的更丰富形态表示。一个双通道可解释AI模块结合了GradCAM++视觉线索和LLM生成的文本推理，将模型推理转化为临床可解释的见解。该框架实现了96.0%的分类准确率，证明了可解释性和高性能可以在基于AI的医学影像系统中共存。XMorph的源代码和材料均可在以下网址公开获取：https://github.com/ALSER-Lab/XMorph。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the interpretability and efficiency of automated brain tumor diagnosis, which is currently hindered by the black-box nature of conventional deep learning models. The authors introduce XMorph, a framework that employs an Information-Weighted Boundary Normalization mechanism to improve the classification of glioma, meningioma, and pituitary tumors by focusing on relevant tumor boundary regions and integrating clinically validated features. The experimental results indicate that XMorph achieves a classification accuracy of 96.0%, demonstrating that it is possible to achieve both explainability and high performance in AI-based medical imaging systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高自动化脑肿瘤诊断的可解释性和计算效率，而传统深度学习模型的黑箱特性限制了这一点。作者提出了XMorph框架，采用信息加权边界归一化（IWBN）机制，重点关注关键肿瘤边界区域，并整合临床验证的特征。实验结果表明，XMorph在胶质瘤、脑膜瘤和垂体肿瘤的分类准确率达到96.0%，证明了高性能与可解释性可以在基于AI的医学影像系统中有效结合。</div>
</details>
</div>
<div class="card">
<div class="title">How much does context affect the accuracy of AI health advice?</div>
<div class="meta-line">Authors: Prashant Garg, Thiemo Fetzer</div>
<div class="meta-line">First: 2025-04-25T12:37:15+00:00 · Latest: 2026-02-24T18:23:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.18310v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.18310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used to provide health advice, yet evidence on how their accuracy varies across languages, topics and information sources remains limited. We assess how linguistic and contextual factors affect the accuracy of AI-based health-claim verification. We evaluated seven widely used LLMs on two datasets: (i) 1,975 legally authorised nutrition and health claims from UK and EU regulatory registers translated into 21 languages; and (ii) 9,088 journalist-vetted public-health claims from the PUBHEALTH corpus spanning COVID-19, abortion, politics and general health, drawn from government advisories, scientific abstracts and media sources. Models classified each claim as supported or unsupported using majority voting across repeated runs. Accuracy was analysed by language, topic, source and model. Accuracy on authorised claims was highest in English and closely related European languages and declined in several widely spoken non-European languages, decreasing with syntactic distance from English. On real-world public-health claims, accuracy was substantially lower and varied systematically by topic and source. Models performed best on COVID-19 and government-attributed claims and worst on general health and scientific abstracts. High performance on English, canonical health claims masks substantial context-dependent gaps. Differences in training data exposure, editorial framing and topic-specific tuning likely contribute to these disparities, which are comparable in magnitude to cross-language differences. LLM accuracy in health-claim verification depends strongly on language, topic and information source. English-language performance does not reliably generalise across contexts, underscoring the need for multilingual, domain-specific evaluation before deployment in public-health communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文对人工智能健康建议准确性的影响有多大？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地用于提供健康建议，但关于它们的准确性在不同语言、主题和信息来源之间的变化的证据仍然有限。我们评估了语言和上下文因素如何影响基于人工智能的健康声明验证的准确性。我们在两个数据集上评估了七个广泛使用的LLM：（i）来自英国和欧盟监管注册的1,975条合法授权的营养和健康声明，翻译成21种语言；（ii）来自PUBHEALTH语料库的9,088条经过记者审查的公共卫生声明，涵盖COVID-19、堕胎、政治和一般健康，来源于政府建议、科学摘要和媒体来源。模型通过重复运行的多数投票将每个声明分类为支持或不支持。根据语言、主题、来源和模型分析准确性。授权声明的准确性在英语和密切相关的欧洲语言中最高，在几种广泛使用的非欧洲语言中下降，且与英语的句法距离增加而减少。在现实世界的公共卫生声明中，准确性显著较低，并且在主题和来源上系统性变化。模型在COVID-19和政府归属的声明上表现最佳，而在一般健康和科学摘要上表现最差。英语和规范健康声明的高性能掩盖了显著的上下文依赖差距。训练数据曝光、编辑框架和主题特定调优的差异可能导致这些差异，其规模与跨语言差异相当。LLM在健康声明验证中的准确性在很大程度上依赖于语言、主题和信息来源。英语语言的表现并不能可靠地推广到不同上下文，强调在公共卫生传播中部署前需要进行多语言、特定领域的评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand how linguistic and contextual factors influence the accuracy of AI-based health advice provided by large language models (LLMs). The study evaluated seven widely used LLMs on two datasets: one consisting of 1,975 legally authorized health claims translated into 21 languages, and another with 9,088 vetted public-health claims covering various topics. The findings revealed that accuracy was highest for authorized claims in English and closely related European languages, while it decreased in widely spoken non-European languages. Additionally, accuracy on real-world public-health claims was lower and varied by topic and source, with the best performance on COVID-19 claims and the worst on general health claims, highlighting the need for multilingual and context-specific evaluations before deploying LLMs in public health communication.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）提供的AI健康建议的准确性，动机在于对这种准确性在不同语言、主题和信息来源之间变化的证据有限。研究使用两个数据集评估了七个LLM：一个包含1975条翻译成21种语言的合法健康声明，另一个包含9088条涵盖各种主题的经过审查的公共健康声明。研究结果显示，授权声明在英语及相关欧洲语言中的准确性最高，而在一些广泛使用的非欧洲语言中准确性下降，并且根据主题和来源的不同，准确性差异显著，COVID-19声明表现最佳，而一般健康主题表现最差。这些结果强调了上下文在AI健康建议准确性中的重要性，以及在部署之前进行多语言评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Through Words: Controlling Visual Retrieval Quality with Language Models</div>
<div class="meta-line">Authors: Jianglin Lu, Simon Jenni, Kushal Kafle, Jing Shi, Handong Zhao, Yun Fu</div>
<div class="meta-line">First: 2026-02-24T18:20:57+00:00 · Latest: 2026-02-24T18:20:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21175v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21175v1">PDF</a> · <a href="https://github.com/Jianglin954/QCQC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>透视文字：通过语言模型控制视觉检索质量</div>
<div class="mono" style="margin-top:8px">文本到图像检索是视觉-语言学习中的一项基本任务，但在现实场景中，短小且不明确的用户查询常常带来挑战。这类查询通常只有一两个词，导致语义模糊，容易在多样的视觉解释中产生冲突，并缺乏对检索图像质量的明确控制。为了解决这些问题，我们提出了一种新的质量可控检索范式，通过上下文细节丰富短查询，同时结合图像质量的明确概念。我们的关键思想是利用生成语言模型作为查询补全函数，将不明确的查询扩展为描述性形式，捕捉细致的视觉属性，如姿势、场景和美学。我们引入了一个通用框架，将查询补全条件化于离散化的质量水平，这些水平源自相关性和美学评分模型，从而使查询丰富不仅在语义上有意义，而且对质量有意识。最终系统提供了三个关键优势：1）灵活性，兼容任何预训练的视觉-语言模型（VLMs）而无需修改；2）透明性，丰富的查询对用户是明确可解释的；3）可控性，使检索结果能够朝向用户偏好的质量水平引导。大量实验表明，我们提出的方法显著改善了检索结果，并提供了有效的质量控制，弥合了现代VLMs的表达能力与短用户查询的不明确性之间的差距。我们的代码可在 https://github.com/Jianglin954/QCQC 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance text-to-image retrieval, which often struggles with short and ambiguous user queries. The authors propose a new paradigm for quality-controllable retrieval that enriches these queries using a generative language model to generate descriptive forms that capture detailed visual attributes. Experimental results show that this approach significantly improves retrieval outcomes and allows for effective control over image quality, demonstrating flexibility, transparency, and controllability in the retrieval process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升文本到图像检索的效果，尤其是在面对短小且模糊的用户查询时，这类查询往往导致检索结果不佳。作者提出了一种新颖的框架，利用生成语言模型来丰富这些不明确的查询，通过添加上下文细节并控制图像质量。实验结果表明，该方法显著改善了检索性能，并允许用户控制检索图像的质量，展示了与现有视觉-语言模型的兼容性，并提供了透明且可解释的查询丰富过程。</div>
</details>
</div>
<div class="card">
<div class="title">NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning</div>
<div class="meta-line">Authors: Ishaan Rawal, Shubh Gupta, Yihan Hu, Wei Zhan</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-24T18:17:21+00:00 · Latest: 2026-02-24T18:17:21+00:00</div>
<div class="meta-line">Comments: Accepted to CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21172v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $&lt;$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NoRD：一种无需推理的高效数据视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过用统一的端到端架构替代模块化管道，推动了自主驾驶的发展。然而，当前的VLA面临两个昂贵的要求：（1）大量数据集收集，和（2）密集的推理注释。在这项工作中，我们通过\modelname（\textbf{无} \textbf{推理}用于\textbf{驾驶}）解决了这两个挑战。与现有的VLA相比，\modelname在$&lt;$60\%的数据和没有推理注释的情况下实现了竞争性能，结果减少了3$\times$的token数量。我们发现，标准的群体相对策略优化（GRPO）在应用于在如此小的、无推理数据集上训练的策略时未能产生显著的改进。我们表明，这一限制源于难度偏差，它不成比例地惩罚来自于在GRPO中产生高方差回报的场景的奖励信号。\modelname通过结合Dr.~GRPO克服了这一问题，Dr.~GRPO是一种旨在减轻LLM中难度偏差的最新算法。因此，\modelname在Waymo和NAVSIM上以较少的训练数据和没有推理开销实现了竞争性能，从而使自主系统更加高效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance autonomous driving capabilities by developing a Vision-Language-Action (VLA) model that reduces the need for extensive dataset collection and reasoning annotations. The authors introduce NoRD, a model that achieves competitive performance while being fine-tuned on less than 60% of the data and without requiring reasoning annotations, resulting in three times fewer tokens. They identify that the standard Group Relative Policy Optimization (GRPO) method does not significantly improve policies trained on small, reasoning-free datasets due to difficulty bias, which they address by implementing Dr. GRPO, an algorithm that mitigates this bias, ultimately demonstrating that NoRD can perform effectively on Waymo and NAVSIM with reduced training data and no reasoning overhead.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过视觉-语言-动作（VLA）模型提升自动驾驶，同时解决对大量数据集和密集推理注释的需求。作者提出了NoRD模型，该模型在不到60%的数据和没有推理注释的情况下有效运行，且以三倍更少的标记实现了竞争性能。他们发现传统的群体相对策略优化（GRPO）在小型无推理数据集上表现不佳，主要是由于困难偏差，作者通过采用Dr. GRPO来减轻这一问题，从而在Waymo和NAVSIM上实现了显著的性能提升，且训练数据大幅减少，无需推理开销。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Round Human-AI Collaboration with User-Specified Requirements</div>
<div class="meta-line">Authors: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas</div>
<div class="meta-line">First: 2026-02-19T18:54:34+00:00 · Latest: 2026-02-24T18:15:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17646v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用户指定要求的多轮人机协作</div>
<div class="mono" style="margin-top:8px">随着人类越来越依赖多轮对话AI进行高风险决策，需要有原则的框架来确保这种互动可靠地提高决策质量。我们采用以人为中心的视角，遵循两个原则：反事实伤害，确保AI不削弱人类的优势；互补性，确保AI在人的易错之处增加价值。我们通过用户定义的规则形式化这些概念，允许用户准确指定其特定任务中伤害和互补性的含义。然后，我们引入了一种在线的、无分布假设的算法，具有有限样本保证，强制执行用户指定的协作动态约束。我们在两个互动设置中评估我们的框架：在医疗诊断任务上的LLM模拟协作和在图像推理任务上的人类众包研究。我们展示了我们的在线程序在非平稳互动动态下仍能维持规定的反事实伤害和互补性违反率。此外，收紧或放宽这些约束会产生可预测的下游人类准确性变化，确认这两个原则作为引导多轮协作朝向更好决策质量的实用杠杆，而无需建模或约束人类行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for reliable frameworks in multi-round human-AI interactions, particularly for high-stakes decisions. The authors propose a human-centric approach based on two principles: counterfactual harm and complementarity, formalized through user-defined rules that specify the meaning of these concepts for specific tasks. Their online, distribution-free algorithm, which adheres to these user-specified constraints, was evaluated in two settings: a medical diagnostic task using LLM simulated collaboration and a human crowdsourcing study on pictorial reasoning. The findings demonstrate that the algorithm effectively maintains the desired rates of counterfactual harm and complementarity violations, and adjustments to these constraints lead to predictable changes in human accuracy, indicating that these principles can enhance decision quality in collaborative settings without needing to model human behavior.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要可靠的框架，以增强高风险情况下多轮对话AI的决策质量。作者提出了一种以人为中心的方法，基于两个原则：反事实伤害和互补性，通过用户定义的规则形式化这些概念，以便为特定任务指定其含义。他们的在线无分布算法遵循这些用户指定的约束，并在两个设置中进行了评估：医学诊断任务和图像推理任务。研究结果表明，该算法成功维持了所需的反事实伤害和互补性违反率，并且对这些约束的调整导致人类准确性发生可预测的变化，证明了这些原则在改善协作环境中的决策质量方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma</div>
<div class="meta-line">Authors: Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</div>
<div class="meta-line">First: 2026-02-24T18:11:23+00:00 · Latest: 2026-02-24T18:11:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21168v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -&gt; AKI -&gt; HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from &quot;what if this feature were different?&quot; to &quot;what if we had intervened earlier, and how would that propagate forward?&quot; --  yielding clinically actionable insights grounded in biological plausibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间临床数据的序列反事实推断：解决时间旅行者困境</div>
<div class="mono" style="margin-top:8px">反事实推断使临床医生能够提出关于患者结果的“如果”问题，但标准方法假设特征独立和同时可修改——这些假设在纵向临床数据中被违反。我们引入了序列反事实框架，该框架通过区分不可变特征（慢性诊断）和可控特征（实验室值）来尊重电子健康记录中的时间依赖性，并建模干预如何随时间传播。应用于2,723名COVID-19患者（383例长期COVID心力衰竭病例，2,340例匹配对照），我们证明在天真的方法下，38-67%的慢性病患者将需要生物学上不可能的反事实。我们识别出一个心肾级联（CKD -&gt; AKI -&gt; HF），每一步的相对风险分别为2.27和1.19，说明了时间传播，序列反事实能够捕捉到这一点，而天真的反事实则无法。我们的框架将反事实解释从“如果这个特征不同会怎样？”转变为“如果我们早些干预，会如何传播？”——从而产生基于生物学合理性的临床可操作见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve counterfactual inference in clinical settings, particularly for longitudinal data where standard methods fail due to assumptions of feature independence. The authors propose the Sequential Counterfactual Framework, which accounts for temporal dependencies in electronic health records by differentiating between immutable features and controllable features, and modeling the propagation of interventions over time. In their analysis of 2,723 COVID-19 patients, they found that naive methods would require biologically impossible counterfactuals for 38-67% of patients with chronic conditions, and identified a cardiorenal cascade with relative risks of 2.27 and 1.19, demonstrating that their sequential approach captures temporal propagation more effectively than naive methods, leading to clinically relevant insights about intervention timing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善临床环境中的反事实推断，特别是针对纵向数据，因为标准方法由于假设特征独立性和同时可修改性而失效。作者提出了顺序反事实框架，该框架通过区分不可变特征和可控特征，并建模干预在时间上的传播，来考虑电子健康记录中的时间依赖性。在对2723名COVID-19患者的应用中，他们发现38-67%的慢性病患者在天真的方法下需要生物学上不可能的反事实，并识别出具有相对风险2.27和1.19的心肾级联，证明他们的顺序反事实能够捕捉天真的方法无法捕捉的时间传播，从而提供更具临床可操作性的见解。</div>
</details>
</div>
<div class="card">
<div class="title">PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data</div>
<div class="meta-line">Authors: Samah Fodeh, Linhai Ma, Yan Wang, Srivani Talakokkul, Ganesh Puthiaraju, Afshan Khan, Ashley Hagaman, Sarah Lowe, Aimee Roundtree</div>
<div class="meta-line">First: 2026-02-24T18:10:00+00:00 · Latest: 2026-02-24T18:10:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PVminer：一种特定领域工具，用于检测患者生成数据中的患者声音</div>
<div class="mono" style="margin-top:8px">患者生成的文本，如安全消息、调查和访谈，包含丰富的患者声音（PV）表达，反映了沟通行为和健康的社会决定因素（SDoH）。传统的定性编码框架劳动密集，无法扩展到跨健康系统的大量患者撰写消息。现有的机器学习（ML）和自然语言处理（NLP）方法提供了部分解决方案，但通常将以患者为中心的沟通（PCC）和SDoH视为独立任务，或依赖于不适合患者语言的模型。我们介绍了PVminer，一种适应领域的NLP框架，用于构建安全的患者-提供者沟通中的患者声音。PVminer将PV检测公式化为多标签、多类别预测任务，整合了特定患者的BERT编码器（PV-BERT-base和PV-BERT-large）、用于主题增强的无监督主题建模（PV-Topic-BERT）和针对代码、子代码和组合级标签的微调分类器。在微调和推理过程中，主题表示被纳入以丰富语义输入。PVminer在层次任务中表现出色，超越生物医学和临床预训练基线，F1分数达到82.25%（代码）、80.14%（子代码）和最高77.87%（组合）。消融研究进一步表明，作者身份和基于主题的增强各自贡献了显著的提升。预训练模型、源代码和文档将公开发布，带注释的数据集可根据请求用于研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to effectively capture the patient voice in patient-generated text, which is often overlooked by traditional qualitative coding frameworks that are not scalable for large datasets. The authors developed PVminer, a domain-specific natural language processing framework that utilizes patient-specific BERT encoders and unsupervised topic modeling to structure patient voice in secure communications. The experimental results demonstrate that PVminer significantly outperforms existing biomedical and clinical models, achieving F1 scores of 82.25% for Code, 80.14% for Subcode, and 77.87% for Combo-level labels, with additional gains noted from incorporating author identity and topic-based augmentation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效捕捉患者生成数据中的患者声音，这通常被传统的定性编码框架所忽视，而这些框架无法扩展。作者开发了PVminer，这是一种适应特定领域的自然语言处理框架，将患者声音检测视为多标签、多类别预测任务，利用患者特定的BERT编码器和无监督主题建模。实验结果表明，PVminer在现有生物医学和临床预训练模型中表现优越，Code的F1得分为82.25%，Subcode为80.14%，Combo为77.87%，并且模型中引入作者身份和基于主题的增强也带来了额外的提升。</div>
</details>
</div>
<div class="card">
<div class="title">MoEMba: A Mamba-based Mixture of Experts for High-Density EMG-based Hand Gesture Recognition</div>
<div class="meta-line">Authors: Mehran Shabanpour, Kasra Rad, Sadaf Khademi, Arash Mohammadi</div>
<div class="meta-line">First: 2025-02-09T17:07:46+00:00 · Latest: 2026-02-24T18:08:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17457v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.17457v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-Density surface Electromyography (HDsEMG) has emerged as a pivotal resource for Human-Computer Interaction (HCI), offering direct insights into muscle activities and motion intentions. However, a significant challenge in practical implementations of HD-sEMG-based models is the low accuracy of inter-session and inter-subject classification. Variability between sessions can reach up to 40% due to the inherent temporal variability of HD-sEMG signals. Targeting this challenge, the paper introduces the MoEMba framework, a novel approach leveraging Selective StateSpace Models (SSMs) to enhance HD-sEMG-based gesture recognition. The MoEMba framework captures temporal dependencies and cross-channel interactions through channel attention techniques. Furthermore, wavelet feature modulation is integrated to capture multi-scale temporal and spatial relations, improving signal representation. Experimental results on the CapgMyo HD-sEMG dataset demonstrate that MoEMba achieves a balanced accuracy of 56.9%, outperforming its state-of-the-art counterparts. The proposed framework&#x27;s robustness to session-to-session variability and its efficient handling of high-dimensional multivariate time series data highlight its potential for advancing HD-sEMG-powered HCI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoEMba：基于Mamba的高密度EMG手势识别专家混合模型</div>
<div class="mono" style="margin-top:8px">高密度表面肌电图（HDsEMG）已成为人机交互（HCI）的关键资源，提供对肌肉活动和运动意图的直接洞察。然而，HD-sEMG模型在实际应用中的一个重大挑战是会话间和个体间分类的低准确性。由于HD-sEMG信号固有的时间变异性，会话间的变异性可达到40%。针对这一挑战，本文提出了MoEMba框架，这是一种利用选择性状态空间模型（SSMs）来增强HD-sEMG手势识别的新方法。MoEMba框架通过通道注意力技术捕捉时间依赖性和跨通道交互。此外，集成小波特征调制以捕捉多尺度的时间和空间关系，改善信号表示。在CapgMyo HD-sEMG数据集上的实验结果表明，MoEMba实现了56.9%的平衡准确率，优于其最先进的对手。所提框架对会话间变异性的鲁棒性及其高效处理高维多变量时间序列数据的能力突显了其在推动HD-sEMG驱动的HCI系统方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the low accuracy of inter-session and inter-subject classification in high-density surface electromyography (HDsEMG) for hand gesture recognition, which can vary significantly due to temporal variability. The authors propose the MoEMba framework, which utilizes Selective State-Space Models (SSMs) combined with channel attention techniques and wavelet feature modulation to enhance the recognition of gestures by capturing temporal dependencies and cross-channel interactions. Experimental results on the CapgMyo HD-sEMG dataset indicate that MoEMba achieves a balanced accuracy of 56.9%, surpassing existing state-of-the-art methods and demonstrating its robustness against session variability and effectiveness in managing high-dimensional multivariate time series data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决高密度表面肌电图（HDsEMG）在手势识别中的会话间和个体间分类准确性低的问题，这种准确性因时间变异性而显著波动。作者提出了MoEMba框架，该框架利用选择性状态空间模型（SSMs），结合通道注意力技术和小波特征调制，以改善HD-sEMG信号的表示。对CapgMyo HD-sEMG数据集的实验结果表明，MoEMba实现了56.9%的平衡准确率，超越了现有的最先进方法，并在应对会话变异性方面表现出稳健性，同时有效管理高维多变量时间序列数据。</div>
</details>
</div>
<div class="card">
<div class="title">Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions</div>
<div class="meta-line">Authors: Mame Diarra Toure, David A. Stephens</div>
<div class="meta-line">First: 2026-02-24T18:05:51+00:00 · Latest: 2026-02-24T18:05:51+00:00</div>
<div class="meta-line">Comments: 8 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21160v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model&#x27;s ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=σ_k^{2}/(2μ_k)$, with $μ_k{=}\mathbb{E}[p_k]$ and $σ_k^2{=}\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/μ_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\sum_k C_k \approx \mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\% over MI and 56.2\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of asymmetric failure costs in safety-critical classification, where traditional Bayesian deep learning methods summarize epistemic uncertainty with a single scalar, mutual information (MI), which fails to differentiate between benign and critical classes. The authors propose a decomposition of MI into a per-class vector that accounts for class-specific contributions, derived from a second-order Taylor expansion of entropy, allowing for better comparison across classes. Experimental validation on three tasks demonstrates that this approach significantly reduces selective risk in diabetic retinopathy prediction, enhances out-of-distribution detection performance, and shows improved robustness to label noise, indicating that the method&#x27;s effectiveness is influenced by both the quality of posterior approximation and the uncertainty propagation through the network.</div>
<div class="mono" style="margin-top:8px">该研究解决了贝叶斯深度学习在安全关键分类中的局限性，因为不同类别的失败成本差异显著。作者提出了一种将互信息分解为每类向量的方法，从而更细致地理解认知不确定性。实验结果表明，该方法在糖尿病视网膜病变的选择性预测中将选择性风险降低了34.7%，在分布外检测中实现了最高的AUROC，并在控制研究中对标签噪声的敏感性降低，表明不确定性在网络中的传播对有效测量至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</div>
<div class="meta-line">Authors: Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray, Hua Wei</div>
<div class="meta-line">First: 2026-02-24T18:04:54+00:00 · Latest: 2026-02-24T18:04:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21158v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SELAUR：通过不确定性感知奖励的自我进化LLM代理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用作多步骤决策代理，其中有效的奖励设计对于指导学习至关重要。尽管最近的研究探索了各种形式的奖励塑造和步骤级信用分配，但一个关键信号仍然被大多数人忽视：LLMs的内在不确定性。不确定性反映了模型的信心，揭示了需要探索的地方，并在失败的轨迹中提供有价值的学习线索。我们介绍了SELAUR：通过不确定性感知奖励的自我进化LLM代理，这是一个将不确定性直接纳入奖励设计的强化学习框架。SELAUR将基于熵、最小置信度和边际的度量整合为一个组合的令牌级不确定性估计，提供密集的信心对齐监督，并采用一种关注失败的奖励重塑机制，将这些不确定性信号注入到步骤和轨迹级奖励中，以提高探索效率和学习稳定性。在两个基准测试ALFWorld和WebShop上的实验表明，我们的方法在强基线之上始终提高了成功率。消融研究进一步证明了不确定性信号如何增强探索和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of reward design in large language models (LLMs) used as multi-step decision-making agents by incorporating intrinsic uncertainty signals. The authors propose SELAUR, a reinforcement learning framework that integrates various uncertainty metrics into a combined token-level uncertainty estimate, which is then used to reshape rewards at both step and trajectory levels. Experimental results on the ALFWorld and WebShop benchmarks indicate that SELAUR consistently outperforms strong baselines in success rates, with ablation studies highlighting the role of uncertainty signals in improving exploration efficiency and learning stability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过引入内在不确定性信号来改善大型语言模型（LLMs）作为多步骤决策代理的奖励设计，从而提高其性能。作者提出了SELAUR，这是一种强化学习框架，将各种不确定性指标整合为一个组合的令牌级不确定性估计，并利用该估计在步骤和轨迹级别重塑奖励。实验结果表明，在ALFWorld和WebShop基准上，SELAUR的成功率显著高于强基线，消融研究进一步表明，不确定性信号的引入增强了探索效率和学习稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</div>
<div class="meta-line">Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</div>
<div class="meta-line">First: 2026-02-23T18:59:27+00:00 · Latest: 2026-02-24T18:03:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20156v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20156v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skill-Inject：测量智能体对技能文件攻击的脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）智能体正在迅速发展，得益于代码执行、工具和最近引入的智能体技能功能。技能允许用户通过专业的第三方代码、知识和指令扩展LLM应用。尽管这可以将智能体能力扩展到新领域，但也创造了一个日益复杂的智能体供应链，为提示注入攻击提供了新的表面。我们将基于技能的提示注入识别为一个重大威胁，并引入SkillInject，一个评估广泛使用的LLM智能体对技能文件注入的易受攻击性的基准。SkillInject包含202个注入任务对，攻击范围从明显恶意的注入到隐藏在其他合法指令中的微妙、依赖上下文的攻击。我们在SkillInject上评估前沿LLM，测量其在避免有害指令方面的安全性和在遵守合法指令方面的效用。我们的结果表明，今天的智能体高度脆弱，前沿模型的攻击成功率高达80%，经常执行极具危害性的指令，包括数据外泄、破坏性行为和类似勒索软件的行为。此外，它们还表明，这个问题不会通过模型扩展或简单的输入过滤来解决，而是需要上下文感知的授权框架来实现稳健的智能体安全。我们的基准可在https://www.skill-inject.com/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the increasing complexity of LLM agents due to the introduction of agent skills, which can lead to new vulnerabilities from prompt injection attacks. The authors developed SkillInject, a benchmark designed to assess the vulnerability of popular LLM agents to skill-based prompt injections, comprising 202 task pairs that range from overtly malicious to subtle attacks. Experimental results reveal that current LLM agents exhibit significant vulnerabilities, with attack success rates reaching up to 80%, often resulting in harmful actions such as data exfiltration and destructive behavior, indicating that enhancing agent security will require more than just scaling models or filtering inputs, but rather the implementation of context-aware authorization frameworks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于应对技能基础的提示注入攻击对LLM代理的威胁，这种威胁因引入允许集成第三方代码和指令的代理技能而变得更加复杂。作者开发了SkillInject，一个基准测试，旨在评估流行LLM代理对这些类型攻击的脆弱性，包含202个注入任务对，从明显恶意到微妙的上下文依赖注入不等。使用SkillInject对领先的LLM进行评估显示出高度脆弱，攻击成功率高达80%，导致执行数据外泄和破坏性行为等有害指令，表明提高代理安全性需要的不仅仅是模型扩展或输入过滤，而是实施上下文感知的授权框架。</div>
</details>
</div>
<div class="card">
<div class="title">From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</div>
<div class="meta-line">Authors: Dawei Li, Abdullah Alnaibari, Arslan Bisharat, Manny Sandoval, Deborah Hall, Yasin Silva, Huan Liu</div>
<div class="meta-line">First: 2025-12-02T18:31:18+00:00 · Latest: 2026-02-24T18:01:52+00:00</div>
<div class="meta-line">Comments: Accepted by PAKDD 2026 special session on Data Science: Foundations and Applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03005v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.03005v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从调解到中介：大型语言模型能否在在线争论中充当中介？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速发展为积极应用的人工智能开辟了新可能性。随着LLMs越来越多地调解在线沟通，它们在促进同理心和建设性对话方面的潜力成为负责任的人工智能研究的重要前沿。本研究探讨LLMs是否不仅能作为检测有害内容的调解者，还能作为理解和缓解在线冲突的中介。我们的框架将调解分解为两个子任务：判断，LLM评估对话的公平性和情感动态；引导，LLM生成同理心和缓解冲突的信息，引导参与者走向解决方案。为了评估调解质量，我们构建了一个基于Reddit的大型数据集，并提出了一个多阶段评估流程，结合基于原则的评分、用户模拟和人工比较。实验表明，在进行调解时，基于API的模型在推理和干预一致性方面优于开源模型。我们的发现突显了当前LLMs作为新兴在线社会调解者的潜力和局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the potential of large language models (LLMs) to act as mediators in online conflicts, moving beyond their traditional role as moderators. The study develops a framework that breaks down mediation into two tasks: judgment, where LLMs assess conversation fairness and emotional dynamics, and steering, where they create empathetic messages to facilitate resolution. Experimental results demonstrate that API-based models significantly outperform open-source models in both reasoning and intervention alignment, indicating the promise and limitations of LLMs in online social mediation.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在在线冲突中作为调解者的潜力，超越了其传统的监督者角色。该研究采用一个框架，将调解分解为两个任务：判断，即LLMs评估对话的公平性和情感动态；引导，即生成同理心的信息以促进解决。实验结果表明，基于API的模型在推理和干预一致性方面显著优于开源模型，突显了LLMs在在线社会调解中的能力和局限性。</div>
</details>
</div>
<div class="card">
<div class="title">CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning</div>
<div class="meta-line">Authors: Ziwei Niu, Hao Sun, Shujun Bian, Xihong Yang, Lanfen Lin, Yuxin Liu, Yueming Jin</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-24T17:59:21+00:00 · Latest: 2026-02-24T17:59:21+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21154v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21154v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CG-DMER：用于解耦多模态ECG表示学习的混合对比生成框架</div>
<div class="mono" style="margin-top:8px">准确解读心电图（ECG）信号对诊断心血管疾病至关重要。最近将ECG与临床报告结合的多模态方法显示出强大的潜力，但从模态角度来看仍面临两个主要问题：（1）同模态：现有模型以无导联的方式处理ECG，忽视了导联间的时空依赖性，限制了其在建模细粒度诊断模式中的有效性；（2）跨模态：现有方法直接将ECG信号与临床报告对齐，由于报告的自由文本特性，引入了模态特定的偏差。针对这两个问题，我们提出了CG-DMER，一种用于解耦多模态ECG表示学习的对比生成框架，依赖于两个关键设计：（1）空间-时间掩蔽建模旨在通过在空间和时间维度上应用掩蔽并重建缺失信息，更好地捕捉细粒度的时间动态和导联间的空间依赖性；（2）表示解耦和对齐策略旨在通过引入模态特定和模态共享的编码器，减轻不必要的噪声和模态特定的偏差，确保模态不变和模态特定表示之间的更清晰分离。在三个公共数据集上的实验表明，CG-DMER在多样的下游任务中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the interpretation of electrocardiogram (ECG) signals for better diagnosis of cardiovascular diseases, addressing limitations in current multimodal approaches that fail to adequately consider spatial-temporal dependencies and modality-specific biases. The authors propose CG-DMER, a hybrid contrastive-generative framework that employs spatial-temporal masked modeling to enhance the capture of fine-grained temporal dynamics and inter-lead spatial dependencies, along with a representation disentanglement and alignment strategy to reduce noise and biases. Experimental results on three public datasets indicate that CG-DMER achieves state-of-the-art performance in various downstream tasks, demonstrating its effectiveness in multimodal ECG representation learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善心电图（ECG）信号的解读，以便更好地诊断心血管疾病，解决当前将ECG与临床报告结合的多模态方法中的局限性。作者提出了CG-DMER，这是一种混合对比生成框架，采用空间-时间掩蔽建模来捕捉细粒度的时间动态和导联间的空间依赖关系，并结合表示解耦和对齐策略以减少模态特定的偏差。在三个公共数据集上的实验结果表明，CG-DMER在各种下游任务中实现了最先进的性能，证明了其在增强ECG表示学习方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Very Big Video Reasoning Suite</div>
<div class="meta-line">Authors: Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, Thaddäus Wiedemer, Qingying Gao, Dezhi Luo, Yaoyao Qian, Lianyu Huang, Zelong Hong, Jiahui Ge, Qianli Ma, Hang He, Yifan Zhou, Lingzi Guo, Lantao Mei, Jiachen Li, Hanwen Xing, Tianqi Zhao, Fengyuan Yu, Weihang Xiao, Yizheng Jiao, Jianheng Hou, Danyang Zhang, Pengcheng Xu, Boyang Zhong, Zehong Zhao, Gaoyun Fang, John Kitaoka, Yile Xu, Hua Xu, Kenton Blacutt, Tin Nguyen, Siyuan Song, Haoran Sun, Shaoyue Wen, Linyang He, Runming Wang, Yanzhi Wang, Mengyue Yang, Ziqiao Ma, Raphaël Millière, Freda Shi, Nuno Vasconcelos, Daniel Khashabi, Alan Yuille, Yilun Du, Ziming Liu, Bo Li, Dahua Lin, Ziwei Liu, Vikash Kumar, Yijiang Li, Lei Yang, Zhongang Cai, Hokin Deng</div>
<div class="meta-line">First: 2026-02-23T18:59:41+00:00 · Latest: 2026-02-24T17:59:15+00:00</div>
<div class="meta-line">Comments: Homepage: https://video-reason.com/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20159v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20159v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个非常大的视频推理套件</div>
<div class="mono" style="margin-top:8px">视频模型的快速进展主要集中在视觉质量上，推理能力尚未得到充分探索。视频推理将智能建立在时空一致的视觉环境中，这超出了文本自然捕捉的范围，使得对时空结构（如连续性、互动和因果关系）进行直观推理成为可能。然而，系统研究视频推理及其扩展行为受到缺乏大规模训练数据的限制。为了解决这一问题，我们引入了非常大的视频推理（VBVR）数据集，这是一个前所未有的大规模资源，涵盖200个经过精心策划的推理任务，遵循原则性分类法，并包含超过一百万个视频片段，规模大约是现有数据集的三个数量级。我们进一步提出了VBVR-Bench，这是一个可验证的评估框架，通过结合基于规则的、与人类对齐的评分者，超越了基于模型的评判，能够实现视频推理能力的可重复和可解释的诊断。利用VBVR套件，我们进行了一项大规模视频推理扩展研究的初步探索，并观察到对未见推理任务的早期泛化迹象。VBVR为可推广视频推理研究的下一个阶段奠定了基础。数据、基准工具包和模型可在https://video-reason.com/上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of video models, which have primarily focused on visual quality. To address the lack of large-scale training data for video reasoning, the authors introduce the Very Big Video Reasoning (VBVR) Dataset, which includes over one million video clips and 200 curated reasoning tasks. The study employs the VBVR-Bench evaluation framework to assess video reasoning capabilities through rule-based and human-aligned scoring methods. The findings reveal early signs of emergent generalization to unseen reasoning tasks, indicating the potential for significant advancements in generalizable video reasoning research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强视频模型的推理能力，而这些模型主要集中于视觉质量。为了填补视频推理的大规模训练数据的缺乏，作者引入了非常大的视频推理（VBVR）数据集，该数据集包含200个精心策划的推理任务和超过一百万个视频片段，规模远超现有数据集。该研究采用VBVR-Bench评估框架来评估视频推理能力，并揭示了对未见推理任务的早期泛化迹象，从而为未来可推广视频推理的研究奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">SPRITETOMESH: Automatic Mesh Generation for 2D Skeletal Animation Using Learned Segmentation and Contour-Aware Vertex Placement</div>
<div class="meta-line">Authors: Bastien Gimbert</div>
<div class="meta-line">First: 2026-02-24T17:58:31+00:00 · Latest: 2026-02-24T17:58:31+00:00</div>
<div class="meta-line">Comments: 11 pages, 17 figures. Code available at https://github.com/BastienGimbert/SpriteToMesh</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21153v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21153v1">PDF</a> · <a href="https://github.com/BastienGimbert/SpriteToMesh">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SPRITETOMESH, a fully automatic pipeline for converting 2D game sprite images into triangle meshes compatible with skeletal animation frameworks such as Spine2D. Creating animation-ready meshes is traditionally a tedious manual process requiring artists to carefully place vertices along visual boundaries, a task that typically takes 15-60 minutes per sprite. Our method addresses this through a hybrid learned-algorithmic approach. A segmentation network (EfficientNet-B0 encoder with U-Net decoder) trained on over 100,000 sprite-mask pairs from 172 games achieves an IoU of 0.87, providing accurate binary masks from arbitrary input images. From these masks, we extract exterior contour vertices using Douglas-Peucker simplification with adaptive arc subdivision, and interior vertices along visual boundaries detected via bilateral-filtered multi-channel Canny edge detection with contour-following placement. Delaunay triangulation with mask-based centroid filtering produces the final mesh. Through controlled experiments, we demonstrate that direct vertex position prediction via neural network heatmap regression is fundamentally not viable for this task: the heatmap decoder consistently fails to converge (loss plateau at 0.061) while the segmentation decoder trains normally under identical conditions. We attribute this to the inherently artistic nature of vertex placement - the same sprite can be meshed validly in many different ways. This negative result validates our hybrid design: learned segmentation where ground truth is unambiguous, algorithmic placement where domain heuristics are appropriate. The complete pipeline processes a sprite in under 3 seconds, representing a speedup of 300x-1200x over manual creation. We release our trained model to the game development community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPRITETOMESH：基于学习分割和轮廓感知顶点放置的2D骨骼动画自动网格生成</div>
<div class="mono" style="margin-top:8px">我们提出了SPRITETOMESH，一个完全自动化的管道，用于将2D游戏精灵图像转换为与骨骼动画框架（如Spine2D）兼容的三角网格。创建动画准备好的网格传统上是一个繁琐的手动过程，要求艺术家仔细沿视觉边界放置顶点，这一任务通常需要每个精灵15-60分钟。我们的方法通过混合学习-算法方法解决了这个问题。一个在172个游戏中训练的分割网络（EfficientNet-B0编码器与U-Net解码器）基于超过100,000个精灵-掩码对实现了0.87的IoU，提供了来自任意输入图像的准确二进制掩码。通过这些掩码，我们使用Douglas-Peucker简化和自适应弧细分提取外部轮廓顶点，并通过双边滤波的多通道Canny边缘检测与轮廓跟随放置检测视觉边界上的内部顶点。基于掩码的重心过滤的Delaunay三角剖分生成最终网格。通过控制实验，我们证明了通过神经网络热图回归直接预测顶点位置在此任务中根本不可行：热图解码器在相同条件下始终无法收敛（损失停滞在0.061），而分割解码器正常训练。我们将此归因于顶点放置的固有艺术性——同一精灵可以以多种不同方式有效地网格化。这个负面结果验证了我们的混合设计：在真实值不明确的情况下使用学习分割，在领域启发式适用的情况下使用算法放置。完整的管道在3秒内处理一个精灵，代表了比手动创建快300x-1200x的速度提升。我们将训练好的模型发布给游戏开发社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to automate the labor-intensive process of converting 2D game sprite images into triangle meshes for skeletal animation, which typically requires significant manual effort from artists. The authors developed SPRITETOMESH, a fully automatic pipeline that employs a hybrid learned-algorithmic approach, utilizing a segmentation network trained on over 100,000 sprite-mask pairs to achieve an IoU of 0.87 for accurate binary mask generation. Key experimental findings reveal that while direct vertex position prediction via neural network heatmap regression is ineffective, the combination of learned segmentation and algorithmic vertex placement significantly enhances efficiency, allowing the pipeline to process sprites in under 3 seconds, achieving a speedup of 300x-1200x compared to manual methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是自动化将2D游戏精灵图像转换为适合骨骼动画的三角网格的劳动密集型过程，这一过程传统上需要艺术家投入大量的手工工作。作者开发了SPRITETOMESH，一个完全自动化的管道，采用混合的学习-算法方法，利用在超过100,000个精灵-掩码对上训练的分割网络，实现了0.87的IoU以生成准确的二进制掩码。关键实验结果表明，尽管通过神经网络热图回归直接预测顶点位置是无效的，但学习分割与算法顶点放置的结合显著提高了效率，使得该管道能够在3秒内处理精灵，相比手动方法实现了300到1200倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">Complexity-aware fine-tuning</div>
<div class="meta-line">Authors: Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev</div>
<div class="meta-line">First: 2025-06-26T13:13:24+00:00 · Latest: 2026-02-24T17:50:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21220v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.21220v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across three small open models ($\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.58$ vs $0.45$ average accuracy) and outperforms the distillation approach ($0.58$ vs $0.56$ average accuracy) while using $81\%$ less data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复杂性意识的微调</div>
<div class="mono" style="margin-top:8px">通用大型语言模型（LLMs）通常通过监督微调（SFT）来增强在特定领域的性能。通过提炼更大模型的思维链，可以在付出大量昂贵调用和更多数据的代价下获得更好的结果。我们提出了一种高效微调的新蓝图，仅对通过熵识别的复杂数据使用推理。具体而言，在三个小型开放模型（约3B）中，我们通过单个标记答案熵（ROC AUC 0.73）将训练数据分为复杂性类别，通过SFT和蒸馏微调大型语言模型（LLMs），并显示我们的管道显著优于标准SFT方法（0.58 vs 0.45平均准确率），并且优于蒸馏方法（0.58 vs 0.56平均准确率），同时使用了81%的数据更少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the fine-tuning process of general-purpose Large Language Models (LLMs) for specific domains while minimizing data usage. The authors propose a novel method that categorizes training data based on complexity, determined by a single token answer entropy, and employs supervised fine-tuning (SFT) and distillation selectively for complex data. Experimental results demonstrate that this approach achieves an average accuracy of 0.58, significantly surpassing the standard SFT method (0.45) and also outperforming the distillation method (0.56), while utilizing 81% less data.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善通用大型语言模型（LLMs）在特定领域的微调过程，同时最小化数据使用。作者提出了一种方法，通过单个令牌答案的熵来对训练数据进行复杂性分类，然后仅对复杂数据使用监督微调（SFT）和蒸馏来微调模型。实验结果表明，该方法的平均准确率为0.58，显著超过标准SFT方法的0.45和蒸馏方法的0.56，同时使用的数据减少了81%。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling State-Space Models on Multiple GPUs with Tensor Parallelism</div>
<div class="meta-line">Authors: Anurag Dutt, Nimit Shah, Hazem Masarani, Anshul Gandhi</div>
<div class="meta-line">First: 2026-02-24T17:47:54+00:00 · Latest: 2026-02-24T17:47:54+00:00</div>
<div class="meta-line">Comments: Submitted to 46th IEEE International Conference on Distributed Computing Systems (ICDCS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.
  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer&#x27;s packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用张量并行性在多个GPU上扩展状态空间模型</div>
<div class="mono" style="margin-top:8px">选择性状态空间模型（SSMs）迅速成为大型语言模型的一个重要支柱，特别是在长上下文工作负载中。然而，在部署中，它们的推理性能通常受到单个GPU的内存容量、带宽和延迟限制的制约，使得多GPU执行变得越来越必要。尽管张量并行性（TP）被广泛用于扩展Transformer推理，但将其应用于选择性SSM块并非易事，因为SSM混合器将大投影与序列级递归状态更新和局部混合耦合在一起，其效率依赖于保持局部性并避免在关键路径中的同步。
  本文提出了一种用于选择性SSM推理的通信高效TP设计，解决了三个实际工程挑战：通过SSM状态缓存在预填充和解码之间实现TTFT改进，划分混合器的打包参数张量，以便递归更新保持局部同时最小化通信，以及通过量化AllReduce减少TP聚合开销。我们在NVIDIA A6000和A100集群上评估了三种代表性的基于SSM的LLM，涵盖纯SSM和混合架构 - Mamba、Falcon-Mamba和Zamba。我们的实验表明，张量并行SSM推理带来了显著的吞吐量提升，在2个GPU上提高了约1.6-2.1倍，在4个GPU上提高了约2.6-4.0倍，Mamba在长上下文长度时获得了最大的收益，并通过降低同步带宽开销实现了量化全规约带来的进一步约10-18%的吞吐量提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the inference performance of selective state space models (SSMs) for large language models, particularly in long-context scenarios, by overcoming the limitations of single GPU execution. The authors propose a communication-efficient tensor parallelism (TP) design that addresses key challenges such as enabling improvements through an SSM state cache, optimizing the mixer&#x27;s parameter tensor for local recurrent updates, and minimizing aggregation overhead with quantized AllReduce. Experimental results demonstrate significant throughput enhancements for SSM inference, achieving approximately 1.6-2.1 times improvement on 2 GPUs and 2.6-4.0 times on 4 GPUs for the Mamba model, with the most substantial gains observed at longer context lengths and an additional 10-18% improvement from quantized AllReduce.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过克服单个GPU的内存、带宽和延迟限制，提升选择性状态空间模型（SSMs）在长上下文场景中的推理性能。作者提出了一种通信高效的张量并行（TP）设计，解决了多个挑战，包括通过SSM状态缓存实现TTFT改进、优化混合器的参数张量以实现局部递归更新，以及通过量化AllReduce最小化TP聚合开销。实验结果表明，SSM推理的吞吐量显著提高，在2个GPU上实现了1.6-2.1倍的提升，在4个GPU上实现了2.6-4.0倍的提升，尤其在较长上下文长度下获得了最大的收益，并且通过量化All-Reduce进一步提高了10-18%的吞吐量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260225_0400.html">20260225_0400</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0335.html">20260223_0335</a>
<a href="archive/20260222_0334.html">20260222_0334</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0348.html">20260220_0348</a>
<a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
