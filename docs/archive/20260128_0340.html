<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 03:40</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0340</div>
    <div class="row"><div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权在线探索学习在困难问题上的推理</div>
<div class="mono" style="margin-top:8px">强化学习（RL）提高了大型语言模型（LLMs）的推理能力，但最先进的方法在许多训练问题上仍然无法学习。在困难问题上，在线RL几乎从不探索单个正确的回合，导致零奖励和没有学习信号来推动改进。我们发现，经典RL中解决这一探索问题的自然解决方案，如熵奖励、对重要性比率的更宽松裁剪或直接优化pass@k目标，并未解决此问题，且往往使优化不稳定而不改善可解性。一个自然的替代方案是利用来自更简单问题的迁移。然而，我们表明，在RL训练期间混合简单和困难问题是适得其反的，因为光线干扰，优化集中在已经可解的问题上，从而积极抑制对更困难问题的进展。为了解决这一挑战，我们引入了特权在线探索（POPE），这是一种利用人类或其他神谕解决方案作为特权信息来指导困难问题探索的方法，与将神谕解决方案用作训练目标的方法（例如，离线RL方法或从SFT热启动）不同。POPE通过神谕解决方案的前缀增强困难问题，使RL在引导回合中获得非零奖励。至关重要的是，所产生的行为通过遵循指令和推理之间的协同作用转移回原始的无引导问题。实证表明，POPE扩展了可解问题的集合，并显著提高了在具有挑战性的推理基准上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of reinforcement learning (RL) in large language models (LLMs), particularly in addressing the limitations faced on hard problems where traditional methods fail to yield learning signals. The authors propose a novel method called Privileged On-Policy Exploration (POPE), which utilizes human or oracle solutions to guide exploration in RL, rather than using these solutions as mere training targets. Experimental results demonstrate that POPE significantly increases the range of solvable problems and improves performance on challenging reasoning benchmarks by allowing RL to achieve non-zero rewards during guided rollouts, thereby facilitating better transfer of learned behaviors to unguided problems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高强化学习（RL）在大型语言模型（LLMs）中的推理能力，特别是在传统方法难以有效学习的挑战性问题上。作者提出了一种新方法，称为特权在线探索（POPE），该方法利用人类或oracle解决方案作为特权信息，以促进在困难问题场景中的探索。实验结果表明，POPE显著扩大了可解决问题的范围，并提高了在困难推理基准上的表现，通过允许RL在引导回合中获得非零奖励，从而促进了学习行为向原始无引导问题的有益转移。</div>
</details>
</div>
<div class="card">
<div class="title">Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T18:59:27+00:00 · Latest: 2026-01-26T17:06:02+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16912v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16912v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索与利用：通过剪切、熵和虚假奖励重新思考可验证奖励的强化学习</div>
<div class="mono" style="margin-top:8px">本文探讨了可验证奖励强化学习（RLVR）中的探索与利用权衡，这是一个旨在改善大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制在LLM中引发强大的数学推理：虚假奖励通过奖励与真实情况无关的结果来抑制利用，而熵最小化则通过推动模型朝向更自信和确定的输出抑制探索，突显出一个令人困惑的动态：同时抑制利用和抑制探索都能提高推理性能，但调和这些效应的基本原理仍然不清楚。我们关注两个基本问题：（i）策略熵与性能的关系，以及（ii）虚假奖励是否能带来收益，可能通过剪切偏差和模型污染的相互作用。我们的结果表明，在虚假奖励下，剪切偏差降低了策略熵，导致更自信和确定的输出，而单靠熵最小化不足以改善。我们进一步提出了一个奖励失调模型，解释了为什么虚假奖励可以在污染环境中提升性能。我们的发现阐明了虚假奖励益处背后的机制，并为更有效的RLVR训练提供了原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR) to enhance the reasoning capabilities of Large Language Models (LLMs). The authors analyze how policy entropy affects performance and the role of spurious rewards, which can paradoxically improve reasoning by reducing policy entropy through clipping bias. The key findings indicate that while entropy minimization alone does not enhance performance, the presence of spurious rewards leads to more confident outputs, suggesting a complex interplay that can be explained by a proposed reward-misalignment model.</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励的强化学习（RLVR）中的探索与利用权衡，以增强大型语言模型（LLMs）的推理能力。作者分析了虚假奖励和熵最小化对模型性能的影响，重点关注策略熵如何影响结果以及虚假奖励的潜在好处。研究结果表明，与虚假奖励相关的剪切偏差降低了策略熵，导致输出更为自信，而仅靠熵最小化并未带来性能提升。此外，提出了一种奖励失调模型，以解释虚假奖励如何在污染环境中增强性能，从而阐明虚假奖励在RLVR训练中优势的机制。</div>
</details>
</div>
<div class="card">
<div class="title">MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</div>
<div class="meta-line">Authors: Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-09T08:07:19+00:00 · Latest: 2026-01-26T16:58:19+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07915v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07915v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \textit{retrieve-then-compress} strategy using a \textbf{Visual Memory Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame&#x27;s tokens -- reducing visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by \textbf{23.9\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARC：基于记忆增强的强化学习令牌压缩用于高效视频理解</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速进展为多模态模型奠定了基础。然而，视觉语言模型（VLMs）在从图像扩展到视频时仍面临高帧率和长时长带来的高计算成本。令牌压缩是一种有前景的解决方案，但大多数现有的无训练方法会导致信息丢失和性能下降。为此，我们提出了\textbf{基于记忆增强的强化学习令牌压缩（MARC）}，它结合了结构化检索和基于强化学习的蒸馏。MARC采用\textit{先检索后压缩}策略，使用\textbf{视觉记忆检索器（VMR）}选择关键片段，并通过\textbf{压缩组相对策略优化（C-GRPO）}框架将推理能力从教师模型蒸馏到学生模型。在六个视频基准上的实验表明，MARC仅使用一帧的令牌就能达到接近基线的准确率——将视觉令牌减少了\textbf{95\%}，GPU内存减少了\textbf{72\%}，延迟减少了\textbf{23.9\%}。这证明了其在资源受限环境下（如视频问答、监控和自动驾驶）实现高效实时视频理解的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computational costs associated with extending visual language models to video understanding, particularly due to the challenges posed by high frame rates and long durations. The authors propose a novel method called Memory-Augmented Reinforcement Learning-based Token Compression (MARC), which utilizes a retrieve-then-compress strategy that combines structured retrieval with reinforcement learning-based distillation. Experimental results on six video benchmarks indicate that MARC can achieve near-baseline accuracy while significantly reducing visual tokens by 95%, GPU memory usage by 72%, and latency by 23.9%, highlighting its effectiveness for efficient video understanding in resource-constrained environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉语言模型（VLMs）在处理视频时面临的高计算成本，这在高帧率和长时长的情况下尤为明显。作者提出了一种新方法，称为基于记忆增强的强化学习令牌压缩（MARC），该方法采用检索后压缩策略，利用视觉记忆检索器（VMR）选择关键片段，并通过压缩组相对策略优化（C-GRPO）框架将知识从教师模型蒸馏到学生模型。六个视频基准实验的结果表明，MARC在仅使用一帧的令牌的情况下，能够实现接近基线的准确率，同时将视觉令牌减少95%、GPU内存使用减少72%和延迟减少23.9%，显示出其在资源有限环境中进行高效视频理解的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP&#x27;s Visual Embedding Projector is a Few-shot Cornucopia</div>
<div class="meta-line">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-10-07T17:59:59+00:00 · Latest: 2026-01-26T14:50:34+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05270v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.05270v4">PDF</a> · <a href="https://github.com/astra-vision/ProLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation&#x27;&#x27; and ``validation-free&#x27;&#x27; settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP&#x27;s lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP的视觉嵌入投影器是一个少样本的丰饶之地</div>
<div class="mono" style="margin-top:8px">我们介绍了ProLIP，这是一种简单且与架构无关的方法，用于将对比预训练的视觉-语言模型（如CLIP）适应于少样本分类。ProLIP通过对预训练权重的偏差进行Frobenius范数正则化，微调视觉编码器的投影矩阵。它在11个少样本分类基准测试中，在“少样本验证”和“无验证”设置下均实现了最先进的性能。此外，通过ProLIP的视角重新思考非线性CLIP-Adapter，我们设计了一个正则化线性适配器（RLA），其性能更佳，无需超参数调优，对学习率值的敏感性较低，并在模型权重不可访问的黑箱场景中提供了ProLIP的替代方案。除了少样本分类，ProLIP在跨数据集迁移、领域泛化、基础到新类泛化和测试时适应方面表现出色——在训练速度上比提示调优快一个数量级。代码可在https://github.com/astra-vision/ProLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve few-shot classification performance in contrastively pretrained vision-language models like CLIP. The authors introduce ProLIP, a method that fine-tunes the projection matrix of the vision encoder using Frobenius norm regularization to align it with pretrained weights. Experimental results demonstrate that ProLIP achieves state-of-the-art performance across 11 few-shot classification benchmarks and excels in various tasks such as cross-dataset transfer and domain generalization, while also offering a Regularized Linear Adapter as a more efficient alternative.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高对比预训练视觉-语言模型（如CLIP）在少样本分类中的表现。作者提出了ProLIP，这是一种通过Frobenius范数正则化微调视觉编码器投影矩阵的方法，以使其与预训练权重对齐。实验结果表明，ProLIP在11个少样本分类基准测试中实现了最先进的性能，并在跨数据集迁移和领域泛化等多种任务中表现出色，同时还提供了一种正则化线性适配器，该适配器在不需要超参数调整的情况下表现更好。</div>
</details>
</div>
<div class="card">
<div class="title">Closing the Modality Gap Aligns Group-Wise Semantics</div>
<div class="meta-line">Authors: Eleonora Grassucci, Giordano Cicchetti, Emanuele Frasca, Aurelio Uncini, Danilo Comminiello</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-26T14:36:04+00:00 · Latest: 2026-01-26T14:36:04+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>闭合模态差距对齐组级语义</div>
<div class="mono" style="margin-top:8px">在多模态学习中，CLIP被认为是学习跨多个模态共享潜在空间的\textit{事实上的}方法，将相似的表示靠近并将其与不相似的表示远离。尽管基于CLIP的损失有效地在语义层面对齐模态，但结果潜在空间往往仅部分共享，揭示了一种称为模态差距的结构不匹配现象。尽管解决这一现象的必要性仍存在争议，特别是考虑到其对实例级任务（例如检索）的有限影响，但我们证明其在组级任务（例如聚类）中的影响却非常显著。为了支持这一主张，我们提出了一种新方法，旨在在双模态设置中持续减少这种差异，并可以简单扩展到一般的$n$模态情况。通过我们的广泛评估，我们展示了我们的新见解：虽然减少差距在传统的实例级任务中仅提供边际或不一致的改进，但它显著增强了组级任务。这些发现可能会重塑我们对模态差距的理解，突显其在提高需要语义分组的任务性能中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the modality gap in multimodal learning, specifically focusing on the limitations of CLIP in aligning latent spaces across different modalities. The authors propose a novel method aimed at reducing this gap, particularly in two-modal settings, with potential extensions to n-modal cases. Experimental results indicate that while the reduction of the modality gap yields minimal improvements in instance-wise tasks, it significantly enhances performance in group-level tasks such as clustering, suggesting that the modality gap plays a crucial role in semantic grouping tasks.</div>
<div class="mono" style="margin-top:8px">本研究关注多模态学习中的模态差距，特别是在CLIP的背景下，该方法以对齐不同模态的表示而闻名。作者提出了一种新方法，旨在减少这一差距，他们认为这一差距对群体级任务（如聚类）有显著影响，而对实例级任务（如检索）的影响有限。通过广泛的评估，他们发现虽然减少模态差距在实例级任务中仅带来微小的改善，但在群体级任务中却显著提升了性能，这表明模态差距在语义分组性能中起着关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation</div>
<div class="meta-line">Authors: Luca Cazzola, Ahed Alboody</div>
<div class="meta-line">First: 2025-12-12T15:32:28+00:00 · Latest: 2026-01-26T11:40:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11654v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11654v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lucazzola.github.io/publications/kinemic">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR&#x27;s requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at https://lucazzola.github.io/publications/kinemic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文中的动量挖掘：通过文本到运动蒸馏的少样本动作合成</div>
<div class="mono" style="margin-top:8px">大型标注运动数据集的获取成本仍然是基于骨架的人类活动识别（HAR）的一个关键瓶颈。尽管文本到运动（T2M）生成模型提供了一个引人注目的、可扩展的合成数据源，但其训练目标强调一般艺术运动，且数据集结构与HAR对运动学精确、类别区分动作的要求根本不同。这种差异造成了显著的领域差距，使得通用的T2M模型不适合生成适合HAR分类器的动作。为了解决这一挑战，我们提出了KineMIC（上下文中的动量挖掘），这是一个用于少样本动作合成的迁移学习框架。KineMIC通过假设文本编码空间中的语义对应关系可以为运动学蒸馏提供软监督，将T2M扩散模型适应于HAR领域。我们通过一种动量挖掘策略来实现这一点，该策略利用CLIP文本嵌入建立稀疏HAR标签与T2M源数据之间的对应关系。这个过程指导微调，将通用的T2M主干转变为专门的少样本动作到运动生成器。我们使用HumanML3D作为源T2M数据集，NTU RGB+D 120的一个子集作为目标HAR领域，随机选择每个动作类别的10个样本来验证KineMIC。我们的方法生成了显著更连贯的动作，提供了一个强大的数据增强源，带来了+23.1%的准确率提升。动画插图和补充材料可在https://lucazzola.github.io/publications/kinemic获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of acquiring large, annotated motion datasets for skeletal-based Human Activity Recognition (HAR), which is hindered by high costs. To overcome this, the authors propose KineMIC, a transfer learning framework that adapts a Text-to-Motion (T2M) diffusion model for few-shot action synthesis by utilizing semantic correspondences in text encoding to provide soft supervision for kinematic distillation. Experimental results demonstrate that KineMIC significantly enhances motion coherence and serves as a robust data augmentation source, achieving a 23.1% improvement in accuracy when validated on the HumanML3D dataset and a subset of NTU RGB+D 120 with only 10 samples per action class.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大型注释运动数据集的高获取成本，这阻碍了基于骨架的人类活动识别（HAR）。作者提出了KineMIC，这是一种迁移学习框架，通过利用文本编码中的语义对应关系为运动蒸馏提供软监督，从而将文本到运动（T2M）扩散模型适应于少样本动作合成。实验结果表明，KineMIC显著提高了运动的一致性，并作为有效的数据增强源，在HumanML3D数据集和NTU RGB+D 120的子集上验证时，准确率提高了23.1%，仅使用每个动作类别的10个样本。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning</div>
<div class="meta-line">Authors: Zhixian Zhao, Wenjie Tian, Xiaohai Tian, Jun Zhang, Lei Xie</div>
<div class="meta-line">First: 2026-01-26T10:03:26+00:00 · Latest: 2026-01-26T10:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18321v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18321v1">PDF</a> · <a href="https://github.com/zxzhao0/SABER-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a &quot;perceive-then-reason&quot; separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>集成细粒度音视频证据以实现稳健的多模态情感推理</div>
<div class="mono" style="margin-top:8px">多模态情感分析正从静态分类转向生成推理。稳健的情感推理不仅仅是简单的标签预测，还必须综合细粒度信号，如面部微表情和韵律，以解码复杂社会背景中的潜在因果关系。然而，当前的多模态大型语言模型（MLLMs）在细粒度感知方面面临重大限制，主要由于数据稀缺和跨模态融合不足。因此，这些模型往往表现出单模态主导，导致在复杂多模态交互中出现幻觉，特别是在视觉和声学线索微妙、模糊或甚至矛盾（例如，在讽刺场景中）时。为了解决这个问题，我们引入了SABER-LLM，一个旨在实现稳健多模态推理的框架。首先，我们构建了SABER，一个大规模情感推理数据集，包含60万段视频剪辑，采用一种新颖的六维模式进行注释，联合捕捉音视频线索和因果逻辑。其次，我们提出了结构化证据分解范式，强制在证据提取和推理之间进行“感知-再推理”分离，以减轻单模态主导。通过一致性感知的直接偏好优化，进一步增强了感知复杂场景的能力，明确鼓励在模糊或冲突的感知条件下各模态之间的对齐。在EMER、EmoBench-M和SABER-Test上的实验表明，SABER-LLM显著优于开源基线，并在解码复杂情感动态方面与闭源模型的稳健性相当。数据集和模型可在https://github.com/zxzhao0/SABER-LLM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multimodal emotion analysis by moving from static classification to generative reasoning, addressing the limitations of current Multimodal Large Language Models (MLLMs) in fine-grained perception. The authors introduce SABER-LLM, a framework that includes the construction of a large-scale emotion reasoning dataset with 600K video clips annotated with a six-dimensional schema and a structured evidence decomposition paradigm that separates evidence extraction from reasoning. Experimental results show that SABER-LLM significantly outperforms existing open-source models and achieves robustness comparable to closed-source models in decoding complex emotional dynamics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过从静态分类转向生成推理来增强多模态情感分析，解决当前多模态大语言模型（MLLMs）在细粒度感知方面因数据稀缺和单模态主导而面临的局限性。作者提出了SABER-LLM框架，包括构建一个包含60万段视频片段的大规模情感推理数据集，并用六维框架进行注释，以及一种结构化证据分解范式，将证据提取与推理分开。实验结果表明，SABER-LLM在多个基准测试中显著优于现有的开源模型，并在解码复杂情感动态方面达到了与闭源模型相当的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment</div>
<div class="meta-line">Authors: Fu-An Chao, Bi-Cheng Yan, Berlin Chen</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-18T08:10:24+00:00 · Latest: 2026-01-26T08:58:34+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16387v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the untapped potential of Whisper, a well-established automatic speech recognition (ASR) foundation model, in the context of L2 spoken language assessment (SLA). Unlike prior studies that extrinsically analyze transcriptions produced by Whisper, our approach goes a step further to probe its latent capabilities by extracting acoustic and linguistic features from hidden representations. With only a lightweight classifier being trained on top of Whisper&#x27;s intermediate and final outputs, our method achieves strong performance on the GEPT picture-description dataset, outperforming existing cutting-edge baselines, including a multimodal approach. Furthermore, by incorporating image and text-prompt information as auxiliary relevance cues, we demonstrate additional performance gains. Finally, we conduct an in-depth analysis of Whisper&#x27;s embeddings, which reveals that, even without task-specific fine-tuning, the model intrinsically encodes both ordinal proficiency patterns and semantic aspects of speech, highlighting its potential as a powerful foundation for SLA and other spoken language understanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究ASR基础模型在L2英语口语评估中的隐藏潜力</div>
<div class="mono" style="margin-top:8px">本文探讨了Whisper这一成熟的自动语音识别（ASR）基础模型在L2口语语言评估（SLA）中的未开发潜力。与之前对Whisper生成的转录进行外部分析的研究不同，我们的方法进一步探测其潜在能力，通过从隐藏表示中提取声学和语言特征。仅在Whisper的中间和最终输出上训练一个轻量级分类器，我们的方法在GEPT图片描述数据集上取得了强劲的表现，超越了现有的尖端基线，包括多模态方法。此外，通过将图像和文本提示信息作为辅助相关线索，我们展示了额外的性能提升。最后，我们对Whisper的嵌入进行了深入分析，揭示了即使没有特定任务的微调，该模型也内在地编码了顺序熟练度模式和语音的语义方面，突显了其作为SLA和其他口语理解任务强大基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential of the Whisper automatic speech recognition (ASR) model for assessing L2 spoken language, motivated by the need for effective evaluation tools in language learning. The researchers extract acoustic and linguistic features from Whisper&#x27;s hidden representations and train a lightweight classifier on its outputs. The results show that this method significantly outperforms existing state-of-the-art approaches on the GEPT picture-description dataset, with further improvements achieved by integrating image and text-prompt information, while an analysis of Whisper&#x27;s embeddings indicates that the model captures proficiency patterns and semantic features of speech without requiring task-specific fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Whisper自动语音识别（ASR）模型在L2口语评估中的潜力，旨在改善语言学习中的评估方法。研究人员从Whisper的隐藏表示中提取声学和语言特征，并在其输出上训练了一个轻量级分类器。结果表明，该方法在GEPT图片描述数据集上显著优于现有基准，且加入图像和文本提示进一步提升了性能，揭示了Whisper的嵌入在没有特定任务微调的情况下编码了熟练度模式和语音的语义方面。</div>
</details>
</div>
<div class="card">
<div class="title">Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach</div>
<div class="meta-line">Authors: Sahil Naik, Soham Bagayatkar, Pavankumar Singh</div>
<div class="meta-line">First: 2026-01-26T07:29:50+00:00 · Latest: 2026-01-26T07:29:50+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于EfficientNetB2的FER-2013面部情感识别方法</div>
<div class="mono" style="margin-top:8px">基于面部图像在人类情感检测中的应用在现实场景中是一项困难的任务，原因包括图像质量低、光照变化、姿态变化、背景干扰、类间小变异、嘈杂的众包标签以及严重的类别不平衡，这些在FER-2013数据集中48x48灰度图像中得到了体现。尽管最近使用大型CNN（如VGG和ResNet）的方法取得了合理的准确性，但它们计算成本高且内存占用大，限制了其在实时应用中的实用性。我们通过基于EfficientNetB2的轻量级高效面部情感识别管道来解决这些挑战，该管道采用两阶段的预热和微调策略进行训练。模型通过AdamW优化、解耦权重衰减、标签平滑（epsilon = 0.06）以减少注释噪声、剪裁类别权重以缓解类别不平衡，以及使用dropout、混合精度训练和广泛的实时数据增强进行增强。模型采用分层的87.5%/12.5%训练-验证划分进行训练，同时保持官方测试集不变，测试准确率达到68.78%，参数数量几乎是基于VGG16的基线的十分之一。实验结果，包括每类指标和学习动态，表明训练稳定且具有强泛化能力，使得所提方法适合实时和边缘计算应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve facial emotion recognition in real-world scenarios, which is challenged by factors such as low image quality and class imbalance in the FER-2013 dataset. The authors propose a lightweight approach using EfficientNetB2, employing a two-stage warm-up and fine-tuning strategy, along with techniques like AdamW optimization, label smoothing, and extensive data augmentation. The model achieves a test accuracy of 68.78% while utilizing nearly ten times fewer parameters than VGG16-based models, demonstrating stable training and strong generalization suitable for real-time applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善在现实场景中面部情感识别的效果，该任务受到低图像质量和FER-2013数据集中类别不平衡等因素的挑战。作者提出了一种基于EfficientNetB2的轻量高效方法，采用两阶段的预热和微调策略，以及AdamW优化和广泛的数据增强等技术。实验结果表明，该模型在参数数量显著少于VGG16模型的情况下，达到了68.78%的测试准确率，表明其训练稳定且具有良好的泛化能力，适合实时应用。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</div>
<div class="meta-line">Authors: Yifan Li, Shiying Wang, Jianqiang Huang</div>
<div class="meta-line">First: 2026-01-26T06:16:53+00:00 · Latest: 2026-01-26T06:16:53+00:00</div>
<div class="meta-line">Comments: 7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18190v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18190v1">PDF</a> · <a href="https://github.com/Lcrucial1f/MPS-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关键词引导的多视角子图CLIP用于遥感图像-文本检索</div>
<div class="mono" style="margin-top:8px">视觉-语言预训练（VLP）模型如CLIP显著推动了遥感图像-文本检索（RSITR）的发展。然而，现有方法主要依赖粗粒度的全局对齐，常常忽视了航拍图像中固有的密集多尺度语义。此外，通过全量微调适应这些重型模型会产生高昂的计算成本，并且存在灾难性遗忘的风险。为了解决这些挑战，我们提出了MPS-CLIP，一个参数高效的框架，旨在将检索范式从全局匹配转变为关键词引导的细粒度对齐。具体而言，我们利用大型语言模型（LLM）提取核心语义关键词，引导Segment Anything Model（SamGeo）生成语义相关的子视角。为了高效适应冻结的主干网络，我们引入了门控全局注意力（G^2A）适配器，以最小的开销捕捉全局上下文和长距离依赖。此外，多视角表示（MPR）模块将这些局部线索聚合成强健的多视角嵌入。该框架通过结合多视角对比损失和加权三元组损失的混合目标进行优化，动态选择最大响应视角以抑制噪声并强制精确的语义匹配。在RSICD和RSITMD基准上的广泛实验表明，MPS-CLIP分别实现了35.18%和48.40%的平均召回率（mR），显著优于全量微调基线和最近的竞争方法。代码可在https://github.com/Lcrucial1f/MPS-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing Vision-Language Pre-training models like CLIP in Remote Sensing Image-Text Retrieval, which often rely on coarse global alignment and incur high computational costs during fine-tuning. The authors propose MPS-CLIP, a parameter-efficient framework that utilizes a Large Language Model to extract semantic keywords, guiding the Segment Anything Model to create semantically relevant sub-perspectives. Experimental results on the RSICD and RSITMD benchmarks show that MPS-CLIP achieves state-of-the-art performance with mean Recall rates of 35.18% and 48.40%, significantly surpassing traditional fine-tuning approaches and other competitive methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法依赖粗粒度全局对齐的局限性来改善遥感图像-文本检索（RSITR），因为这些方法往往忽视了高空图像中的密集语义。作者提出了一种新颖的框架MPS-CLIP，该框架利用大型语言模型提取语义关键词，并指导Segment Anything Model生成语义相关的子视角。关键实验结果表明，MPS-CLIP在RSICD和RSITMD基准测试中实现了最先进的性能，平均召回率分别为35.18%和48.40%，显著优于传统的微调方法和其他竞争性方法。</div>
</details>
</div>
<div class="card">
<div class="title">VideoPro: Adaptive Program Reasoning for Long Video Understanding</div>
<div class="meta-line">Authors: Chenglin Li, Feng Han, Yikun Wang, Ruilin Li, Shuai Dong, Haowen Hou, Haitao Li, Qianglong Chen, Feng Tao, Jingqi Tong, Yin Zhang, Jiaqi Wang</div>
<div class="meta-line">First: 2025-09-22T13:06:17+00:00 · Latest: 2026-01-26T02:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17743v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.17743v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models&#x27; ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoPro：用于长视频理解的自适应程序推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生成视觉任务的程序工作流方面显示出潜力。然而，以前的方法往往依赖于闭源模型，缺乏系统推理，并且在长视频问答（videoQA）中表现不佳。为了解决这些挑战，我们引入了FS-VisPR框架，这是一种自适应视觉程序推理方法，平衡了简单查询的快速推理与困难查询的慢速推理。首先，我们设计了高效的视觉模块（例如，关键片段检索和字幕检索）以支持长视频任务。然后，我们构建了一个多样化且高质量的快慢推理数据集，结合强大的LLM，以对齐开源语言模型生成视觉程序工作流的能力，称为FS-LLM。接下来，我们设计了一个快慢推理框架与FS-LLM：简单查询由VideoLLMs直接解决，而困难查询则调用视觉程序推理，受到类人推理过程的启发。在此过程中，低置信度的快速思考答案将触发第二阶段的慢推理过程，如果程序执行失败，则激活快速推理的回退机制。此外，我们通过在训练和推理期间的参数搜索来改进视觉程序。通过调整程序中视觉模块的参数，生成多个变体：在训练期间，选择产生正确答案的程序，而在推理期间，应用置信度最高的程序。实验表明，FS-VisPR提高了视觉程序工作流的效率和可靠性。在LVBench上取得了50.4%的准确率，超越了GPT-4o，匹配了Qwen2.5VL-72B在VideoMME上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance long-form video question answering (videoQA) by addressing the limitations of previous approaches that rely on closed-source models and lack systematic reasoning. The authors introduce the FS-VisPR framework, which employs an adaptive visual program reasoning method that differentiates between simple and complex queries through efficient visual modules and a fast-slow reasoning dataset aligned with open-source language models. Experimental results demonstrate that FS-VisPR improves both efficiency and reliability in visual program workflows, achieving 50.4% accuracy on LVBench, outperforming GPT-4o and matching the performance of Qwen2.5VL-72B on VideoMME.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过大型语言模型增强长视频理解和问答能力，解决了以往方法依赖闭源模型和缺乏系统推理的局限性。作者提出了FS-VisPR框架，采用自适应视觉程序推理方法，通过高效的视觉模块和快慢推理数据集区分简单和复杂查询。实验结果表明，FS-VisPR在LVBench上达到了50.4%的准确率，超越了GPT-4o，并与Qwen2.5VL-72B在VideoMME上的表现相匹配，表明视觉程序工作流程在效率和可靠性方面都有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment</div>
<div class="meta-line">Authors: Yuxun Tang, Lan Liu, Wenhao Feng, Yiwen Zhao, Jionghao Han, Yifeng Yu, Jiatong Shi, Qin Jin</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-02T08:53:49+00:00 · Latest: 2026-01-25T09:43:41+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01812v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01812v3">PDF</a> · <a href="https://huggingface.co/datasets/TangRain/SingMOS-Pro">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro expands annotations of the additional part to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent advances. Each clip receives at least five ratings from professional annotators, ensuring reliability and consistency. Furthermore, we explore how to effectively utilize MOS data annotated under different standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset can be accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SingMOS-Pro：一个全面的歌唱质量评估基准</div>
<div class="mono" style="margin-top:8px">歌唱声音生成进展迅速，但评估歌唱质量仍然是一个关键挑战。人类主观评估通常以听力测试的形式进行，成本高且耗时，而现有的客观指标仅捕捉有限的感知方面。在这项工作中，我们介绍了SingMOS-Pro，一个用于自动歌唱质量评估的数据集。基于我们之前的版本SingMOS，后者仅提供总体评分，SingMOS-Pro扩展了附加部分的注释，包括歌词、旋律和整体质量，提供更广泛的覆盖和更大的多样性。该数据集包含由41个模型在12个数据集上生成的7,981个歌唱片段，涵盖了从早期系统到最近进展的范围。每个片段至少获得五个专业注释者的评分，确保可靠性和一致性。此外，我们探讨如何有效利用在不同标准下注释的MOS数据，并在SingMOS-Pro上基准测试几种广泛使用的评估方法，为未来的研究建立强有力的基线和实用参考。该数据集可在https://huggingface.co/datasets/TangRain/SingMOS-Pro访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in evaluating singing quality, as traditional human assessments are expensive and time-consuming, while existing objective metrics are limited in their perceptual coverage. The authors introduce SingMOS-Pro, a comprehensive dataset for automatic singing quality assessment that builds upon a previous version by including additional annotations for lyrics, melody, and overall quality, thus enhancing diversity and coverage. The dataset comprises 7,981 singing clips generated by 41 models across 12 datasets, with each clip receiving multiple ratings from professional annotators, and the study benchmarks various evaluation methods, establishing strong baselines for future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决评估歌唱质量的挑战，这通常依赖于昂贵且耗时的人类评估。作者介绍了SingMOS-Pro，这是一个全面的自动歌唱质量评估数据集，扩展了他们之前的工作，包含了歌词、旋律和整体质量的详细注释，共有7,981个由41个模型生成的歌唱片段。主要发现表明，该数据集提供了来自专业评估者的可靠评分，并为评估各种方法建立了强基准，从而为未来的歌唱质量评估研究提供了有价值的参考。</div>
</details>
</div>
<div class="card">
<div class="title">The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</div>
<div class="meta-line">Authors: Chenyu Mu, Xin He, Qu Yang, Wanshun Chen, Jiadi Yao, Huang Liu, Zihao Yi, Bo Zhao, Xingyu Chen, Ruotian Ma, Fanghua Ye, Erkun Yang, Cheng Deng, Zhaopeng Tu, Xiaolong Li, Linus</div>
<div class="meta-line">First: 2026-01-25T08:10:28+00:00 · Latest: 2026-01-25T08:10:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap&#x27;&#x27; between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>剧本就是你所需的一切：一种用于长时段对话到电影视频生成的代理框架</div>
<div class="mono" style="margin-top:8px">最近的视频生成进展产生了能够从简单文本提示合成惊人视觉内容的模型。然而，这些模型在从对话等高层次概念生成长篇连贯叙事方面存在困难，揭示了创意想法与其电影执行之间的“语义差距”。为了解决这个问题，我们提出了一种新颖的端到端代理框架，用于对话到电影视频生成。我们框架的核心是ScripterAgent，一个训练用于将粗略对话翻译为细致可执行电影剧本的模型。为此，我们构建了ScriptBench，一个新的大规模基准，具有丰富的多模态上下文，通过专家指导的流程进行注释。生成的剧本随后指导DirectorAgent，该代理使用跨场景连续生成策略协调最先进的视频模型，以确保长时段的一致性。我们的综合评估，结合了AI驱动的CriticAgent和新的视觉-剧本对齐（VSA）指标，显示我们的框架显著提高了剧本的忠实度和时间保真度，适用于所有测试的视频模型。此外，我们的分析揭示了当前SOTA模型在视觉壮观与严格剧本遵循之间的关键权衡，为未来的自动化电影制作提供了宝贵的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by existing video generation models in creating long-form, coherent narratives from high-level dialogue concepts. The authors propose an end-to-end agentic framework for dialogue-to-cinematic video generation, which includes a model called ScripterAgent that translates coarse dialogue into detailed cinematic scripts. Their experimental results demonstrate that this framework significantly enhances script faithfulness and temporal fidelity across various video models, while also revealing a trade-off between visual appeal and adherence to the script in current state-of-the-art models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视频生成模型在从高层对话概念创建长篇连贯叙事方面面临的挑战。作者提出了一种端到端的代理框架，用于对话到电影视频的生成，其中包括一个名为ScripterAgent的模型，该模型将粗略对话翻译为详细的电影剧本。他们的实验使用新的视觉-剧本对齐指标和一个AI驱动的CriticAgent进行评估，结果表明该框架显著提高了各种视频模型的剧本忠实度和时间一致性，同时揭示了当前最先进模型在视觉吸引力与剧本遵循之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-domain EEG-based Emotion Recognition with Contrastive Learning</div>
<div class="meta-line">Authors: Rui Yan, Yibo Li, Han Ding, Fei Wang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-11-07T14:55:10+00:00 · Latest: 2026-01-25T07:14:05+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05293v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05293v2">PDF</a> · <a href="https://github.com/Departure2021/EmotionCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69\% and 73.50\%, and cross-time accuracies of 88.46\% and 77.54\%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition. The code is available at https://github.com/Departure2021/EmotionCLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于跨域EEG的情感识别与对比学习</div>
<div class="mono" style="margin-top:8px">基于脑电图（EEG）的情感识别对情感计算至关重要，但在特征利用和跨域泛化方面面临挑战。本研究引入EmotionCLIP，将识别重新构建为CLIP框架下的EEG-文本匹配任务。定制的主干网络SST-LegoViT通过多尺度卷积和Transformer模块捕捉空间、频谱和时间特征。在SEED和SEED-IV数据集上的实验显示，跨受试者准确率分别为88.69\%和73.50\%，跨时间准确率为88.46\%和77.54\%，优于现有模型。结果证明了多模态对比学习在稳健的EEG情感识别中的有效性。代码可在https://github.com/Departure2021/EmotionCLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of feature utilization and cross-domain generalization in EEG-based emotion recognition, which is crucial for affective computing. The authors propose EmotionCLIP, an innovative approach that reformulates emotion recognition as an EEG-text matching task within the CLIP framework, utilizing a custom backbone called SST-LegoViT to effectively capture spatial, spectral, and temporal features through multi-scale convolution and Transformer modules. Experimental results on the SEED and SEED-IV datasets demonstrate significant improvements in cross-subject accuracies of 88.69% and 73.50%, as well as cross-time accuracies of 88.46% and 77.54%, indicating the effectiveness of multimodal contrastive learning in enhancing EEG emotion recognition performance.</div>
<div class="mono" style="margin-top:8px">本研究解决了基于脑电图（EEG）的情感识别中，特征利用和跨领域泛化的挑战，这对情感计算至关重要。作者提出了EmotionCLIP，将识别任务重新定义为EEG-文本匹配问题，并在CLIP框架内使用定制的骨干网络SST-LegoViT，有效捕捉空间、频谱和时间特征。在SEED和SEED-IV数据集上的实验结果显示，跨受试者的准确率分别达到88.69%和73.50%，跨时间的准确率达到88.46%和77.54%，超越了现有模型，突显了多模态对比学习在增强EEG情感识别中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</div>
<div class="meta-line">Authors: Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi</div>
<div class="meta-line">First: 2026-01-25T02:32:01+00:00 · Latest: 2026-01-25T02:32:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17657v1">PDF</a> · <a href="https://github.com/taewan2002/space-clip">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPACE-CLIP：通过自适应CLIP嵌入进行单目深度估计的空间感知</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）在语义理解方面取得了非凡的成功，但在感知几何结构方面固有地存在困难。现有方法试图通过文本提示查询CLIP来弥补这一差距，这一过程往往间接且低效。本文提出了一种根本不同的方法，使用双通道解码器。我们提出了SPACE-CLIP，一种从冻结的CLIP视觉编码器直接解锁和解释潜在几何知识的架构，完全绕过文本编码器及其相关的文本提示。语义通道解释高层特征，动态地基于全局上下文使用特征线性调制（FiLM）。此外，结构通道从早期层提取细粒度空间细节。这些互补流被分层融合，使语义上下文和精确几何的强健合成成为可能。在KITTI基准上的大量实验表明，SPACE-CLIP显著优于以前的基于CLIP的方法。我们的消融研究验证了双通道的协同融合对这一成功至关重要。SPACE-CLIP为重新利用大规模视觉模型提供了一种新的、高效的、架构优雅的蓝图。所提出的方法不仅是一个独立的深度估计器，而是一个可直接集成的空间感知模块，适用于下一代具身AI系统，如视觉-语言-行动（VLA）模型。我们的模型可在https://github.com/taewan2002/space-clip获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance geometric perception in semantic understanding, as existing methods using CLIP struggle with depth estimation due to their reliance on indirect textual prompts. The authors propose SPACE-CLIP, a novel architecture that utilizes a dual-pathway decoder to directly extract geometric knowledge from a frozen CLIP vision encoder, bypassing the text encoder. Experimental results on the KITTI benchmark demonstrate that SPACE-CLIP significantly outperforms previous CLIP-based methods, with ablation studies confirming the importance of the synergistic fusion of its dual pathways for achieving this improved performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高深度估计中几何感知的能力，而对比语言-图像预训练（CLIP）模型在这方面传统上存在困难。作者提出了SPACE-CLIP，这是一种新颖的架构，利用双通道解码器直接从冻结的CLIP视觉编码器中提取几何知识，绕过文本提示的需求。在KITTI基准上的实验结果表明，SPACE-CLIP显著优于现有的基于CLIP的方法，消融研究确认了其双通道协同融合在提升性能中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#x27; Contextual and Cultural Knowledge and Thinking</div>
<div class="meta-line">Authors: Xilin Jiang, Qiaolin Wang, Junkai Wu, Xiaomin He, Zhongweiyang Xu, Yinghao Ma, Minshuo Piao, Kaiyi Yang, Xiuwen Zheng, Riki Shimizu, Yicong Chen, Arsalan Firoozi, Gavin Mischler, Sukru Samet Dindar, Richard Antonello, Linyang He, Tsun-An Hsieh, Xulin Fan, Yulun Wu, Yuesheng Ma, Chaitanya Amballa, Weixiong Chen, Jiarui Hai, Ruisi Li, Vishal Choudhari, Cong Han, Yinghao Aaron Li, Adeen Flinker, Mounya Elhilali, Emmanouil Benetos, Mark Hasegawa-Johnson, Romit Roy Choudhury, Nima Mesgarani</div>
<div class="meta-line">First: 2026-01-25T01:40:15+00:00 · Latest: 2026-01-25T01:40:15+00:00</div>
<div class="meta-line">Comments: avmemeexam.github.io/public</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17645v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17645v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&amp;A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AVMeme考试：针对LLMs的上下文和文化知识及思维的多模态多语言多文化基准</div>
<div class="mono" style="margin-top:8px">互联网音视频片段通过时间变化的声音和运动传达意义，超越了文本所能表达的内容。为了检验AI模型是否能够理解人类文化背景中的这些信号，我们引入了AVMeme考试，这是一个由人类策划的基准，包含超过一千个标志性的互联网声音和视频，涵盖演讲、歌曲、音乐和音效。每个模因都配有独特的问答，评估从表面内容到上下文、情感、使用和世界知识的理解水平，以及原始年份、转录、摘要和敏感性等元数据。我们使用该基准系统地评估最先进的多模态大型语言模型（MLLMs）与人类参与者的表现。我们的结果揭示了一个一致的局限性：当前模型在无文本的音乐和音效上表现不佳，且在上下文和文化思维方面相较于表面内容存在困难。这些发现突显了人类对齐的多模态智能中的一个关键差距，并呼吁开发能够超越听到和看到的表面进行上下文和文化感知的模型。项目页面：avmemeexam.github.io/public</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to assess AI models&#x27; understanding of audio-visual content in cultural contexts, which traditional text-based evaluations may overlook. The authors introduce the AVMeme Exam, a benchmark consisting of over one thousand curated Internet sounds and videos, each accompanied by a Q&amp;A to evaluate understanding across various dimensions such as context and emotion. Experimental results indicate that current multimodal large language models perform inadequately on non-verbal audio content, particularly music and sound effects, revealing significant limitations in their contextual and cultural comprehension compared to their ability to process surface-level information.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估人工智能模型是否能够理解人类文化背景下的音频视觉信号，这些信号往往无法仅通过文本充分表达。作者引入了AVMeme Exam，这是一个包含超过一千个策划的互联网声音和视频的基准，每个都配有独特的问答，评估理解的不同维度，包括上下文和情感。对最先进的多模态大型语言模型与人类参与者的评估显示，这些模型在非文本音频元素（如音乐和音效）上的表现持续不佳，表明它们在处理上下文和文化细微差别方面与表面内容相比存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation</div>
<div class="meta-line">Authors: Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran</div>
<div class="meta-line">First: 2026-01-24T23:01:43+00:00 · Latest: 2026-01-24T23:01:43+00:00</div>
<div class="meta-line">Comments: 21 pages,7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17625v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BrainDistill：具有任务特定知识蒸馏的可植入运动解码</div>
<div class="mono" style="margin-top:8px">基于变换器的神经解码器具有大量参数，经过大规模数据集的预训练，最近在脑-计算机接口（BCI）任务中超越了经典机器学习模型和小型神经网络。然而，它们的大量参数和高计算需求阻碍了在功率受限的可植入系统中的部署。为了解决这一挑战，我们提出了BrainDistill，一种新颖的可植入运动解码管道，将可植入神经解码器（IND）与任务特定知识蒸馏（TSKD）框架相结合。与试图完整保留教师表示的标准特征蒸馏方法不同，TSKD通过监督投影明确优先考虑对解码至关重要的特征。在多个神经数据集上，IND在运动解码任务中始终优于先前的神经解码器，而其TSKD蒸馏变体在少量样本校准设置中进一步超越了替代蒸馏方法。最后，我们提出了一种量化感知训练方案，使得在训练期间学习的激活裁剪范围内进行仅整数推理成为可能。量化后的IND使得在可植入BCI的严格功率限制下部署成为可能，且性能损失最小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the deployment of transformer-based neural decoders in power-constrained implantable brain-computer interfaces (BCIs), which are hindered by their large parameter counts and computational demands. The authors propose BrainDistill, an implantable motor decoding pipeline that combines an implantable neural decoder with a task-specific knowledge distillation framework that focuses on critical features for decoding. Experimental results demonstrate that the implantable neural decoder consistently outperforms previous models in motor decoding tasks, and its distilled variant shows superior performance compared to other distillation methods in few-shot calibration scenarios, while also incorporating a quantization-aware training scheme for efficient deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高神经解码器在受限功耗的植入式脑机接口（BCI）中的部署能力，因为传统的大型模型受到其高计算需求的限制。作者提出了BrainDistill，这是一种创新的运动解码管道，结合了植入式神经解码器和任务特定知识蒸馏框架，专注于保留解码所需的关键特征。实验结果表明，植入式神经解码器在运动解码任务中始终优于先前的模型，而其蒸馏版本在少量样本校准场景中相比其他蒸馏方法表现更佳，同时还结合了量化感知训练方案以实现高效的整数推理。</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-24T17:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>它会零-shot吗？：预测任意查询的零-shot分类性能</div>
<div class="mono" style="margin-top:8px">视觉-语言模型如CLIP为文本和图像创建了对齐的嵌入空间，使任何人都可以通过简单命名他们想要区分的类别来构建视觉分类器。然而，在一个领域表现良好的模型在另一个领域可能会失败，非专家用户没有简单的方法来评估他们选择的VLM是否适用于他们的问题。我们基于先前的工作，使用仅文本的比较来评估模型在给定自然语言任务中的表现，并探索生成与该任务相关的合成图像的方法，以评估和改进零-shot准确性的预测。我们展示了生成的图像显著提高了基于文本的基线分数的预测质量。此外，它为用户提供了关于用于评估的图像类型的反馈。在标准CLIP基准数据集上的实验表明，基于图像的方法帮助用户在没有任何标记示例的情况下预测VLM是否对他们的应用有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge faced by non-expert users in assessing the effectiveness of Vision-Language Models (VLMs) for zero-shot classification tasks across different domains. The authors build upon previous methods that utilized text-only comparisons and introduce an approach that incorporates synthetic image generation relevant to the classification task to enhance the prediction of zero-shot accuracy. Experimental results on standard CLIP benchmark datasets reveal that the inclusion of generated images significantly improves the accuracy of predictions compared to text-only evaluations, providing users with valuable feedback on the types of images influencing their assessments.</div>
<div class="mono" style="margin-top:8px">本研究解决了非专家用户在不同领域预测视觉语言模型（VLM）零样本分类任务性能时面临的挑战。作者在先前使用文本比较的方法基础上，提出了一种新颖的方法，结合与分类任务相关的合成图像生成，从而增强零样本准确性的预测。对标准CLIP基准数据集的实验结果表明，这种基于图像的方法显著提高了预测质量，相较于仅使用文本评估，向用户提供了关于影响其预测的图像类型的有价值反馈。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Unlabeled Data with Multiple Expert Teachers for Open Vocabulary Aerial Object Detection and Its Orientation Adaptation</div>
<div class="meta-line">Authors: Yan Li, Weiwei Guo, Xue Yang, Ning Liao, Shaofeng Zhang, Yi Yu, Wenxian Yu, Junchi Yan</div>
<div class="meta-line">First: 2024-11-04T12:59:13+00:00 · Latest: 2026-01-24T15:00:31+00:00</div>
<div class="meta-line">Comments: Accepted by International Journal of Computer Vision (IJCV&#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.02057v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.02057v2">PDF</a> · <a href="https://github.com/VisionXLab/CastDet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, aerial object detection has been increasingly pivotal in various earth observation applications. However, current algorithms are limited to detecting a set of pre-defined object categories, demanding sufficient annotated training samples, and fail to detect novel object categories. In this paper, we put forth a novel formulation of the aerial object detection problem, namely open-vocabulary aerial object detection (OVAD), which can detect objects beyond training categories without costly collecting new labeled data. We propose CastDet, a CLIP-activated student-teacher detection framework that serves as the first OVAD detector specifically designed for the challenging aerial scenario, where objects often exhibit weak appearance features and arbitrary orientations. Our framework integrates a robust localization teacher along with several box selection strategies to generate high-quality proposals for novel objects. Additionally, the RemoteCLIP model is adopted as an omniscient teacher, which provides rich knowledge to enhance classification capabilities for novel categories. A dynamic label queue is devised to maintain high-quality pseudo-labels during training. By doing so, the proposed CastDet boosts not only novel object proposals but also classification. Furthermore, we extend our approach from horizontal OVAD to oriented OVAD with tailored algorithm designs to effectively manage bounding box representation and pseudo-label generation. Extensive experiments for both tasks on multiple existing aerial object detection datasets demonstrate the effectiveness of our approach. The code is available at https://github.com/VisionXLab/CastDet.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用多个专家教师的无标签数据进行开放词汇空中物体检测及其方向适应</div>
<div class="mono" style="margin-top:8px">近年来，空中物体检测在各种地球观测应用中变得越来越重要。然而，当前算法仅限于检测一组预定义的物体类别，需要足够的标注训练样本，并且无法检测新物体类别。本文提出了一种新的空中物体检测问题的表述，即开放词汇空中物体检测（OVAD），可以在不需要昂贵收集新标注数据的情况下检测超出训练类别的物体。我们提出了CastDet，一个激活CLIP的学生-教师检测框架，作为第一个专门为具有挑战性的空中场景设计的OVAD检测器，其中物体通常表现出弱的外观特征和任意方向。我们的框架集成了一个强大的定位教师以及几种框选策略，以生成高质量的新物体提案。此外，RemoteCLIP模型被采用为全知教师，提供丰富的知识以增强新类别的分类能力。设计了一个动态标签队列，以在训练期间保持高质量的伪标签。通过这样做，所提出的CastDet不仅提升了新物体提案的质量，还增强了分类能力。此外，我们将方法从水平OVAD扩展到定向OVAD，采用定制的算法设计有效管理边界框表示和伪标签生成。在多个现有空中物体检测数据集上进行的广泛实验证明了我们方法的有效性。代码可在https://github.com/VisionXLab/CastDet获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current aerial object detection algorithms, which are restricted to predefined categories and require extensive labeled data. The authors propose a novel approach called open-vocabulary aerial object detection (OVAD) using a CLIP-activated student-teacher detection framework named CastDet, which allows for the detection of novel object categories without the need for new labeled data. Experimental results show that CastDet significantly improves the generation of novel object proposals and enhances classification performance, demonstrating its effectiveness on multiple aerial object detection datasets, including adaptations for both horizontal and oriented detection tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要能够识别新物体类别的航空物体检测系统，而无需大量标注的训练数据。作者提出了一种新的方法，称为开放词汇航空物体检测（OVAD），并提出了一个名为CastDet的框架，该框架利用CLIP激活的学生-教师检测模型来增强在特征较弱和方向变化的航空场景中的检测能力。实验结果表明，CastDet显著提高了新物体的检测和分类准确性，证明了其在多个航空物体检测数据集上的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection</div>
<div class="meta-line">Authors: Chunze Yang, Wenjie Zhao, Yue Tang, Junbo Lu, Jiusong Ge, Qidong Liu, Zeyu Gao, Chen Li</div>
<div class="meta-line">First: 2026-01-24T10:31:21+00:00 · Latest: 2026-01-24T10:31:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17405v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17405v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAAF：基础模型的层次适应与对齐用于少量样本病理异常检测</div>
<div class="mono" style="margin-top:8px">精准病理依赖于在特定兴趣区域（ROI）内检测细微的形态异常，因为这些局部、富有纹理的线索——而非全局幻灯片上下文——驱动专家的诊断推理。尽管视觉-语言（V-L）模型通过利用语义先验承诺数据效率，但其适应面临关键的粒度不匹配问题，通用表示无法解决如此细微的缺陷。目前的适应方法通常将模态视为独立流，未能将语义提示与ROI特定的视觉上下文结合起来。为了解决这一问题，我们提出了层次适应与对齐框架（HAAF）。其核心是一个新颖的跨层级缩放对齐（CLSA）机制，强制执行顺序校准：视觉特征首先将上下文注入文本提示，以生成内容自适应描述符，然后空间引导视觉编码器聚焦于异常。此外，双分支推理策略将语义分数与几何原型结合，以确保在少量样本设置中的稳定性。在四个基准上的实验表明，HAAF显著优于最先进的方法，并在低资源场景中有效地与领域特定的骨干网络（例如CONCH）进行扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for precise pathology that focuses on detecting fine-grained morphological abnormalities in specific Regions of Interest (ROIs), which are crucial for expert diagnostics. The authors introduce the Hierarchical Adaptation and Alignment Framework (HAAF), which employs a Cross-Level Scaled Alignment (CLSA) mechanism to sequentially calibrate visual features and text prompts, enhancing the detection of subtle defects. Experimental results demonstrate that HAAF significantly outperforms existing methods across four benchmarks and effectively adapts to domain-specific backbones in low-resource environments.</div>
<div class="mono" style="margin-top:8px">本研究解决了病理学中检测细微形态异常的挑战，现有的视觉-语言模型因通用表示与特定兴趣区域之间的粒度不匹配而面临困难。作者提出了分层适应与对齐框架（HAAF），该框架采用跨层缩放对齐机制，顺序校准视觉特征和文本提示，从而增强异常检测。实验结果表明，HAAF在四个基准测试中显著优于最先进的方法，并能有效适应低资源环境中的领域特定骨干网络。</div>
</details>
</div>
<div class="card">
<div class="title">Phase Transition for Budgeted Multi-Agent Synergy</div>
<div class="meta-line">Authors: Bang Liu, Linglong Kong, Jian Pei</div>
<div class="meta-line">First: 2026-01-24T05:32:50+00:00 · Latest: 2026-01-24T05:32:50+00:00</div>
<div class="meta-line">Comments: 55 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17311v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17311v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s&gt;β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预算多智能体协同的相变</div>
<div class="mono" style="margin-top:8px">多智能体系统可以提高可靠性，但在固定推理预算下，它们往往会帮助、饱和甚至崩溃。我们开发了一种最小且可校准的理论，从现代智能体堆栈的三个约束条件预测这些状态：有限的上下文窗口、损失的智能体间通信和相似智能体之间的共享失败。每个叶子智能体由计算性能缩放指数 $β$ 概括；通信由消息长度保真度曲线 $γ(m)$ 捕获；依赖性由有效共享错误相关性 $ρ$ 捕获；上下文窗口 $W$ 强加了硬性输入限制，使得层次结构成为必要。对于具有多数聚合的二元成功/失败任务，我们证明了具有相关输入和损失通信的深 $b$ 叉树的锐利相变：一个标量 $α_ρ$（结合 $γ(m)$、$ρ$ 和输入 $b$）决定了弱信号是被放大到非平凡固定点还是被冲淡到偶然。在放大状态下，我们推导出一个组织指数 $s$，并表明预算协同，即在相同总预算下超越最佳单一智能体，恰好发生在 $s&gt;β$ 时，得出封闭形式的计算分配规则和明确的预算阈值。我们进一步通过混合深度表征饱和，并提供一个保守的剪裁预测器，在增长和饱和过程中保持准确。连续性能的热身为星形、链形和树形组织提供了封闭形式的风险，使得相关性和通信引起的底线显而易见，并在平滑设置中暴露核心设计权衡。最后，我们在受控的合成模拟中验证了预测的相边界，并展示了相同机制如何解释最近大规模匹配预算研究中报告的主要瓶颈。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the performance of multi-agent systems under fixed inference budgets, motivated by the need to understand their reliability and potential failure modes. The authors develop a theoretical framework that incorporates constraints such as finite context windows, lossy communication, and shared failures among agents, using parameters like compute-performance scaling exponent and message-length fidelity curve. Key findings include the identification of a phase transition in performance based on a scalar that combines communication fidelity and error correlation, revealing that budgeted synergy occurs when a derived organization exponent exceeds the scaling exponent, along with explicit compute allocation rules and thresholds validated through synthetic simulations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在固定推理预算下多智能体系统的性能限制，旨在理解这些系统如何成功、饱和或失败。作者开发了一个理论框架，结合了有限上下文窗口、损失通信和智能体之间共享故障等约束，使用计算性能缩放指数、消息长度保真曲线和共享误差相关性等参数。主要发现包括在相关输入的深b-叉树中识别出相变，其中一个标量决定了弱信号的放大，以及推导出当特定组织指数超过缩放指数时使预算协同成为可能的计算分配规则，通过合成模拟验证了这些发现，与大规模研究中观察到的瓶颈一致。</div>
</details>
</div>
<div class="card">
<div class="title">LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups</div>
<div class="meta-line">Authors: Masih Aminbeidokhti, Subhankar Roy, Eric Granger, Elisa Ricci, Marco Pedersoli</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-11-11T14:15:57+00:00 · Latest: 2026-01-23T23:22:15+00:00</div>
<div class="meta-line">Comments: Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10683v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10683v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LT-Soups：通过子采样模型汤桥接头尾类</div>
<div class="mono" style="margin-top:8px">现实世界的数据集通常表现出长尾（LT）分布，其中少数头类占主导地位，而许多尾类严重不足。尽管最近的研究表明，像LoRA和AdaptFormer这样的参数高效微调（PEFT）方法在基础模型（如CLIP）上保持了尾类性能，但我们发现这以牺牲头类准确性为代价。我们确定头尾比，即头类与尾类的比例，是影响这一权衡的重要但被忽视的因素。通过对CIFAR100进行控制实验，改变不平衡比（$ρ$）和头尾比（$η$），我们展示了PEFT在尾重场景中表现出色，但在更平衡和头重分布中表现下降。为克服这些限制，我们提出了LT-Soups，一个旨在跨越多样LT模式的两阶段模型汤框架。在第一阶段，LT-Soups对在平衡子集上微调的模型进行平均，以减少头类偏差；在第二阶段，它仅对完整数据集上的分类器进行微调，以恢复头类准确性。对六个基准数据集的实验表明，LT-Soups在广泛的不平衡模式下相比于PEFT和传统模型汤实现了更优的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by long-tailed distributions in real-world datasets, where head classes dominate and tail classes are underrepresented. The authors propose a two-stage framework called LT-Soups, which first averages models fine-tuned on balanced subsets to mitigate head-class bias and then fine-tunes the classifier on the full dataset to enhance head-class accuracy. Experimental results on six benchmark datasets demonstrate that LT-Soups provides better trade-offs in performance compared to parameter-efficient fine-tuning methods and traditional model soups across various imbalance scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现实世界数据集中长尾分布带来的挑战，其中头类占主导地位而尾类严重不足。作者提出了一种名为LT-Soups的两阶段模型汤框架，首先对在平衡子集上微调的模型进行平均，以减轻头类偏差，然后在完整数据集上微调分类器以提高头类准确性。在六个基准数据集上的实验结果表明，LT-Soups在各种不平衡场景下相较于参数高效微调方法和传统模型汤提供了更好的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models</div>
<div class="meta-line">Authors: Zahraa Al Sahili, Ioannis Patras, Matthew Purver</div>
<div class="meta-line">First: 2025-01-22T21:08:30+00:00 · Latest: 2026-01-23T15:12:35+00:00</div>
<div class="meta-line">Comments: Published at TMLR; updated version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.13223v7">Abs</a> · <a href="https://arxiv.org/pdf/2501.13223v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors -- model size, training-data scale, and training-data source -- by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M/2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies -- Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据最重要：审计对比视觉语言模型中的社会偏见</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在零样本识别中表现强劲，但经常从训练数据中继承社会偏见。我们通过比较CLIP和OpenCLIP这两个共享相同对比目标但在编码器宽度和预训练图像-文本语料库（400M专有对比对与400M/2B LAION）上存在差异的模型，系统性地解开三个设计因素——模型大小、训练数据规模和训练数据来源。在平衡的面部分析基准测试中，增大编码器在CLIP中减少了性别偏差，但在OpenCLIP中加剧了性别和种族偏差；将LAION语料库从400M增加到2B进一步增加了OpenCLIP的偏见。在匹配的模型和数据预算下，用LAION替代专有数据提高了性别公平性，但增加了种族偏差，强调数据来源是偏见模式的主要驱动因素。我们还评估了三种后期、测试时去偏见策略——偏见提示、提示数组和SANER。去偏见减少了但并未消除伤害，其有效性依赖于来源和规模：在较小模型规模下，偏见提示最有效地减少了CLIP中的性别偏差，而提示数组和SANER更可靠地减少了OpenCLIP中的种族偏差；扩展LAION重新配置了最公平的方法。综合来看，这些发现挑战了更大模型或数据集自动更公平的假设，并突出了训练数据来源作为偏见和缓解有效性的关键决定因素。我们发布了代码和评估脚本，以便对未来的VLM进行透明、可重复的审计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the social biases present in vision-language models (VLMs), motivated by their strong zero-shot recognition capabilities yet problematic bias inheritance from training data. The study systematically examines the effects of model size, training-data scale, and training-data source by comparing CLIP and OpenCLIP, which differ in encoder width and pre-training datasets. Key findings reveal that increasing model size reduces gender bias in CLIP but exacerbates both gender and racial biases in OpenCLIP, while expanding the LAION dataset heightens bias further; substituting proprietary data with LAION improves gender fairness but increases racial skew, highlighting the significance of data source in bias patterns. Additionally, three debiasing strategies are evaluated, showing varying effectiveness based on model size and data source, ultimately challenging the notion that larger models or datasets inherently lead to fairer outcomes.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLM）中存在的社会偏见，这些偏见源于其训练数据，旨在理解和减轻这些偏见。研究系统地考察了模型大小、训练数据规模和训练数据来源的影响，通过比较CLIP和OpenCLIP这两个具有相同目标但在架构和训练数据集上有所不同的模型。研究结果表明，尽管在CLIP中增加编码器大小可以减少性别偏见，但在OpenCLIP中却加剧了性别和种族偏见，尤其是在LAION数据集从4亿扩展到20亿时。此外，评估了各种去偏见策略，结果显示这些策略可以减少偏见，但其有效性因模型和数据来源而异，强调了训练数据在偏见缓解中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis</div>
<div class="meta-line">Authors: Yinqi Cai, Jichang Li, Zhaolun Li, Weikai Chen, Rushi Lan, Xi Xie, Xiaonan Luo, Guanbin Li</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-10-29T07:35:29+00:00 · Latest: 2026-01-23T14:32:04+00:00</div>
<div class="meta-line">Comments: ICCV 2025. Code is available at https://github.com/lijichang/DeepShield</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25237v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25237v2">PDF</a> · <a href="https://github.com/lijichang/DeepShield">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks. Code is available at https://github.com/lijichang/DeepShield.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepShield：通过局部和全局伪造分析强化深度伪造视频检测</div>
<div class="mono" style="margin-top:8px">最近深度生成模型的进展使得操纵人脸视频变得更加容易，这引发了对其潜在滥用进行欺诈和传播错误信息的重大担忧。现有检测器在领域内场景中表现良好，但由于依赖于特定伪造伪影，无法在多样化的操纵技术中进行泛化。在本研究中，我们介绍了DeepShield，这是一种新颖的深度伪造检测框架，平衡局部敏感性和全局泛化，以提高对未见伪造的鲁棒性。DeepShield通过两个关键组件增强CLIP-ViT编码器：局部补丁引导（LPG）和全局伪造多样化（GFD）。LPG应用时空伪影建模和补丁级监督，以捕捉全球模型常常忽视的细粒度不一致性。GFD引入领域特征增强，利用领域桥接和边界扩展特征生成合成多样化伪造，减轻过拟合并增强跨领域适应性。通过整合新颖的局部和全局分析进行深度伪造检测，DeepShield在跨数据集和跨操纵评估中超越了最先进的方法，实现了对未见深度伪造攻击的卓越鲁棒性。代码可在 https://github.com/lijichang/DeepShield 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the growing concern over the misuse of manipulated face videos due to advancements in deep generative models, which existing detection methods struggle to address effectively across diverse manipulation techniques. The authors propose DeepShield, a deepfake detection framework that enhances the CLIP-ViT encoder by incorporating Local Patch Guidance (LPG) and Global Forgery Diversification (GFD) to improve robustness against unseen forgeries. Experimental results demonstrate that DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, showing superior adaptability and resilience against various deepfake attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于深度生成模型的进步使得操纵面部视频变得更加容易，这对欺诈和错误信息构成了风险。作者提出了DeepShield，一个通过结合局部敏感性和全局泛化来提高鲁棒性的深度伪造检测框架。关键实验结果表明，DeepShield通过其局部补丁引导和全局伪造多样化组件，在跨数据集和跨操纵评估中显著优于现有方法，展示了对未见深度伪造攻击的增强适应性。</div>
</details>
</div>
<div class="card">
<div class="title">LATTLE: LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains</div>
<div class="meta-line">Authors: Ibna Kowsar, Kazi F. Akhter, Manar D. Samad</div>
<div class="meta-line">First: 2025-11-08T23:05:31+00:00 · Latest: 2026-01-23T13:15:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06161v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06161v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transfer learning on tabular data is challenging due to disparate feature spaces across domains, in contrast to the homogeneous structures of image and text. Large language models (LLMs) offer a knowledge base to improve the limited effectiveness of cross-domain transfer learning for tabular data. However, LLM performance often stagnates due to subjective text prompts and the computational limitations of in-context learning. We present a novel language-to-tabular context-learning method that uses attention-specific transformer weights, enabling seamless transfer learning across disparate tabular data sets. The LLM attention transplant mechanism facilitates a domain-agnostic transfer learning, eliminating the need for shared features between tables, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of disjoint source-target data sets and 12 baseline methods demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and models trained on thousands to billions of tabular samples. The proposed cross-domain attention transfer demonstrates an effective solution for adapting LLMs to learning non-text tabular data in a low-resource environment. The source code of the LATTLE implementation is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LATTLE：用于跨不同领域表格数据迁移学习的LLM注意力移植</div>
<div class="mono" style="margin-top:8px">由于不同领域之间特征空间的差异，表格数据的迁移学习面临挑战，这与图像和文本的同质结构形成对比。大型语言模型（LLMs）提供了一个知识基础，以提高跨领域表格数据迁移学习的有限效果。然而，由于主观文本提示和上下文学习的计算限制，LLM的性能往往停滞不前。我们提出了一种新颖的语言到表格上下文学习方法，使用特定于注意力的变换器权重，实现跨不同表格数据集的无缝迁移学习。LLM注意力移植机制促进了领域无关的迁移学习，消除了表格之间共享特征、LLM提示工程和大规模预训练模型的需求。我们使用十对不相交的源-目标数据集和12种基线方法的实验表明，所提出的用于迁移学习的LLM注意力移植（LATTLE）方法优于传统机器学习模型、最先进的深度表格架构以及在数千到数十亿个表格样本上训练的模型。所提出的跨领域注意力转移展示了在低资源环境中将LLM适应于学习非文本表格数据的有效解决方案。LATTLE实现的源代码已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of transfer learning on tabular data, which often involves disparate feature spaces across different domains, unlike the more uniform structures found in image and text data. The authors propose a novel method called LATTLE, which utilizes attention-specific transformer weights to facilitate transfer learning without the need for shared features or extensive prompt engineering. Experimental results show that LATTLE outperforms traditional machine learning models and state-of-the-art deep tabular architectures across ten pairs of disjoint source-target datasets, demonstrating its effectiveness in adapting large language models to tabular data in low-resource settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决表格数据迁移学习中的挑战，这通常涉及不同领域之间的特征空间差异，而与图像和文本数据中更统一的结构不同。作者提出了一种新方法LATTLE，利用特定于注意力的变换器权重来促进领域无关的迁移学习，无需共享特征或大量的提示工程。实验结果表明，LATTLE在十对不相交的源-目标数据集上优于传统机器学习模型和最先进的深度表格架构，证明了其在低资源环境中将大型语言模型适应于非文本表格数据的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</div>
<div class="meta-line">Authors: Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-23T11:06:51+00:00 · Latest: 2026-01-23T11:06:51+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16645v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16645v1">PDF</a> · <a href="https://github.com/gongms00/SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model&#x27;s generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的边缘感知图像处理与新颖的结构保留损失</div>
<div class="mono" style="margin-top:8px">最近的图像编辑进展利用潜在扩散模型（LDMs）进行多样化、基于文本提示的编辑。然而，保持像素级边缘结构——对于如照片真实感风格转移或图像色调调整等任务至关重要——仍然是潜在扩散编辑的一大挑战。为克服这一限制，我们提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异。我们的无训练方法将SPL直接集成到扩散模型的生成过程中，以确保结构的保真性。该核心机制通过后处理步骤来减轻LDM解码失真、精确编辑定位的掩蔽策略以及保留未编辑区域色调的颜色保留损失进行补充。实验确认SPL增强了结构保真性，在基于潜在扩散的图像编辑中实现了最先进的性能。我们的代码将在https://github.com/gongms00/SPL公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of maintaining pixel-level edge structures in image editing using latent diffusion models, which is essential for tasks like photorealistic style transfer. The authors propose a novel Structure Preservation Loss (SPL) that quantifies structural differences between input and edited images using local linear models, integrating this loss into the generative process of the diffusion model without requiring additional training. Experimental results demonstrate that the SPL significantly enhances structural fidelity in image editing, achieving state-of-the-art performance in latent-diffusion-based methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用潜在扩散模型（LDM）进行图像编辑时保持像素级边缘结构的挑战，这对于如逼真风格转移等任务至关重要。作者提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异，并将该损失直接整合到扩散模型的生成过程中，无需额外训练。实验结果表明，SPL显著增强了结构保真度，在基于潜在扩散的图像编辑任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval</div>
<div class="meta-line">Authors: Hongyu Guo, Xiangzhao Hao, Jiarui Guo, Haiyun Guo, Jinqiao Wang, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-08-06T07:02:39+00:00 · Latest: 2026-01-23T10:16:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04136v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.04136v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniFGVC：通过属性感知多模态检索实现通用无训练少样本细粒度视觉分类</div>
<div class="mono" style="margin-top:8px">少样本细粒度视觉分类（FGVC）旨在利用有限数据使模型能够区分微妙的类别差异。近期的研究大多对预训练的视觉语言模型进行了微调以提高性能，但却遭遇了过拟合和弱泛化的问题。为了解决这个问题，我们提出了UniFGVC，一个通用的无训练框架，将少样本FGVC重新表述为多模态检索。首先，我们提出了类别区分视觉描述生成器（CDV-Captioner），利用多模态大型语言模型（MLLMs）的开放世界知识生成结构化文本描述，以捕捉区分密切相关类别的细粒度属性特征。CDV-Captioner使用思维链提示和视觉相似的参考图像来减少幻觉并增强生成描述的区分性。通过它，我们可以将每个图像转换为图像-描述对，从而实现更全面的特征表示，并使用少样本样本构建多模态类别模板，以便于后续的检索管道。然后，现成的视觉和文本编码器嵌入查询和模板对，FGVC通过在联合空间中检索最近的模板来完成。UniFGVC确保与多种MLLM和编码器的广泛兼容性，提供可靠的泛化和适应性，适用于少样本FGVC场景。在12个FGVC基准上的广泛实验表明，它在性能上始终优于先前的少样本基于CLIP的方法，甚至优于一些完全监督的基于MLLM的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot fine-grained visual classification (FGVC) by addressing issues of overfitting and weak generalization in existing methods that rely on fine-tuning pre-trained visual language models. The authors introduce UniFGVC, a training-free framework that reformulates FGVC as a multimodal retrieval task, utilizing a Category-Discriminative Visual Captioner (CDV-Captioner) to generate structured text descriptions that highlight distinguishing attributes of closely related classes. Experimental results across 12 FGVC benchmarks show that UniFGVC consistently outperforms previous few-shot CLIP-based methods and several fully-supervised MLLMs-based approaches, demonstrating its effectiveness and adaptability in various few-shot scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有依赖于微调预训练模型的方法中的过拟合和弱泛化问题，来改善少样本细粒度视觉分类（FGVC）。作者提出了UniFGVC，这是一种无训练框架，将FGVC重新定义为多模态检索任务，利用类别区分视觉描述生成器（CDV-Captioner）生成结构化文本描述，以捕捉密切相关类别的细粒度属性。通过在12个FGVC基准上的实验结果表明，UniFGVC在性能上始终优于先前的少样本CLIP方法以及几种完全监督的多模态语言模型方法，展示了其在各种少样本场景中的有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding</div>
<div class="meta-line">Authors: Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu, Yifei Dong, Shuyuan Tu, Qiyu Hu, Huiting Huang, Yuxiang Lin, Jun-Yan He, Kai Wang, Zheng Lian, Zhi-Qi Cheng</div>
<div class="meta-line">First: 2026-01-23T05:02:43+00:00 · Latest: 2026-01-23T05:02:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16449v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Emotion-LLaMAv2 和 MMEVerse：多模态情感理解的新框架和基准</div>
<div class="mono" style="margin-top:8px">从多模态信号理解人类情感在情感计算和人机交互中面临重大挑战。尽管多模态大型语言模型（MLLMs）在一般视觉-语言任务中表现出色，但它们在情感推理方面的能力仍然有限。该领域目前缺乏高质量、描述性情感注释的大规模数据集，并且缺乏标准化的评估基准。我们的初步框架Emotion-LLaMA开创了情感推理的指令调优多模态学习，但受到显式人脸检测、隐式融合策略和低质量训练数据的限制。为了解决这些局限性，我们提出了Emotion-LLaMAv2和MMEVerse基准，建立了一个端到端的管道以及情感识别和推理的标准化评估设置。Emotion-LLaMAv2引入了三个关键进展。首先，端到端的多视角编码器消除了外部人脸检测，通过更丰富的空间和时间多视角标记捕捉细微的情感线索。其次，Conv Attention预融合模块旨在实现LLM主干外部的局部和全局多模态特征交互。第三，LLaMA2主干中的感知到认知课程指令调优方案统一了情感识别和自由形式的情感推理。为了支持大规模训练和可重复评估，MMEVerse将包括IEMOCAP、MELD、DFEW和MAFW在内的十二个公开可用的情感数据集聚合为统一的多模态指令格式。数据通过涉及Qwen2 Audio、Qwen2.5 VL和GPT 4o的多代理管道重新注释，生成了130k训练片段和36k测试片段，涵盖18个评估基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of understanding human emotions from multimodal signals, which is crucial for affective computing and human-robot interaction. The authors developed Emotion-LLaMAv2 and the MMEVerse benchmark to enhance emotional reasoning capabilities in multimodal large language models (MLLMs) by introducing an end-to-end multiview encoder, a Conv Attention pre-fusion module, and a perception-to-cognition curriculum instruction tuning scheme. Experimental results demonstrate that the new framework, supported by a comprehensive dataset of 130k training clips and 36k testing clips, significantly improves emotion recognition and reasoning across 18 evaluation benchmarks compared to previous models.</div>
<div class="mono" style="margin-top:8px">本研究解决了从多模态信号理解人类情感的挑战，这对情感计算和人机交互至关重要。作者开发了Emotion-LLaMAv2和MMEVerse基准，以增强多模态大语言模型（MLLMs）在情感推理方面的能力。主要发现包括引入一种端到端的多视角编码器，该编码器在不依赖外部面部检测的情况下捕捉情感线索，设计了Conv Attention预融合模块以改善特征交互，以及一种新的课程指令调优方案，将情感识别与推理整合在一起，支持130k训练片段和36k测试片段的大规模数据集，涵盖18个基准。</div>
</details>
</div>
<div class="card">
<div class="title">AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose</div>
<div class="meta-line">Authors: Jongmin Yu, Hyeontaek Oh, Zhongtian Sun, Angelica I Aviles-Rivero, Moongu Jeon, Jinhong Yang</div>
<div class="meta-line">First: 2026-01-23T04:01:49+00:00 · Latest: 2026-01-23T04:01:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16429v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16429v1">PDF</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git">Code1</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git&amp;#39">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git&#x27;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaFace：高保真且实时的面部交换器，能够适应面部姿态</div>
<div class="mono" style="margin-top:8px">现有的面部交换方法在受限环境中通常能提供竞争力的结果，但在处理极端面部姿态时质量显著下降。为了提高面部姿态的鲁棒性，应用了显式几何特征，但这种方法仍然存在问题，因为它引入了额外的依赖关系并增加了计算成本。基于扩散的方法取得了显著成果；然而，它们在实时处理方面不切实际。我们引入了AlphaFace，它利用开源的视觉-语言模型和CLIP图像与文本嵌入，应用新颖的视觉和文本语义对比损失。AlphaFace实现了更强的身份表示和更精确的属性保留，同时保持实时性能。在FF++、MPIE和LPFF上的全面实验表明，AlphaFace在姿态挑战案例中超越了最先进的方法。该项目在`https://github.com/andrewyu90/Alphaface_Official.git`上公开可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing face-swapping methods, which struggle with quality degradation in extreme facial poses. The authors introduce AlphaFace, a novel approach that utilizes an open-source vision-language model and CLIP image and text embeddings, applying innovative visual and textual semantic contrastive losses to enhance identity representation and attribute preservation while ensuring real-time performance. Experimental results across datasets such as FF++, MPIE, and LPFF indicate that AlphaFace outperforms state-of-the-art methods, particularly in challenging pose scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有换脸方法在极端面部姿势下质量下降的问题。作者提出了AlphaFace，这是一种新颖的方法，利用开源视觉语言模型和CLIP图像及文本嵌入，实施视觉和文本语义对比损失，从而增强身份表示和属性保留，同时确保实时性能。在FF++、MPIE和LPFF等数据集上的实验结果表明，AlphaFace在面部姿势挑战的情况下优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction</div>
<div class="meta-line">Authors: Shuang Wu, Meijie Wang, Lun Yu</div>
<div class="meta-line">First: 2026-01-23T03:51:19+00:00 · Latest: 2026-01-23T03:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16426v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model&#x27;s out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **&quot;safe multitask&quot;** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全的多任务分子图网络用于蒸汽压和气味阈值预测</div>
<div class="mono" style="margin-top:8px">我们研究了与气味相关的属性建模中的两个重要任务：蒸汽压（VP）和气味阈值（OP）。为了评估模型的分布外（OOD）能力，我们采用了Bemis-Murcko支架分割。在特征方面，我们引入了丰富的A20/E17分子图特征（20维原子特征 + 17维键特征），并系统地比较了GINE和PNA骨干网络。结果显示：对于VP，PNA与简单回归头的验证均方误差约为0.21（归一化空间）；对于在相同支架分割下的OP单任务，使用A20/E17和稳健训练（Huber/winsor）实现了验证均方误差约为0.60-0.61。对于多任务训练，我们提出了一种**“安全多任务”**方法：将VP作为主要任务，将OP作为辅助任务，使用延迟激活 + 梯度裁剪 + 小权重，避免对主要任务造成损害，同时实现最佳的VP泛化性能。本文提供了完整的可重复实验、消融研究和误差相似性分析，同时讨论了数据噪声和方法限制的影响。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
