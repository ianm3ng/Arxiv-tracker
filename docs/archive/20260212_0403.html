<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 04:03</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0403</div>
    <div class="row"><div class="card">
<div class="title">Biases in the Blind Spot: Detecting What LLMs Fail to Mention</div>
<div class="meta-line">Authors: Iván Arcuschin, David Chanin, Adrià Garriga-Alonso, Oana-Maria Camburu</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2026-02-10T18:59:56+00:00 · Latest: 2026-02-10T18:59:56+00:00</div>
<div class="meta-line">Comments: 10 pages, Under review at ICML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model&#x27;s CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>盲点中的偏见：检测大型语言模型未提及的内容</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常提供看似合理的思维链（CoT）推理痕迹，但可能隐藏内部偏见。我们称这些为*未表述的偏见*。通过模型的陈述推理监控模型因此不可靠，现有的偏见评估通常需要预定义类别和手工制作的数据集。在本研究中，我们引入了一种完全自动化的黑箱管道，用于检测特定任务的未表述偏见。给定一个任务数据集，该管道使用LLM自动评分器生成候选偏见概念。然后，它通过生成正负变体在逐渐增大的输入样本上测试每个概念，并应用多重测试和早停的统计技术。如果一个概念在模型的CoT中未被引用作为理由，但产生了统计显著的性能差异，则该概念被标记为未表述的偏见。我们在三个决策任务（招聘、贷款批准和大学录取）上对六个LLM评估了我们的管道。我们的技术自动发现了这些模型中以前未知的偏见（例如，西班牙语流利度、英语能力、写作正式性）。在同一次运行中，该管道还验证了先前工作手动识别的偏见（性别、种族、宗教、民族）。更广泛地说，我们提出的方法提供了一条实用、可扩展的自动任务特定偏见发现路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of unverbalized biases in Large Language Models (LLMs), which can lead to unreliable reasoning outputs. The authors developed a fully automated black-box pipeline that detects task-specific unverbalized biases by generating candidate bias concepts using LLM autoraters and testing these concepts on varying input samples with statistical methods for validation. The findings reveal that the pipeline can automatically identify previously unknown biases, such as those related to language fluency and writing formality, while also confirming biases previously documented in the literature, including those based on gender and race, across six LLMs in three decision-making tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）中的未言明偏见问题，这可能导致误导性的推理痕迹，掩盖内部偏见。作者开发了一种完全自动化的黑箱管道，通过利用LLM自动评分器从任务数据集中生成候选偏见概念，测试这些概念在不同输入样本上的表现，并应用统计技术进行验证，以检测特定任务的未言明偏见。实验结果表明，该管道成功识别出先前未知的偏见，如西班牙语流利度和英语能力，同时确认了与性别、种族、宗教和民族相关的偏见，这些偏见在先前研究中已被识别，展示了其在三个决策任务（包括招聘和贷款批准）上对六个LLM的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation</div>
<div class="meta-line">Authors: Mingyang Wu, Ashirbad Mishra, Soumik Dey, Shuo Xing, Naveen Ravipati, Hansi Wu, Binbin Li, Zhengzhong Tu</div>
<div class="meta-line">First: 2026-02-10T18:59:51+00:00 · Latest: 2026-02-10T18:59:51+00:00</div>
<div class="meta-line">Comments: Project page: https://myangwu.github.io/ConsID-Gen</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10113v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://myangwu.github.io/ConsID-Gen">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConsID-Gen：视图一致性与身份保持的图像到视频生成</div>
<div class="mono" style="margin-top:8px">图像到视频生成（I2V）将静态图像动画化为遵循文本指令的时间一致视频序列，但在变化视角下保持细粒度对象身份仍然是一个持续的挑战。与文本到视频模型不同，现有的I2V管道往往受到外观漂移和几何失真的影响，这些伪影归因于单视图2D观察的稀疏性和跨模态对齐的弱性。我们从数据和模型的角度解决这个问题。首先，我们策划了ConsIDVid，这是一个大型对象中心数据集，采用可扩展管道构建高质量、时间对齐的视频，并建立了ConsIDVid-Bench，在这里我们提出了一种新颖的基准和评估框架，用于使用对细微几何和外观偏差敏感的指标进行多视图一致性评估。我们进一步提出了ConsID-Gen，这是一种视图辅助的I2V生成框架，通过未摆姿的辅助视图增强第一帧，并通过双流视觉-几何编码器以及文本-视觉连接器融合语义和结构线索，为扩散变换器主干提供统一的条件。ConsIDVid-Bench上的实验表明，ConsID-Gen在多个指标上始终表现优异，最佳整体性能超越了领先的视频生成模型，如Wan2.1和HunyuanVideo，在具有挑战性的现实场景中提供了卓越的身份保真度和时间一致性。我们将在https://myangwu.github.io/ConsID-Gen发布我们的模型和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image-to-video (I2V) generation by addressing the challenges of preserving object identity and avoiding appearance drift and geometric distortion in videos created from static images. The authors developed a large-scale dataset called ConsIDVid and a benchmarking framework, ConsIDVid-Bench, to evaluate multi-view consistency. They proposed a novel I2V generation framework, ConsID-Gen, which utilizes unposed auxiliary views and a dual-stream visual-geometric encoder to enhance the generation process. Experimental results show that ConsID-Gen outperforms existing models in terms of identity fidelity and temporal coherence, demonstrating superior performance in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善图像到视频生成（I2V），解决在从静态图像生成视频时保持物体身份和避免外观漂移及几何失真的挑战。作者整理了一个名为ConsIDVid的大规模数据集，并建立了一个基准框架ConsIDVid-Bench，以评估多视角一致性。他们提出了一种新颖的I2V生成框架ConsID-Gen，该框架利用辅助视图和双流视觉几何编码器来增强生成过程。实验结果表明，ConsID-Gen在身份保真度和时间一致性方面优于现有模型，尤其是在复杂的现实场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Olaf-World: Orienting Latent Actions for Video World Modeling</div>
<div class="meta-line">Authors: Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou</div>
<div class="meta-line">First: 2026-02-10T18:58:41+00:00 · Latest: 2026-02-10T18:58:41+00:00</div>
<div class="meta-line">Comments: Project page: https://showlab.github.io/Olaf-World/ Code: https://github.com/showlab/Olaf-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10104v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10104v1">PDF</a> · <a href="https://github.com/showlab/Olaf-World">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://showlab.github.io/Olaf-World/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Olaf-World：面向视频世界建模的潜在动作定向</div>
<div class="mono" style="margin-top:8px">可控动作世界模型的扩展受到动作标签稀缺的限制。尽管潜在动作学习承诺从未标记的视频中提取控制接口，但学习到的潜在动作往往无法在不同上下文中转移：它们纠缠于场景特定的线索，缺乏共享坐标系统。这是因为标准目标仅在每个片段内操作，未提供跨上下文对齐动作语义的机制。我们的关键见解是，尽管动作是未观察到的，但它们的语义效果是可观察的，可以作为共享参考。我们引入了Seq$Δ$-REPA，一种序列级控制效果对齐目标，将集成的潜在动作锚定到来自冻结的自监督视频编码器的时间特征差异上。在此基础上，我们提出了Olaf-World，一个从大规模被动视频中预训练动作条件视频世界模型的管道。大量实验表明，我们的方法学习到更结构化的潜在动作空间，导致比最先进的基线更强的零-shot动作转移和更高的数据效率适应新控制接口。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of scaling action-controllable world models due to the lack of action labels, which hampers the transferability of learned latent actions across different contexts. The authors propose a novel method called Seq$Δ$-REPA, which aligns latent actions with observable semantic effects using a sequence-level control-effect alignment objective based on a frozen, self-supervised video encoder. Experimental results show that Olaf-World, the proposed pipeline for pretraining action-conditioned video world models from large-scale passive video, achieves a more structured latent action space, resulting in improved zero-shot action transfer and enhanced data efficiency in adapting to new control interfaces compared to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于缺乏动作标签而限制可控世界模型扩展的问题，这导致学习的潜在动作在不同上下文中的可转移性较差。作者提出了Seq$Δ$-REPA，一种序列级控制效果对齐目标，利用未观察到的动作的可观察语义效果创建共享参考，以对齐潜在动作。实验结果表明，他们在Olaf-World管道中实现的方法实现了更结构化的潜在动作空间，从而在零-shot动作转移和适应新控制接口的数据效率方面优于现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">VideoWorld 2: Learning Transferable Knowledge from Real-world Videos</div>
<div class="meta-line">Authors: Zhongwei Ren, Yunchao Wei, Xiao Yu, Guixun Luo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin</div>
<div class="meta-line">First: 2026-02-10T18:58:19+00:00 · Latest: 2026-02-10T18:58:19+00:00</div>
<div class="meta-line">Comments: Code and models are released at: https://maverickren.github.io/VideoWorld2.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10102v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10102v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://maverickren.github.io/VideoWorld2.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoWorld 2：从真实视频中学习可转移知识</div>
<div class="mono" style="margin-top:8px">从未标记的视频数据中学习可转移知识并在新环境中应用是智能体的一项基本能力。本研究提出了VideoWorld 2，扩展了VideoWorld，并首次探讨了直接从原始真实视频中学习可转移知识。在其核心，VideoWorld 2引入了一种动态增强的潜在动态模型（dLDM），将动作动态与视觉外观解耦：一个预训练的视频扩散模型处理视觉外观建模，使dLDM能够学习专注于紧凑且有意义的任务相关动态的潜在编码。这些潜在编码随后以自回归方式建模，以学习任务策略并支持长时间推理。我们在具有挑战性的真实手工制作任务上评估VideoWorld 2，之前的视频生成和潜在动态模型在可靠性方面存在困难。值得注意的是，VideoWorld 2在任务成功率上提高了多达70%，并生成连贯的长执行视频。在机器人领域，我们展示了VideoWorld 2可以从Open-X数据集中获取有效的操作知识，显著提高CALVIN上的任务表现。本研究揭示了直接从原始视频中学习可转移世界知识的潜力，所有代码、数据和模型将开源以供进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enable intelligent agents to learn transferable knowledge from unlabeled real-world video data for application in new environments. The authors present VideoWorld 2, which utilizes a dynamic-enhanced Latent Dynamics Model (dLDM) that separates action dynamics from visual appearance, employing a pretrained video diffusion model for visual modeling. Experimental results demonstrate that VideoWorld 2 achieves up to a 70% improvement in task success rates on complex handcraft making tasks and effectively acquires manipulation knowledge from the Open-X dataset, significantly enhancing performance on the CALVIN platform.</div>
<div class="mono" style="margin-top:8px">本研究的动机是使智能体能够从未标记的真实世界视频数据中学习可转移的知识，以便在新环境中应用。作者提出了VideoWorld 2，采用动态增强的潜在动态模型（dLDM），将动作动态与视觉外观分离，利用预训练的视频扩散模型进行视觉建模。实验结果表明，VideoWorld 2在真实世界手工制作任务中的成功率提高了多达70%，并有效地从Open-X数据集中获取操作知识，提升了CALVIN上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy</div>
<div class="meta-line">Authors: Júlio Oliveira, Rodrigo Ferreira, André Riker, Glaucio H. S. Carvalho, Eirini Eleni Tsilopoulou</div>
<div class="meta-line">First: 2026-02-10T18:58:11+00:00 · Latest: 2026-02-10T18:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10100v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向可解释的联邦学习：理解差分隐私的影响</div>
<div class="mono" style="margin-top:8px">数据隐私和可解释人工智能（XAI）是现代机器学习系统的两个重要方面。为了增强数据隐私，最近的机器学习模型被设计为联邦学习（FL）系统。在此基础上，可以通过差分隐私（DP）添加额外的隐私层。另一方面，为了提高可解释性，机器学习必须考虑更可解释的方法，减少特征数量并简化内部架构。在这种背景下，本文旨在实现一个结合增强数据隐私和可解释性的机器学习（ML）模型。因此，我们提出了一种名为带有差分隐私的联邦可解释树（FEXT-DP）的FL解决方案： (i) 基于决策树，因为它们比基于神经网络的FL系统更轻量且具有更好的可解释性； (ii) 通过将差分隐私（DP）应用于基于树的模型提供额外的数据隐私保护层。然而，添加DP会产生副作用：它会损害系统的可解释性。因此，本文还展示了DP保护对机器学习模型可解释性的影响。进行的性能评估显示，FEXT-DP在训练速度（即轮次数量）、均方误差和可解释性方面有所改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance data privacy and explainability in machine learning systems, particularly through Federated Learning (FL) and Differential Privacy (DP). The authors propose a novel approach called Federated EXplainable Trees with Differential Privacy (FEXT-DP), which utilizes Decision Trees for their interpretability and lightweight nature while incorporating DP for added privacy protection. Experimental results indicate that FEXT-DP achieves faster training times, improved Mean Squared Error, and maintains a level of explainability, although the introduction of DP does negatively impact the model&#x27;s explainability to some extent.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强机器学习系统中的数据隐私和可解释性，特别是通过联邦学习（FL）和差分隐私（DP）。作者提出了一种新方法，称为带有差分隐私的联邦可解释树（FEXT-DP），该方法利用决策树的轻量特性和相较于基于神经网络的FL系统更优的可解释性。实验结果表明，FEXT-DP在训练时间、均方误差方面有所改善，并保持了一定的可解释性，尽管引入DP在一定程度上对模型的可解释性产生了负面影响。</div>
</details>
</div>
<div class="card">
<div class="title">Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders</div>
<div class="meta-line">Authors: Amandeep Kumar, Vishal M. Patel</div>
<div class="meta-line">First: 2026-02-10T18:58:04+00:00 · Latest: 2026-02-10T18:58:04+00:00</div>
<div class="meta-line">Comments: Technical Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10099v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10099v1">PDF</a> · <a href="https://github.com/amandpkr/RJF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流形学习：通过表示编码器解锁标准扩散变换器</div>
<div class="mono" style="margin-top:8px">利用表示编码器进行生成建模为高效、高保真合成提供了一条路径。然而，标准扩散变换器无法直接在这些表示上收敛。尽管最近的研究将此归因于容量瓶颈，提出了计算成本高昂的扩散变换器宽度扩展，但我们证明这一失败在根本上是几何性的。我们将几何干扰确定为根本原因：标准欧几里得流匹配强迫概率路径穿过表示编码器的超球面特征空间的低密度内部，而不是沿着流形表面。为了解决这个问题，我们提出了带有雅可比正则化的黎曼流匹配（RJF）。通过将生成过程约束在流形测地线并纠正因曲率引起的误差传播，RJF使标准扩散变换器架构能够在不扩展宽度的情况下收敛。我们的方法RJF使标准DiT-B架构（131M参数）有效收敛，达到FID 3.37，而之前的方法未能收敛。代码：https://github.com/amandpkr/RJF</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the convergence of standard diffusion transformers when utilizing representation encoders for generative modeling, as previous methods have struggled due to a capacity bottleneck. The authors introduce Riemannian Flow Matching with Jacobi Regularization (RJF) as a novel method that addresses the geometric issues causing convergence failures by constraining the generative process to follow the manifold geodesics. Experimental results demonstrate that RJF allows the standard DiT-B architecture to converge effectively, achieving a Fréchet Inception Distance (FID) of 3.37, significantly outperforming prior methods that could not converge.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用表示编码器高效合成高保真生成模型的挑战，因为标准扩散变换器在这些表示上难以收敛。作者确定几何干扰是根本问题，传统的欧几里得流匹配将概率路径引导通过超球面特征空间的低密度区域，而不是沿着流形表面。为此，他们提出了带有雅可比正则化的黎曼流匹配（RJF），该方法将生成过程约束在流形测地线上，并纠正曲率引起的误差，从而使标准扩散变换器架构能够有效收敛，达到3.37的Fréchet Inception Distance（FID），显著优于之前未能收敛的方法。</div>
</details>
</div>
<div class="card">
<div class="title">VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model</div>
<div class="meta-line">Authors: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin, Zhibo Chen</div>
<div class="meta-line">First: 2026-02-10T18:58:01+00:00 · Latest: 2026-02-10T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLA-JEPA：通过潜在世界模型增强视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">在互联网规模的视频上预训练视觉-语言-动作（VLA）策略是有吸引力的，但当前的潜在动作目标往往学习错误的内容：它们仍然依赖于像素变化而非与动作相关的状态转变，使其容易受到外观偏差、干扰运动和信息泄漏的影响。我们引入了VLA-JEPA，一种JEPA风格的预训练框架，旨在设计上避免这些陷阱。关键思想是“无泄漏状态预测”：目标编码器从未来帧生成潜在表示，而学生路径仅看到当前观察——未来信息仅用作监督目标，绝不作为输入。通过在潜在空间而非像素空间进行预测，VLA-JEPA学习到对相机运动和无关背景变化具有鲁棒性的动态抽象。这产生了一个简单的两阶段方案——JEPA预训练后跟随动作头微调——而没有先前潜在动作管道的多阶段复杂性。在LIBERO、LIBERO-Plus、SimplerEnv和真实世界操作任务上的实验表明，VLA-JEPA在泛化和鲁棒性方面相较于现有方法取得了一致的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the pretraining of Vision-Language-Action (VLA) models, which often suffer from biases due to their reliance on pixel variations rather than meaningful action-relevant state transitions. The authors propose VLA-JEPA, a pretraining framework that utilizes a leakage-free state prediction approach, where a target encoder generates latent representations from future frames while the student pathway only accesses current observations, ensuring that future information serves solely as supervision. Experimental results demonstrate that VLA-JEPA significantly enhances generalization and robustness across various benchmarks, including LIBERO, LIBERO-Plus, SimplerEnv, and real-world manipulation tasks, outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉-语言-动作（VLA）模型的预训练，这些模型通常因依赖像素变化而非与动作相关的状态转变而受到偏见。作者提出了VLA-JEPA，这是一种预训练框架，采用无泄漏状态预测方法，其中目标编码器从未来帧生成潜在表示，而学生路径仅访问当前观察。实验结果表明，VLA-JEPA在LIBERO、LIBERO-Plus、SimplerEnv和真实世界操作任务等各种任务中显著提高了泛化能力和鲁棒性，优于现有方法，并采用了更简单的两阶段训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">Step-resolved data attribution for looped transformers</div>
<div class="meta-line">Authors: Georgios Kaissis, David Mildenberger, Juan Felipe Gomez, Martin J. Menten, Eleni Triantafillou</div>
<div class="meta-line">First: 2026-02-10T18:57:53+00:00 · Latest: 2026-02-10T18:57:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10097v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10097v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>循环变换器的逐步分解数据归因</div>
<div class="mono" style="margin-top:8px">我们研究了单个训练示例如何影响循环变换器的内部计算，其中一个共享块在 $τ$ 次递归迭代中应用，以实现潜在推理。现有的训练数据影响估计器如 TracIn 产生一个单一的标量分数，聚合所有循环迭代，模糊了训练示例在递归计算中的重要时刻。我们引入了 \textit{逐步分解影响 (SDI)}，通过展开递归计算图并将影响归因于特定循环迭代，将 TracIn 分解为长度为 $τ$ 的影响轨迹。为了使 SDI 在变换器规模上实用，我们提出了一种 TensorSketch 实现，永远不生成每个示例的梯度。在循环 GPT 风格模型和算法推理任务上的实验表明，SDI 具有出色的扩展性，低误差地匹配全梯度基线，并支持广泛的数据归因和可解释性任务，提供对潜在推理过程的逐步洞察。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates how individual training examples influence the internal computations of looped transformers, which utilize recurrent iterations for latent reasoning. The authors introduce a novel method called Step-Decomposed Influence (SDI) that decomposes existing influence estimators into a detailed influence trajectory across loop iterations, allowing for specific attribution of influence to individual steps in the computation. Experimental results demonstrate that SDI effectively scales with transformer models, aligns closely with full-gradient baselines while maintaining low error rates, and provides valuable insights into data attribution and interpretability across various reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了单个训练样本如何影响循环变压器的内部计算，这些变压器在多个迭代中使用共享块进行潜在推理。作者提出了一种新方法，称为步骤分解影响（SDI），该方法将现有的影响估计器如TracIn分解为跨循环迭代的详细影响轨迹，从而允许在每个步骤上具体归因影响。实验结果表明，SDI在变压器模型中有效扩展，与全梯度基线紧密对齐，同时保持低误差率，并为算法推理任务中的数据归因和可解释性提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Causality in Video Diffusers is Separable from Denoising</div>
<div class="meta-line">Authors: Xingjian Bai, Guande He, Zhengqi Li, Eli Shechtman, Xun Huang, Zongze Wu</div>
<div class="meta-line">First: 2026-02-10T18:57:21+00:00 · Latest: 2026-02-10T18:57:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10095v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10095v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频扩散中的因果关系与去噪分离</div>
<div class="mono" style="margin-top:8px">因果关系——指组件之间的时间性单向因果关系——是许多复杂生成过程的基础，包括视频、语言和机器人轨迹。目前的因果扩散模型将时间推理与迭代去噪纠缠在一起，在每个去噪步骤和整个上下文中对所有层应用因果注意力。本文表明，这些模型中的因果推理可以与多步骤去噪过程分离。通过对自回归视频扩散器的系统探测，我们发现了两个关键规律：（1）早期层在去噪步骤中产生高度相似的特征，表明在扩散轨迹上存在冗余计算；（2）深层层次表现出稀疏的跨帧注意力，主要进行帧内渲染。基于这些发现，我们引入了可分离因果扩散（SCD），一种新架构，通过因果变换器编码器显式地将每帧的时间推理与通过轻量级扩散解码器的多步骤帧渲染解耦。对合成和真实基准的预训练和后训练任务的广泛实验表明，SCD显著提高了吞吐量和每帧延迟，同时匹配或超过了强因果扩散基线的生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the role of causality in generative processes, particularly in video diffusion models where temporal reasoning is often intertwined with denoising. The authors propose a new architecture called Separable Causal Diffusion (SCD) that separates causal reasoning from the denoising process by utilizing a causal transformer encoder for temporal reasoning and a lightweight diffusion decoder for rendering. Experimental results demonstrate that SCD enhances throughput and reduces per-frame latency while achieving comparable or superior generation quality compared to existing causal diffusion models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前因果扩散模型中因果推理与迭代去噪的纠缠，这使得视频和其他领域的生成过程变得复杂。作者提出了一种新的架构，称为可分离因果扩散（SCD），通过使用因果变换器编码器进行时间推理，并使用轻量级扩散解码器进行渲染，从而将时间推理与多步去噪解耦。实验结果表明，SCD显著提高了吞吐量并减少了每帧延迟，同时保持或超过了现有因果扩散模型的生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning</div>
<div class="meta-line">Authors: Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, Wenzhe Jiao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T18:22:30+00:00 · Latest: 2026-02-10T18:56:09+00:00</div>
<div class="meta-line">Comments: Published at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01278v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01278v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Positive-Unlabeled (PU) learning aims to train a binary classifier (positive vs. negative) where only limited positive data and abundant unlabeled data are available. While widely applicable, state-of-the-art PU learning methods substantially underperform their supervised counterparts on complex datasets, especially without auxiliary negatives or pre-estimated parameters (e.g., a 14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the challenge of learning discriminative representations under unreliable supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU learning framework that requires no auxiliary information. NcPU combines a noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns intra-class representations despite unreliable supervision, with a phantom label disambiguation (PLD) scheme that supplies conservative negative supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can iteratively benefit each other from the perspective of the Expectation-Maximization framework. Empirically, extensive experiments demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive performance; and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging datasets on post-disaster building damage mapping, highlighting its promise for real-world applications. Code: Code will be open-sourced after review.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声对鲁棒表示对齐的正无标学习</div>
<div class="mono" style="margin-top:8px">正无标（PU）学习旨在训练一个二元分类器（正类与负类），其中仅有有限的正类数据和丰富的无标数据可用。尽管广泛适用，最先进的PU学习方法在复杂数据集上的表现显著低于其监督学习的对应方法，尤其是在没有辅助负类或预估参数的情况下（例如，在CIFAR-100数据集上存在14.26%的差距）。我们将主要瓶颈识别为在不可靠监督下学习判别表示的挑战。为了解决这一挑战，我们提出了NcPU，一个不需要辅助信息的非对比PU学习框架。NcPU结合了一个噪声对鲁棒监督非对比损失（NoiSNCL），该损失在不可靠监督下对齐类内表示，以及一个通过基于遗憾的标签更新提供保守负监督的幻影标签消歧（PLD）方案。从期望最大化框架的角度来看，理论上，NoiSNCL和PLD可以相互迭代受益。经验上，大量实验表明：（1）NoiSNCL使简单的PU方法能够实现竞争性能；（2）NcPU在各种数据集上显著优于最先进的PU方法，包括灾后建筑损坏映射等具有挑战性的数据集，突显了其在现实世界应用中的潜力。代码：代码将在审核后开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Positive-Unlabeled (PU) learning, which struggles to match the performance of supervised learning methods on complex datasets due to the lack of reliable negative samples. The authors propose NcPU, a non-contrastive PU learning framework that integrates a noisy-pair robust supervised non-contrastive loss (NoiSNCL) and a phantom label disambiguation (PLD) scheme to enhance representation learning without requiring auxiliary information. Experimental results show that NcPU significantly outperforms existing PU methods across various datasets, including challenging applications like post-disaster building damage mapping, thereby demonstrating its effectiveness in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善正负标记学习（PU学习），由于监督不可靠，PU学习在复杂数据集上的表现难以与监督学习方法相匹敌。作者提出了NcPU，这是一种非对比的PU学习框架，结合了噪声对鲁棒监督非对比损失（NoiSNCL）和幻影标签消歧方案（PLD），使得在不需要辅助信息的情况下有效对齐表示。实验结果表明，NcPU显著提升了PU学习方法在多个数据集上的表现，包括与灾后建筑损坏映射相关的数据集，从而展示了其在实际应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He</div>
<div class="meta-line">First: 2026-02-10T18:55:41+00:00 · Latest: 2026-02-10T18:55:41+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10090v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10090v1">PDF</a> · <a href="https://github.com/Snowflake-Labs/agent-world-model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理世界模型：用于代理强化学习的无限合成环境</div>
<div class="mono" style="margin-top:8px">最近大型语言模型（LLM）的进展使自主代理能够执行需要与工具和环境进行多轮交互的复杂任务。然而，扩展此类代理训练受到缺乏多样化和可靠环境的限制。本文提出了代理世界模型（AWM），一个完全合成的环境生成管道。通过该管道，我们扩展到1,000个涵盖日常场景的环境，代理可以与丰富的工具集（每个环境平均35个工具）进行交互并获得高质量的观察。值得注意的是，这些环境是代码驱动的，并由数据库支持，提供比LLM模拟的环境更可靠和一致的状态转移。此外，与从现实环境中收集轨迹相比，它们还使代理交互更高效。为了证明这一资源的有效性，我们对多轮工具使用代理进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还可以设计可靠的奖励函数。在三个基准上的实验表明，仅在合成环境中训练，而不是特定基准的环境，能够产生强大的分布外泛化。代码可在 https://github.com/Snowflake-Labs/agent-world-model 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations in scaling agent training due to the lack of diverse and reliable environments for autonomous agents. The authors propose the Agent World Model (AWM), a synthetic environment generation pipeline that creates 1,000 diverse environments with rich toolsets for agents to interact with. Experimental results demonstrate that training agents exclusively in these synthetic environments leads to strong out-of-distribution generalization, outperforming traditional benchmark-specific training methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由于缺乏多样化和可靠环境而限制自主体训练规模的问题。作者提出了代理世界模型（AWM），这是一种完全合成的环境生成管道，创建了1000个多样化的环境，供代理与丰富的工具集进行交互。实验结果表明，专门在这些合成环境中训练代理能够实现强大的分布外泛化，优于传统的基准特定训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">Story-Iter: A Training-free Iterative Paradigm for Long Story Visualization</div>
<div class="meta-line">Authors: Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, Zeyu Zheng, Zirui Wang, Cihang Xie, Yuyin Zhou</div>
<div class="meta-line">First: 2024-10-08T17:59:30+00:00 · Latest: 2026-02-10T18:53:34+00:00</div>
<div class="meta-line">Comments: 31 pages, 33 figures, The project page and associated code can be accessed via https://jwmao1.github.io/storyiter/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06244v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.06244v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jwmao1.github.io/storyiter/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces Story-Iter, a new training-free iterative paradigm to enhance long-story generation. Unlike existing methods that rely on fixed reference images to construct a complete story, our approach features a novel external iterative paradigm, extending beyond the internal iterative denoising steps of diffusion models, to continuously refine each generated image by incorporating all reference images from the previous round. To achieve this, we propose a plug-and-play, training-free global reference cross-attention (GRCA) module, modeling all reference frames with global embeddings, ensuring semantic consistency in long sequences. By progressively incorporating holistic visual context and text constraints, our iterative paradigm enables precise generation with fine-grained interactions, optimizing the story visualization step-by-step. Extensive experiments in the official story visualization dataset and our long story benchmark demonstrate that Story-Iter&#x27;s state-of-the-art performance in long-story visualization (up to 100 frames) excels in both semantic consistency and fine-grained interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Story-Iter：一种无训练的长故事可视化迭代范式</div>
<div class="mono" style="margin-top:8px">本文介绍了Story-Iter，一种新的无训练的迭代范式，以增强长故事生成。与依赖固定参考图像构建完整故事的现有方法不同，我们的方法具有新颖的外部迭代范式，超越了扩散模型的内部迭代去噪步骤，通过结合前一轮的所有参考图像，持续优化每个生成图像。为此，我们提出了一种即插即用的无训练全局参考交叉注意力（GRCA）模块，使用全局嵌入建模所有参考帧，确保长序列中的语义一致性。通过逐步结合整体视觉上下文和文本约束，我们的迭代范式实现了精确生成与细粒度交互，逐步优化故事可视化。在官方故事可视化数据集和我们的长故事基准上的大量实验表明，Story-Iter在长故事可视化（最多100帧）中的最先进性能在语义一致性和细粒度交互方面均表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve long-story visualization, which has been limited by existing methods that depend on fixed reference images. The authors introduce Story-Iter, a training-free iterative paradigm that utilizes a global reference cross-attention (GRCA) module to refine generated images by integrating all reference images from previous iterations. Experimental results demonstrate that Story-Iter achieves state-of-the-art performance in long-story visualization, effectively maintaining semantic consistency and enabling fine-grained interactions across sequences of up to 100 frames.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善长故事可视化，而现有方法依赖固定参考图像，限制了其效果。作者提出了Story-Iter，这是一种无训练的迭代范式，利用全局参考交叉注意力（GRCA）模块，通过整合所有先前的参考图像来优化生成图像。实验结果表明，Story-Iter在长故事可视化方面达到了最先进的性能，在官方数据集和新的长故事基准测试中，展示了在语义一致性和细粒度交互方面的显著改善，支持多达100帧的生成。</div>
</details>
</div>
<div class="card">
<div class="title">CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</div>
<div class="meta-line">Authors: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully</div>
<div class="meta-line">First: 2026-02-10T18:51:39+00:00 · Latest: 2026-02-10T18:51:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10085v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/code-sharp/homepage}{here}$">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CODE-SHARP：作为层次奖励程序的持续开放式技能发现与演化</div>
<div class="mono" style="margin-top:8px">开发能够开放式发现和学习新技能的智能体是人工智能中的一项重大挑战。尽管强化学习为训练智能体掌握复杂技能提供了强大的框架，但它通常依赖于手工设计的奖励函数。这对于开放式技能发现来说是不可行的，因为有意义的技能集合并不事先已知。尽管最近的方法在自动化奖励函数设计方面显示出良好的结果，但它们仍然局限于为预定义任务优化奖励。为了解决这一限制，我们引入了作为层次奖励程序的持续开放式技能发现与演化（CODE-SHARP），这是一个新颖的框架，利用基础模型（FM）开放式扩展和优化层次技能档案，结构为可执行奖励函数的有向图。我们展示了一个仅在发现的SHARP技能生成的奖励上训练的目标条件智能体能够在Craftax环境中解决越来越长的目标。当由基于FM的高层规划器组合时，发现的技能使得单个目标条件智能体能够解决复杂的长时间任务，平均超越预训练智能体和任务特定专家策略超过134%。我们将开源我们的代码并提供额外的视频$\href{https://sites.google.com/view/code-sharp/homepage}{这里}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop agents capable of open-ended skill discovery, which is a significant challenge in Artificial Intelligence, particularly when traditional reinforcement learning methods rely on predefined reward functions. The authors propose a novel framework called CODE-SHARP, which utilizes Foundation Models to continuously discover and evolve a hierarchical archive of skills structured as a directed graph of executable reward functions. Experimental results demonstrate that a goal-conditioned agent trained on rewards from the discovered skills can effectively solve increasingly complex long-horizon tasks in the Craftax environment, achieving over 134% improvement compared to pretrained agents and task-specific expert policies on average.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于开发能够持续发现和学习新技能的智能体，而不依赖于预定义的奖励函数，这在人工智能领域是一个重大挑战。作者提出了CODE-SHARP框架，利用基础模型创建和完善一个以可执行奖励函数的有向图表示的技能层次档案。实验结果表明，仅基于发现的技能奖励训练的目标条件智能体能够有效地解决Craftax环境中的复杂长远任务，平均相比于预训练智能体和特定任务专家策略提高了超过134%。</div>
</details>
</div>
<div class="card">
<div class="title">CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment</div>
<div class="meta-line">Authors: Nanda Rani, Kimberly Milner, Minghao Shao, Meet Udeshi, Haoran Xi, Venkata Sai Charan Putrevu, Saksham Aggarwal, Sandeep K. Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Muhammad Shafique, Ramesh Karri</div>
<div class="meta-line">First: 2026-02-08T15:56:22+00:00 · Latest: 2026-02-10T18:48:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08023v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08023v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CyberExplorer：在真实攻击模拟环境中评估LLM的进攻安全能力</div>
<div class="mono" style="margin-top:8px">现实世界的进攻安全操作本质上是开放式的：攻击者探索未知的攻击面，在不确定性下修正假设，并在没有保证成功的情况下进行操作。现有基于LLM的进攻代理评估依赖于具有预定义目标和二元成功标准的封闭世界设置。为了解决这一差距，我们引入了CyberExplorer，一个评估套件，包含两个核心组件：（1）一个开放环境基准，建立在一个虚拟机上，托管40个来自真实CTF挑战的易受攻击的网络服务，代理在没有先前知识的情况下自主进行侦察、目标选择和利用；（2）一个支持动态探索的反应式多代理框架，无需预定义计划。CyberExplorer使得评估超越标志恢复，捕捉交互动态、协调行为、失败模式和漏洞发现信号——弥合基准测试与现实多目标攻击场景之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of large language model (LLM) offensive security capabilities in realistic scenarios, as existing methods rely on closed-world settings with predefined goals. The authors developed CyberExplorer, an evaluation suite featuring an open-environment benchmark with a virtual machine containing 40 vulnerable web services and a reactive multi-agent framework that allows for dynamic exploration without predefined plans. Key experimental findings indicate that CyberExplorer facilitates a more nuanced assessment of offensive security operations by capturing interaction dynamics, coordination behavior, and vulnerability discovery signals, thus better reflecting real-world multi-target attack scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型语言模型（LLM）在更现实和动态环境中进行攻防安全能力评估的有效性，因为现有评估仅限于具有固定目标的封闭世界设置。作者开发了CyberExplorer评估套件，其中包括一个开放环境基准，具有一个虚拟机，托管40个脆弱的网络服务，以及一个反应式多代理框架，允许代理在没有先前知识的情况下自主探索和利用漏洞。主要实验结果表明，CyberExplorer促进了对代理交互、协调和漏洞发现的细致评估，从而提供了对现实世界攻防安全操作复杂性的深入洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Anagent For Enhancing Scientific Table &amp; Figure Analysis</div>
<div class="meta-line">Authors: Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang</div>
<div class="meta-line">First: 2026-02-10T18:46:28+00:00 · Latest: 2026-02-10T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10081v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xhguo7.github.io/Anagent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \&amp; figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \&amp; figure analysis. Our project page: https://xhguo7.github.io/Anagent/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强科学表格与图形分析的Anagent</div>
<div class="mono" style="margin-top:8px">在科学研究中，分析需要准确解读复杂的多模态知识，整合来自不同来源的证据，并基于特定领域的知识进行推理。然而，当前的人工智能（AI）系统在持续展示这些能力方面存在困难。科学表格和图形的复杂性和变异性，加上异构结构和长上下文要求，构成了科学表格与图形分析的基本障碍。为了量化这些挑战，我们引入了AnaBench，这是一个大规模基准，包含来自九个科学领域的$63,178$个实例，系统地按七个复杂性维度进行分类。为了解决这些挑战，我们提出了Anagent，一个通过四个专门代理增强科学表格与图形分析的多代理框架：Planner将任务分解为可操作的子任务，Expert通过针对性工具执行检索任务特定信息，Solver综合信息生成连贯分析，Critic通过五维质量评估进行迭代优化。我们进一步开发了模块化训练策略，利用监督微调和专门的强化学习来优化个体能力，同时保持有效的协作。对170个子领域的全面评估表明，Anagent在无训练设置下实现了高达$\uparrow 13.43\%$的显著提升，在微调下实现了高达$\uparrow 42.12\%$的提升，同时揭示了面向任务的推理和上下文感知的问题解决对高质量科学表格与图形分析的重要性。我们的项目页面：https://xhguo7.github.io/Anagent/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by current AI systems in accurately interpreting complex multimodal knowledge in scientific tables and figures. The authors introduce AnaBench, a benchmark with 63,178 instances from nine scientific domains, categorized by complexity dimensions, to quantify these challenges. They propose Anagent, a multi-agent framework that includes specialized agents for task decomposition, information retrieval, synthesis, and iterative refinement. Experimental results show that Anagent significantly improves performance, achieving up to 13.43% enhancement in training-free settings and 42.12% with finetuning, highlighting the importance of task-oriented reasoning and context-aware problem-solving in scientific analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前人工智能系统在准确分析复杂科学表格和图形方面所面临的挑战，这些分析需要整合多模态知识和领域特定的推理。作者提出了Anagent，一个多代理框架，采用四个专业代理来增强分析：Planner、Expert、Solver和Critic，各自负责任务的不同方面。实验结果表明，Anagent显著提高了性能，在无训练设置下提升了最多13.43%，而经过微调则提升了42.12%，突显了任务导向推理和上下文感知问题解决在科学分析中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach</div>
<div class="meta-line">Authors: Soumyaroop Nandi, Prem Natarajan</div>
<div class="meta-line">First: 2026-02-10T18:46:04+00:00 · Latest: 2026-02-10T18:46:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10079v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像拼接和复制移动伪造能否通过同一模型检测？Forensim：一种基于注意力的状态空间方法</div>
<div class="mono" style="margin-top:8px">我们介绍了Forensim，一种基于注意力的状态空间框架，用于图像伪造检测，能够同时定位被操纵（目标）和源区域。与传统方法仅依赖伪影线索检测拼接或伪造区域不同，Forensim旨在捕捉对理解上下文至关重要的重复模式。在抗议图像等场景中，仅检测伪造区域，例如插入和平人群中的重复暴力行为，可能会误导解读，突显了联合源-目标定位的必要性。Forensim输出三类掩码（原始、源、目标），并支持在统一架构中检测拼接和复制移动伪造。我们提出了一种视觉状态空间模型，利用归一化的注意力图识别内部相似性，并配合基于区域的块注意力模块区分被操纵区域。该设计实现了端到端训练和精确定位。Forensim在标准基准测试中达到了最先进的性能。我们还发布了CMFD-Anything，一个新的数据集，解决了现有复制移动伪造数据集的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective detection of image forgery, particularly in contexts where understanding both manipulated and source regions is crucial for accurate interpretation. The authors introduce Forensim, an attention-based state-space framework that jointly localizes both target and source regions in images, moving beyond traditional methods that focus solely on artifact cues. The experimental results demonstrate that Forensim achieves state-of-the-art performance on standard benchmarks for detecting both splicing and copy-move forgeries, while also providing a new dataset, CMFD-Anything, to address existing limitations in the field.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效检测图像伪造，特别是在理解被操纵区域和源区域对准确解读至关重要的背景下。作者提出了Forensim，这是一种基于注意力的状态空间框架，能够同时定位目标和源区域，并检测拼接和复制移动伪造。关键实验结果表明，Forensim在标准基准测试中实现了最先进的性能，并有效输出三类掩膜，从而实现对操纵区域的精确定位，并通过引入CMFD-Anything解决了现有数据集的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals</div>
<div class="meta-line">Authors: Puqi Zhou, Ali Asgarov, Aafiya Hussain, Wonjoon Park, Amit Paudyal, Sameep Shrestha, Chia-wei Tang, Michael F. Lighthiser, Michael R. Hieb, Xuesu Xiao, Chris Thomas, Sungsoo Ray Hong</div>
<div class="meta-line">First: 2026-02-09T16:43:37+00:00 · Latest: 2026-02-10T18:41:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08882v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08882v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals&#x27; burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>与公共安全专业人员共同设计多机器人地面视频感知</div>
<div class="mono" style="margin-top:8px">来自地面机器人车队的视频可以通过提供可扩展的情境意识和减轻专业人员的负担来促进公共安全。然而，关于如何将多机器人视频设计和整合到公共安全工作流程中知之甚少。我们与六个警察机构合作，研究了如何使这些视频变得实用。在研究1中，我们展示了多机器人地面视频感知的第一个测试平台。该测试平台包括38个与公共安全相关的事件（EoI），一个包含20个机器人巡逻视频（10对昼夜）的数据集，涵盖EoI类型，以及6个旨在改善当前视频感知实践的设计要求。在研究2中，我们构建了MRVS，一个通过提示工程视频理解模型增强多机器人巡逻视频流的工具。参与者报告了手动工作负担的减少和对基于LLM的解释的更大信心，同时指出了对误报和隐私的担忧。我们总结了对未来多机器人视频感知工具设计的启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance public safety by integrating multi-robot video feeds into existing workflows, addressing the gap in practical applications for such technologies. The study involved collaboration with six police agencies and included two main components: the development of a testbed featuring 38 events-of-interest and a dataset of 20 robot patrol videos, followed by the creation of a tool called MRVS that utilizes a prompt-engineered video understanding model. Key findings indicate that participants experienced a reduction in manual workload and increased confidence in the system&#x27;s explanations, although they expressed concerns regarding false alarms and privacy issues.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过将多机器人视频系统整合到现有工作流程中来增强公共安全，解决该领域缺乏实用设计知识的问题。研究与六个警察机构合作，分为两个主要阶段：第一阶段建立了一个测试平台，包含38个与公共安全相关的事件、20个机器人巡逻视频的数据集以及六个视频理解的设计要求。第二阶段引入了MRVS工具，通过提示工程模型改善视频流，参与者报告手动工作负担减少和对解释的信心增加，但也提出了对误报和隐私的担忧。</div>
</details>
</div>
<div class="card">
<div class="title">Data-efficient and Interpretable Inverse Materials Design using a Disentangled Variational Autoencoder</div>
<div class="meta-line">Authors: Cheng Zeng, Zulqarnain Khan, Nathan L. Post</div>
<div class="meta-line">Venue: AI Mater. 2025(1):0002</div>
<div class="meta-line">First: 2024-09-10T02:21:13+00:00 · Latest: 2026-02-10T18:34:17+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/cengc13/d_vae_hea</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.06740v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.06740v3">PDF</a> · <a href="https://github.com/cengc13/d_vae_hea">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse materials design has proven successful in accelerating novel material discovery. Many inverse materials design methods use unsupervised learning where a latent space is learned to offer a compact description of materials representations. A latent space learned this way is likely to be entangled, in terms of the target property and other properties of the materials. This makes the inverse design process ambiguous. Here, we present a semi-supervised learning approach based on a disentangled variational autoencoder to learn a probabilistic relationship between features, latent variables and target properties. This approach is data efficient because it combines all labelled and unlabelled data in a coherent manner, and it uses expert-informed prior distributions to improve model robustness even with limited labelled data. It is in essence interpretable, as the learnable target property is disentangled out of the other properties of the materials, and an extra layer of interpretability can be provided by a post-hoc analysis of the classification head of the model. We demonstrate this new approach on an experimental high-entropy alloy dataset with chemical compositions as input and single-phase formation as the single target property. High-entropy alloys were chosen as example materials because of the vast chemical space of their possible combinations of compositions and atomic configurations. While single property is used in this work, the disentangled model can be extended to customize for inverse design of materials with multiple target properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于解耦变分自编码器的数据高效且可解释的逆材料设计</div>
<div class="mono" style="margin-top:8px">逆材料设计已被证明在加速新材料发现方面取得成功。许多逆材料设计方法使用无监督学习，通过学习潜在空间来提供材料表示的紧凑描述。以这种方式学习的潜在空间可能在目标属性和材料的其他属性之间是纠缠的，这使得逆设计过程变得模糊。在这里，我们提出了一种基于解耦变分自编码器的半监督学习方法，以学习特征、潜在变量和目标属性之间的概率关系。这种方法数据高效，因为它以一致的方式结合了所有标记和未标记的数据，并使用专家信息的先验分布来提高模型的鲁棒性，即使在标记数据有限的情况下。本质上，它是可解释的，因为可学习的目标属性从材料的其他属性中解耦出来，并且通过对模型分类头的事后分析可以提供额外的可解释性。我们在一个实验高熵合金数据集上演示了这种新方法，以化学成分作为输入，单相形成作为单一目标属性。选择高熵合金作为示例材料是因为其可能的成分和原子配置组合的广泛化学空间。虽然本工作中使用了单一属性，但解耦模型可以扩展以定制具有多个目标属性的材料的逆设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency and interpretability of inverse materials design, which is crucial for accelerating novel material discovery. The authors propose a semi-supervised learning method utilizing a disentangled variational autoencoder to establish a probabilistic relationship between material features, latent variables, and target properties. Experimental results on a high-entropy alloy dataset indicate that this approach effectively disentangles the target property from other material properties, demonstrating improved data efficiency and robustness even with limited labeled data, while also allowing for post-hoc interpretability analysis of the model&#x27;s classification head.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高逆材料设计的效率和可解释性，这对加速新材料的发现至关重要。作者提出了一种基于解缠变分自编码器的半监督学习方法，以建立材料特征、潜变量和目标属性之间的概率关系。在高熵合金数据集上的实验结果表明，该方法有效地将目标属性与其他材料属性解缠，从而提高了模型的鲁棒性和可解释性，即使在标记数据有限的情况下，同时也为未来多目标属性的逆设计提供了适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-10T18:33:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征作为奖励：通过可解释性实现开放式任务的可扩展监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能够学习编码抽象概念（如事实性或意图）的特征。这些特征传统上用于测试时监控或引导。我们提出了一种替代方法：将特征作为开放式任务的可扩展监督。我们考虑减少幻觉的情况作为一种理想的、但开放式的行为，并设计了一个强化学习（RL）管道，称为RLFR（基于特征奖励的强化学习），该管道使用特征作为奖励函数。基于一种新颖的探测框架，该框架识别候选的幻觉声明，我们的管道教会模型在不确定其事实性时进行干预和纠正其完成。此外，该管道还实现了可扩展的测试时计算，再次由我们的奖励特征引导。该端到端过程在Gemma-3-12B-IT上实现，结果显示该策略的幻觉概率比原始模型低58%，同时在标准基准上保持性能。综上所述，通过将监督基于特征的语言，这篇论文引入了可解释性在学习开放式任务中的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of hallucination in language models, proposing a novel approach to supervision for open-ended tasks by utilizing features as reward functions. The authors developed a reinforcement learning pipeline, RLFR (Reinforcement Learning from Feature Rewards), which employs a probing framework to identify potential hallucinated claims and teaches the model to correct its outputs when uncertain about their factuality. Experimental results demonstrate that this method significantly reduces hallucination rates by 58% compared to the original model while maintaining performance on standard benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用语言模型学习到的特征作为开放式任务的可扩展监督，特别是在减少幻觉方面。作者提出了一种名为RLFR（基于特征奖励的强化学习）的强化学习管道，利用识别出的特征作为奖励函数来指导模型行为。实验结果表明，该方法应用于Gemma-3-12B-IT模型时，幻觉减少了58%，同时在标准基准测试中保持了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification</div>
<div class="meta-line">Authors: Jack Michael Solomon, Rishi Leburu, Matthias Chung</div>
<div class="meta-line">First: 2026-02-03T00:46:29+00:00 · Latest: 2026-02-10T18:33:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02948v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.
  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>变分稀疏配对自编码器（vsPAIR）用于逆问题和不确定性量化</div>
<div class="mono" style="margin-top:8px">逆问题是许多科学和工程学科的基础；当人们试图从噪声测量中重建隐藏的基础量时，就会出现逆问题。许多应用不仅需要点估计，还需要可解释的不确定性。在众多应用中，提供快速推断和不确定性估计仍然具有挑战性但又是可取的。
我们提出了变分稀疏配对自编码器（vsPAIR）来应对这一挑战。该架构将标准的变分自编码器（VAE）与稀疏VAE编码的感兴趣量配对，通过学习的潜在映射连接。变分结构使得不确定性估计成为可能，配对架构通过将感兴趣量（QoI）表示锚定到干净数据上来鼓励可解释性，而稀疏编码通过将信息集中到可识别的因素中而不是在所有维度上扩散来提供结构。为了验证我们提出的架构的有效性，我们在盲图像修复和计算机断层扫描上进行了实验，证明vsPAIR是一个能够提供可解释和结构化不确定性估计的有效逆问题求解器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenges of reconstructing hidden quantities from noisy measurements in inverse problems while providing interpretable uncertainty estimates. The authors propose the Variational Sparse Paired Autoencoder (vsPAIR), which combines a standard variational autoencoder (VAE) for observations with a sparse VAE for quantities of interest, linked through a learned latent mapping. Experimental results on blind inpainting and computed tomography show that vsPAIR effectively solves inverse problems and generates interpretable, structured uncertainty estimates.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效解决从噪声测量中重建隐藏量的挑战，同时提供可解释的不确定性估计，这在各种科学和工程应用中至关重要。作者提出了变分稀疏配对自编码器（vsPAIR），该模型将标准变分自编码器（VAE）与稀疏VAE相结合，通过学习的潜在映射连接。对盲图像修复和计算机断层扫描的实验结果表明，vsPAIR成功解决了逆问题，并提供了可解释和结构化的不确定性估计。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维链：使用自适应认知模式进行推理</div>
<div class="mono" style="margin-top:8px">人类解决问题从来不是单一思维模式的重复，我们所指的思维模式是指一种独特的认知处理方式。在处理特定任务时，我们并不依赖于单一思维模式；相反，我们在单一解决过程中整合多种思维模式。然而，现有的大型语言模型推理方法陷入了一个共同的陷阱：它们在所有步骤中应用相同的固定思维模式，忽视了解决同一问题的不同阶段需要根本不同的思维模式。这种单一思维的假设阻碍了模型达到更高的智能水平。为了解决这一局限性，我们提出了思维链（CoM），这是一个无训练的代理框架，能够实现逐步自适应思维模式的协调。CoM将推理分解为四种功能异质的思维模式：空间、收敛、发散和算法。一个元代理根据不断变化的推理状态动态选择最佳思维模式，而双向上下文门则过滤跨模块的信息流，以保持有效性和效率。跨越数学、代码生成、科学问答和空间推理的六个具有挑战性的基准实验表明，CoM实现了最先进的性能，在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash的整体准确率上分别超越了最强基线4.96\%和4.72\%，同时平衡了推理效率。我们的代码公开可用，链接为\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance human-like problem-solving in large language models (LLMs) by addressing the limitation of fixed mindsets in reasoning processes. The authors propose a novel framework called Chain of Mindset (CoM), which allows for adaptive orchestration of four distinct cognitive modes: Spatial, Convergent, Divergent, and Algorithmic. Through experiments on six challenging benchmarks, CoM demonstrates significant improvements in performance, achieving state-of-the-art results with a 4.96% and 4.72% increase in overall accuracy compared to the strongest baseline models, while also maintaining reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决大型语言模型（LLMs）在推理过程中固定思维模式的局限性，来增强类人问题解决能力。作者提出了一种新框架，称为思维链（CoM），该框架允许对四种不同的认知模式进行自适应编排：空间、收敛、发散和算法。在六个具有挑战性的基准测试中的实验表明，CoM表现出色，与最强基线模型相比，在整体准确率上提高了4.96%和4.72%，同时保持了推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">Vendi Novelty Scores for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Amey P. Pasarkar, Adji Bousso Dieng</div>
<div class="meta-line">First: 2026-02-10T18:30:29+00:00 · Latest: 2026-02-10T18:30:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Score (VNS), an OOD detector based on the Vendi Scores (VS), a family of similarity-based diversity metrics. VNS quantifies how much a test sample increases the VS of the in-distribution feature set, providing a principled notion of novelty that does not require density modeling. VNS is linear-time, non-parametric, and naturally combines class-conditional (local) and dataset-level (global) novelty signals. Across multiple image classification benchmarks and network architectures, VNS achieves state-of-the-art OOD detection performance. Remarkably, VNS retains this performance when computed using only 1% of the training data, enabling deployment in memory- or access-constrained settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于分布外检测的Vendi新颖性评分</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测对于机器学习系统的安全部署至关重要。现有的后验检测器通常依赖于模型置信度评分或特征空间中的似然估计，通常在限制性分布假设下进行。在本研究中，我们引入了第三种范式，并从多样性视角对OOD检测进行公式化。我们提出了Vendi新颖性评分（VNS），这是一种基于Vendi评分（VS）的OOD检测器，属于相似性基础的多样性度量家族。VNS量化了测试样本在多大程度上增加了分布内特征集的VS，提供了一种不需要密度建模的原则性新颖性概念。VNS是线性时间、非参数的，并自然结合了类条件（局部）和数据集级别（全局）新颖性信号。在多个图像分类基准和网络架构中，VNS实现了最先进的OOD检测性能。值得注意的是，当仅使用1%的训练数据计算时，VNS仍能保持这一性能，使其能够在内存或访问受限的环境中部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance out-of-distribution (OOD) detection for safer deployment of machine learning systems, as existing methods often rely on restrictive assumptions. The authors introduce the Vendi Novelty Score (VNS), a novel OOD detection method that leverages similarity-based diversity metrics to quantify the novelty of test samples without requiring density modeling. Experimental results demonstrate that VNS achieves state-of-the-art OOD detection performance across various image classification benchmarks and network architectures, maintaining this performance even when using only 1% of the training data, which is advantageous for memory- or access-constrained environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强分布外（OOD）检测，这对于机器学习系统的安全实施至关重要。作者提出了Vendi新颖性评分（VNS），这是一种新颖的OOD检测方法，利用基于相似性的多样性度量，而不依赖于限制性的分布假设。实验结果表明，VNS在多个图像分类基准和网络架构中实现了最先进的性能，即使仅使用1%的训练数据也能保持高效能，适合内存或访问受限的环境。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Disentangled Representations for Controllable Music Generation</div>
<div class="meta-line">Authors: Laura Ibáñez-Martínez, Chukwuemeka Nkama, Andrea Poltronieri, Xavier Serra, Martín Rocamora</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-10T18:25:04+00:00 · Latest: 2026-02-10T18:25:04+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10058v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10058v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估可控音乐生成的解耦表示</div>
<div class="mono" style="margin-top:8px">最近的音乐生成方法依赖于解耦表示，通常标记为结构和音色或局部和全局，以实现可控合成。然而，这些嵌入的基本属性仍然未被充分探索。在本研究中，我们在一组音乐音频模型中评估这种解耦表示，以可控生成为目标，采用超越标准下游任务的探测框架。所选模型反映了多种无监督解耦策略，包括归纳偏差、数据增强、对抗目标和分阶段训练程序。我们进一步隔离特定策略以分析其影响。我们的分析涵盖四个关键轴心：信息性、等变性、不变性和解耦性，这些在数据集、任务和受控变换中进行评估。我们的发现揭示了嵌入的预期语义与实际语义之间的不一致，表明当前策略未能产生真正的解耦表示，并促使我们重新审视音乐生成中的可控性方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the effectiveness of disentangled representations in music generation, which are crucial for enabling controllable synthesis. The authors employ a probing-based framework to evaluate various music audio models that utilize different unsupervised disentanglement strategies, including inductive biases and adversarial objectives. The key findings indicate that there are inconsistencies between the intended and actual semantics of the embeddings, revealing that current methods do not achieve truly disentangled representations and highlighting the need for a reassessment of controllability in music generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨在音乐生成中解耦表示的有效性，这对实现可控合成至关重要。作者采用基于探测的框架评估了多种使用不同无监督解耦策略的音乐音频模型，包括归纳偏差和对抗目标。主要发现表明，嵌入的预期语义与实际语义之间存在不一致，揭示了当前方法未能实现真正的解耦表示，并强调了重新评估音乐生成中的可控性需求。</div>
</details>
</div>
<div class="card">
<div class="title">WildCat: Near-Linear Attention in Theory and Practice</div>
<div class="meta-line">Authors: Tobias Schröder, Lester Mackey</div>
<div class="meta-line">First: 2026-02-10T18:22:32+00:00 · Latest: 2026-02-10T18:22:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Crucially, we select the coreset using a fast but spectrally-accurate subsampling algorithm -- randomly pivoted Cholesky -- and weight the elements optimally to minimise reconstruction error. Remarkably, given bounded inputs, WildCat approximates exact attention with super-polynomial $O(n^{-\sqrt{\log(\log(n))}})$ error decay while running in near-linear $O(n^{1+o(1)})$ time. In contrast, prior practical approximations either lack error guarantees or require quadratic runtime to guarantee such high fidelity. We couple this advance with a GPU-optimized PyTorch implementation and a suite of benchmark experiments demonstrating the benefits of WildCat for image generation, image classification, and language model KV cache compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WildCat：理论与实践中的近线性注意力</div>
<div class="mono" style="margin-top:8px">我们介绍了WildCat，这是一种高精度、低成本的神经网络注意力机制压缩方法。尽管注意力是现代网络架构的基础，但由于资源需求与输入序列长度 $n$ 的平方成比例，部署成本极高。WildCat通过仅在一个小的加权核心集上进行注意力计算，避免了这些平方成本。关键是，我们使用一种快速但谱准确的子采样算法——随机主轴Cholesky，来选择核心集，并最优加权元素以最小化重构误差。值得注意的是，在有界输入的情况下，WildCat以超多项式 $O(n^{-\sqrt{\log(\log(n))}})$ 的误差衰减近似精确注意力，同时运行时间接近线性 $O(n^{1+o(1)})$。相比之下，之前的实际近似要么缺乏误差保证，要么需要平方时间来保证如此高的保真度。我们将这一进展与GPU优化的PyTorch实现和一系列基准实验结合，展示了WildCat在图像生成、图像分类和语言模型KV缓存压缩中的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to address the high resource costs associated with the attention mechanism in neural networks, which scale quadratically with input sequence length. The authors introduce WildCat, a method that compresses attention by attending to a small weighted coreset selected through a fast subsampling algorithm called randomly pivoted Cholesky. Experimental results show that WildCat achieves super-polynomial error decay of O(n^{-√log(log(n))}) while operating in near-linear time O(n^{1+o(1)}), outperforming previous approximations that either lacked error guarantees or required quadratic runtime, and demonstrating its effectiveness in applications such as image generation, image classification, and language model KV cache compression.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决神经网络中注意力机制的高资源成本，该成本随着输入序列长度的增加而呈平方级别增长。作者提出了WildCat，一种通过关注小的加权核心集来压缩注意力的方法，该核心集是通过一种称为随机主轴Cholesky的快速子采样算法选择的。实验结果表明，WildCat在近线性时间O(n^{1+o(1)})下实现了O(n^{-\sqrt{\log(\log(n))}})的超多项式误差衰减，优于之前的近似方法，这些方法要么缺乏误差保证，要么需要平方时间运行，并且在图像生成、图像分类和语言模型KV缓存压缩等应用中表现出有效性。</div>
</details>
</div>
<div class="card">
<div class="title">From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</div>
<div class="meta-line">Authors: Dawei Li, Abdullah Alnaibari, Arslan Bisharat, Manny Sandoval, Deborah Hall, Yasin Silva, Huan Liu</div>
<div class="meta-line">First: 2025-12-02T18:31:18+00:00 · Latest: 2026-02-10T18:19:05+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03005v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.03005v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从调解到仲裁：大型语言模型能否在在线争论中充当调解者？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速发展为积极应用的人工智能开辟了新的可能性。随着LLMs越来越多地调解在线沟通，它们在促进同理心和建设性对话方面的潜力成为负责任的人工智能研究的重要前沿。本研究探讨LLMs是否不仅能作为检测有害内容的调解者，还能作为理解和缓解在线冲突的调解者。我们的框架将调解分解为两个子任务：判断，LLM评估对话的公平性和情感动态；引导，LLM生成同理心和缓解性的信息，引导参与者朝着解决方案前进。为了评估调解质量，我们构建了一个基于Reddit的大型数据集，并提出了一个多阶段评估流程，结合了基于原则的评分、用户模拟和人工比较。实验表明，在进行调解时，基于API的模型在推理和干预一致性方面优于开源模型。我们的研究结果突显了当前LLMs作为新兴在线社会调解者的潜力和局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the potential of large language models (LLMs) to act as mediators in online conflicts, moving beyond their traditional role as moderators that detect harmful content. The authors develop a framework that breaks down mediation into two tasks: judgment, where LLMs assess the fairness and emotional dynamics of conversations, and steering, where they generate empathetic messages to facilitate resolution. Experimental results indicate that API-based models significantly outperform open-source models in reasoning and intervention alignment, underscoring both the capabilities and limitations of LLMs in the context of online social mediation.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在在线冲突中作为调解者的潜力，超越了它们传统的识别有害内容的调解角色。作者开发了一个框架，将调解分为两个任务：判断，即LLMs评估讨论的公平性和情感方面；引导，即它们生成同情的信息以促进解决。通过使用来自Reddit的大型数据集和多阶段评估过程，实验结果表明，基于API的模型在推理和干预有效性方面显著优于开源模型，突显了LLMs在社会调解中的能力和局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving</div>
<div class="meta-line">Authors: Serin Varghese, Kevin Ross, Fabian Hueger, Kira Maag</div>
<div class="meta-line">First: 2026-02-10T18:18:37+00:00 · Latest: 2026-02-10T18:18:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10052v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶中一致性视频语义分割的时空注意力</div>
<div class="mono" style="margin-top:8px">深度神经网络，特别是基于变换器的架构，在环境感知的语义分割中取得了显著成功。然而，现有模型独立处理视频帧，未能利用时间一致性，这可能显著提高动态场景中的准确性和稳定性。在本研究中，我们提出了一种时空注意力（STA）机制，扩展了变换器注意力块，以纳入多帧上下文，从而为视频语义分割提供稳健的时间特征表示。我们的方法修改了标准自注意力，以处理时空特征序列，同时保持计算效率，并对现有架构的更改最小。STA在各种变换器架构中具有广泛的适用性，并在轻量级和大规模模型中均有效。在Cityscapes和BDD100k数据集上的全面评估显示，与单帧基线相比，时间一致性指标提高了9.20个百分点，平均交并比提高了最多1.76个百分点。这些结果表明，STA是视频语义分割应用的有效架构增强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the accuracy and stability of video semantic segmentation in automated driving by addressing the limitations of existing models that process video frames independently. The authors propose a Spatio-Temporal Attention (STA) mechanism that integrates multi-frame context into transformer architectures, allowing for robust temporal feature representations while maintaining computational efficiency. Experimental results on the Cityscapes and BDD100k datasets reveal significant improvements, with a 9.20 percentage point increase in temporal consistency metrics and up to a 1.76 percentage point increase in mean intersection over union compared to single-frame baselines, indicating the effectiveness of STA in video-based semantic segmentation applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有模型独立处理视频帧的局限性，来提高自动驾驶中语义分割的准确性和稳定性。作者提出了一种时空注意力（STA）机制，通过结合多帧上下文来增强变换器架构，以实现稳健的时间特征表示。对Cityscapes和BDD100k数据集的实验结果表明，STA在时间一致性指标上提高了9.20个百分点，在平均交并比上提高了最多1.76个百分点，相较于单帧基线表现出显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</div>
<div class="meta-line">Authors: Abdul Matin, Rupasree Dey, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-13T19:59:04+00:00 · Latest: 2026-02-10T18:18:19+00:00</div>
<div class="meta-line">Comments: Accepted to the KGML Bridge at AAAI 2026 (non-archival)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12445v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识引导的掩码自编码器，结合线性光谱混合和光谱角感知重建</div>
<div class="mono" style="margin-top:8px">将领域知识融入深度学习已成为提高模型可解释性、泛化能力和数据效率的有前景方向。在本研究中，我们提出了一种新颖的基于知识引导的ViT掩码自编码器，在自监督重建过程中嵌入科学领域知识。我们的方法不仅依赖于数据驱动的优化，还将线性光谱混合模型（LSMM）作为物理约束，并使用基于物理的光谱角映射器（SAM），确保学习到的表示遵循观察信号与其潜在成分之间已知的结构关系。该框架联合优化LSMM和SAM损失与传统的Huber损失目标，促进特征空间中的数值准确性和几何一致性。这种知识引导的设计增强了重建保真度，在有限监督下稳定训练，并产生基于物理原理的可解释潜在表示。实验结果表明，所提出的模型显著提高了重建质量，并改善了下游任务性能，突显了在基于变换器的自监督学习中嵌入物理知识引导偏置的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve model interpretability, generalization, and data efficiency by integrating domain knowledge into deep learning. The authors propose a knowledge-guided ViT-based Masked Autoencoder that utilizes the Linear Spectral Mixing Model (LSMM) and Spectral Angle Mapper (SAM) as physical constraints during the self-supervised reconstruction process. Experimental results demonstrate that this approach significantly enhances reconstruction quality and improves performance on downstream tasks, indicating the effectiveness of incorporating physics-informed inductive biases in transformer-based self-supervised learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将领域知识融入深度学习来提高模型的可解释性、泛化能力和数据效率。作者提出了一种基于知识引导的ViT掩蔽自编码器，该模型在自监督重建过程中利用线性光谱混合模型（LSMM）和光谱角映射器（SAM）作为物理约束。实验结果表明，该方法显著提高了重建质量，并改善了下游任务的性能，表明在基于变换器的自监督学习中嵌入物理知识引导偏置的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization</div>
<div class="meta-line">Authors: Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2026-02-10T18:15:58+00:00 · Latest: 2026-02-10T18:15:58+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10048v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10048v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过细粒度组策略优化进行长链思维压缩</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常生成不必要冗长的链思维（CoT）推理，增加计算成本和延迟而没有相应的性能提升。本文提出了细粒度组策略优化（FGO），这是一种强化学习（RL）算法，通过细分组响应并根据长度和熵分配适当的权重，从而实现有效的链思维压缩。同时，作为组相对策略优化（GRPO）的增强变体，FGO成功解决了GRPO的两个主要限制：数据利用效率低和熵崩溃。我们在多个推理LLM和基准上评估FGO，包括MATH500、AIME24、AMC23和Minerva。实验结果表明，FGO实现了高效的链思维压缩而不降低性能，同时解决了GRPO的关键限制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of Large Language Models generating overly verbose Chain-of-Thought reasoning, which leads to increased computational costs without significant performance improvements. The authors propose Fine-grained Group policy Optimization (FGO), a Reinforcement Learning algorithm that refines group responses by subdividing them and assigning weights based on length and entropy to achieve effective CoT compression. Experimental results demonstrate that FGO successfully compresses CoT reasoning efficiently while maintaining performance and overcoming the limitations of the previous Group Relative Policy Optimization method.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型生成过于冗长的思维链推理的问题，这导致计算成本和延迟增加，而性能提升却不明显。作者提出了细粒度组策略优化（FGO），这是一种强化学习算法，通过细分组响应并根据响应长度和熵分配权重，从而增强思维链压缩。实验结果表明，FGO有效地压缩了思维链，同时保持了性能，并克服了先前组相对策略优化方法在数据利用和熵崩溃方面的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Prediction Sets for Instance Segmentation</div>
<div class="meta-line">Authors: Kerri Lu, Dan M. Kluger, Stephen Bates, Sherrie Wang</div>
<div class="meta-line">First: 2026-02-10T18:15:06+00:00 · Latest: 2026-02-10T18:15:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10045v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10045v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实例分割的保形预测集</div>
<div class="mono" style="margin-top:8px">当前的实例分割模型在平均预测上表现良好，但缺乏原则性的不确定性量化：它们的输出没有经过校准，且无法保证预测的掩膜接近真实值。为了解决这一限制，我们引入了一种保形预测算法，以生成适应性置信集用于实例分割。给定一张图像和一个像素坐标查询，我们的算法为该像素生成实例预测的置信集，并提供了至少一个预测与真实物体实例掩膜具有高交并比（IoU）的概率的可证明保证。我们将算法应用于农业领域划分、细胞分割和车辆检测的实例分割示例。实证结果表明，我们的预测集的大小根据查询难度而变化，并达到了目标覆盖率，优于现有基线，如学习后测试、保形风险控制和基于形态膨胀的方法。我们提供了具有渐近和有限样本保证的算法版本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve uncertainty quantification in instance segmentation models, which typically provide high average performance but lack calibrated outputs. The authors propose a conformal prediction algorithm that generates adaptive confidence sets for instance segmentation, ensuring that the predicted masks are close to the ground truth with a provable probability. Experimental results demonstrate that the algorithm effectively adjusts the size of prediction sets based on query difficulty and achieves the desired coverage, outperforming existing methods such as Learn Then Test and Conformal Risk Control in various applications including agricultural field delineation and vehicle detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善实例分割模型中的不确定性量化，这些模型通常缺乏校准，且无法保证预测掩膜的准确性。作者提出了一种保形预测算法，该算法生成自适应置信集以进行实例分割，确保至少有一个预测与真实物体实例掩膜高度重合的概率很高。实验结果表明，预测集的大小随查询难度而变化，并成功实现了目标覆盖率，在农业领域划分、细胞分割和车辆检测等应用中优于现有方法，如先学习后测试和保形风险控制。</div>
</details>
</div>
<div class="card">
<div class="title">How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets</div>
<div class="meta-line">Authors: Xiwen Huang, Pierre Pinson</div>
<div class="meta-line">First: 2025-11-25T18:34:33+00:00 · Latest: 2026-02-10T18:12:04+00:00</div>
<div class="meta-line">Comments: Accepted for publication in INFORMS Journal on Data Science (IJDS). This is the authors&#x27; preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20605v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.20605v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to benchmark baselines including random sampling and a greedy knapsack heuristic. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如何购买标签？一种使用主动学习市场的成本效益方法</div>
<div class="mono" style="margin-top:8px">我们介绍并分析了主动学习市场作为购买标签的一种方式，适用于分析师希望获取额外数据以改善模型拟合或更好地训练预测分析应用模型的情况。这与已有的许多购买特征和示例的提案形成对比。通过最初将市场清算形式化为优化问题，我们将预算约束和改进阈值整合到标签获取过程中。我们专注于单买家-多卖家设置，并提出使用两种主动学习策略（基于方差和基于委员会查询），配合不同的定价机制。它们与包括随机抽样和贪婪背包启发式在内的基准基线进行比较。所提策略在两个关键应用领域的真实数据集上得到了验证：房地产定价和能源预测。结果表明我们的方法具有鲁棒性，与传统方法相比，获得更少的标签却始终实现更优的性能。我们的提案是一个易于实施的实用解决方案，旨在优化资源受限环境中的数据获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently acquiring labels to enhance model fitting and predictive analytics, contrasting with existing methods focused on purchasing features and examples. The authors formalize the label acquisition process as an optimization problem, incorporating budget constraints and improvement thresholds, and propose two active learning strategies—variance based and query-by-committee based—within a single-buyer-multiple-seller framework. Experimental validation on real-world datasets from real estate pricing and energy forecasting shows that the proposed methods outperform traditional approaches, achieving better performance with fewer labels, thus providing a practical solution for data acquisition in resource-limited settings.</div>
<div class="mono" style="margin-top:8px">本研究解决了有效获取标签以增强模型拟合和预测分析的挑战，与现有的关注购买特征和示例的方法形成对比。作者提出了一种新的主动学习市场框架，将标签获取过程形式化为一个优化问题，考虑了预算约束和改进阈值。通过实施两种主动学习策略——基于方差和基于委员会查询，并将其与随机抽样和贪婪背包启发式等基准进行比较，研究发现他们的方法在房地产定价和能源预测等实际应用中，始终优于传统方法，以更少的标签实现更好的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
