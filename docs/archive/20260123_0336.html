<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 03:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0336</div>
    <div class="row"><div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考具身世界的视频生成模型</div>
<div class="mono" style="margin-top:8px">视频生成模型显著推动了具身智能的发展，为生成多样化的机器人数据开辟了新可能，这些数据捕捉了物理世界中的感知、推理和行动。然而，合成高质量视频以准确反映现实世界中的机器人交互仍然具有挑战性，缺乏标准化基准限制了公平比较和进展。为了解决这一问题，我们引入了一个全面的机器人基准RBench，旨在评估面向机器人的视频生成，涵盖五个任务领域和四种不同的具身形式。它通过可重复的子指标评估任务级正确性和视觉保真度，包括结构一致性、物理合理性和行动完整性。对25个代表性模型的评估突显了生成物理现实机器人行为的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。虽然RBench提供了识别这些不足的必要视角，但实现物理现实需要超越评估，解决高质量训练数据的严重短缺。基于这些见解，我们引入了一个精细的四阶段数据管道，最终形成RoVid-X，这是最大的开源机器人视频生成数据集，包含400万个带注释的视频片段，涵盖数千个任务，并配有全面的物理属性注释。总体而言，这一评估和数据的协同生态系统为视频模型的严格评估和可扩展训练奠定了坚实基础，加速了具身人工智能向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video generation models for embodied intelligence, particularly in generating high-quality videos that accurately depict robotic interactions in the physical world. The authors introduce RBench, a comprehensive robotics benchmark that evaluates robot-oriented video generation across five task domains and four embodiments, focusing on task-level correctness and visual fidelity through various reproducible metrics. Experimental results reveal significant shortcomings in the physical realism of robot behaviors across 25 evaluated models, while RBench demonstrates a high correlation with human evaluations, achieving a Spearman coefficient of 0.96. To address the identified deficiencies, the authors propose RoVid-X, an extensive open-source dataset containing 4 million annotated video clips, which aims to improve the training data quality necessary for advancing video generation models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强具身智能的视频生成模型，特别是在生成物理世界中真实的机器人交互方面。作者引入了RBench，一个全面的机器人基准，评估五个任务领域和四种具身形式的视频生成，重点关注通过各种可重复的指标评估任务级正确性和视觉保真度。他们对25个模型的评估揭示了在生成物理真实的机器人行为方面的显著不足，而RBench与人类评估之间的高相关性表明其有效性。此外，RoVid-X的引入是一个包含400万个注释视频片段的大型开源数据集，旨在解决提高视频生成物理真实感所需的高质量训练数据的短缺问题。</div>
</details>
</div>
<div class="card">
<div class="title">Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</div>
<div class="meta-line">Authors: Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-28T17:57:05+00:00 · Latest: 2026-01-21T16:16:08+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24709v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object binding, the brain&#x27;s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型预训练视觉变换器中对象绑定是否自然出现？</div>
<div class="mono" style="margin-top:8px">对象绑定是大脑将共同代表一个对象的多个特征结合成一个连贯整体的能力，这对人类认知至关重要。它将低级感知特征分组为高级对象表示，能够高效且组合地存储这些对象，并支持人类对单个对象实例的推理。尽管之前的研究通常明确施加以对象为中心的注意力（例如，插槽注意力）来探讨这些好处，但尚不清楚这种能力是否在预训练的视觉变换器（ViTs）中自然出现。直观上，它们可能会：识别哪些补丁属于同一对象对于下游预测应该是有用的，从而引导注意力。基于自注意力的二次特性，我们假设ViTs表示两个补丁是否属于同一对象，我们称之为IsSameObject。我们使用二次相似性探针从ViT层的补丁嵌入中解码IsSameObject，准确率超过90%。重要的是，这种对象绑定能力在DINO、CLIP和ImageNet监督的ViTs中可靠地出现，但在MAE中明显较弱，这表明绑定不是一个简单的架构伪影，而是通过特定预训练目标获得的能力。我们进一步发现IsSameObject在对象特征之上的低维子空间中编码，并且该信号积极引导注意力。从模型激活中消除IsSameObject会降低下游性能，并与学习目标相悖，这意味着新兴的对象绑定自然服务于预训练目标。我们的发现挑战了ViTs缺乏对象绑定的观点，并强调了“哪些部分属于一起”的符号知识如何在连接主义系统中自然出现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether object binding, the cognitive ability to integrate various features into coherent object representations, naturally emerges in large pretrained Vision Transformers (ViTs). The authors hypothesize that ViTs can represent the relationship between patches belonging to the same object, termed IsSameObject, and they employ a quadratic similarity probe to decode this property from patch embeddings across different ViT layers, achieving over 90% accuracy. The results indicate that while IsSameObject is robustly present in DINO, CLIP, and ImageNet-supervised ViTs, it is significantly weaker in MAE, suggesting that effective object binding is linked to specific pretraining objectives and that this capability actively influences attention and downstream performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了物体绑定这一认知能力是否在大型预训练视觉变换器（ViTs）中自然出现，即将各种特征整合为连贯的物体表征。作者假设ViTs能够表示两个补丁是否属于同一物体，称为IsSameObject，并采用二次相似性探针从补丁嵌入中解码该属性，准确率超过90%。结果表明，这种物体绑定能力在DINO、CLIP和ImageNet监督的ViTs中始终存在，但在MAE中明显较弱，暗示有效的物体绑定与特定的预训练目标相关，并在引导注意力和增强下游性能方面发挥重要作用。</div>
</details>
</div>
<div class="card">
<div class="title">WavLink: Compact Audio--Text Embeddings with a Global Whisper Token</div>
<div class="meta-line">Authors: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:55:58+00:00 · Latest: 2026-01-21T15:55:58+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WavLink：紧凑音频-文本嵌入与全局Whisper令牌</div>
<div class="mono" style="margin-top:8px">Whisper已成为提取通用音频特征的大型音频语言模型的事实标准编码器，其中30秒的音频片段通常由1500帧特征表示并投影到LLM中。相比之下，像CLAP模型这样的音频-文本嵌入模型在很大程度上依赖于替代音频编码器（例如HTS-AT、PaSST），并未有效利用Whisper。我们提出了WavLink，这是一种紧凑的音频-文本嵌入模型，通过可学习的全局令牌增强Whisper编码器，并与文本编码器共同训练。通过对设计选择的系统研究，包括预训练文本编码器、损失函数、训练模式和数据混合，我们识别出能够实现最先进检索性能的配置。我们在三种模型规模上的两阶段训练方案，结合Matryoshka风格的监督，提高了可扩展性，使得嵌入体积缩小8倍且性能损失最小。WavLink在AIR-Bench的多项选择题和零-shot分类中也表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance audio-text embedding models by effectively leveraging the Whisper encoder, which has been underutilized in this domain. The authors introduce WavLink, a compact audio-text embedding model that incorporates a learnable global token and is trained jointly with a text encoder. Key experimental findings reveal that through a systematic exploration of design choices and a two-stage training approach, WavLink achieves state-of-the-art retrieval performance while producing embeddings that are 8 times smaller with minimal performance degradation, and it shows competitive results on the AIR-Bench benchmark for multiple-choice questions and zero-shot classification tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过利用Whisper编码器的能力来增强音频-文本嵌入模型的有效性，而Whisper在这一背景下尚未得到充分利用。作者提出了WavLink，这是一种紧凑的音频-文本嵌入模型，结合了可学习的全局标记，并与文本编码器共同训练。通过对不同预训练文本编码器和训练配置的系统探索，研究发现WavLink在实现最先进的检索性能的同时，生成的嵌入体积缩小了8倍，且性能仅有微小下降，并且在AIR-Bench数据集的多项选择题和零-shot分类任务中也表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background</div>
<div class="meta-line">Authors: Tianyu Li, Songyue Cai, Zongqian Wu, Ping Hu, Xiaofeng Zhu</div>
<div class="meta-line">First: 2026-01-21T15:12:11+00:00 · Latest: 2026-01-21T15:12:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15065v1">PDF</a> · <a href="https://github.com/lounwb/FoBoR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过前景和背景的细化增强少样本分布外检测</div>
<div class="mono" style="margin-top:8px">基于CLIP的前景-背景（FG-BG）分解方法在提高少样本分布外（OOD）检测性能方面表现出显著的有效性。然而，现有方法仍然存在若干局限性。对于从分解中获得的背景区域，现有方法对所有补丁采用统一的抑制策略，忽视了不同补丁对预测的不同贡献。对于前景区域，现有方法未能充分考虑某些局部补丁可能与其他类别在外观或语义上相似，这可能误导训练过程。为了解决这些问题，我们提出了一种新的即插即用框架。该框架由三个核心组件组成：（1）前景-背景分解模块，遵循之前的FG-BG方法将图像分为前景和背景区域；（2）自适应背景抑制模块，自适应地加权补丁分类熵；（3）可混淆前景校正模块，识别并校正可混淆的前景补丁。大量实验结果表明，所提出的即插即用框架显著提高了现有FG-BG分解方法的性能。代码可在：https://github.com/lounwb/FoBoR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance few-shot out-of-distribution detection performance, which is hindered by limitations in existing CLIP-based foreground-background decomposition methods. The authors propose a new framework that includes a Foreground-Background Decomposition module, an Adaptive Background Suppression module, and a Confusable Foreground Rectification module to address these limitations. Experimental results show that this framework significantly improves the performance of existing methods by effectively managing the contributions of different patches and rectifying confusable foreground regions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高少样本的分布外检测性能，而现有的前景-背景分解方法存在局限性。作者提出了一个新框架，包括前景-背景分解模块、自适应背景抑制模块和混淆前景修正模块，以解决均匀抑制策略和局部补丁误分类的问题。实验结果表明，该框架显著提高了现有方法在分布外样本检测中的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Leakage with Generative Flow Matching Denoiser</div>
<div class="meta-line">Authors: Isaac Baglin, Xiatian Zhu, Simon Hadfield</div>
<div class="meta-line">First: 2026-01-21T14:51:01+00:00 · Latest: 2026-01-21T14:51:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15049v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度泄漏与生成流匹配去噪器</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）已成为一种强大的去中心化模型训练范式，但仍然容易受到深度泄漏（DL）攻击，这些攻击通过共享模型更新重建私有客户端数据。尽管先前的DL方法已显示出不同程度的成功，但它们通常在现实FL环境下存在不稳定性、有限的保真度或较差的鲁棒性。我们提出了一种新的DL攻击，将生成流匹配（FM）先验整合到重建过程中。通过引导优化朝向真实图像的分布（由流匹配基础模型表示），我们的方法在不需要私有数据知识的情况下增强了重建保真度。在多个数据集和目标模型上的广泛实验表明，我们的方法在像素级、感知和基于特征的相似性度量上始终优于最先进的攻击。重要的是，该方法在不同的训练周期、更大的客户端批量大小以及在常见防御措施（如噪声注入、裁剪和稀疏化）下仍然有效。我们的发现呼吁开发新的防御策略，明确考虑配备强大生成先验的对手。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerability of Federated Learning (FL) to deep leakage attacks, which can reconstruct private client data from model updates. The authors propose a novel deep leakage attack that utilizes a generative Flow Matching prior to improve the fidelity of data reconstruction without needing access to the private data itself. Experimental results show that this method significantly outperforms existing state-of-the-art attacks across various metrics, including pixel-level, perceptual, and feature-based similarities, while maintaining effectiveness under different training conditions and common defenses, highlighting the need for new defense strategies against such advanced attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决联邦学习（FL）在深度泄露（DL）攻击下的脆弱性，这种攻击可以从模型更新中重建私有客户端数据。作者提出了一种新颖的DL攻击，利用生成流匹配（FM）先验来改善重建过程。实验结果表明，该方法显著提高了重建的保真度，并在各种指标上持续超越现有的最先进攻击，同时在不同的训练条件和常见防御机制下保持有效性，强调了针对这种先进对抗技术的新防御策略的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</div>
<div class="meta-line">Authors: Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers</div>
<div class="meta-line">First: 2026-01-21T14:42:33+00:00 · Latest: 2026-01-21T14:42:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于课程的深度强化学习框架用于电动车路径规划问题</div>
<div class="mono" style="margin-top:8px">带时间窗的电动车路径规划问题（EVRPTW）是可持续物流中的一个复杂优化问题，路径决策必须在满足严格客户时间约束的同时，最小化总旅行距离、车队规模和电池使用。尽管深度强化学习（DRL）作为经典启发式算法和精确求解器的替代方案显示出巨大潜力，但现有的DRL模型在约束密集时往往难以保持训练稳定性，无法收敛或泛化。在本研究中，我们提出了一种基于课程的深度强化学习（CB-DRL）框架，旨在解决这种不稳定性。该框架利用结构化的三阶段课程，逐步增加问题复杂性：代理首先学习距离和车队优化（阶段A），然后是电池管理（阶段B），最后是完整的EVRPTW（阶段C）。为了确保各阶段的稳定学习，该框架采用了修改后的近端策略优化算法，具有阶段特定的超参数、价值和优势裁剪，以及自适应学习率调度。策略网络基于异构图注意力编码器构建，增强了全局-局部注意力和特征线性调制。这种专门的架构明确捕捉了仓库、客户和充电站的不同特性。模型仅在N=10客户的小实例上训练，显示出对N=5到N=100的未见实例的强泛化能力，在中等规模问题上显著优于标准基线。实验结果确认，这种课程引导的方法在标准DRL基线失败的分布外实例上实现了高可行性率和竞争性解质量，有效弥合了神经速度与操作可靠性之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of the electric vehicle routing problem with time windows (EVRPTW), which involves optimizing travel distance, fleet size, and battery usage under strict time constraints. The authors propose a curriculum-based deep reinforcement learning (CB-DRL) framework that systematically increases problem complexity through a three-phase learning process, focusing first on distance and fleet optimization, then battery management, and finally the complete EVRPTW. Experimental results show that the CB-DRL framework, utilizing a modified proximal policy optimization algorithm and a specialized policy network, achieves high feasibility rates and outperforms standard baselines on medium-scale problems, demonstrating robust generalization to unseen instances with varying customer numbers.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决带时间窗口的电动汽车路径规划问题（EVRPTW），该问题涉及在严格的时间约束下优化行驶距离、车队规模和电池使用。作者提出了一种基于课程的深度强化学习（CB-DRL）框架，该框架通过三个阶段的学习过程系统地增加问题复杂性，首先关注行驶距离和车队优化，然后是电池管理，最后是完整的EVRPTW。实验结果表明，CB-DRL框架利用专门的策略网络和修改后的近端策略优化算法，在中等规模问题上优于标准基线，且在不同客户数量的未见实例上表现出强大的泛化能力，达到了高可行性率。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：用于遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有的视觉语言模型（VLMs）主要在网络抓取的嘈杂图像-文本数据上训练，缺乏对遥感（RS）专业领域的充分接触。这一缺陷导致在RS特定任务上的表现不佳，因为常用数据集往往缺乏详细、科学准确的文本描述，而仅强调日期和位置等属性。为弥补这一关键缺口，我们推出了GAIA，一个为多尺度、多传感器和多模态RS图像分析设计的新数据集。GAIA包含201,005对精心策划的RS图像-文本对，代表了与不同空间分辨率相关的多样化RS模态。与现有的RS视觉语言数据集不同，GAIA特别关注捕捉多样化的RS应用，提供有关环境变化、自然灾害和其他各种动态现象的独特信息。该数据集提供了空间和时间上平衡的分布，覆盖全球，涵盖过去25年，观察的时间分布也很平衡。GAIA的构建涉及两个阶段的过程：（1）从信誉良好的RS相关来源有针对性地抓取图像及其伴随文本，和（2）为每张图像生成五个高质量、科学基础的合成标题，使用精心设计的提示，利用GPT-4o的先进视觉语言能力。我们的广泛实验，包括对CLIP和BLIP2模型的微调，表明GAIA显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。我们在项目的GitHub仓库上公开了我们的数据集、自动处理框架和微调模型权重：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing Vision-Language Models (VLMs) that are primarily trained on noisy web-scraped data, which inadequately supports remote sensing (RS) tasks due to a lack of scientifically accurate textual descriptions. The authors introduce GAIA, a comprehensive dataset consisting of 201,005 curated RS image-text pairs, designed for multi-scale and multi-modal analysis, created through targeted web-scraping and the generation of high-quality synthetic captions using GPT-4o. Experimental results indicate that fine-tuning models like CLIP and BLIP2 on the GAIA dataset leads to significant improvements in RS image classification, cross-modal retrieval, and image captioning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视觉语言模型（VLMs）在噪声较大的网络抓取数据上训练不足的问题，这导致其在遥感（RS）任务上的表现不佳，因为缺乏科学准确的文本描述。作者提出了GAIA，一个包含201,005对精心策划的RS图像-文本对的综合数据集，该数据集通过针对性网络抓取和使用GPT-4o生成高质量合成标题的两阶段过程创建。实验结果表明，在GAIA上微调CLIP和BLIP2等模型显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</div>
<div class="meta-line">Authors: Kangcheng Zhou, Jun Jiang, Qing Zhang, Shuang Zheng, Qingli Li, Shugong Xu</div>
<div class="meta-line">First: 2026-01-21T08:21:35+00:00 · Latest: 2026-01-21T08:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14757v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14757v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReinPath：一种用于病理学的多模态强化学习方法</div>
<div class="mono" style="margin-top:8px">可解释性在计算病理学中至关重要，促使了来自组织病理图像和相应文本数据的多模态信息集成的发展。然而，现有的多模态方法由于缺乏支持明确推理和推断的高质量数据集以及简单的推理过程，导致可解释性有限。为了解决上述问题，我们引入了一种具有强大推理能力的新型多模态病理大语言模型。为了提高准确且上下文相关的文本描述的生成，我们设计了一种与组相对策略优化相结合的语义奖励策略。我们构建了一个高质量的病理视觉问答（VQA）数据集，专门设计用于支持复杂推理任务。在该数据集上进行的全面实验表明，我们的方法在性能上优于最先进的方法，即使仅用20%的数据进行训练。我们的方法在下游零样本图像分类任务中也与CLIP相比表现出可比的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for improved interpretability in computational pathology, particularly through the integration of histopathological images and text data. To address the limitations of existing multimodal methods, the authors developed a novel multimodal pathology large language model that incorporates a semantic reward strategy and group relative policy optimization to enhance the generation of accurate textual descriptions. Experimental results on a newly constructed high-quality pathology visual question answering dataset show that this approach outperforms state-of-the-art methods, achieving superior performance even with only 20% of the training data, and demonstrates comparable results in downstream zero-shot image classification tasks when compared to CLIP.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高计算病理学中的可解释性，特别是通过整合组织病理图像和文本数据。作者提出了一种新颖的多模态病理大语言模型，结合了语义奖励策略和组相对策略优化，以增强准确文本描述的生成。实验结果表明，该方法在新构建的高质量视觉问答数据集上优于现有的最先进方法，即使仅使用20%的训练数据也能取得优异的表现，并在零样本图像分类任务中与CLIP的结果相当。</div>
</details>
</div>
<div class="card">
<div class="title">Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</div>
<div class="meta-line">Authors: Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</div>
<div class="meta-line">First: 2025-12-19T01:39:43+00:00 · Latest: 2026-01-21T07:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17160v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成图像能否作为有效且高效的类别原型？</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在零样本图像分类任务中表现出色。然而，现有方法，包括对比语言-图像预训练（CLIP），都依赖于注释的文本-图像对来对齐视觉和文本模态。这种依赖引入了准备高质量数据集的巨大成本和准确性要求。同时，从两种模式处理数据也需要双塔编码器，这也阻碍了它们的轻量化。为了解决这些限制，我们提出了“基于大型语言模型生成的对比语言-图像预训练（LGCLIP）”框架。LGCLIP利用大型语言模型（LLM）生成特定类别的提示，引导扩散模型合成参考图像。随后，这些生成的图像作为视觉原型，真实图像的视觉特征被提取并与这些原型的视觉特征进行比较，以实现比较预测。通过优化LLM的提示生成并仅使用视觉编码器，LGCLIP保持轻量和高效。关键是，我们的框架在整个实验过程中仅需要类别标签作为输入，消除了对手动注释的图像-文本对和额外预处理的需求。实验结果验证了LGCLIP的可行性和效率，在零样本分类任务中表现出色，并建立了分类的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and effectiveness of class prototypes in zero-shot image classification tasks, addressing the limitations of existing methods that rely on annotated text-to-image pairs. The authors propose a new framework called LGCLIP, which utilizes a Large Language Model to generate class-specific prompts that guide a diffusion model in synthesizing reference images, thus allowing for the use of visual prototypes without the need for dual-tower encoders. Experimental results demonstrate that LGCLIP achieves strong performance in zero-shot classification tasks while remaining lightweight and requiring only class labels as input, thereby eliminating the need for manually annotated datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有依赖注释文本-图像对的方法的局限性，来提高零-shot图像分类的效率和有效性。作者提出了一种名为LGCLIP的新框架，该框架利用大型语言模型生成特定类别的提示，指导扩散模型合成参考图像，从而使用这些合成图像作为视觉原型。实验结果表明，LGCLIP在零-shot分类任务中表现出色，同时保持轻量和高效，因为它仅需要类别标签，而无需手动注释的图像-文本对。</div>
</details>
</div>
<div class="card">
<div class="title">RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models</div>
<div class="meta-line">Authors: Sha Luo, Yogesh Prabhu, Timothy Ossowski, Kaiping Chen, Junjie Hu</div>
<div class="meta-line">First: 2026-01-06T19:14:49+00:00 · Latest: 2026-01-21T06:11:42+00:00</div>
<div class="meta-line">Comments: *updated author email in this version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03369v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03369v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RiskCueBench：基于早期风险线索的预测推理基准测试</div>
<div class="mono" style="margin-top:8px">随着以视频为中心的社交媒体的快速增长，从视觉数据中预测风险事件的能力是确保公共安全和防止现实世界事故的一个有前景的方向。之前的研究广泛探讨了在驾驶、抗议和自然灾害等领域的监督视频风险评估。然而，许多现有数据集允许模型访问完整的视频序列，包括事故本身，这大大降低了任务的难度。为了更好地反映现实世界的条件，我们引入了一个新的视频理解基准RiskCueBench，其中视频经过仔细注释，以识别风险信号片段，定义为指示潜在安全隐患的最早时刻。实验结果揭示了当前系统在解释不断变化的情况和从早期视觉信号中预测未来风险事件方面的显著差距，突显了在实践中部署视频风险预测模型的重要挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance public safety by improving the ability to anticipate risky events from visual data in video-centered social media. The authors introduce a new benchmark called RiskCueBench, which focuses on identifying risk signal clips in videos, representing the earliest moments that indicate potential safety concerns, rather than providing access to full video sequences. Experimental findings demonstrate a significant gap in the current systems&#x27; capabilities to interpret evolving situations and predict future risks from early visual cues, underscoring the challenges faced in deploying effective video risk prediction models in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提高从视频社交媒体中的视觉数据预测风险事件的能力来增强公共安全。作者提出了一个新的基准，称为RiskCueBench，专注于识别视频中的风险信号片段，这些片段代表了指示潜在安全隐患的最早时刻，而不是提供完整视频序列的访问。实验结果表明，当前系统在解释不断变化的情况和从早期视觉线索预测未来风险方面存在显著差距，突显了在实际应用中部署有效视频风险预测模型所面临的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning</div>
<div class="meta-line">Authors: Jiaying Wu, Can Gao, Jinglu Hu, Hui Li, Xiaofeng Cao, Jingcai Guo</div>
<div class="meta-line">First: 2026-01-20T16:06:23+00:00 · Latest: 2026-01-20T16:06:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMCE：基于概率的多粒度语义与标题引导增强的少样本学习</div>
<div class="mono" style="margin-top:8px">少样本学习旨在仅从少量标记样本中识别新类别，其中从稀缺数据估计的原型往往存在偏差且泛化能力差。基于语义的方法通过引入粗略的类别级信息来缓解这一问题，但它们主要应用于支持侧，查询表示保持不变。本文提出了PMCE，一种利用多粒度语义与标题引导增强的概率少样本框架。PMCE构建了一个非参数知识库，存储每个类别的视觉统计信息以及基础类别的CLIP编码类名嵌入。在元测试时，根据每个新类别的类名嵌入的相似性检索最相关的基础类别。这些统计信息随后被聚合为类别特定的先验信息，并通过简单的MAP更新与支持集原型融合。同时，一个冻结的BLIP标题生成器提供无标签的实例级图像描述，而一个在基础类别上训练的轻量级增强器在归纳协议下优化支持原型和查询特征，并通过一致性正则化来稳定噪声标题。在四个基准上的实验表明，PMCE在强基线之上持续改进，在1-shot设置下在MiniImageNet上实现了最高7.71%的绝对增益。我们的代码可在https://anonymous.4open.science/r/PMCE-275D获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot learning, which struggles with biased prototypes and poor generalization due to limited labeled samples. The authors propose PMCE, a probabilistic framework that incorporates multi-granularity semantics and caption-guided enhancement, utilizing a nonparametric knowledge bank to store visual statistics and class name embeddings. Experimental results demonstrate that PMCE outperforms strong baselines, achieving an absolute gain of up to 7.71% over the leading semantic competitor on the MiniImageNet dataset in the 1-shot setting.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善少样本学习，该领域面临着有限标记样本导致的原型偏差和泛化能力差的问题。作者提出了PMCE，这是一种利用多粒度语义和基于标题增强的概率框架，构建了一个非参数知识库来存储视觉统计数据和类别名称嵌入。实验结果表明，PMCE显著优于强基线，在MiniImageNet数据集的1-shot设置中，相较于最佳语义竞争者实现了高达7.71%的绝对增益。</div>
</details>
</div>
<div class="card">
<div class="title">Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model</div>
<div class="meta-line">Authors: Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu, Yuyang Zhang, Longxiang Gao, Shuaiguang Li, Xingyu Li, Yanran Xu, Changwei Wang</div>
<div class="meta-line">First: 2026-01-20T15:06:10+00:00 · Latest: 2026-01-20T15:06:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你也需要的视野：利用多模态大语言模型导航分布外检测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测是一项关键任务，受到了广泛关注。CLIP的出现促进了零样本OOD检测的广泛研究，通常采用无训练的方法。目前的方法利用大型语言模型（LLMs）的专家知识来识别潜在的异常值。然而，这些方法往往过于依赖文本空间的知识，忽视了在图像空间中检测分布外样本所涉及的固有挑战。本文提出了一种新颖的管道MM-OOD，利用多模态大语言模型（MLLMs）的多模态推理能力及其进行多轮对话的能力，以增强异常值检测。我们的方法旨在提高近OOD和远OOD任务的性能。具体而言，(1) 对于近OOD任务，我们直接将ID图像和相应的文本提示输入MLLMs以识别潜在的异常值；(2) 对于远OOD任务，我们引入了草图-生成-详细框架：首先，我们使用文本提示草拟异常值暴露，然后生成相应的视觉OOD样本，最后通过使用多模态提示进行详细说明。实验表明，我们的方法在广泛使用的多模态数据集（如Food-101）上取得了显著的改进，同时验证了其在ImageNet-1K上的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of out-of-distribution (OOD) detection, particularly in the context of multimodal data, where existing methods often rely heavily on textual information. The authors propose a novel pipeline called MM-OOD that utilizes the multimodal reasoning capabilities of large language models (LLMs) to enhance outlier detection through multi-round conversations. Experimental results indicate that MM-OOD significantly improves performance on both near OOD and far OOD tasks, achieving notable advancements on datasets like Food-101 and demonstrating scalability on ImageNet-1K.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决分布外（OOD）检测中的挑战，特别是在多模态数据的背景下，现有方法往往过于依赖文本信息。作者提出了一种名为MM-OOD的新型管道，利用大型语言模型（LLM）的多模态推理能力，通过多轮对话增强异常值检测。实验结果表明，MM-OOD在近OOD和远OOD任务上显著提高了性能，在Food-101等数据集上取得了显著进展，并在ImageNet-1K上展示了可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3</div>
<div class="meta-line">Authors: Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li</div>
<div class="meta-line">First: 2026-01-20T12:25:41+00:00 · Latest: 2026-01-20T12:25:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniOVCD：利用SAM 3 简化开放词汇变化检测</div>
<div class="mono" style="margin-top:8px">变化检测（CD）是遥感中的一项基本任务，监测土地覆盖的演变。基于此，开放词汇变化检测（OVCD）引入了新的要求，旨在减少对预定义类别的依赖。现有的无训练OVCD方法主要使用CLIP来识别类别，同时需要额外的模型如DINO来提取特征。然而，结合不同模型往往会导致特征匹配问题，使系统不稳定。最近，推出了Segment Anything Model 3（SAM 3），它将分割和识别能力集成在一个可提示模型中，为OVCD任务提供了新的可能性。本文提出了OmniOVCD，一个专为OVCD设计的独立框架。通过利用SAM 3的解耦输出头，我们提出了一种协同融合到实例解耦（SFID）策略。SFID首先融合SAM 3的语义、实例和存在输出以构建土地覆盖掩膜，然后将其分解为单个实例掩膜以进行变化比较。该设计在类别识别中保持高准确性，并在图像间保持实例级一致性。因此，模型能够生成准确的变化掩膜。在四个公共基准（LEVIR-CD、WHU-CD、S2Looking和SECOND）上的实验表明，模型达到了SOTA性能，分别实现了67.2、66.5、24.5和27.1（类别平均）的IoU分数，超越了所有先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Open-Vocabulary Change Detection (OVCD) in remote sensing by reducing reliance on predefined categories. The authors propose OmniOVCD, a standalone framework that utilizes the Segment Anything Model 3 (SAM 3) and introduces a Synergistic Fusion to Instance Decoupling (SFID) strategy, which fuses and then decouples outputs to create accurate land-cover masks. Experimental results on four public benchmarks show that OmniOVCD achieves state-of-the-art performance with Intersection over Union (IoU) scores of 67.2, 66.5, 24.5, and 27.1, outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过减少对预定义类别的依赖来增强遥感中的开放词汇变化检测（OVCD）。作者提出了一种新的框架OmniOVCD，该框架利用Segment Anything Model 3（SAM 3）来简化这一过程。通过实施协同融合到实例解耦（SFID）策略，该框架有效地融合和解耦SAM 3的输出，以创建准确的土地覆盖掩膜和用于变化检测的单个实例掩膜。在四个公共基准上的实验结果表明，OmniOVCD实现了最先进的性能，交并比（IoU）得分分别为67.2、66.5、24.5和27.1，超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍然存在分歧：视觉-语言模型（如CLIP）在全局语义对齐方面表现出色，但缺乏空间精度，而自监督方法（如MAE、DINO）捕捉复杂的局部结构，但在高层语义上下文中表现不佳。我们认为这些范式在根本上是互补的，可以整合到一个有原则的多任务框架中，并通过密集的空间监督进一步增强。我们介绍了MTV，一个多任务视觉预训练框架，联合优化视觉-语言对比、自监督和密集空间目标的共享主干。为了减少对手动标注的需求，我们利用高容量的“专家”模型——如Depth Anything V2和OWLv2——在大规模上合成密集的、结构化的伪标签。除了框架之外，我们还系统地研究了多任务视觉学习的机制，分析：（i）每个目标的边际收益，（ii）任务协同与干扰，以及（iii）在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了“兼得其利”的性能，显著增强了细粒度空间推理，而不妨碍全局语义理解。我们的发现表明，多任务学习在高质量伪监督的推动下，是通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current visual representation learning methods, which either excel in global semantic alignment or in capturing local structures but struggle to integrate both effectively. The authors propose a multi-task visual pretraining framework called MTV, which optimizes a shared backbone across various objectives, including vision-language contrastive, self-supervised, and dense spatial tasks, while utilizing high-capacity expert models to generate structured pseudo-labels. Experimental results indicate that MTV significantly improves fine-grained spatial reasoning and maintains strong global semantic understanding, demonstrating the effectiveness of multi-task learning enhanced by high-quality pseudo-supervision for developing more general visual encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前视觉表征学习方法的局限性，这些方法要么在语义对齐方面表现出色，要么在捕捉局部结构方面表现良好，但两者不能兼得。作者提出了一种名为MTV的多任务视觉预训练框架，该框架整合了视觉-语言对比、自监督和密集空间目标，同时利用高容量专家模型生成结构化伪标签，而无需人工标注。实验结果表明，MTV在细粒度空间推理和全局语义理解方面表现优越，证明了通过高质量伪监督增强的多任务学习作为开发更通用视觉编码器的可扩展方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive Generative Augmentation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Wei Liu, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-20T08:09:38+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v3">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应生成增强与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两个协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样且语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能，提升幅度为+2.5%。在视觉-语言学习中，它在十个数据集上将平均零-shot分类准确率提高了+12.31%（相较于CLIP）和+5.31%（相较于SLIP），并进一步将Flickr30k文本检索的R@5提高了+3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance contrastive learning by addressing the limitations in constructing high-quality positive pairs and the lack of a quality assessment mechanism in current methods. The authors propose GenView++, a unified framework that incorporates a multi-source adaptive view generation mechanism to create diverse and semantically coherent views, along with a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results show that GenView++ improves MoCov2 by +2.5% on ImageNet linear classification and increases zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, while also enhancing Flickr30k text retrieval R@5 by +3.2%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决高质量正样本对构建中的局限性以及学习过程中缺乏质量评估机制的问题，来提高对比学习的有效性。作者提出了GenView++，一个统一框架，结合了多源自适应视图生成机制，用于创建多样且语义一致的视图，以及一个质量驱动的对比学习机制，根据语义对齐和多样性动态调整训练贡献。实验结果表明，GenView++在ImageNet线性分类上使MoCov2提高了2.5%，在十个数据集上使CLIP的零样本分类准确率提高了12.31%，使SLIP提高了5.31%，同时在Flickr30k文本检索R@5上提高了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</div>
<div class="meta-line">Authors: Donghee Lee, Rui Cai, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-20T05:44:33+00:00 · Latest: 2026-01-20T05:44:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13622v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARPE：通过集成实现的大型视觉语言模型的上下文感知图像表示优先级</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）的最新进展使其更接近成为通用助手。尽管表现强劲，LVLMs在图像分类等以视觉为中心的任务上仍然存在困难，表现不及其基础视觉编码器，后者通常是基于CLIP的模型。为了解决这一限制，我们提出了上下文感知图像表示优先级通过集成（CARPE），这是一种新颖的模型无关框架，引入了视觉集成层和上下文感知集成策略，以识别何时优先考虑图像表示或依赖语言模型的推理能力。该设计增强了模型自适应加权视觉和文本模态的能力，使模型能够捕捉图像表示的各个方面，从而在分类和视觉语言基准测试中实现一致的泛化改进。大量实验表明，CARPE不仅提高了图像分类基准的性能，还增强了各种视觉语言基准的结果。最后，CARPE旨在与大多数由视觉编码器和语言模型组成的开源LVLMs有效集成，确保其在多种架构中的适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of Large Vision-Language Models (LVLMs) in vision-centric tasks, where they currently underperform compared to traditional vision encoders. The authors propose a novel framework called Context-Aware Image Representation Prioritization via Ensemble (CARPE), which incorporates vision-integration layers and a context-aware ensemble strategy to optimize the use of image representations and language model reasoning. Experimental results show that CARPE significantly enhances performance in image classification tasks and improves generalization across various vision-language benchmarks, demonstrating its effectiveness and adaptability with existing LVLM architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型视觉语言模型（LVLMs）在视觉中心任务中的表现，而这些任务中它们的表现通常低于传统的视觉编码器。作者提出了一种名为上下文感知图像表示优先级排序的集成框架（CARPE），该框架结合了视觉集成层和上下文感知的集成策略，以优化图像表示和语言推理的使用。实验结果表明，CARPE显著提高了图像分类基准上的表现，并增强了在各种视觉语言任务中的泛化能力，证明了其有效性和与现有LVLM架构的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</div>
<div class="meta-line">Authors: Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour</div>
<div class="meta-line">First: 2025-09-03T17:56:46+00:00 · Latest: 2026-01-19T23:50:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03515v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.03515v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Waymo开放运动数据集能否支持现实行为建模？基于自然轨迹的验证研究</div>
<div class="mono" style="margin-top:8px">Waymo开放运动数据集（WOMD）已成为数据驱动的自动驾驶汽车（AV）行为建模的热门资源。然而，由于专有后处理、缺乏误差量化以及将轨迹分割为20秒片段，其在行为分析中的有效性仍不确定。本研究考察WOMD是否准确捕捉到现实世界AV操作中的动态和交互。利用在亚利桑那州凤凰城（PHX）进行的4级AV操作独立收集的自然数据集，我们对三个典型城市驾驶场景进行比较分析：在信号交叉口的卸载、跟车和变道行为。在卸载分析中，手动从航拍视频中提取车距，以确保测量误差微乎其微。对于跟车和变道案例，我们应用模拟外推（SIMEX）方法来考虑PHX数据中经验估计的误差，并使用动态时间规整（DTW）距离量化行为差异。所有场景的结果一致表明，PHX中的行为超出了WOMD的行为范围。值得注意的是，WOMD对短车距和突然减速的表现不足。这些发现表明，仅基于WOMD校准的行为模型可能系统性低估自然驾驶的变异性、风险和复杂性。因此，在没有与独立收集的数据进行适当验证的情况下，使用WOMD进行行为建模时应谨慎。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to assess the validity of the Waymo Open Motion Dataset (WOMD) for behavioral modeling of autonomous vehicles, given concerns about its proprietary processing and lack of error quantification. The researchers utilized an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona, and conducted comparative analyses across three urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. The results indicate that the behaviors observed in Phoenix significantly deviate from those in WOMD, particularly showing that WOMD underrepresents short headways and abrupt decelerations, suggesting that models based solely on WOMD may underestimate the complexities of real-world driving scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估Waymo开放运动数据集（WOMD）在自动驾驶车辆行为建模中的有效性，因为存在关于其专有处理和缺乏误差量化的担忧。研究人员使用来自亚利桑那州凤凰城的四级自动驾驶操作的独立收集的自然数据集进行了比较分析，重点关注三个城市驾驶场景：在信号交叉口的卸载、跟车和变道行为。结果表明，PHX数据集中捕获的行为与WOMD中的行为显著不同，WOMD明显低估了短车距和急刹车，这表明仅基于WOMD的模型可能会低估与现实驾驶场景相关的复杂性和风险。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</div>
<div class="meta-line">Authors: Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</div>
<div class="meta-line">First: 2026-01-19T22:55:30+00:00 · Latest: 2026-01-19T22:55:30+00:00</div>
<div class="meta-line">Comments: 10 pages,4 images</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13440v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的异常分类与分割方法分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM），特别是CLIP，通过实现零样本和少样本缺陷识别，彻底改变了异常检测，无需大量标注数据集。通过学习图像和文本的对齐表示，VLM通过自然语言描述正常和异常状态，促进异常分类和分割，消除了对特定任务训练或缺陷示例的传统要求。本项目对基于VLM的异常分类（AC）和异常分割（AS）方法进行了全面分析。我们系统地研究了关键架构范式，包括基于滑动窗口的密集特征提取（WinCLIP）、具有可学习投影的多阶段特征对齐（AprilLab框架）和组合提示集成策略。我们的分析在关键维度上评估这些方法：特征提取机制、文本-视觉对齐策略、提示工程技术、零样本与少样本的权衡、计算效率和跨领域泛化。通过在MVTec AD和VisA等基准上的严格实验，我们比较了分类准确性、分割精度和推理效率。主要贡献是对VLM在异常检测中成功的基础理解，综合了方法选择的实用见解并识别当前的局限性。本研究旨在促进VLM方法在工业质量控制中的知情采用，并指导未来的研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the capabilities of Vision-Language Models (VLMs), particularly CLIP, in enhancing anomaly detection without the need for extensive labeled datasets. The study employs a comprehensive analysis of various VLM-based approaches for anomaly classification and segmentation, investigating architectural paradigms such as sliding window-based dense feature extraction, multi-stage feature alignment, and compositional prompt ensemble strategies. Key experimental findings reveal that these methods provide significant insights into feature extraction, text-visual alignment, and trade-offs between zero-shot and few-shot learning, ultimately demonstrating their effectiveness in improving classification accuracy and segmentation precision on benchmarks like MVTec AD and VisA, while also identifying limitations for future research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于利用视觉-语言模型（VLMs），如CLIP，增强异常检测，这些模型允许在没有大量标记数据集的情况下进行零样本和少样本缺陷识别。该研究对多种基于VLM的方法进行了全面分析，系统研究了滑动窗口密集特征提取、多阶段特征对齐和组合提示集成策略等架构范式。关键实验结果揭示了特征提取机制、文本-视觉对齐策略以及零样本与少样本方法之间的权衡，展示了在MVTec AD和VisA等基准上分类准确性、分割精度和推理效率的提升，最终提供了对VLM在异常检测中有效性的基础理解，并指导未来的研究和工业应用。</div>
</details>
</div>
<div class="card">
<div class="title">Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations</div>
<div class="meta-line">Authors: Tim Lachmann, Alexandra Israelsson, Christina Tornberg, Teimuraz Saghinadze, Michal Balazia, Philipp Müller, Petri Laukka</div>
<div class="meta-line">First: 2026-01-19T16:59:45+00:00 · Latest: 2026-01-19T16:59:45+00:00</div>
<div class="meta-line">Comments: Accepted for publication at IEEE Face &amp; Gesture 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有混合情感都是相等的：带有相对显著性注释的混合情感表达BLEMORE数据集</div>
<div class="mono" style="margin-top:8px">人类常常同时体验多种情感的混合，而不仅仅是一种基本情感，且这些情感的显著性各不相同。尽管混合情感的重要性不言而喻，但大多数基于视频的情感识别方法仅设计用于识别单一情感。少数尝试识别混合情感的方法通常无法评估混合情感中各情感的相对显著性。这一局限性主要源于缺乏包含大量带有相对显著性注释的混合情感样本的数据集。为了解决这一不足，我们引入了BLEMORE，一个用于多模态（视频、音频）混合情感识别的新数据集，其中包含每种情感在混合中的相对显著性信息。BLEMORE包含来自58位演员的3000多个片段，表演6种基本情感和10种不同的混合情感，每种混合情感有3种不同的显著性配置（50/50、70/30和30/70）。利用该数据集，我们对两项混合情感预测任务进行了广泛评估： (1) 预测给定样本中情感的存在， (2) 预测混合情感中情感的相对显著性。我们的结果表明，单模态分类器在验证集上实现了最高29%的存在准确率和13%的显著性准确率，而多模态方法则显著改善，ImageBind + WavLM达到了35%的存在准确率，HiCMAE达到了18%的显著性准确率。在保留的测试集上，最佳模型实现了33%的存在准确率（VideoMAEv2 + HuBERT）和18%的显著性准确率（HiCMAE）。总之，BLEMORE数据集为推进考虑混合情感表达复杂性和重要性的情感识别系统研究提供了宝贵资源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve emotion recognition systems that can handle blended emotions, which are often experienced in real life but inadequately addressed by existing methods that focus on single emotions. To tackle this issue, the authors introduce the BLEMORE dataset, which consists of over 3,000 video clips featuring 58 actors expressing 6 basic emotions and 10 distinct blends, each annotated with relative salience configurations. Experimental results demonstrate that while unimodal classifiers achieve up to 29% accuracy in detecting emotion presence and 13% in assessing salience, multimodal approaches significantly enhance performance, with the best models reaching 35% presence accuracy and 18% salience accuracy on the validation set, indicating the dataset&#x27;s potential to advance blended emotion recognition research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善对混合情感的识别，混合情感是人类常常体验的，但现有的视频情感识别系统主要关注单一情感，未能充分解决这一问题。为了解决这一问题，作者引入了BLEMORE数据集，该数据集包含3000多个视频片段，展示58名演员表达6种基本情感和10种不同的混合情感，每种混合情感都带有相对显著性配置的注释。实验结果表明，单模态分类器在情感存在性预测上最高可达29%的准确率，而在显著性预测上为13%；而多模态方法显著提高了性能，最佳模型在测试集上达到33%的情感存在性准确率和18%的显著性准确率。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</div>
<div class="meta-line">Authors: Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan</div>
<div class="meta-line">First: 2026-01-19T15:19:28+00:00 · Latest: 2026-01-19T15:19:28+00:00</div>
<div class="meta-line">Comments: Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的可适应自监督学习用于以人为中心的视觉任务</div>
<div class="mono" style="margin-top:8px">以人为中心的视觉分析在监控、医疗保健和人机交互等多种应用中发挥着关键作用。随着大规模无标签人类图像数据集的出现，对能够支持多样化以人为中心的下游任务的一般无监督预训练模型的需求日益增加。为实现这一目标，我们提出了CLASP（基于CLIP的可适应自监督学习），这是一个旨在进行以人为中心的视觉任务无监督预训练的新框架。CLASP利用强大的视觉-语言模型CLIP生成低级（例如，身体部位）和高级（例如，属性）语义伪标签。这些多层次的语义线索被整合到学习的视觉表示中，丰富了其表现力和泛化能力。考虑到不同的下游任务对语义粒度的需求不同，CLASP结合了一个提示控制的专家混合（MoE）模块。MoE根据任务特定的提示动态调整特征提取，减轻潜在的特征冲突并增强可迁移性。此外，CLASP采用多任务预训练策略，其中来自CLIP的部分和属性级伪标签指导表示学习过程。在多个基准上的广泛实验表明，CLASP始终优于现有的无监督预训练方法，推动了以人为中心的视觉分析领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for a general unsupervised pre-training model that can effectively support various human-centric visual tasks, given the rise of large-scale unlabeled human image datasets. The authors propose CLASP, a novel framework that utilizes the CLIP vision-language model to generate multi-level semantic pseudo-labels, which are integrated into visual representations to enhance their expressiveness. Experimental results show that CLASP, through its Prompt-Controlled Mixture-of-Experts module and multi-task pre-training strategy, consistently outperforms existing unsupervised pre-training methods across multiple benchmarks in human-centric visual analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在大规模无标签人类图像数据集出现的背景下，迫切需要一个能够有效支持各种以人为中心的视觉任务的通用无监督预训练模型。作者提出了CLASP，这是一种新颖的框架，利用CLIP视觉语言模型生成低层次和高层次的语义伪标签，并将其整合到视觉表示中以增强其表现力。实验结果表明，CLASP通过其提示控制的专家混合模块和多任务预训练策略，在多个基准测试中始终优于现有的无监督预训练方法，从而推动了以人为中心的视觉分析的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</div>
<div class="meta-line">Authors: Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</div>
<div class="meta-line">First: 2025-10-11T10:17:38+00:00 · Latest: 2026-01-19T15:00:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10150v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process carries a critical risk: entropy collapse. This phenomenon is a rapid decrease in policy entropy, which severely limits exploration and diminishes learning effectiveness. Recent methods attempt to mitigate this collapse via heuristic entropy interventions, yet the underlying mechanisms governing entropy remain unclear. In this work, we conduct a theoretical and quantitative analysis of GRPO&#x27;s entropy dynamics, revealing that token-level entropy change in each update step is jointly governed by four key factors: clipping strategy, advantage, token probability, and token entropy. These findings not only explain the mechanisms of existing methods, but also reveal their limitations: they rely on heuristic adjustments to only one or two factors, leaving other relevant factors unconsidered and reducing their effectiveness. This motivates us to propose a new method, STEER, which adaptively reweights tokens based on their estimated entropy change to regulate entropy in a principled manner. Experiments on both math and coding benchmarks demonstrate that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考RLVR中的熵干预：熵变化视角</div>
<div class="mono" style="margin-top:8px">尽管可验证奖励的强化学习（RLVR）可以增强大型语言模型（LLM）的推理能力，但其训练过程存在一个关键风险：熵崩溃。这一现象是策略熵的快速下降，严重限制了探索并降低了学习效果。最近的方法试图通过启发式熵干预来减轻这种崩溃，但控制熵的基本机制仍不清楚。在本研究中，我们对GRPO的熵动态进行了理论和定量分析，揭示了每次更新步骤中令牌级熵变化由四个关键因素共同控制：裁剪策略、优势、令牌概率和令牌熵。这些发现不仅解释了现有方法的机制，还揭示了它们的局限性：它们依赖于对一两个因素的启发式调整，忽略了其他相关因素，从而降低了有效性。这促使我们提出了一种新方法STEER，该方法根据估计的熵变化自适应地重新加权令牌，以原则性地调节熵。在数学和编码基准上的实验表明，STEER有效减轻了熵崩溃，并始终优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the critical issue of entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR), which hampers exploration and learning effectiveness. The authors conduct a theoretical and quantitative analysis of the entropy dynamics in GRPO, identifying four key factors that influence token-level entropy change during updates. Their findings indicate that existing heuristic methods inadequately address these factors, leading to reduced effectiveness. To overcome this limitation, they propose a new method called STEER, which adaptively reweights tokens based on their estimated entropy change. Experimental results on math and coding benchmarks show that STEER effectively mitigates entropy collapse and outperforms current state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决可验证奖励的强化学习（RLVR）中熵崩溃的关键问题，这一问题妨碍了探索和学习的有效性。作者对GRPO中的熵动态进行了理论和定量分析，识别出在更新过程中影响令牌级熵变化的四个关键因素。他们的发现突显了现有启发式方法的局限性，这些方法仅调整一两个因素，因此促使他们开发了一种新方法STEER，该方法根据估计的熵变化自适应地重新加权令牌。数学和编码基准上的实验结果表明，STEER有效地缓解了熵崩溃，并且在性能上超越了当前的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Proxy Robustness in Vision Language Models is Effortlessly Transferable</div>
<div class="meta-line">Authors: Xiaowei Fu, Fuxiang Huang, Lei Zhang</div>
<div class="meta-line">First: 2026-01-19T09:23:11+00:00 · Latest: 2026-01-19T09:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12865v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12865v1">PDF</a> · <a href="http://github.com/fxw13/HPT-GPD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的代理鲁棒性易于转移</div>
<div class="mono" style="margin-top:8px">作为提高深度模型防御能力的关键技术，通过蒸馏实现对抗鲁棒性转移在传统图像分类任务中取得了显著成功。然而，当应用于视觉语言模型（VLM）（例如CLIP）时，这一范式面临重大挑战：为大规模多模态模型构建对抗鲁棒教师需要极高的计算资源。我们通过揭示一个有趣的现象来弥补这一差距：普通CLIP（未经过对抗训练）对由不同架构的另一个CLIP生成的对抗样本表现出内在的防御能力。我们正式将其定义为代理对抗鲁棒性，并自然提出了一个异构代理转移（HPT）框架，该框架在CLIP变体之间建立跨架构鲁棒性蒸馏通道，轻松实现从代理到目标模型的VLM鲁棒性转移。然而，这种代理转移范式容易导致严重的过拟合，导致零-shot自然泛化的急剧下降。为了解决这个问题，我们通过利用学习率调度的差异设计了泛化-支点解耦（GPD）。这将代理转移过程解耦为一个保持泛化的泛化锚定预热和一个促进对抗鲁棒性的泛化拉动HPT，以实现自然泛化与对抗鲁棒性之间的平衡。在15个零-shot数据集上的大量实验证明了我们HPT-GPD方法的有效性。代码可在github.com/fxw13/HPT-GPD网站上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enhancing adversarial robustness in vision-language models (VLMs), which typically require substantial computational resources for robust teacher construction. The authors introduce a Heterogeneous Proxy Transfer (HPT) framework that leverages the intrinsic defensive capabilities of vanilla CLIP models against adversarial examples from differently architected CLIPs, facilitating robustness transfer without extensive resources. To mitigate overfitting and improve zero-shot natural generalization, they propose Generalization-Pivot Decoupling (GPD), which separates the proxy transfer into a warm-up phase that preserves generalization and a subsequent phase that enhances adversarial robustness. Experimental results across 15 zero-shot datasets validate the effectiveness of the HPT-GPD method.</div>
<div class="mono" style="margin-top:8px">本研究解决了增强视觉语言模型（VLM）对抗鲁棒性所面临的挑战，这通常需要大量计算资源来实现有效的防御机制。作者提出了一种异构代理转移（HPT）框架，利用普通CLIP模型对不同架构生成的对抗样本的内在防御能力。为了减轻转移过程中的过拟合，他们还提出了一种泛化-支点解耦（GPD）策略，以实现自然泛化和对抗鲁棒性之间的平衡。在15个零样本数据集上的实验结果验证了HPT-GPD方法在提高鲁棒性而不牺牲泛化性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</div>
<div class="meta-line">Authors: Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</div>
<div class="meta-line">First: 2026-01-19T08:16:11+00:00 · Latest: 2026-01-19T08:16:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP风格的视觉-语言模型在合成空间关系数据上的左右对称性破缺</div>
<div class="mono" style="margin-top:8px">空间理解仍然是视觉-语言模型中的一个关键挑战。然而，目前尚不清楚这种理解是否真正获得，以及如果获得，是通过什么机制。我们提出了一个可控的1D图像-文本测试平台，以探究在使用CLIP风格对比目标训练的基于Transformer的视觉和文本编码器中，左右关系理解是如何出现的。我们在一对一和一对二物体场景的描述上端到端训练轻量级的基于Transformer的视觉和文本编码器，并在系统性变化标签和布局多样性的同时评估对未见物体对的泛化能力。我们发现对比训练学习了左右关系，并且在这种情况下，标签多样性比布局多样性更是泛化的主要驱动因素。为了获得机制理解，我们进行了注意力分解，显示位置和标记嵌入之间的交互引发了一个水平注意力梯度，破坏了编码器中的左右对称性；消除这一贡献会显著降低左右区分能力。我们的结果提供了CLIP风格模型何时以及如何获得关系能力的机制性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the mechanisms behind spatial understanding in vision-language models, particularly focusing on left-right relational understanding. The authors developed a controllable 1D image-text testbed and trained lightweight Transformer-based encoders on paired descriptions of one- and two-object scenes using a CLIP-style contrastive objective. The findings reveal that contrastive training effectively learns left-right relations, with label diversity being the key factor driving generalization, while attention decomposition indicates that interactions between positional and token embeddings create a horizontal attention gradient that disrupts left-right symmetry, significantly affecting discrimination capabilities.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉-语言模型中空间理解的机制，特别关注左右关系理解。作者开发了一个可控的1D图像-文本测试平台，并使用CLIP风格的对比目标对轻量级Transformer编码器进行了一对一和两对象场景描述的训练。研究结果表明，对比训练有效地学习了左右关系，其中标签多样性是泛化的关键因素，而注意力分解揭示了位置嵌入和标记嵌入之间的相互作用产生了一个水平注意力梯度，破坏了左右对称性，显著影响了区分性能。</div>
</details>
</div>
<div class="card">
<div class="title">Open Vocabulary Panoptic Segmentation With Retrieval Augmentation</div>
<div class="meta-line">Authors: Nafis Sadeq, Qingfeng Liu, Mostafa El-Khamy</div>
<div class="meta-line">First: 2026-01-19T07:16:45+00:00 · Latest: 2026-01-19T07:16:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇全景分割与检索增强</div>
<div class="mono" style="margin-top:8px">给定输入图像和类别名称集合，全景分割旨在为图像中的每个像素标注类别标签和实例标签。相比之下，开放词汇全景分割旨在根据用户输入促进任意类别的分割。挑战在于，训练于特定数据集的全景分割系统通常无法很好地推广到训练数据之外的未见类别。在这项工作中，我们提出了RetCLIP，一种检索增强的全景分割方法，旨在提高未见类别的性能。具体而言，我们使用配对的图像-文本数据构建了一个掩码段特征数据库。在推理时，我们使用输入图像的掩码段特征作为查询键，从数据库中检索相似特征和相关类别标签。掩码段的分类分数基于查询特征和检索特征之间的相似性进行分配。基于检索的分类分数与基于CLIP的分数结合，以生成最终输出。我们将我们的解决方案与之前的SOTA方法（FC-CLIP）结合。当在COCO上训练时，所提方法在ADE20k数据集上表现出30.9 PQ、19.3 mAP、44.0 mIoU，相较于基线实现了+4.5 PQ、+2.5 mAP、+10.0 mIoU的绝对提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance panoptic segmentation by enabling the segmentation of arbitrary classes based on user input, addressing the limitations of traditional systems that struggle with unseen classes. The authors propose a method called RetCLIP, which utilizes a retrieval-augmented approach that constructs a masked segment feature database from paired image-text data. Experimental results show that when trained on the COCO dataset, RetCLIP achieves significant improvements on the ADE20k dataset, with scores of 30.9 PQ, 19.3 mAP, and 44.0 mIoU, representing absolute gains of +4.5 PQ, +2.5 mAP, and +10.0 mIoU over the baseline method FC-CLIP.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使基于用户输入的任意类别的分割成为可能，从而增强全景分割，解决现有系统在处理未见类别时的局限性。作者提出了一种名为RetCLIP的方法，该方法利用检索增强的方式，从配对的图像-文本数据中构建掩蔽段特征数据库。实验结果表明，在COCO数据集上训练时，RetCLIP在ADE20k数据集上取得了显著的提升，得分为30.9 PQ、19.3 mAP和44.0 mIoU，分别比基线方法FC-CLIP提高了+4.5 PQ、+2.5 mAP和+10.0 mIoU。</div>
</details>
</div>
<div class="card">
<div class="title">Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval</div>
<div class="meta-line">Authors: Zequn Xie, Boyun Zhang, Yuxiao Lin, Tao Jin</div>
<div class="meta-line">First: 2026-01-19T06:55:33+00:00 · Latest: 2026-01-19T06:55:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12768v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12768v1">PDF</a> · <a href="https://github.com/boyun-zhang/HVP-Net">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&#x27;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入探讨：用于鲁棒视频-文本检索的层次视觉感知</div>
<div class="mono" style="margin-top:8px">视频-文本检索（VTR）旨在使用自然语言查询定位相关视频。当前的方法通常基于预训练模型如CLIP，但受到视频固有冗余和对粗糙最终层特征的依赖，限制了匹配精度。为了解决这个问题，我们引入了HVP-Net（层次视觉感知网络），这是一个通过从视觉编码器的多个中间层提取和精炼特征来挖掘更丰富视频语义的框架。我们的方法逐步从不同语义层次的原始补丁标记中提炼显著的视觉概念，减轻冗余，同时保留对齐所需的关键细节。这导致了更鲁棒的视频表示，在包括MSRVTT、DiDeMo和ActivityNet在内的挑战性基准上实现了新的最先进性能。我们的工作验证了利用层次特征推进视频-文本检索的有效性。我们的代码可在https://github.com/boyun-zhang/HVP-Net获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of video-text retrieval (VTR), which is currently limited by the redundancy in videos and the reliance on coarse features from pre-trained models like CLIP. The authors propose the HVP-Net (Hierarchical Visual Perception Network), which extracts and refines features from multiple intermediate layers of a vision encoder to capture richer video semantics. Experimental results demonstrate that this method significantly enhances video representation and achieves state-of-the-art performance on benchmarks such as MSRVTT, DiDeMo, and ActivityNet, confirming the benefits of utilizing hierarchical features for VTR tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视频文本检索（VTR）的准确性，目前的技术受到视频冗余和对预训练模型（如CLIP）粗糙特征依赖的限制。作者提出了HVP-Net（层次视觉感知网络），该网络从视觉编码器的多个中间层提取和精炼特征，以捕捉更丰富的视频语义。实验结果表明，该方法显著增强了视频表示，在MSRVTT、DiDeMo和ActivityNet等基准测试中实现了最先进的性能，从而验证了在VTR任务中利用层次特征的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition</div>
<div class="meta-line">Authors: Hanyu Zhu, Zhihao Zhan, Yuhang Ming, Liang Li, Dibo Hou, Javier Civera, Wanzeng Kong</div>
<div class="meta-line">First: 2026-01-19T05:19:56+00:00 · Latest: 2026-01-19T05:19:56+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DC-VLAQ：用于鲁棒视觉位置识别的查询残差聚合</div>
<div class="mono" style="margin-top:8px">视觉位置识别（VPR）中的一个核心挑战是学习在大视角变化、光照变化和严重领域转移下仍然具有区分性的鲁棒全局表示。虽然视觉基础模型（VFM）提供了强大的局部特征，但大多数现有方法依赖于单一模型，忽视了不同VFM提供的互补线索。然而，利用这些互补信息不可避免地改变了标记分布，这对现有基于查询的全局聚合方案的稳定性构成挑战。为了解决这些挑战，我们提出了DC-VLAQ，一个以表示为中心的框架，集成了互补VFM的融合和鲁棒的全局聚合。具体而言，我们首先引入了一种轻量级的残差引导互补融合，将表示锚定在DINOv2特征空间，同时通过学习的残差校正注入来自CLIP的互补语义。此外，我们提出了局部聚合查询向量（VLAQ），这是一种查询-残差全局聚合方案，通过其对可学习查询的残差响应对局部标记进行编码，从而提高了稳定性并保留了细粒度的区分线索。在标准VPR基准测试（包括Pitts30k、Tokyo24/7、MSLS、Nordland、SPED和AmsterTime）上的大量实验表明，DC-VLAQ始终优于强基线，并在具有挑战性的领域转移和长期外观变化下实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance visual place recognition (VPR) by developing a robust global representation that can withstand significant changes in viewpoint, illumination, and domain shifts. The authors propose a novel framework called DC-VLAQ, which employs a lightweight residual-guided complementary fusion to combine features from different visual foundation models (VFMs) while maintaining stability in token distributions. Experimental results on various VPR benchmarks, including Pitts30k and Tokyo24/7, show that DC-VLAQ outperforms existing methods, achieving state-of-the-art results, especially in challenging conditions involving domain shifts and long-term appearance variations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过开发一种能够抵御显著视角变化、光照变化和领域转移的鲁棒全局表示，来增强视觉地点识别（VPR）。作者提出了一种名为DC-VLAQ的新框架，该框架整合了互补的视觉基础模型（VFM），并采用查询-残差全局聚合方法。对多个VPR基准（包括Pitts30k和Tokyo24/7）的实验结果表明，DC-VLAQ始终超越现有的强基线，并在面临领域转移和长期外观变化等挑战条件下实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Source-Free Domain Adaptation</div>
<div class="meta-line">Authors: Song Tang, Wenxin Su, Mao Ye, Boyu Wang, Xiatian Zhu</div>
<div class="meta-line">First: 2024-03-12T12:40:08+00:00 · Latest: 2026-01-19T03:29:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.07601v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.07601v4">PDF</a> · <a href="https://github.com/tntek/CausalDA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including Closed-set, Open-set, Partial-set, and Generalized settings. Existing methods, focusing on specific scenarios, not only address a limited subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. In this paper, we propose a novel approach latent Causal factors discovery for unified SFDA (CausalDA). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate CausalDA from a causality perspective. The objective is to uncover potential causality between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that CausalDA can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization. Our code and data are available at https://github.com/tntek/CausalDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一无源领域适应</div>
<div class="mono" style="margin-top:8px">在没有访问源训练数据的情况下，将源模型转移到目标领域的过程中，无源领域适应（SFDA）在包括闭集、开集、部分集和广义设置等各种场景中得到了广泛探索。现有方法专注于特定场景，不仅解决了有限的挑战子集，还需要对目标领域的先验知识，这显著限制了它们的实际效用和可部署性。考虑到这些因素，我们引入了一个更实用但具有挑战性的问题，称为统一SFDA，它以统一的方式全面整合所有特定场景。在本文中，我们提出了一种新颖的方法，称为统一SFDA的潜在因果因素发现（CausalDA）。与强调学习现实统计描述的先前替代方案不同，我们从因果性角度构建CausalDA。其目标是揭示潜在变量与模型决策之间的潜在因果关系，提高所学模型对领域变化的可靠性和鲁棒性。为了整合广泛的世界知识，我们利用了预训练的视觉-语言模型，如CLIP。这有助于在缺乏监督的情况下形成和发现潜在因果因素，结合新设计的信息瓶颈及其理论保证。大量实验表明，CausalDA能够在不同的SFDA设置中实现新的最先进结果，以及无源的分布外泛化。我们的代码和数据可在https://github.com/tntek/CausalDA获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing Source-Free Domain Adaptation (SFDA) methods, which often require prior knowledge of the target domain and focus on specific scenarios, thereby reducing their practical applicability. The authors propose a novel approach called CausalDA, which formulates the problem from a causality perspective to discover latent causal factors that influence model decisions. Experimental results show that CausalDA achieves state-of-the-art performance across various SFDA settings and demonstrates effective source-free out-of-distribution generalization.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有无源领域适应（SFDA）方法的局限性，这些方法通常需要目标领域的先验知识，并且仅处理特定场景。作者提出了一种新方法CausalDA，从因果关系的角度来制定问题，以在不依赖源训练数据的情况下发现潜在的因果因素。实验结果表明，CausalDA在各种SFDA设置中达到了最先进的性能，并展示了有效的无源分布外泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-19T02:37:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0%的图像-AUROC和92.2%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that struggle with balancing semantic generalization and structural discriminability. The authors propose a novel method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through the Hierarchical Semantic-Visual Synergy (HSVS) mechanism and employs a Vision-Conditioned Prompt Generator (VCPG) for dynamic prompt generation. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临限制，这种方法无法有效平衡语义泛化和结构可区分性。作者提出了一种新方法，称为协同语义-视觉提示（SSVP），通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，将DINOv3的多尺度结构先验与CLIP语义空间结合。实验结果表明，SSVP在七个工业基准测试中实现了最先进的性能，在MVTec-AD上获得93.0%的图像AUROC和92.2%的像素AUROC，显著超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">SiLVR: A Simple Language-based Video Reasoning Framework</div>
<div class="meta-line">Authors: Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius</div>
<div class="meta-line">First: 2025-05-30T17:59:19+00:00 · Latest: 2026-01-18T22:02:25+00:00</div>
<div class="meta-line">Comments: Accepted by TMLR (01/2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24869v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.24869v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/cs.unc.edu/silvr">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SILVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SILVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an Adaptive Context Reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. More details can be found at https://sites.google.com/cs.unc.edu/silvr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SiLVR：一种简单的基于语言的视频推理框架</div>
<div class="mono" style="margin-top:8px">最近的测试时优化进展使大型语言模型（LLMs）在推理能力上取得了显著进展，使其能够解决数学和编码中的高度复杂问题。然而，多模态LLMs（MLLMs）的推理能力仍显著滞后，尤其是在复杂的视频语言任务中。为了解决这个问题，我们提出了SILVR，一种简单的基于语言的视频推理框架，将复杂的视频理解分解为两个阶段。在第一阶段，SILVR使用多感官输入（如短片字幕和音频/语音字幕）将原始视频转换为基于语言的表示。在第二阶段，语言描述被输入到一个强大的推理LLM中，以解决复杂的视频语言理解任务。为了处理长上下文的多感官输入，我们使用了一种自适应上下文缩减方案，动态确定采样令牌的时间粒度。我们的简单、模块化且无需训练的视频推理框架在Video-MME（长）、Video-MMMU（理解）、Video-MMLU、CGBench和EgoLife上取得了最佳报告结果。此外，我们的实证研究集中于视频推理能力，表明尽管没有针对视频进行明确训练，强大的推理LLM仍能有效聚合来自视频、语音和音频的多感官输入信息，以应对视频中的复杂时间、因果、长上下文和知识获取推理任务。更多细节请访问https://sites.google.com/cs.unc.edu/silvr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of multimodal large language models (MLLMs) for complex video-language tasks, which have been underperforming compared to their single-modal counterparts. The authors propose SiLVR, a two-stage framework that first converts raw video into language-based representations using multisensory inputs, and then utilizes a reasoning LLM to address video understanding challenges. The experimental results demonstrate that SiLVR achieves state-of-the-art performance on various benchmarks, indicating that even without explicit video training, strong reasoning LLMs can effectively integrate information from video, speech, and audio for complex reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高多模态大型语言模型（MLLMs）在复杂视频语言任务中的推理能力，而目前这些能力远落后于传统的大型语言模型。所提出的方法SiLVR将视频理解分为两个阶段：首先，利用多感官输入将原始视频转换为基于语言的表示，其次，使用推理LLM处理这些表示以进行视频语言理解任务。实验结果表明，SiLVR在多个基准测试上实现了最先进的性能，包括Video-MME和Video-MMMU，并显示出强大的推理LLM能够有效整合多感官信息以应对复杂的视频推理任务，尽管它们并未在视频数据上进行明确训练。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Defense in Vision-Language Models: An Overview</div>
<div class="meta-line">Authors: Xiaowei Fu, Lei Zhang</div>
<div class="meta-line">First: 2026-01-18T14:57:51+00:00 · Latest: 2026-01-18T14:57:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12443v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的对抗防御：概述</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs，例如CLIP）的广泛使用引发了对其易受复杂且不可察觉的对抗攻击的担忧。这些攻击可能会影响模型性能和跨模态任务的系统安全。为应对这一挑战，提出了三种主要的防御范式：训练时防御、测试时适应防御和无训练防御。训练时防御涉及修改训练过程，通常通过对抗微调来提高对抗样本的鲁棒性。尽管有效，但这种方法需要大量计算资源，并且可能无法在所有对抗攻击中泛化。测试时适应防御专注于在推理时适应模型，通过更新其参数来处理未标记的对抗样本，提供灵活性，但通常以增加复杂性和计算开销为代价。无训练防御避免修改模型本身，而是专注于改变对抗输入或其特征嵌入，通过强制输入扰动来减轻攻击的影响，而无需额外训练。本调查回顾了VLMs对抗防御策略的最新进展，突出了这些方法的优缺点，并讨论了增强VLMs鲁棒性所面临的持续挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing reliance on Vision Language Models (VLMs) has highlighted their susceptibility to adversarial attacks, which can undermine their performance and security in cross-modal applications. This paper reviews three primary defense strategies: Training-time Defense, which enhances robustness through adversarial fine-tuning; Test-time Adaptation Defense, which modifies model parameters during inference to counteract adversarial examples; and Training-free Defense, which alters adversarial inputs without changing the model itself. The findings indicate that while Training-time Defense is effective, it demands significant computational resources and may not generalize well, whereas Test-time Adaptation offers flexibility but introduces complexity, and Training-free Defense provides a less resource-intensive alternative but may have limitations in effectiveness.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）的广泛应用暴露了其对对抗攻击的脆弱性，这可能会影响其在跨模态应用中的性能和安全性。本文回顾了三种主要的防御策略：训练时防御通过对抗微调增强模型的鲁棒性；测试时适应防御在推理阶段修改模型参数以应对对抗样本；以及无训练防御，通过改变对抗输入或其特征而不重新训练模型来减轻攻击影响。研究结果表明，尽管训练时防御有效，但资源消耗大且可能无法很好地推广，而测试时适应提供灵活性但增加了复杂性，无训练防御则提供了一种不干扰模型的方式来减轻攻击，而无需额外的训练。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
