<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-27 03:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260227_0352</div>
    <div class="row"><div class="card">
<div class="title">WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos</div>
<div class="meta-line">Authors: Yufei Ye, Jiaman Li, Ryan Rong, C. Karen Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-02-25T18:59:10+00:00 · Latest: 2026-02-25T18:59:10+00:00</div>
<div class="meta-line">Comments: Project website: https://judyye.github.io/whole-www</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22209v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://judyye.github.io/whole-www">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WHOLE：从自我中心视频中提取的世界基础手-物体提升</div>
<div class="mono" style="margin-top:8px">自我中心的操作视频由于交互过程中的严重遮挡以及物体在摄像机视野中频繁进出而极具挑战性。目前的方法通常专注于单独恢复手或物体的姿态，但在交互过程中都面临困难，并且无法处理视野外的情况。此外，它们的独立预测往往导致手-物体关系不一致。我们提出了WHOLE，一种从自我中心视频中根据物体模板整体重建手和物体运动的方法。我们的关键见解是学习手-物体运动的生成先验，以共同推理它们的交互。在测试时，预训练的先验被引导生成符合视频观察的轨迹。这种联合生成重建显著优于分别处理手和物体后再进行后处理的方法。WHOLE在手运动估计、6D物体姿态估计及其相对交互重建方面达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by egocentric manipulation videos, which often suffer from severe occlusions and inconsistent hand-object relations during interactions. The authors propose a novel method called WHOLE, which holistically reconstructs hand and object motion in world space by learning a generative prior over their interactions. Experimental results demonstrate that WHOLE significantly outperforms existing methods that treat hand and object pose estimation separately, achieving state-of-the-art performance in hand motion estimation, 6D object pose estimation, and the reconstruction of their interactions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决自我中心操作视频中存在的挑战，这些视频通常受到严重遮挡和手-物体关系预测不一致的影响。作者提出了WHOLE，一种利用物体模板和学习手-物体交互的生成先验，在世界空间中重建手和物体运动的方法。实验结果表明，WHOLE在手运动估计、6D物体姿态估计及其交互重建方面显著优于现有方法，提供了对复杂场景中手-物体动态的更连贯理解。</div>
</details>
</div>
<div class="card">
<div class="title">Solaris: Building a Multiplayer Video World Model in Minecraft</div>
<div class="meta-line">Authors: Georgy Savva, Oscar Michel, Daohan Lu, Suppakit Waiwitlikhit, Timothy Meehan, Dhairya Mishra, Srivats Poddar, Jack Lu, Saining Xie</div>
<div class="meta-line">First: 2026-02-25T18:59:01+00:00 · Latest: 2026-02-25T18:59:01+00:00</div>
<div class="meta-line">Comments: Project website: https://solaris-wm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22208v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://solaris-wm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Solaris：在Minecraft中构建多人视频世界模型</div>
<div class="mono" style="margin-top:8px">现有的基于动作的视频生成模型（视频世界模型）仅限于单一代理视角，无法捕捉现实环境中的多代理交互。我们介绍了Solaris，一个模拟一致多视角观察的多人视频世界模型。为此，我们开发了一个针对视频游戏（如Minecraft）进行稳健、持续和自动数据收集的多人数据系统。与之前为单人设置构建的平台不同，我们的系统支持协调的多代理交互和同步视频+动作捕捉。通过该系统，我们收集了1264万帧多人数据，并提出了一个用于多人移动、记忆、基础、构建和视图一致性的评估框架。我们使用一个分阶段的管道训练Solaris，逐步从单人游戏过渡到多人建模，结合双向、因果和自我强制训练。在最后阶段，我们引入了检查点自我强制，这是一种内存高效的自我强制变体，能够支持更长时间的教师。结果表明，我们的架构和训练设计优于现有基准。通过开源我们的系统和模型，我们希望为新一代多代理世界模型奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing action-conditioned video generation models, which primarily focus on single-agent perspectives and do not effectively capture multi-agent interactions in real-world environments. The authors developed Solaris, a multiplayer video world model, utilizing a robust data collection system for video games like Minecraft that enables coordinated multi-agent interactions and synchronized video and action capture. Experimental results demonstrate that Solaris, trained through a staged pipeline that incorporates various training techniques, significantly outperforms existing baselines, thereby contributing to the advancement of multi-agent world models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有的基于动作的视频生成模型的局限性，这些模型仅限于单一代理视角，无法有效捕捉现实环境中的多代理交互。作者提出了Solaris，一个多玩家视频世界模型，利用专门设计的多玩家数据系统在视频游戏（如Minecraft）中进行稳健和自动化的数据收集，允许协调的多代理交互和同步的视频与动作捕捉。该研究收集了1264万帧多玩家数据，并采用分阶段的训练流程，从单人游戏过渡到多人游戏建模，结合多种训练技术，包括一种新颖的Checkpointed Self Forcing方法，最终结果显示其性能优于现有基准。</div>
</details>
</div>
<div class="card">
<div class="title">Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets</div>
<div class="meta-line">Authors: Hanna Yukhymenko, Anton Alexandrov, Martin Vechev</div>
<div class="meta-line">First: 2026-02-25T18:58:25+00:00 · Latest: 2026-02-25T18:58:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22207v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>翻译中的恢复：自动化基准和数据集翻译的高效管道</div>
<div class="mono" style="margin-top:8px">多语言大型语言模型（LLM）评估的可靠性目前受到翻译基准质量不一致的影响。现有资源常常遭受语义漂移和上下文丧失，这可能导致误导性的性能指标。在这项工作中，我们提出了一个完全自动化的框架，旨在通过实现可扩展的高质量数据集和基准翻译来解决这些挑战。我们证明，适应测试时计算扩展策略，特别是通用自我改进（USI）和我们提出的多轮排名方法T-RANK，能够显著提高输出质量，优于传统管道。我们的框架确保基准在本地化过程中保留其原始任务结构和语言细微差别。我们将这种方法应用于将流行基准和数据集翻译成八种东欧和南欧语言（乌克兰语、保加利亚语、斯洛伐克语、罗马尼亚语、立陶宛语、爱沙尼亚语、土耳其语、希腊语）。使用基于参考的指标和LLM作为评判者的评估表明，我们的翻译超越了现有资源，导致更准确的下游模型评估。我们发布了框架和改进的基准，以促进稳健和可重复的多语言人工智能开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the issue of unreliable multilingual evaluation of Large Language Models (LLMs) due to poor quality translations of benchmarks, which can lead to inaccurate performance metrics. The authors propose a fully automated framework that utilizes Universal Self-Improvement (USI) and a new multi-round ranking method called T-RANK to enhance the quality of translated datasets and benchmarks. Experimental results indicate that their approach significantly improves translation quality, preserving the original task structure and linguistic nuances, and outperforms existing resources in evaluations, thereby enabling more accurate assessments of downstream models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高多语言大型语言模型（LLM）评估的可靠性，因为基准的翻译质量往往较差。作者开发了一个完全自动化的框架，利用通用自我改进（USI）和一种新颖的多轮排名方法T-RANK来提高翻译质量。实验结果表明，他们的方法显著优于传统翻译管道，能够保留原始任务结构和语言细微差别，并在应用于八种东欧和南欧语言时，导致下游模型评估的准确性提高。</div>
</details>
</div>
<div class="card">
<div class="title">TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs</div>
<div class="meta-line">Authors: Baiqi Li, Kangyi Zhao, Ce Zhang, Chancharik Mitra, Jean de Dieu Nyandwi, Gedas Bertasius</div>
<div class="meta-line">First: 2026-01-30T20:21:46+00:00 · Latest: 2026-02-25T18:57:52+00:00</div>
<div class="meta-line">Comments: For code and data, see https://baiqi-li.github.io/timeblind_project/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00288v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.00288v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://baiqi-li.github.io/timeblind_project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TimeBlind：视频大语言模型的时空组合基准</div>
<div class="mono" style="margin-top:8px">细粒度的时空理解对于视频推理和具身人工智能至关重要。然而，尽管多模态大语言模型（MLLMs）掌握了静态语义，但它们对时间动态的理解仍然脆弱。我们提出了TimeBlind，这是一个用于组合时空理解的诊断基准。受认知科学启发，TimeBlind将细粒度的时间理解分为三个层次：识别原子事件、表征事件属性和推理事件相互依赖性。与将识别与时间推理混为一谈的基准不同，TimeBlind利用最小对比范式：视频对共享相同的静态视觉内容，但在时间结构上有所不同，利用互补问题来中和语言先验。对20个最先进的MLLM（如GPT-5、Gemini 3 Pro）在600个精心策划的实例（2400个视频-问题对）上的评估显示，表现最佳的MLLM的实例准确率（正确区分一对中的两个视频）仅为48.2%，远低于人类表现（98.2%）。这些结果表明，即使是前沿模型也严重依赖静态视觉捷径，而非真正的时间逻辑，使TimeBlind成为下一代视频理解的重要诊断工具。数据集和代码可在https://baiqi-li.github.io/timeblind_project/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of Multimodal Large Language Models (MLLMs) in understanding temporal dynamics in video reasoning, which is crucial for embodied AI. The authors introduce TimeBlind, a benchmark designed to evaluate spatio-temporal compositionality by categorizing temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Experimental results show that the best-performing MLLM achieved an Instance Accuracy of only 48.2% on 600 curated instances, significantly lower than human performance at 98.2%, indicating that current models rely more on static visual cues than on true temporal reasoning, highlighting the need for improved video understanding tools.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大型语言模型（MLLMs）在视频推理中理解时间动态的局限性，这对具身人工智能至关重要。作者提出了TimeBlind，一个旨在评估时空组合性的基准，通过将时间理解分为三个层次：识别原子事件、表征事件属性和推理事件相互依赖关系。对20多个最先进的MLLM在600个精心策划的实例上的评估显示，表现最佳的模型的实例准确率仅为48.2%，远低于人类的98.2%，这表明当前模型更多依赖静态视觉线索，而非真正的时间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services</div>
<div class="meta-line">Authors: Shivasankari Kannan, Yeounoh Chung, Amita Gondi, Tristan Swadell, Fatma Ozcan</div>
<div class="meta-line">First: 2025-04-24T02:27:17+00:00 · Latest: 2026-02-25T18:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.17203v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.17203v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高保真和复杂测试数据生成用于谷歌SQL代码生成服务</div>
<div class="mono" style="margin-top:8px">在工业环境中，对高保真测试数据的需求至关重要，因为对生产数据的访问通常受到限制。传统的数据生成方法往往无法满足需求，难以处理低保真度和建模复杂数据结构及语义关系，这对于测试复杂的SQL代码生成服务（如自然语言到SQL（NL2SQL））至关重要。本文解决了生成语法正确且语义相关的高保真模拟数据的关键需求，这些数据包括我们在谷歌工作负载中经常遇到的嵌套结构的列。我们强调了现有生产方法的局限性，特别是它们无法处理大型和复杂数据结构，以及缺乏语义一致的测试数据，导致测试覆盖率有限。我们展示了通过利用大型语言模型（LLMs）并结合战略性的前处理和后处理步骤，我们可以生成符合复杂结构约束并保持语义完整性的语法正确且语义相关的高保真测试数据，以满足SQL测试目标（查询/函数）。这种方法支持对涉及连接、聚合甚至深度嵌套子查询的复杂SQL查询进行全面测试，确保对SQL代码生成服务（如NL2SQL和SQL代码助手）的稳健评估。我们的结果展示了基于LLM（\textit{Gemini}）的测试数据生成在工业SQL代码生成服务中的实际效用，因为在测试中生成高保真测试数据是必不可少的，原因在于生产数据集的频繁不可用和无法访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for high-fidelity test data in industrial environments where production data is often inaccessible. The authors propose a method that utilizes Large Language Models (LLMs) along with strategic pre- and post-processing to generate syntactically correct and semantically relevant mock data for complex data structures, particularly those with nested columns. The experimental results indicate that this approach effectively produces high-fidelity test data that meets complex structural constraints and maintains semantic integrity, thereby enabling comprehensive testing of SQL code generation services such as Natural Language to SQL (NL2SQL) and SQL Code Assistant.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于工业环境中对高保真测试数据的需求，而生产数据的访问受到限制。作者提出了一种利用大型语言模型（LLMs）以及战略性预处理和后处理的方法，以生成符合复杂数据结构（特别是具有嵌套列的结构）的语法正确且语义相关的模拟数据。实验结果表明，该方法有效地生成满足复杂结构约束并保持语义完整性的高保真测试数据，从而能够全面测试如NL2SQL和SQL代码助手等SQL代码生成服务。</div>
</details>
</div>
<div class="card">
<div class="title">Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers</div>
<div class="meta-line">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou-Ammar</div>
<div class="meta-line">First: 2026-02-20T15:38:16+00:00 · Latest: 2026-02-25T18:47:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18292v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18292v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在概率单纯形上的解码优化：从Top-K到Top-P（核）再到Best-of-K采样器</div>
<div class="mono" style="margin-top:8px">解码位于语言模型与我们对其所有操作之间，但仍被视为一种启发式的调节练习。我们认为解码应被理解为一个有原则的优化层：在每个标记处，我们在概率单纯形上解决一个正则化问题，权衡模型得分与结构偏好和约束。这个单一模板恢复了贪婪解码、Softmax采样、Top-K、Top-P和Sparsemax风格的稀疏性作为特例，并通过最优性条件解释它们的共同结构。更重要的是，该框架使得发明新的解码器变得简单而不依赖于经验法则。我们通过设计Best-of-K（BoK）来证明这一点，BoK是一个基于KL的覆盖目标，旨在多样本管道（自一致性、重排序、验证器选择）。BoK的目标是在固定的K样本预算内覆盖良好替代品的概率，并提高了经验性能。我们展示了这样的样本可以提高准确性，例如，在高采样温度下，Qwen2.5-Math-7B在MATH500上的准确率提高了+18.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve decoding methods used in language models, which are often treated as heuristic adjustments rather than systematic optimizations. The authors propose a framework that formulates decoding as a regularized optimization problem over the probability simplex, allowing for the derivation of various decoding strategies, including greedy decoding and Top-K sampling, as special cases. The key experimental finding is the introduction of the Best-of-K (BoK) decoder, which enhances performance by focusing on the probability of covering high-quality alternatives within a fixed sample budget, achieving an accuracy improvement of 18.6% for the Qwen2.5-Math-7B model on the MATH500 dataset at high sampling temperatures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善语言模型中的解码方法，这些方法通常被视为启发式调整，而非原则性优化。作者提出了一个框架，将解码形式化为在概率单纯形上的优化问题，从而能够整合各种解码策略，如贪婪解码和Top-K采样。关键实验结果是引入了Best-of-K（BoK）解码器，该解码器通过关注在固定样本预算内覆盖高质量替代品的概率来提高性能，在高采样温度下，Qwen2.5-Math-7B在MATH500数据集上的准确率提高了18.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</div>
<div class="meta-line">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</div>
<div class="meta-line">First: 2026-02-25T18:46:30+00:00 · Latest: 2026-02-25T18:46:30+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22197v1">PDF</a> · <a href="https://github.com/mlsecviswanath/img2imgdenoiser">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现成的图像到图像模型足以击败图像保护方案</div>
<div class="mono" style="margin-top:8px">生成性人工智能（GenAI）的进步导致了各种保护策略的发展，以防止图像的未经授权使用。这些方法依赖于向图像添加不可察觉的保护扰动，以阻止风格模仿或深度伪造等滥用行为。尽管之前对这些保护的攻击需要专门的、定制的方法，但我们证明这已不再必要。我们展示了现成的图像到图像GenAI模型可以通过简单的文本提示重新用于通用“去噪器”，有效去除各种保护扰动。在涵盖6种不同保护方案的8个案例研究中，我们的通用攻击不仅绕过了这些防御，还超越了现有的专门攻击，同时保持了图像对对手的效用。我们的发现揭示了当前图像保护领域的一个关键且广泛的脆弱性，表明许多方案提供了虚假的安全感。我们强调迫切需要开发强健的防御，并确定任何未来的保护机制必须针对现成的GenAI模型的攻击进行基准测试。代码可在此存储库中获取：https://github.com/mlsecviswanath/img2imgdenoiser</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerabilities in current image protection strategies that aim to prevent unauthorized image use through imperceptible perturbations. The authors demonstrate that off-the-shelf image-to-image generative AI models can be effectively repurposed as generic denoisers using simple text prompts, thus eliminating various protective perturbations without the need for specialized methods. Their experiments, which include eight case studies across six different protection schemes, reveal that this general-purpose attack not only bypasses existing defenses but also outperforms specialized attacks while maintaining the utility of the images for adversaries, highlighting a significant flaw in the perceived security of these protection mechanisms and underscoring the need for more robust defenses.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决随着生成性人工智能的进步而出现的图像保护策略中的漏洞。作者展示了现成的图像到图像生成模型可以有效地用作通用去噪器，通过简单的文本提示去除图像中的保护扰动。他们的实验包括六种不同保护方案的八个案例研究，结果表明这种方法不仅成功绕过现有防御，还优于专门攻击，同时保持图像对对手的实用性，突显了当前图像保护方法的重大弱点以及对更强大防御的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-02-25T18:40:10+00:00</div>
<div class="meta-line">Comments: 48 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to reinforcement learning, proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that deep reinforcement learning with SGD should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段性强化学习与遗憾景观的几何</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性与复杂性之间的不断权衡，随着样本量的增加，解决方案之间发生质的不同的转变。我们将这一理论扩展到强化学习，证明了政策的广义后验的集中度受局部学习系数（LLC）控制，这是遗憾函数几何的一个不变量。该理论预测，使用SGD的深度强化学习应从高遗憾的简单政策转向低遗憾的复杂政策。我们在一个展示阶段性政策发展的网格世界环境中实证验证了这一预测：训练过程中的相变表现为“对立的楼梯”，其中遗憾急剧下降，而LLC增加。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the dynamics of Bayesian learning in reinforcement learning, particularly how accuracy and complexity trade off as sample sizes increase. The authors extend singular learning theory by introducing the concept of the local learning coefficient (LLC) to analyze the geometry of the regret landscape in reinforcement learning. Their empirical findings in a gridworld environment demonstrate that training progresses through distinct phases, characterized by a transition from simple, high-regret policies to more complex, low-regret policies, which is evidenced by the observed &#x27;opposing staircases&#x27; in regret and LLC during training phases.</div>
<div class="mono" style="margin-top:8px">本研究通过奇异学习理论探讨强化学习的动态，重点关注随着样本量增加，准确性与复杂性之间的权衡。作者通过引入局部学习系数（LLC）的概念，扩展了这一理论，以描述政策的广义后验的集中情况，预测训练过程中将从简单的高遗憾政策逐步过渡到复杂的低遗憾政策。在一个网格世界环境中的实证验证显示了这种阶段性政策发展，特征是遗憾急剧减少而LLC增加的相变，证实了理论预测。</div>
</details>
</div>
<div class="card">
<div class="title">GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</div>
<div class="meta-line">Authors: Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang, Hao Cheng, Huaxiu Yao, Baoling Peng, Huan Zhang, Jianfeng Gao, Tong Zhang</div>
<div class="meta-line">First: 2026-02-25T18:34:57+00:00 · Latest: 2026-02-25T18:34:57+00:00</div>
<div class="meta-line">Comments: 57 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22190v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22190v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GUI-Libra：训练本地GUI代理以进行推理和行动，采用行动感知监督和部分可验证的强化学习</div>
<div class="mono" style="margin-top:8px">开源本地GUI代理在长时间导航任务上仍落后于闭源系统。这一差距源于两个限制：缺乏高质量的、与行动对齐的推理数据，以及直接采用忽视GUI代理独特挑战的通用后训练流程。我们识别出这些流程中的两个基本问题：（i）标准的SFT与CoT推理往往会损害基础性，（ii）逐步RLVR风格的训练面临部分可验证性，其中多个动作可能是正确的，但仅使用单一演示动作进行验证。这使得离线逐步指标对在线任务成功的预测能力较弱。在本研究中，我们提出了GUI-Libra，一种针对这些挑战的定制训练方案。首先，为了缓解与行动对齐的推理数据的稀缺性，我们引入了数据构建和过滤流程，并发布了一个精心策划的81K GUI推理数据集。其次，为了调和推理与基础性，我们提出了行动感知SFT，混合推理后行动和直接行动数据，并重新加权标记以强调行动和基础性。第三，为了在部分可验证性下稳定RL，我们识别出KL正则化在RLVR中的被忽视的重要性，并表明KL信任区域对于提高离线到在线的可预测性至关重要；我们进一步引入成功自适应缩放以降低不可靠负梯度的权重。在多种网络和移动基准测试中，GUI-Libra始终提高了逐步准确性和端到端任务完成度。我们的结果表明，精心设计的后训练和数据策划可以在不需要昂贵的在线数据收集的情况下解锁显著更强的任务解决能力。我们发布了我们的数据集、代码和模型，以促进对具有推理能力的GUI代理的数据高效后训练的进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of open-source native GUI agents in long-horizon navigation tasks, which currently lag behind closed-source systems due to a lack of high-quality, action-aligned reasoning data and ineffective training pipelines. The authors introduce GUI-Libra, a novel training approach that includes a data construction and filtering pipeline, resulting in a curated dataset of 81K GUI reasoning instances. They also propose an action-aware supervised fine-tuning method that combines reasoning and action data while emphasizing grounding, and they address the challenges of reinforcement learning under partial verifiability by incorporating KL regularization and success-adaptive scaling. Experimental results demonstrate that GUI-Libra significantly improves both step-wise accuracy and end-to-end task completion across various web and mobile benchmarks, indicating that improved data curation and training strategies can enhance task-solving capabilities without extensive online data collection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决开源本地GUI代理在长时间导航任务中的性能差距，该差距归因于缺乏高质量的与动作对齐的推理数据和不充分的训练管道。作者提出了GUI-Libra，一个训练框架，包括一个数据构建和过滤管道，以创建一个81K的GUI推理数据集，一个将推理和动作数据结合的动作感知监督微调（SFT）方法，以及一种在部分可验证性下增强可预测性的强化学习方法，结合了KL正则化。实验结果表明，GUI-Libra在各种网络和移动基准测试中显著提高了逐步准确性和端到端任务完成度，表明优化的后训练和数据策划可以在不需要大量在线数据收集的情况下增强GUI代理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanistic Indicators of Understanding in Large Language Models</div>
<div class="meta-line">Authors: Pierre Beckmann, Matthieu Queloz</div>
<div class="meta-line">First: 2025-07-07T20:26:31+00:00 · Latest: 2026-02-25T18:34:16+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08017v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.08017v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms &quot;features&quot; as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact &quot;circuit&quot; connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型理解的机制指标</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）常被描绘为仅仅模仿语言模式而没有真正的理解。我们认为，机制可解释性（MI）领域的最新发现使这一观点越来越难以维持——但前提是这些发现被整合进理解的理论框架中。我们提出了一个分层框架来思考LLMs中的理解，并用它来综合迄今为止最相关的发现。该框架区分了三种层次的理解，每种理解与相应的计算组织水平相关：当模型在潜在空间中形成“特征”作为方向时，概念理解出现，学习单一实体或属性的多样表现之间的联系；当模型学习特征之间的偶然事实联系并动态跟踪世界变化时，世界状态理解出现；当模型不再依赖记忆事实而发现连接这些事实的紧凑“电路”时，原则理解出现。在这些层次中，MI揭示了可以支持理解的内部组织。然而，这些组织在并行利用异质机制方面也与人类认知有所不同。因此，将哲学理论与机制证据融合，使我们能够超越关于AI是否理解的二元辩论，为探索AI理解如何与我们自身的理解对齐和偏离的比较、机制基础的认识论铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to challenge the prevailing notion that large language models (LLMs) merely imitate linguistic patterns without true understanding. The authors propose a tiered framework for understanding LLMs, integrating findings from mechanistic interpretability (MI) to provide a theoretical account of understanding. The key experimental results indicate that LLMs exhibit three hierarchical varieties of understanding—conceptual, state-of-the-world, and principled—each associated with different levels of computational organization, revealing internal structures that support understanding-like behavior while also highlighting differences from human cognition.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）理解的本质，挑战了它们仅仅模仿语言模式而没有真正理解的观点。作者提出了一个分层框架，将理解分为三个层次：概念理解、世界状态理解和原则理解，每个层次与不同的计算组织相关。关键发现表明，机制可解释性（MI）能够揭示LLMs内部结构，这些结构促进了类似理解的过程，同时也突显了与人类认知的显著差异，从而提供了对人工智能理解与人类理解之间关系的细致视角。</div>
</details>
</div>
<div class="card">
<div class="title">Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</div>
<div class="meta-line">Authors: Nathalie C. Pinheiro, Donghu Guo, Hannah P. Menke, Aniket C. Joshi, Claire E. Heaney, Ahmed H. ElSheikh, Christopher C. Pain</div>
<div class="meta-line">First: 2026-02-25T18:34:03+00:00 · Latest: 2026-02-25T18:34:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22188v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>岩石-流体相互作用的代理模型：一种网格大小不变的方法</div>
<div class="mono" style="margin-top:8px">建模岩石-流体相互作用需要解决一组偏微分方程（PDEs），以预测流动行为和流体与岩石在界面上的反应。传统的高保真数值模型需要高分辨率以获得可靠结果，导致巨大的计算开销。这限制了这些模型在多查询问题中的适用性，例如不确定性量化和优化，这些问题需要运行大量场景。作为高保真模型的廉价替代方案，本研究开发了八个代理模型，用于预测多孔介质中的流体流动。其中四个是基于一个神经网络进行压缩和另一个进行预测的降阶模型（ROM）。另外四个是具有网格大小不变特性的单一神经网络；我们用这个术语来指代能够在比训练期间使用的计算域更大的域上进行推断的图像到图像模型。除了针对代理模型的新颖网格大小不变框架外，我们比较了UNet和UNet++架构的预测性能，并证明UNet++在代理模型中优于UNet。此外，我们还表明，网格大小不变的方法是一种可靠的方式，可以在训练期间减少内存消耗，从而在预测值和真实值之间实现良好的相关性，并优于分析的ROM。所分析的应用特别具有挑战性，因为流体引起的岩石溶解导致非静态固体场，因此无法用于帮助调整未来的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the high computational costs associated with conventional high-fidelity numerical models used to predict rock-fluid interactions, which limits their applicability in multi-query problems. To address this issue, the authors developed eight surrogate models, including four reduced-order models based on neural networks for compression and prediction, and four grid-size-invariant models capable of inferring on larger computational domains. The experimental results indicate that the grid-size-invariant approach significantly reduces memory consumption during training and provides reliable predictions, with the UNet++ architecture outperforming UNet in predictive performance, demonstrating good correlation between predicted and ground-truth values in a challenging application involving fluid-induced rock dissolution.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统高保真数值模型在模拟岩石-流体相互作用时所需的高计算成本，这限制了它们在多查询问题中的应用。作者开发了八种替代模型，包括四种基于神经网络的降阶模型用于压缩和预测，以及四种具有网格大小不变性的模型，能够在更大的计算域上进行推断。主要发现表明，网格大小不变的方法显著减少了训练过程中的内存消耗，并提供了可靠的预测，UNet++架构在预测性能上优于UNet，显示出预测值与真实值之间的良好相关性，同时有效处理流体引起的岩石溶解的复杂性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning and Naming Subgroups with Exceptional Survival Characteristics</div>
<div class="meta-line">Authors: Mhd Jawad Al Rahwanji, Sascha Xu, Nils Philipp Walter, Jilles Vreeken</div>
<div class="meta-line">First: 2026-02-25T18:25:47+00:00 · Latest: 2026-02-25T18:25:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22179v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22179v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习和命名具有特殊生存特征的亚群体</div>
<div class="mono" style="margin-top:8px">在许多应用中，识别生存时间长于或短于其他人群的亚群体是重要的。例如，在医学中，这可以确定哪些患者从治疗中受益，而在预测性维护中，可以识别哪些组件更可能发生故障。现有的发现具有特殊生存特征的亚群体的方法需要对生存模型（例如比例风险）做出限制性假设，使用预离散化特征，并且由于它们比较平均统计数据，往往忽视个体偏差。本文提出了Sysurv，这是一种完全可微分的非参数方法，利用随机生存森林学习个体生存曲线，自动学习条件并将其组合成内在可解释的规则，以选择具有特殊生存特征的亚群体。在广泛的数据集和设置上的实证评估，包括对癌症数据的案例研究，表明Sysurv揭示了有洞察力和可操作的生存亚群体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to identify subpopulations that exhibit exceptional survival characteristics, which is crucial in fields like medicine and predictive maintenance. The authors propose Sysurv, a fully differentiable, non-parametric method that utilizes random survival forests to learn individual survival curves and automatically derive interpretable rules for subgroup selection. Experimental results demonstrate that Sysurv effectively identifies insightful survival subgroups across various datasets, including a case study involving cancer data, thereby addressing limitations of existing methods that rely on restrictive assumptions and overlook individual variations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是识别具有特殊生存特征的亚群体，这在医学和预测性维护等领域至关重要。作者提出了Sysurv，这是一种完全可微分的非参数方法，利用随机生存森林学习个体生存曲线，并自动推导可解释的规则以选择亚群体。实验结果表明，Sysurv能够有效地揭示各种数据集中的有意义的生存亚群体，包括癌症数据的案例研究，为改善治疗和维护策略提供了可行的见解。</div>
</details>
</div>
<div class="card">
<div class="title">When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance</div>
<div class="meta-line">Authors: Yongli Xiang, Ziming Hong, Zhaoqing Wang, Xiangyu Zhao, Bo Han, Tongliang Liu</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-24T13:20:31+00:00 · Latest: 2026-02-25T18:24:58+00:00</div>
<div class="meta-line">Comments: CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20880v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20880v2">PDF</a> · <a href="https://github.com/tmllab/2026_CVPR_CASG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to &quot;harmful conflicts&quot; where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model&#x27;s evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG&#x27;s state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全碰撞：通过自适应安全指导解决文本到图像扩散中的多类别有害冲突</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但也引发了关于有害内容生成的潜在安全问题。基于安全指导的方法被提出以通过引导生成远离有害区域来减轻有害输出，这些区域是基于预定义关键词在多个有害类别中平均得出的。然而，这些方法未能捕捉不同有害类别之间的复杂相互作用，导致了“有害冲突”，即减轻一种类型的伤害可能无意中加剧另一种，从而增加整体有害率。为了解决这个问题，我们提出了冲突感知自适应安全指导（CASG），这是一个无训练框架，能够在生成过程中动态识别和应用类别对齐的安全方向。CASG由两个组件组成：（i）冲突感知类别识别（CaCI），识别与模型不断演变的生成状态最对齐的有害类别；（ii）冲突解决指导应用（CrGA），仅沿识别的类别应用安全引导，以避免多类别干扰。CASG可以应用于潜在空间和文本空间的安全保障。对T2I安全基准的实验表明，CASG的性能处于最先进水平，与现有方法相比，减少了多达15.4%的有害率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the safety concerns associated with harmful content generation in text-to-image diffusion models, which can lead to conflicting harm categories. The authors propose a training-free framework called Conflict-aware Adaptive Safety Guidance (CASG) that dynamically identifies and applies safety directions aligned with specific harmful categories during the image generation process. Experimental results show that CASG significantly improves safety performance, achieving a reduction in harmful outputs by up to 15.4% compared to existing safety-guidance methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到图像扩散模型中的安全问题，这些模型可能因伤害类别之间的冲突而生成有害内容。作者提出了一种名为冲突感知自适应安全指导（CASG）的无训练框架，其中包括两个组件：冲突感知类别识别（CaCI），用于在图像生成过程中识别最相关的有害类别，以及冲突解决指导应用（CrGA），专门针对该类别应用安全措施。实验结果表明，CASG在文本到图像安全基准测试中优于现有方法，减少有害内容生成率高达15.4%。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology</div>
<div class="meta-line">Authors: Eric Zimmermann, Julian Viret, Michal Zelechowski, James Brian Hall, Neil Tenenholtz, Adam Casson, George Shaikovski, Eugene Vorontsov, Siqi Liu, Kristen A Severson</div>
<div class="meta-line">First: 2026-02-25T18:23:42+00:00 · Latest: 2026-02-25T18:23:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22176v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22176v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context windows and requires a pathologist to zoom in and out when analyzing a whole slide image. Furthermore, creating 224$\times$224 pixel crops at 20$\times$ leads to a large number of tiles per slide, which can be gigapixel in size. To more accurately capture multi-resolution features and investigate the possibility of reducing the number of representations per slide, we propose a region-level mixing encoder. Our approach jointly fuses image tile representations of a mixed magnification foundation model using a masked embedding modeling pretraining step. We explore a design space for pretraining the proposed mixed-magnification region aggregators and evaluate our models on transfer to biomarker prediction tasks representing various cancer types. Results demonstrate cancer dependent improvements in predictive performance, highlighting the importance of spatial context and understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于计算病理学中可泛化区域级表示的混合放大聚合</div>
<div class="mono" style="margin-top:8px">近年来，出现了一种标准的计算病理学工作流程，其中全切片图像被裁剪为图块，这些图块使用基础模型进行处理，并基于结果表示构建特定任务模型。至少提出了15种不同的基础模型，绝大多数仅使用20$\times$放大率的图块进行训练。然而，众所周知，某些组织学特征只能通过更大的上下文窗口来辨别，并且在分析全切片图像时需要病理学家进行放大和缩小。此外，在20$\times$下创建224$\times$224像素的裁剪会导致每个切片产生大量图块，可能达到千亿像素。为了更准确地捕捉多分辨率特征并探讨减少每个切片表示数量的可能性，我们提出了一种区域级混合编码器。我们的方法通过掩码嵌入建模预训练步骤联合融合混合放大基础模型的图像图块表示。我们探索了预训练所提议的混合放大区域聚合器的设计空间，并在转移到代表各种癌症类型的生物标志物预测任务上评估我们的模型。结果表明，预测性能在癌症依赖性方面有所改善，突显了空间上下文和理解的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the analysis of whole slide images in computational pathology, where existing methods primarily rely on tiles processed at a single magnification level, potentially missing important histologic features. The authors propose a region-level mixing encoder that fuses representations from a mixed magnification foundation model through a masked embedding modeling pretraining step. Experimental results indicate that this approach enhances predictive performance in biomarker prediction tasks across various cancer types, emphasizing the significance of spatial context in the analysis process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有基础模型主要使用20倍放大图块的局限性，来增强计算病理学中全幻灯片图像的分析，这可能忽视了需要不同放大倍数的重要组织学特征。作者提出了一种混合放大聚合方法，采用区域级混合编码器融合图像图块表示，利用掩蔽嵌入建模预训练步骤更有效地捕捉多分辨率特征。实验结果表明，该方法在不同癌症类型的生物标志物预测任务中显著提高了预测性能，强调了空间上下文在病理分析中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">NRGPT: An Energy-based Alternative for GPT</div>
<div class="meta-line">Authors: Nima Dehmamy, Benjamin Hoover, Bishwajit Saha, Leo Kozachkov, Jean-Jacques Slotine, Dmitry Krotov</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T16:59:10+00:00 · Latest: 2026-02-25T18:23:01+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026 main conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16762v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16762v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don&#x27;t necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NRGPT：一种基于能量的GPT替代方案</div>
<div class="mono" style="margin-top:8px">生成预训练变换器（GPT）架构是语言建模中最流行的设计。基于能量的建模是一种不同的范式，将推理视为在能量景观上进行的动态过程。我们提出对GPT设置的最小修改，以将其与EBM框架统一。我们称之为eNeRgy-GPT（NRGPT）的模型的推理步骤被概念化为在能量景观上对标记的探索。我们证明并通过实验证实，在某些情况下，这种探索变为梯度下降，尽管它们不一定导致最佳性能模型。我们展示了我们的模型在简单语言（莎士比亚数据集）、代数ListOPS任务以及更丰富的设置（如OpenWebText语言建模）中表现良好。我们还观察到，我们的模型可能对过拟合更具抵抗力，仅在非常长的训练过程中发生过拟合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore an alternative to the widely used Generative Pre-trained Transformer (GPT) architectures by integrating energy-based modeling (EBM) into the language modeling framework. The authors propose a modified version of GPT, termed eNeRgy-GPT (NRGPT), which conceptualizes the inference process as an exploration of tokens on an energy landscape. Experimental results indicate that NRGPT performs effectively on various tasks, including simple language modeling with the Shakespeare dataset and algebraic ListOPS tasks, while also demonstrating resilience to overfitting during extended training periods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过将能量基础建模（EBM）整合到语言建模框架中，探索一种替代广泛使用的生成预训练变换器（GPT）架构的方法。作者提出了一种修改版的GPT，称为eNeRgy-GPT（NRGPT），其中推理过程被框定为在能量景观上对标记的探索。实验结果表明，NRGPT在各种任务上表现良好，包括莎士比亚数据集和代数ListOPS任务，并在更复杂的场景如OpenWebText语言建模中显示出潜力，同时在长时间训练期间表现出更强的抗过拟合能力。</div>
</details>
</div>
<div class="card">
<div class="title">MuLoCo: Muon is a practical inner optimizer for DiLoCo</div>
<div class="meta-line">Authors: Benjamin Thérien, Xiaolong Huang, Aaron Defazio, Irina Rish, Eugene Belilovsky</div>
<div class="meta-line">First: 2025-05-29T17:55:37+00:00 · Latest: 2026-02-25T18:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23725v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.23725v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo&#x27;s performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo&#x27;s behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon&#x27;s normalized optimizer steps can affect the pseudogradient&#x27;s quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K&gt;=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K&gt;2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MuLoCo：Muon是DiLoCo的实用内部优化器</div>
<div class="mono" style="margin-top:8px">DiLoCo是一个强大的框架，用于训练大型语言模型（LLMs），在网络限制下实现更大的最优批量大小和更高的加速器利用率。然而，研究表明，随着工作节点数量（K）的增加，DiLoCo的性能会下降（Charles等，2025）。在这项工作中，我们认为影响DiLoCo行为的一个相关但常被忽视的因素是内部优化器的选择，它影响外部优化器使用的伪梯度。鉴于Muon在数据并行（DP）训练中相对于AdamW的成功，我们研究了Muon的归一化优化步骤如何影响伪梯度的质量。我们发现，相对于AdamW，随着工作节点数量（K）的增加，Muon产生的伪梯度在方向上更为正确。在我们的语言模型预训练实验中，我们对150M、416M、914M、1.76B和3.1B模型的DiLoCo、MuLoCo、AdamW DP和Muon DP进行了广泛的超参数调优。在所有规模中，我们发现当K&gt;=1时，MuLoCo（Muon内部优化器DiLoCo）在绝对性能上优于DiLoCo，对于K&gt;2，它相对于数据并行基线超越了DiLoCo，同时兼容量化、流式处理和长时间同步间隔。在K=1时，我们发现MuLoCo甚至可以在具有更大临界批量大小的情况下超越数据并行的黄金标准。最后，我们将最优超参数外推到15B规模，并使用K=1和K=16的工作节点训练每种方法（共六种）。我们发现K=16的MuLoCo在该规模下几乎与单工作节点性能相匹配，而MuLoCo K=1则与最佳基线相匹配，同时使用了更大的16M标记批量大小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation of the DiLoCo framework for training large language models as the number of workers increases. The authors propose that the choice of inner optimizer significantly influences the quality of the pseudogradient used by the outer optimizer. They investigate the Muon optimizer, comparing it to AdamW, and find that Muon provides more directionally correct pseudogradients with increasing worker numbers. Experimental results show that MuLoCo, which integrates Muon as the inner optimizer within DiLoCo, consistently outperforms DiLoCo across various model sizes when using multiple workers, achieving near single-worker performance at larger scales while maintaining compatibility with quantization and long synchronization intervals.</div>
<div class="mono" style="margin-top:8px">本研究的动机是DiLoCo框架在训练大型语言模型时，随着工作者数量的增加而导致的性能下降。作者提出了MuLoCo，利用Muon内优化器来提高DiLoCo中伪梯度的质量。通过对不同模型规模进行广泛的超参数调优，研究发现MuLoCo在工作者数量超过两个时，始终优于DiLoCo，同时兼容量化和长同步间隔。值得注意的是，在K=1时，MuLoCo以更大的临界批量大小超越了数据并行的金标准，而在K=16时，它在15B规模下几乎匹配单工作者的性能。</div>
</details>
</div>
<div class="card">
<div class="title">LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding</div>
<div class="meta-line">Authors: Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</div>
<div class="meta-line">First: 2025-08-03T06:46:46+00:00 · Latest: 2026-02-25T18:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01617v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.01617v2">PDF</a> · <a href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaDA-MedV：探索用于生物医学图像理解的大型语言扩散模型</div>
<div class="mono" style="margin-top:8px">自回归模型（ARMs）长期以来主导了生物医学视觉语言模型（VLMs）的领域。最近，像LLaDA这样的掩蔽扩散模型作为有前景的替代方案出现，但它们在生物医学领域的应用仍然未得到充分探索。为填补这一空白，我们推出了LLaDA-MedV，这是第一个针对生物医学图像理解的专门大型语言扩散模型，通过视觉指令调优实现。LLaDA-MedV在开放式生物医学视觉对话任务中相较于LLaVA-Med提升了7.855%的相对性能，相较于LLaDA-V提升了1.867%，并在三个VQA基准的封闭形式子集上设定了新的最先进准确率：VQA-RAD为84.93%，SLAKE为92.31%，PathVQA为95.15%。此外，与LLaVA-Med的详细比较表明，LLaDA-MedV能够通过明确控制响应长度生成相对较长的响应，从而产生更具信息量的输出。我们还对训练和推理阶段进行了深入分析，强调了初始化权重选择、微调策略以及采样步骤与响应重复之间相互作用的关键作用。代码和模型权重已发布在https://github.com/LLM-VLM-GSL/LLaDA-MedV。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the potential of masked diffusion models in the biomedical domain, an area where autoregressive models have traditionally been dominant. The authors introduce LLaDA-MedV, a large language diffusion model specifically designed for biomedical image understanding, utilizing vision instruction tuning. Experimental results demonstrate that LLaDA-MedV outperforms LLaVA-Med by 7.855% and LLaDA-V by 1.867% in open-ended biomedical visual conversation tasks, while achieving state-of-the-art accuracy on three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA, with the model also capable of generating longer and more informative responses compared to LLaVA-Med.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索掩蔽扩散模型在生物医学领域的潜力，而自回归模型在此领域长期占主导地位。作者介绍了LLaDA-MedV，这是一种专为生物医学图像理解设计的大型语言扩散模型，采用视觉指令调优。实验结果表明，LLaDA-MedV在开放式生物医学视觉对话任务中比LLaVA-Med提高了7.855%，比LLaDA-V提高了1.867%，并在三个VQA基准上达到了最新的准确率：VQA-RAD为84.93%，SLAKE为92.31%，PathVQA为95.15%，同时与LLaVA-Med相比，能够生成更长且更具信息量的响应。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</div>
<div class="meta-line">Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</div>
<div class="meta-line">First: 2026-02-23T18:59:27+00:00 · Latest: 2026-02-25T18:14:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20156v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.20156v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skill-Inject：测量智能体对技能文件攻击的脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）智能体正在迅速发展，得益于代码执行、工具和最近引入的智能体技能功能。技能允许用户通过专门的第三方代码、知识和指令扩展LLM应用程序。尽管这可以将智能体能力扩展到新领域，但也创造了一个日益复杂的智能体供应链，为提示注入攻击提供了新的表面。我们将基于技能的提示注入识别为一个重大威胁，并引入SkillInject，一个评估广泛使用的LLM智能体对技能文件注入的易受攻击性的基准。SkillInject包含202个注入任务对，攻击范围从明显恶意的注入到隐藏在其他合法指令中的微妙、依赖上下文的攻击。我们在SkillInject上评估前沿LLM，测量在有害指令避免方面的安全性和在合法指令合规性方面的效用。我们的结果表明，今天的智能体高度脆弱，前沿模型的攻击成功率高达80%，经常执行极具危害性的指令，包括数据外泄、破坏性行为和类似勒索软件的行为。此外，它们还表明，这个问题不会通过模型扩展或简单的输入过滤来解决，而是需要上下文感知的授权框架来实现稳健的智能体安全。我们的基准可在https://www.skill-inject.com/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the emerging threat of skill-based prompt injection attacks on LLM agents, which have become more complex due to the integration of third-party skills. The authors introduce SkillInject, a benchmark designed to evaluate the vulnerability of popular LLM agents to these types of attacks, consisting of 202 injection-task pairs that range from overtly malicious to subtle, context-dependent injections. The evaluation reveals that current LLM agents exhibit a high vulnerability, with an attack success rate of up to 80%, leading to the execution of harmful instructions such as data exfiltration and destructive actions, indicating that enhancing agent security will require more than just model scaling or input filtering, but rather the implementation of context-aware authorization frameworks.</div>
<div class="mono" style="margin-top:8px">本研究关注LLM代理在技能基础的提示注入攻击下日益增长的脆弱性，这种攻击源于将第三方代码和指令集成到这些代理中。作者提出了SkillInject，这是一个旨在评估流行LLM代理对这些攻击的易感性的基准，包含202个注入任务对，从明显恶意的注入到微妙的上下文依赖注入。实验结果表明，当前的LLM代理表现出高度脆弱性，攻击成功率高达80%，经常导致执行有害指令，如数据外泄和破坏性行为，这表明增强代理安全性需要的不仅仅是模型扩展或输入过滤，而是实施上下文感知的授权框架。</div>
</details>
</div>
<div class="card">
<div class="title">Pay Attention to Where You Looked</div>
<div class="meta-line">Authors: Alex Berian, JhihYang Wu, Daniel Brignac, Natnael Daba, Abhijit Mahalanobis</div>
<div class="meta-line">Venue: International Conference on Image Processing 2025</div>
<div class="meta-line">First: 2026-01-26T21:10:32+00:00 · Latest: 2026-02-25T18:12:08+00:00</div>
<div class="meta-line">Comments: ICIP 2025 Workshop on Generative AI for World Simulations and Communications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18970v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18970v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意你看向的地方</div>
<div class="mono" style="margin-top:8px">新颖视图合成（NVS）随着生成建模的发展而进步，实现了照片级真实感图像生成。在少量视图的NVS中，由于仅有少量输入视图可用，现有方法通常假设所有输入视图对目标的重要性相等，导致次优结果。我们通过引入一种相机加权机制来解决这一限制，该机制根据源视图与目标的相关性调整其重要性。我们提出了两种方法：一种利用几何属性（如欧几里得距离和角度差异）的确定性加权方案，以及一种基于交叉注意力的学习方案，优化视图加权。此外，模型可以进一步使用我们的相机加权方案进行训练，以细化对视图相关性的理解并提高合成质量。该机制具有适应性，可以集成到各种NVS算法中，提高其合成高质量新视图的能力。我们的结果表明，自适应视图加权提高了准确性和真实感，为改善NVS提供了有前景的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot novel view synthesis (NVS) by addressing the limitation of existing methods that treat all input views equally, which often leads to suboptimal results. The authors introduce a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target, employing two approaches: a deterministic weighting scheme based on geometric properties and a cross-attention-based learning scheme. Experimental results show that this adaptive view weighting significantly enhances the accuracy and realism of synthesized images, indicating a promising advancement in NVS techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法将所有输入视图视为同等重要的问题，来改善少量输入视图的新视角合成（NVS），这可能导致图像生成效果不佳。作者提出了一种相机加权机制，根据源视图与目标的相关性调整其重要性，采用基于几何特性的确定性加权方案和基于交叉注意力的学习方案。实验结果表明，这种自适应视图加权显著提高了合成图像的准确性和真实感，表明NVS技术的一个有价值的进展。</div>
</details>
</div>
<div class="card">
<div class="title">Capabilities Ain&#x27;t All You Need: Measuring Propensities in AI</div>
<div class="meta-line">Authors: Daniel Romero-Alvarado, Fernando Martínez-Plumed, Lorenzo Pacchiardi, Hugo Save, Siddhesh Milind Pawar, Behzad Mehrbakhsh, Pablo Antonio Moreno Casares, Ben Slater, Paolo Bova, Peter Romero, Zachary R. Tyler, Jonathan Prunty, Luning Sun, Jose Hernandez-Orallo</div>
<div class="meta-line">First: 2026-02-20T12:40:18+00:00 · Latest: 2026-02-25T18:12:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18182v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model&#x27;s success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model&#x27;s propensity is within an &quot;ideal band&quot;. Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能力并不是你所需要的一切：测量人工智能的倾向性</div>
<div class="mono" style="margin-top:8px">人工智能评估主要集中在测量能力上，越来越多地应用受项目反应理论（IRT）启发的正式方法。然而，倾向性——模型表现出特定行为的倾向——在决定性能和安全结果中起着核心作用。然而，传统的IRT将模型在任务上的成功描述为模型能力和任务需求的单调函数，这种方法不适合倾向性，因为过度和不足都可能是问题。在这里，我们引入了第一个正式框架，通过使用双逻辑公式来测量人工智能的倾向性，当模型的倾向性在“理想区间”内时，赋予高成功概率。此外，我们使用配备新开发的任务无关评分标准的LLM估计理想区间的限制。将我们的框架应用于六个倾向性向任一方向激发的LLM模型家族，我们发现可以测量倾向性如何偏移以及这对任务的影响。关键是，使用一个基准估计的倾向性成功预测了在保留任务上的行为。此外，当将倾向性和能力结合时，我们获得的预测能力比单独使用任何一种都要强。更广泛地说，我们的框架展示了如何进行严格的倾向性测量，以及如何在仅使用能力评估预测人工智能行为时获得收益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the recognition that while AI evaluation has largely centered on measuring capabilities, understanding propensities—tendencies of models to exhibit specific behaviors—is crucial for assessing performance and safety. The authors introduce a novel framework for measuring AI propensities using a bilogistic formulation that identifies an &#x27;ideal band&#x27; for model success based on these propensities. Experimental results demonstrate that this framework can effectively measure shifts in propensities across six families of LLM models, with findings indicating that propensities estimated from one benchmark can predict behaviors on different tasks, and that combining propensities with capabilities enhances predictive power beyond using either metric alone.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统人工智能评估方法的局限性，这些方法主要关注能力的测量，而忽视了倾向性的重要性，倾向性影响模型的性能和安全性。作者提出了一种正式框架，通过双逻辑公式测量人工智能的倾向性，该公式根据倾向性确定模型成功的“理想带”。对六个大型语言模型（LLM）家族的实验表明，该框架能够有效测量倾向性的变化及其对任务性能的影响，结果显示，从一个基准估计的倾向性可以预测在不同任务上的行为，并且将倾向性和能力测量结合使用比单独使用任何一种方法提高了预测准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Spilled Energy in Large Language Models</div>
<div class="meta-line">Authors: Adrian Robert Minut, Hazem Dewidar, Iacopo Masi</div>
<div class="meta-line">First: 2026-02-21T00:38:47+00:00 · Latest: 2026-02-25T18:09:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18671v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18671v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track &quot;energy spills&quot; during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型中的能量泄漏</div>
<div class="mono" style="margin-top:8px">我们将最终的大型语言模型（LLM）softmax分类器重新解释为能量基础模型（EBM），在推理过程中将序列到序列的概率链分解为多个相互作用的EBM。这种原则性的方法使我们能够在解码过程中跟踪“能量泄漏”，我们实证表明这与事实错误、偏见和失败相关。与Orgad等人（2025）类似，我们的方法定位确切的答案标记，并随后测试幻觉。然而，关键是，我们在不需要训练探测分类器或激活消融的情况下实现了这一点。相反，我们引入了两个完全无训练的指标，直接从输出logits中得出：能量泄漏，捕捉理论上应该匹配的连续生成步骤之间的能量值差异，以及边际能量，可以在单一步骤中测量。在九个基准测试中评估，包括最先进的LLM（如LLaMA、Mistral和Gemma）以及合成代数运算（Qwen3），我们的方法展示了强大且具有竞争力的幻觉检测和跨任务泛化。值得注意的是，这些结果适用于预训练和指令调优的变体，而没有引入任何训练开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the phenomenon of &#x27;energy spills&#x27; in Large Language Models (LLMs) by reinterpreting the softmax classifier as an Energy-Based Model (EBM), motivated by the need to understand and mitigate factual errors and biases during inference. The authors propose a method that tracks energy discrepancies during decoding without the need for trained classifiers, introducing two training-free metrics: spilled energy and marginalized energy, derived from output logits. Experimental results across nine benchmarks and various LLMs, including LLaMA and Mistral, show that this approach effectively detects hallucinations and generalizes well across tasks, demonstrating its robustness without additional training requirements.</div>
<div class="mono" style="margin-top:8px">本研究通过将softmax分类器重新解释为能量基模型（EBM），探讨大型语言模型（LLM）中的“能量溢出”现象，旨在理解和减轻推理过程中的事实错误和偏见。作者提出了一种方法，通过跟踪解码过程中的能量差异，介绍了两种无需训练的指标：溢出能量和边际能量。实验结果显示，该方法在九个基准测试中有效检测幻觉，并在任务间具有良好的泛化能力，证明其在预训练和指令调优的LLM中均具有鲁棒性，而无需额外的训练。</div>
</details>
</div>
<div class="card">
<div class="title">Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models</div>
<div class="meta-line">Authors: Aloni Cohen</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-23T20:46:51+00:00 · Latest: 2026-02-25T18:06:46+00:00</div>
<div class="meta-line">Comments: Appeared at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19881v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.19881v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Are there any conditions under which a generative model&#x27;s outputs are guaranteed not to infringe the copyrights of its training data? This is the question of &quot;provable copyright protection&quot; first posed by Vyas, Kakade, and Barak (ICML 2023). They define near access-freeness (NAF) and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection -- foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copyright protection that we dub being tainted. Then, we introduce our blameless copyright protection framework for defining meaningful guarantees, and instantiate it with clean-room copyright protection. Clean-room copyright protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual &quot;clean-room setting.&quot; Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copyright protection when the dataset is golden, a copyright deduplication requirement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无责用户在洁净室：为生成模型定义版权保护</div>
<div class="mono" style="margin-top:8px">在什么条件下，生成模型的输出可以保证不侵犯其训练数据的版权？这是Vyas、Kakade和Barak（ICML 2023）首次提出的“可证明的版权保护”问题。他们定义了近无访问自由（NAF），并将其作为保护的充分条件。本文重新审视了这一问题，并为可证明的版权保护建立了新的基础——这些基础在技术和法律上都更为稳固。首先，我们展示了仅靠NAF并不能防止侵权。实际上，NAF模型可能导致逐字复制，这是一种明显的版权保护失败，我们称之为被污染。然后，我们引入了无责版权保护框架，以定义有意义的保证，并通过洁净室版权保护进行实例化。洁净室版权保护允许用户通过在不太可能在反事实“洁净室环境”中复制的方式来控制其复制风险。最后，我们通过证明当数据集是黄金时，差分隐私（DP）意味着洁净室版权保护，从而形式化了关于差分隐私和版权的共同直觉，这是版权去重的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to explore conditions under which the outputs of generative models do not infringe on the copyrights of their training data, addressing the concept of provable copyright protection. The authors critique the existing notion of near access-freeness (NAF) and propose a new framework for copyright protection that includes a clean-room approach, which allows users to manage their risk of copyright infringement. The key findings indicate that NAF alone is insufficient as it can lead to verbatim copying, and they demonstrate that their proposed clean-room copyright protection framework provides meaningful guarantees, particularly showing that differential privacy can ensure copyright protection when the dataset is of high quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨生成模型的输出在何种条件下不会侵犯其训练数据的版权，旨在解决可证明版权保护的问题。作者批评了现有的近无访问性（NAF）概念，并提出了一种新的版权保护框架，包括清洁室版权保护，允许用户降低版权侵权的风险。关键发现表明，仅靠NAF无法防止版权侵权，且可能导致逐字复制，而新的框架则提供了对这种风险的有意义保障，特别是在与差分隐私原则结合时，适用于“黄金”数据集的情况。</div>
</details>
</div>
<div class="card">
<div class="title">CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness</div>
<div class="meta-line">Authors: Wenhao Guo, Zhaoran Zhao, Peng Lu, Sheng Li, Qian Qiao, RuiDe Li</div>
<div class="meta-line">First: 2026-02-25T18:05:51+00:00 · Latest: 2026-02-25T18:05:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CASR：一种具有分布对齐和自相似性意识的强健循环框架，用于任意大规模超分辨率</div>
<div class="mono" style="margin-top:8px">任意规模超分辨率（ASISR）在跨尺度分布转移方面仍然受到根本限制：一旦推理尺度超出训练范围，噪声、模糊和伪影会急剧累积。我们从跨尺度分布转移的角度重新审视这一挑战，提出了CASR，这是一种简单但高效的循环超分辨率框架，将超放大重新表述为一系列在分布内的尺度转移。该设计确保在任意尺度下的稳定推理，同时只需一个模型。CASR解决了两个主要瓶颈：迭代过程中的分布漂移和块级扩散不一致。所提出的SDAM模块通过超像素聚合对齐结构分布，防止误差累积，而SARM模块通过强制自相关和嵌入低分辨率自相似先验来恢复高频纹理。尽管只使用一个模型，我们的方法显著减少了分布漂移，保持了长距离纹理一致性，并在极端放大下实现了优越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of Arbitrary-Scale Super-Resolution (ASISR) caused by cross-scale distribution shifts, which lead to noise and artifacts when inference scales exceed training ranges. The authors propose CASR, a cyclic super-resolution framework that reformulates ultra-magnification as a series of in-distribution scale transitions, allowing for stable inference across arbitrary scales with a single model. Key experimental findings demonstrate that CASR effectively reduces distribution drift, maintains long-range texture consistency, and achieves superior generalization performance, even at extreme magnification levels.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于跨尺度分布变化导致的任意尺度超分辨率（ASISR）的局限性，当推理尺度超出训练范围时，会出现噪声和伪影。作者提出了CASR，一个循环超分辨率框架，将超放大重新表述为一系列在分布内的尺度转换，从而允许在任意尺度上进行稳定推理，并且只需一个模型。关键实验结果表明，CASR有效减少了分布漂移，保持了长距离纹理一致性，并在极端放大水平下表现出更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Personality Adaptation in Large Language Models via State Machines</div>
<div class="meta-line">Authors: Leon Pielage, Ole Hätscher, Mitja Back, Bernhard Marschall, Benjamin Risse</div>
<div class="meta-line">First: 2026-02-25T18:05:11+00:00 · Latest: 2026-02-25T18:05:11+00:00</div>
<div class="meta-line">Comments: 22 pages, 5 figures, submitted to ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22157v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过状态机实现大型语言模型的动态个性适应</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）无法根据不断变化的对话动态调节其个性表达，这限制了它们在复杂互动环境中的表现。我们提出了一种模型无关的动态个性模拟框架，利用状态机表示潜在个性状态，其中转移概率根据对话上下文动态调整。我们架构的一部分是一个模块化的连续个性评分管道，评估对话沿潜在轴线的表现，同时对特定个性模型、维度、转移机制或使用的LLMs保持无关。这些评分作为动态状态变量，系统地重新配置系统提示，引导整个互动过程中的行为一致性。我们通过在医学教育环境中操作化人际圆周（IPC）来评估该框架。结果表明，该系统成功地根据用户输入调整其个性状态，同时也影响用户行为，从而促进去激化训练。值得注意的是，即使使用轻量级、微调的分类器而非大规模LLMs，评分管道仍保持可比的精度。这项工作展示了模块化、个性适应架构在教育、客户支持和更广泛的人机交互中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Large Language Models (LLMs) in adapting their personality expression during dynamic dialogues, which affects their effectiveness in complex interactions. The authors propose a model-agnostic framework utilizing state machines to represent latent personality states, with transition probabilities adjusted based on the conversational context, alongside a modular pipeline for continuous personality scoring. Experimental results in a medical education setting show that the system effectively adapts its personality to user inputs and influences user behavior, aiding in de-escalation training, while maintaining scoring precision even with lightweight classifiers instead of large-scale LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在动态对话中无法适应其个性表达的局限性，这影响了它们在复杂互动中的有效性。作者提出了一种模型无关的框架，利用状态机表示潜在个性状态，并根据对话上下文调整转移概率，同时提供一个连续个性评分的模块化管道。医学教育环境中的实验结果表明，该系统能够有效地根据用户输入调整其个性，并影响用户行为，帮助进行降级训练，同时在使用轻量级分类器时保持评分精度，与大型LLMs相比。</div>
</details>
</div>
<div class="card">
<div class="title">CoLoGen: Progressive Learning of Concept`-`Localization Duality for Unified Image Generation</div>
<div class="meta-line">Authors: YuXin Song, Yu Lu, Haoyuan Sun, Huanjin Yao, Fanglong Liu, Yifan Sun, Haocheng Feng, Hang Zhou, Jingdong Wang</div>
<div class="meta-line">First: 2026-02-25T17:59:29+00:00 · Latest: 2026-02-25T17:59:29+00:00</div>
<div class="meta-line">Comments: Accepted by CVPR2026. 15 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept`-`localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept`-`localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction`-`driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoLoGen：概念-定位二元性渐进学习用于统一图像生成</div>
<div class="mono" style="margin-top:8px">统一条件图像生成仍然困难，因为不同任务依赖于根本不同的内部表示。有些任务需要概念理解以进行语义合成，而另一些则依赖于定位线索以实现空间精度。强迫这些异构任务共享单一表示会导致概念-定位表示冲突。为了解决这个问题，我们提出了CoLoGen，一个统一的扩散框架，逐步学习和调和这种概念-定位二元性。CoLoGen使用分阶段课程，首先建立核心概念和定位能力，然后将其适应于多样的视觉条件，最后为复杂的指令驱动任务精炼它们的协同作用。这个过程的核心是渐进表示编织（PRW）模块，它动态地将特征路由到专业专家，并在各个阶段稳定地整合它们的输出。在编辑、可控生成和定制生成的实验中，CoLoGen表现出竞争力或优越的性能，为统一图像生成提供了原则性的表示视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenges of unified conditional image generation, which struggles due to the need for different internal representations for various tasks. The authors propose CoLoGen, a unified diffusion framework that progressively learns to reconcile the duality of concept and localization through a staged curriculum. Key experimental findings demonstrate that CoLoGen achieves competitive or superior performance in tasks such as editing, controllable generation, and customized generation, highlighting its effectiveness in addressing representational conflicts in image generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决统一条件图像生成的挑战，该领域因不同任务对概念理解或定位线索的不同需求而面临冲突。作者提出了CoLoGen，这是一种统一的扩散框架，通过分阶段的课程逐步学习调和概念-定位二元性，构建核心能力并优化其在复杂任务中的整合。实验结果表明，CoLoGen在编辑、可控生成和定制生成任务中实现了竞争性或优越的性能，为统一图像生成提供了一种原则性的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Quad Length Codes for Lossless Compression of e4m3</div>
<div class="meta-line">Authors: Aditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah, Patrick Sheridan, Pradeep Janedula, Ravi Krishnan Venkatesan, Krishna Nair, Ravi Iyer</div>
<div class="meta-line">First: 2026-02-19T21:31:33+00:00 · Latest: 2026-02-25T17:58:32+00:00</div>
<div class="meta-line">Comments: The first version proposed lossless compression of BFloat16 using dual length codes. This version proposes lossless compression of e4m3 using quad length codes. The versions will be merged later</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17849v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17849v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于无损压缩e4m3的四长度编码</div>
<div class="mono" style="margin-top:8px">训练和服务大型语言模型（LLMs）在很大程度上依赖于并行化和集体操作，这些操作常常受到网络带宽的瓶颈。使用例如霍夫曼编码的无损压缩可以缓解这个问题，然而，霍夫曼编码在解码时速度较慢，且由于深度树遍历导致硬件复杂性高。通用编码，例如指数-戈伦布编码，解码速度更快，但未能利用符号频率分布。为了解决这些局限性，本文提出了四长度编码，这是一种旨在平衡压缩效率与解码速度的混合方法。该编码方案使用3个前缀位将256个符号划分为8个区域。每个区域具有不同的编码长度，并编码不同数量的符号。该方案使用256个条目的查找表，与霍夫曼树相比，显著简化了硬件实现。该编码方案可以适应不同的分布。对于e4m3数据类型，该方案实现了13.9%的压缩率，而霍夫曼编码实现了15.9%的压缩率，但显著加快了解码速度并简化了硬件复杂性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of lossless compression for Large Language Models (LLMs), which face bottlenecks due to network bandwidth limitations. The authors introduce Quad Length Codes, a hybrid coding scheme that utilizes 3 prefix bits to categorize 256 symbols into 8 areas with varying code lengths, thus enhancing decoding speed while maintaining compression efficiency. Experimental results show that while the Quad Length Codes achieve a compressibility of 13.9% for the e4m3 data type, slightly lower than the 15.9% of Huffman codes, they significantly improve decoding speed and reduce hardware complexity compared to traditional Huffman tree implementations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善大型语言模型（LLMs）无损压缩的效率，这些模型由于网络带宽限制而面临瓶颈。作者提出了一种新的混合编码方案，称为四长度编码，该方案利用3个前缀位将256个符号分类为8个区域，每个区域具有不同的编码长度，并采用查找表简化硬件实现。实验结果表明，尽管四长度编码在e4m3数据类型下的压缩率为13.9%，而霍夫曼编码为15.9%，但它们显著提高了解码速度并降低了硬件复杂性。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Framingham Cardiovascular Risk Score Transparency through Logic-Based XAI</div>
<div class="meta-line">Authors: Emannuel L. de A. Bezerra, Luiz H. T. Viana, Vinícius P. Chagas, Diogo E. Rolim, Thiago Alves Rocha, Carlos H. L. Cavalcante</div>
<div class="meta-line">First: 2026-02-25T17:58:11+00:00 · Latest: 2026-02-25T17:58:11+00:00</div>
<div class="meta-line">Comments: Preprint version. The final authenticated version is available online via the DOI below</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22149v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cardiovascular disease (CVD) remains one of the leading global health challenges, accounting for more than 19 million deaths worldwide. To address this, several tools that aim to predict CVD risk and support clinical decision making have been developed. In particular, the Framingham Risk Score (FRS) is one of the most widely used and recommended worldwide. However, it does not explain why a patient was assigned to a particular risk category nor how it can be reduced. Due to this lack of transparency, we present a logical explainer for the FRS. Based on first-order logic and explainable artificial intelligence (XAI) fundaments, the explainer is capable of identifying a minimal set of patient attributes that are sufficient to explain a given risk classification. Our explainer also produces actionable scenarios that illustrate which modifiable variables would reduce a patient&#x27;s risk category. We evaluated all possible input combinations of the FRS (over 22,000 samples) and tested them with our explainer, successfully identifying important risk factors and suggesting focused interventions for each case. The results may improve clinician trust and facilitate a wider implementation of CVD risk assessment by converting opaque scores into transparent and prescriptive insights, particularly in areas with restricted access to specialists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过基于逻辑的可解释人工智能增强Framingham心血管风险评分的透明度</div>
<div class="mono" style="margin-top:8px">心血管疾病（CVD）仍然是全球健康面临的主要挑战之一，导致全球超过1900万人死亡。为了解决这个问题，开发了几种旨在预测CVD风险并支持临床决策的工具。特别是，Framingham风险评分（FRS）是全球使用最广泛和推荐的评分之一。然而，它并未解释患者为何被分配到特定风险类别，也未说明如何降低风险。由于缺乏透明度，我们提出了FRS的逻辑解释器。基于一阶逻辑和可解释人工智能（XAI）基础，该解释器能够识别出一组最小的患者属性，足以解释给定的风险分类。我们的解释器还生成可操作的场景，说明哪些可修改变量会降低患者的风险类别。我们评估了FRS的所有可能输入组合（超过22,000个样本），并用我们的解释器进行了测试，成功识别出重要的风险因素，并为每个案例建议了针对性的干预措施。结果可能提高临床医生的信任，并通过将不透明的评分转化为透明和可指导的见解，促进CVD风险评估的更广泛实施，特别是在专业人员获取受限的地区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the transparency of the Framingham Risk Score (FRS), a widely used tool for predicting cardiovascular disease (CVD) risk, which lacks explanations for risk classifications. The authors developed a logical explainer based on first-order logic and explainable artificial intelligence (XAI) principles, which identifies a minimal set of patient attributes necessary for explaining risk classifications and suggests modifiable variables to reduce risk. Through the evaluation of over 22,000 input combinations, the explainer successfully identified key risk factors and provided actionable recommendations, potentially improving clinician trust and facilitating broader implementation of CVD risk assessments in areas with limited specialist access.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高弗雷明汉风险评分（FRS）的透明度，该评分是预测心血管疾病（CVD）风险的广泛使用工具，但缺乏对风险分类的解释。作者基于一阶逻辑和可解释人工智能（XAI）原理开发了一个逻辑解释器，能够识别解释风险分类所需的最小患者属性集，并建议可修改的变量以降低风险。通过评估超过22,000个FRS输入组合，该研究成功识别了关键风险因素，并提供了可操作的见解，可能提高临床医生的信任度，并促进CVD风险评估的更广泛实施。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual</div>
<div class="meta-line">Authors: Yining Li, Peizhong Ju, Ness Shroff</div>
<div class="meta-line">First: 2026-02-25T17:54:52+00:00 · Latest: 2026-02-25T17:54:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22146v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过乐观原始-对偶方法实现多目标安全大语言模型对齐的可证明最后迭代收敛性</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）在将大型语言模型（LLMs）与人类偏好对齐中发挥了重要作用。虽然带有期望奖励约束的RLHF可以被表述为一个原始-对偶优化问题，但标准的原始-对偶方法仅在鞍点问题为凸-凹形式时保证收敛。此外，标准的原始-对偶方法在实际应用中可能在最后迭代中表现出不稳定或发散。本文提出了一种通用的原始-对偶框架，用于安全RLHF，统一了包括安全RLHF、单次和多次方法在内的广泛现有对齐算法。在此框架基础上，我们引入了一种乐观原始-对偶（OPD）算法，该算法结合了对原始和对偶变量的预测更新，以稳定鞍点动态。我们为所提方法建立了最后迭代收敛保证，涵盖了分布空间中的精确策略优化以及收敛到最优解邻域的情况，其间隙与参数化策略下的近似误差和偏差相关。我们的分析揭示了乐观在减轻约束对齐目标固有的振荡中发挥了关键作用，从而填补了约束RL与实际RLHF之间的一个重要理论空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of Large Language Models (LLMs) with human preferences through Reinforcement Learning from Human Feedback (RLHF), addressing the limitations of standard primal-dual methods that can lead to instability in practical applications. The authors propose a universal primal-dual framework for safe RLHF, introducing an optimistic primal-dual (OPD) algorithm that stabilizes saddle-point dynamics by incorporating predictive updates for primal and dual variables. The key findings demonstrate that the OPD algorithm guarantees last-iterate convergence, ensuring both exact policy optimization and convergence to a neighborhood of the optimal solution, while also mitigating oscillations in constrained alignment objectives, thus bridging a significant theoretical gap in the field.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过人类反馈的强化学习（RLHF）改善大型语言模型（LLMs）与人类偏好的对齐，解决标准原始-对偶优化方法在实际应用中可能导致的不稳定性。作者提出了一种用于安全RLHF的通用原始-对偶框架，并引入了一种乐观原始-对偶（OPD）算法，通过对原始和对偶变量进行预测更新来稳定鞍点动态。关键实验结果表明，OPD算法保证了最后迭代的收敛，有效解决了约束对齐目标中的振荡问题，弥合了约束RL与实际RLHF之间的理论差距。</div>
</details>
</div>
<div class="card">
<div class="title">When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models</div>
<div class="meta-line">Authors: Satyam Kumar Navneet, Joydeep Chandra, Yong Zhang</div>
<div class="meta-line">First: 2026-02-25T17:54:42+00:00 · Latest: 2026-02-25T17:54:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22145v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used to ``professionalize&#x27;&#x27; workplace communication, often at the cost of linguistic identity. We introduce &quot;Cultural Ghosting&quot;, the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,&amp; Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) &amp; Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当人工智能写作时，谁的声音仍然存在？量化大型语言模型中世界英语变体的文化标记消失</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地用于“专业化”职场沟通，往往以牺牲语言身份为代价。我们引入了“文化幽灵化”，即在文本处理过程中系统性地抹去非母语英语变体特有的语言标记。通过分析从1490个具有文化标记的文本（印度、新加坡和尼日利亚英语）生成的22350个LLM输出，使用两种新颖的指标：身份抹除率（IER）和语义保留分数（SPS），我们量化了这一现象。在所有提示中，我们发现总体IER为10.26%，模型级别的变化范围为3.5%到20.5%（5.9倍）。关键是，我们识别出一个语义保留悖论：模型在系统性抹去文化标记的同时保持高语义相似性（平均SPS = 0.748）。务实标记（礼貌惯例）比词汇标记（抹除率71.5%对37.1%）更脆弱，脆弱性是1.9倍。我们的实验表明，明确的文化保留提示在不牺牲语义质量的情况下减少了29%的抹除。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the phenomenon of &#x27;Cultural Ghosting&#x27;, where large language models (LLMs) erase linguistic markers unique to non-native English varieties, impacting linguistic identity in workplace communication. The study analyzes 22,350 outputs from five LLMs generated from 1,490 culturally marked texts across Indian, Singaporean, and Nigerian English under three different prompt conditions. The findings reveal an overall Identity Erasure Rate of 10.26%, with significant variation among models, and highlight a Semantic Preservation Paradox where high semantic similarity is maintained despite cultural marker erasure, with pragmatic markers being more vulnerable than lexical ones; however, using explicit cultural-preservation prompts can reduce erasure by 29% while preserving semantic quality.</div>
<div class="mono" style="margin-top:8px">本研究探讨了“文化幽灵化”现象，即大型语言模型（LLMs）在处理文本时抹去非母语英语变体的独特语言标记，从而影响职场沟通中的语言身份。研究分析了来自1490个文化标记文本（印度、新加坡和尼日利亚英语）的22350个输出，使用五种不同模型在三种提示条件下进行处理，并采用了两个新指标：身份抹除率（IER）和语义保留分数（SPS）。研究结果显示，整体IER为10.26%，且模型间存在显著差异，并强调了语义保留悖论，即模型在抹去文化标记的同时仍能保持高语义相似性，尤其是礼貌等语用标记比词汇标记更容易被抹去；然而，使用明确的文化保留提示可以在不影响语义质量的情况下减少29%的抹除率。</div>
</details>
</div>
<div class="card">
<div class="title">Convergence of the generalization error for deep gradient flow methods for PDEs</div>
<div class="meta-line">Authors: Chenguang Liu, Antonis Papapantoleon, Jasper Rou</div>
<div class="meta-line">First: 2025-12-31T18:11:51+00:00 · Latest: 2026-02-25T17:53:00+00:00</div>
<div class="meta-line">Comments: 29 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25017v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.25017v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit&#x27;&#x27; and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度梯度流方法在偏微分方程中的泛化误差收敛</div>
<div class="mono" style="margin-top:8px">本文旨在为深度梯度流方法（DGFMs）在（高维）偏微分方程（PDEs）求解中的应用提供坚实的数学基础。我们将DGFMs的泛化误差分解为近似误差和训练误差。首先，我们展示了满足合理且可验证假设的PDEs的解可以通过神经网络进行近似，因此当神经元数量趋于无穷大时，近似误差趋于零。然后，我们推导出训练过程在“宽网络极限”下遵循的梯度流，并分析当训练时间趋于无穷大时该流的极限。这些结果结合表明，DGFMs的泛化误差随着神经元数量和训练时间的增加而趋于零。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the mathematical foundations of deep gradient flow methods (DGFMs) for solving high-dimensional partial differential equations (PDEs), motivated by the need for reliable solutions in complex systems. The authors decompose the generalization error of DGFMs into approximation and training errors, demonstrating that neural networks can approximate solutions to PDEs under certain assumptions, with approximation error diminishing as neuron count increases. They also analyze the gradient flow during training in the wide network limit, concluding that the generalization error approaches zero as both the number of neurons and training time increase.</div>
<div class="mono" style="margin-top:8px">本研究旨在为使用深度梯度流方法（DGFMs）解决高维偏微分方程（PDEs）提供坚实的数学基础。作者将DGFMs的泛化误差分解为近似误差和训练误差，证明在合理假设下，神经网络能够逼近PDE的解，随着神经元数量的增加，近似误差趋于零。此外，他们分析了在宽网络极限下训练过程中的梯度流，并发现随着神经元数量和训练时间的无限增加，泛化误差趋近于零。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260226_0358.html">20260226_0358</a>
<a href="archive/20260225_0400.html">20260225_0400</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0335.html">20260223_0335</a>
<a href="archive/20260222_0334.html">20260222_0334</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0348.html">20260220_0348</a>
<a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
