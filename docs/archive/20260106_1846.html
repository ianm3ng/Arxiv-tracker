<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-06 18:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260106_1846</div>
    <div class="row"><div class="card">
<div class="title">Heterogeneous Low-Bandwidth Pre-Training of LLMs</div>
<div class="meta-line">Authors: Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky</div>
<div class="meta-line">First: 2026-01-05T18:59:57+00:00 · Latest: 2026-01-05T18:59:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02360v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02360v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异构低带宽大语言模型预训练</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的预训练越来越需要分布式计算，但带宽限制使得超出良好配置的数据中心的扩展变得困难，尤其是在模型并行性迫使频繁进行大量设备间通信时。我们研究了基于不频繁同步和稀疏伪梯度交换的低通信数据并行方法SparseLoCo是否可以与通过激活和激活梯度压缩的低带宽管道模型并行相结合。我们引入了一个异构分布式训练框架，其中一些参与者在高带宽互连上托管完整副本，而资源有限的参与者则被分组以使用具有子空间投影的阶段间通信的管道并行共同实例化一个副本。为了使最近引入的子空间管道压缩与SparseLoCo兼容，我们研究了一些适应性。在标准预训练语料库上的大规模语言建模实验（178M-1B参数）中，我们发现激活压缩与SparseLoCo以适度成本组合，而选择性（异构）压缩在相对于压缩所有副本时，始终改善了损失-通信权衡，尤其是在激进的压缩比下。这些结果表明，将低带宽模型并行性和异构参与者纳入LLM预训练的实际路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of pre-training large language models (LLMs) in distributed computing environments, particularly due to bandwidth constraints that hinder scaling beyond well-equipped datacenters. The authors propose a heterogeneous distributed training framework that combines SparseLoCo, a low-communication data parallel method, with low-bandwidth pipeline model parallelism, utilizing activation and activation-gradient compression. Experimental results demonstrate that activation compression can be effectively integrated with SparseLoCo at a modest cost, and that selective compression improves the loss-communication tradeoff compared to compressing all replicas, especially at high compression ratios, indicating a viable approach for low-bandwidth model parallelism in LLM pre-training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在分布式计算环境中预训练大型语言模型（LLMs）所面临的挑战，特别是带宽限制妨碍了超出良好配置的数据中心的扩展。作者提出了一种异构分布式训练框架，将SparseLoCo（一种低通信数据并行方法）与低带宽管道模型并行性结合，利用激活和激活梯度压缩。实验结果表明，激活压缩可以以合理的成本与SparseLoCo集成，而选择性压缩在所有副本上均匀压缩的情况下显著改善了损失-通信权衡，尤其是在激进的压缩比下，表明在LLM预训练中采用低带宽模型并行性是一种可行的方法。</div>
</details>
</div>
<div class="card">
<div class="title">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</div>
<div class="meta-line">Authors: Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik</div>
<div class="meta-line">First: 2026-01-05T18:59:54+00:00 · Latest: 2026-01-05T18:59:54+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02359v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mapooon.github.io/ExposeAnyonePage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExposeAnyone：个性化音频到表情扩散模型是稳健的零样本人脸伪造检测器</div>
<div class="mono" style="margin-top:8px">检测未知的深度伪造操作仍然是人脸伪造检测中最具挑战性的问题之一。当前的最先进方法无法推广到未见的操作，因为它们主要依赖于现有深度伪造或伪伪造的监督训练，这导致对特定伪造模式的过拟合。相比之下，自监督方法提供了更大的推广潜力，但现有工作在仅通过自监督学习区分性表示方面存在困难。本文提出了ExposeAnyone，一种基于扩散模型的完全自监督方法，该方法从音频生成表情序列。关键思想是，一旦模型使用参考集个性化到特定对象，它可以通过扩散重建误差计算可疑视频与个性化对象之间的身份距离，从而实现关注对象的人脸伪造检测。大量实验表明：1）我们的方法在DF-TIMIT、DFDCP、KoDF和IDForge数据集上的平均AUC比之前的最先进方法提高了4.22个百分点；2）我们的模型也能够检测Sora2生成的视频，而之前的方法表现不佳；3）我们的方法对模糊和压缩等损坏具有很强的鲁棒性，突显了其在现实世界人脸伪造检测中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the challenge of detecting unknown deepfake manipulations, as existing methods often overfit to specific forgery patterns due to reliance on supervised training. The authors propose ExposeAnyone, a fully self-supervised approach utilizing a diffusion model to generate expression sequences from audio, which personalizes the model to specific subjects. Experimental results show that this method outperforms the previous state-of-the-art by 4.22 percentage points in average AUC across multiple datasets, effectively detects Sora2-generated videos where prior methods fail, and demonstrates robustness against corruptions like blur and compression, indicating its potential for real-world applications in face forgery detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决检测未知深度伪造操作的挑战，因为现有方法往往由于依赖监督训练而对特定伪造模式过拟合。作者提出了ExposeAnyone，这是一种完全自监督的方法，利用扩散模型从音频生成表情序列，将模型个性化以特定对象，从而有效进行人脸伪造检测。实验结果表明，该方法在多个数据集上的平均AUC比之前的最先进方法提高了4.22个百分点，成功检测到Sora2生成的视频，而之前的方法表现不佳，并且在模糊和压缩等损坏情况下表现出高度鲁棒性，表明其在现实场景中的实际应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection</div>
<div class="meta-line">Authors: Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</div>
<div class="meta-line">First: 2025-06-11T15:06:59+00:00 · Latest: 2026-01-05T18:59:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09827v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09827v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech emotion recognition (SER) systems are constrained by existing datasets that typically cover only 6-10 basic emotions, lack scale and diversity, and face ethical challenges when collecting sensitive emotional states. We introduce EMONET-VOICE, a comprehensive resource addressing these limitations through two components: (1) EmoNet-Voice Big, a 5,000-hour multilingual pre-training dataset spanning 40 fine-grained emotion categories across 11 voices and 4 languages, and (2) EmoNet-Voice Bench, a rigorously validated benchmark of 4,7k samples with unanimous expert consensus on emotion presence and intensity levels. Using state-of-the-art synthetic voice generation, our privacy-preserving approach enables ethical inclusion of sensitive emotions (e.g., pain, shame) while maintaining controlled experimental conditions. Each sample underwent validation by three psychology experts. We demonstrate that our Empathic Insight models trained on our synthetic data achieve strong real-world dataset generalization, as tested on EmoDB and RAVDESS. Furthermore, our comprehensive evaluation reveals that while high-arousal emotions (e.g., anger: 95% accuracy) are readily detected, the benchmark successfully exposes the difficulty of distinguishing perceptually similar emotions (e.g., sadness vs. distress: 63% discrimination), providing quantifiable metrics for advancing nuanced emotion AI. EMONET-VOICE establishes a new paradigm for large-scale, ethically-sourced, fine-grained SER research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmoNet-Voice：一个细粒度、专家验证的语音情感检测基准</div>
<div class="mono" style="margin-top:8px">语音情感识别（SER）系统受到现有数据集的限制，这些数据集通常仅涵盖6-10种基本情感，缺乏规模和多样性，并在收集敏感情感状态时面临伦理挑战。我们推出了EMONET-VOICE，这是一个全面的资源，通过两个组成部分解决这些限制：（1）EmoNet-Voice Big，一个5000小时的多语言预训练数据集，涵盖11种声音和4种语言中的40种细粒度情感类别；（2）EmoNet-Voice Bench，一个经过严格验证的基准，包含4700个样本，专家一致同意情感的存在和强度水平。通过最先进的合成语音生成，我们的隐私保护方法能够在保持受控实验条件的同时，伦理地纳入敏感情感（例如，痛苦、羞愧）。每个样本经过三位心理学专家的验证。我们展示了在我们的合成数据上训练的同理心洞察模型在真实世界数据集上的强泛化能力，经过EmoDB和RAVDESS的测试。此外，我们的综合评估表明，尽管高唤起情感（例如，愤怒：95%准确率）容易被检测，但基准成功揭示了区分感知相似情感（例如，悲伤与痛苦：63%区分率）的困难，为推进细致情感AI提供了可量化的指标。EMONET-VOICE为大规模、伦理来源的细粒度SER研究建立了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing speech emotion recognition (SER) datasets, which often cover only a few basic emotions and face ethical challenges in collecting sensitive emotional data. The authors introduce EMONET-VOICE, which consists of a 5,000-hour multilingual pre-training dataset featuring 40 fine-grained emotion categories and a benchmark of 4,700 samples validated by experts for emotion presence and intensity. The findings indicate that models trained on this synthetic data demonstrate strong generalization to real-world datasets, achieving high accuracy for high-arousal emotions while highlighting challenges in distinguishing similar emotions, thus providing valuable metrics for the development of nuanced emotion AI.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有语音情感识别（SER）数据集的局限性，这些数据集通常仅涵盖少数基本情感，并在收集敏感情感数据时面临伦理挑战。作者介绍了EMONET-VOICE，该数据集包含5000小时的多语言语音，涵盖40种细粒度情感类别，并且经过专家验证的基准。研究结果表明，基于该合成数据训练的模型在真实数据集上表现出强大的泛化能力，对于高唤起情感的准确率很高，但在区分相似情感方面存在挑战，从而为细致情感人工智能的发展提供了有价值的指标。</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-05T18:56:34+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：一个统一的视觉生成器，具有交错的全模态上下文</div>
<div class="mono" style="margin-top:8px">我们提出了VINO，一个统一的视觉生成器，在一个框架内执行图像和视频的生成与编辑。VINO不依赖于特定任务的模型或每种模态的独立模块，而是使用一个共享的扩散骨干网络，基于文本、图像和视频进行条件化，从而在一个模型下实现广泛的视觉创作和编辑任务。具体而言，VINO将视觉-语言模型（VLM）与多模态扩散变换器（MMDiT）结合，其中多模态输入被编码为交错的条件令牌，然后用于指导扩散过程。该设计支持多参考基础、长格式指令跟随以及在静态和动态内容中保持一致的身份，同时避免特定模态的架构组件。为了训练这样一个统一的系统，我们引入了一个多阶段训练管道，逐步将视频生成基础模型扩展为一个统一的多任务生成器，能够处理图像和视频的输入和输出。在多样的生成和编辑基准测试中，VINO展示了强大的视觉质量、忠实的指令跟随、改进的参考和属性保留，以及更可控的多身份编辑。我们的结果突显了可扩展统一视觉生成的实际路径，以及交错的上下文计算作为通用视觉创作基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the development of VINO is to create a unified visual generator capable of handling both image and video generation and editing tasks within a single framework, eliminating the need for task-specific models. The method involves coupling a vision-language model with a Multimodal Diffusion Transformer, where multimodal inputs are encoded as interleaved conditioning tokens to guide the diffusion process. Key experimental findings indicate that VINO achieves strong visual quality, accurate instruction following, enhanced reference and attribute preservation, and allows for more controllable multi-identity edits across various generation and editing benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个统一的视觉生成器，能够在单一框架内处理图像和视频的生成与编辑，克服任务特定模型的局限性。该方法涉及开发VINO，它将视觉语言模型与多模态扩散变换器集成，利用交错的条件标记来指导不同视觉任务的扩散过程。关键实验结果表明，VINO实现了高视觉质量，有效遵循指令，保持参考和属性，并允许对多个身份进行更可控的编辑，展示了其在可扩展统一视觉生成方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?</div>
<div class="meta-line">Authors: Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le</div>
<div class="meta-line">Venue: NeurIPS 2024</div>
<div class="meta-line">First: 2025-12-26T07:40:11+00:00 · Latest: 2026-01-05T18:55:51+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, 4 tables; NeurIPS 2024 format</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21907v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialBench：代理能否分析真实世界的空间生物数据？</div>
<div class="mono" style="margin-top:8px">空间转录组学检测的规模和复杂性迅速增加，使得计算分析成为生物发现的主要瓶颈。尽管前沿AI代理在软件工程和一般数据分析方面有了显著改善，但尚不清楚它们是否能够从混乱的真实世界空间数据集中提取生物学见解。我们介绍了SpatialBench，这是一个基于来自五种空间技术和七个任务类别的实际空间分析工作流程衍生的146个可验证问题的基准。每个问题提供了分析步骤之前的实验数据快照和一个确定性的评分器，用于评估关键生物结果的恢复。关于前沿模型的基准数据表明，基础模型的准确性仍然较低（各模型系列之间为20-38%），并且存在强烈的模型-任务和模型-平台交互。设计的工具对性能有很大的实证影响，这表明工具、提示、控制流和执行环境应作为一流对象进行评估和改进。SpatialBench既是一个测量工具，也是一个诊断视角，用于开发能够真实、透明和可重复地与真实空间数据集交互的代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational challenges posed by the increasing scale and complexity of spatial transcriptomics assays, which hinder biological discovery. The authors introduce SpatialBench, a benchmark consisting of 146 verifiable problems that reflect practical spatial analysis workflows across various technologies and tasks. Experimental results reveal that the accuracy of frontier AI models in analyzing these datasets is low, ranging from 20-38%, and highlight significant interactions between model-task and model-platform, suggesting that the design of tools and prompts significantly impacts performance and should be prioritized in future developments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决空间转录组测定中日益增加的规模和复杂性所带来的计算挑战。作者提出了SpatialBench，这是一个由146个可验证问题组成的基准，源自于各种技术和任务类别的实际空间分析工作流程。实验结果显示，前沿AI模型在分析这些数据集时的准确率较低，范围为20-38%，并强调了模型任务和模型平台之间的显著互动，表明工具和提示的设计在提高性能方面起着关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">DARC: Drum accompaniment generation with fine-grained rhythm control</div>
<div class="meta-line">Authors: Trey Brosnan</div>
<div class="meta-line">First: 2026-01-05T18:55:43+00:00 · Latest: 2026-01-05T18:55:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARC：具有细粒度节奏控制的鼓伴奏生成</div>
<div class="mono" style="margin-top:8px">在音乐创作中，快速原型制作对于探索和完善想法至关重要，但现有的生成工具在用户需要结构控制和风格灵活性时往往不够理想。之前的干声到干声生成方法可以基于其他音乐干声进行条件生成，但对节奏的控制有限，而音色转移方法允许用户指定特定的节奏，但无法基于音乐上下文进行条件生成。我们介绍了DARC，一种生成鼓伴奏模型，它同时基于其他干声的音乐上下文和明确的节奏提示（如打击乐或敲击轨道）进行条件生成。通过参数高效的微调，我们增强了STAGE，一个最先进的鼓干声生成器，提供细粒度的节奏控制，同时保持音乐上下文的意识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance music creation by providing tools that allow for both structural control and stylistic flexibility in drum accompaniment generation. The authors introduce DARC, a generative model that integrates musical context from other stems with explicit rhythm prompts, utilizing parameter-efficient fine-tuning to improve upon the existing STAGE drum stem generator. Key experimental findings demonstrate that DARC successfully achieves fine-grained rhythm control while maintaining awareness of the musical context, addressing the limitations of previous generative tools in this domain.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过为音乐创作中的快速原型设计提供结构控制和风格灵活性，来增强生成音乐工具。作者提出了DARC，这是一种生成鼓伴奏模型，通过参数高效微调增强了现有的鼓音轨生成器STAGE，使其能够同时基于音乐上下文和明确的节奏提示进行生成。主要实验结果表明，DARC有效地将细粒度节奏控制与上下文意识结合起来，解决了以往生成鼓伴奏方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</div>
<div class="meta-line">Authors: Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</div>
<div class="meta-line">First: 2026-01-05T18:55:32+00:00 · Latest: 2026-01-05T18:55:32+00:00</div>
<div class="meta-line">Comments: Project page: https://sparkstj.github.io/talk2move</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02356v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sparkstj.github.io/talk2move">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Talk2Move：基于强化学习的文本指令对象级几何变换框架</div>
<div class="mono" style="margin-top:8px">我们介绍了Talk2Move，这是一种基于强化学习（RL）的扩散框架，用于文本指令下场景中对象的空间变换。通过自然语言在场景中操控对象对多模态生成系统提出了挑战。虽然现有的基于文本的操控方法可以调整外观或风格，但由于缺乏配对监督和像素级优化限制，它们在执行对象级几何变换（如平移、旋转或缩放对象）方面表现不佳。Talk2Move采用群体相对策略优化（GRPO）通过从输入图像和轻量级文本变体生成的多样化回合探索几何动作，消除了对昂贵配对数据的需求。一个空间奖励引导的模型将几何变换与语言描述对齐，而离策略步骤评估和主动步骤采样通过关注信息丰富的变换阶段提高学习效率。此外，我们设计了以对象为中心的空间奖励，直接评估位移、旋转和缩放行为，实现可解释和一致的变换。在经过精心策划的基准测试中，Talk2Move实现了精确、一致且语义上忠实的对象变换，在空间准确性和场景一致性方面超越了现有的文本引导编辑方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of spatially manipulating objects in scenes using natural language, which existing methods struggle with due to limitations in paired supervision and pixel-level optimization. The authors propose Talk2Move, a reinforcement learning-based diffusion framework that utilizes Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts from input images and lightweight textual variations, eliminating the need for expensive paired data. Experimental results show that Talk2Move achieves precise and semantically faithful object transformations, outperforming current text-guided editing methods in terms of spatial accuracy and scene coherence.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用自然语言在场景中空间操纵物体的挑战，特别是现有方法在执行物体级几何变换方面的局限性。作者提出了Talk2Move，这是一种基于强化学习的框架，利用群体相对策略优化（GRPO）探索几何动作，无需昂贵的配对数据，而是依赖于来自输入图像和轻量文本变体的多样化回滚。针对策划基准的实验结果表明，Talk2Move实现了精确且语义上可信的物体变换，在空间准确性和场景一致性方面超越了现有的文本引导编辑方法。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices</div>
<div class="meta-line">Authors: Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha</div>
<div class="meta-line">First: 2026-01-05T18:55:05+00:00 · Latest: 2026-01-05T18:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02353v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>边缘设备上少样本植物病理的元学习引导剪枝</div>
<div class="mono" style="margin-top:8px">偏远地区的农民需要快速可靠的植物病害识别方法，但他们通常缺乏实验室或高性能计算资源。深度学习模型可以从叶片图像中高精度地检测病害，但这些模型通常过大且计算成本高，无法在如树莓派等低成本边缘设备上运行。此外，收集数千张标记病害图像用于训练既昂贵又耗时。本文通过将神经网络剪枝（去除模型中不必要的部分）与少样本学习相结合，解决了这两个挑战，使模型能够从有限的示例中学习。本文提出了病害感知通道重要性评分（DACIS）方法，识别神经网络中最重要的部分，以区分不同的植物病害，并集成到三阶段的剪枝-元学习-再剪枝（PMP）流程中。在PlantVillage和PlantDoc数据集上的实验表明，所提方法将模型大小减少了78\%，同时保持了92.3\%的原始准确率，压缩后的模型在树莓派4上以每秒7帧的速度运行，使小农户的实时田间诊断成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to provide farmers in remote areas with efficient methods for identifying plant diseases, as they often lack access to advanced computing resources. The authors propose a novel approach that combines neural network pruning with few-shot learning, specifically through a method called Disease-Aware Channel Importance Scoring (DACIS), which identifies critical components of the neural network for disease classification. Experimental results on the PlantVillage and PlantDoc datasets show that this method achieves a 78% reduction in model size while retaining 92.3% of the original accuracy, enabling the compressed model to operate at 7 frames per second on a Raspberry Pi 4, thus facilitating real-time diagnosis for smallholder farmers.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为偏远地区的农民提供高效的植物病害识别方法，因为他们通常缺乏先进的计算资源。作者结合了神经网络剪枝和少样本学习，以解决模型体积和对大量标注数据集需求的挑战。提出的疾病感知通道重要性评分（DACIS）方法集成在一个三阶段的剪枝-元学习-再剪枝流程中，实现了模型体积减少78%，同时保持92.3%的原始准确率，使得压缩后的模型能够在Raspberry Pi 4上以每秒7帧的速度运行，从而为小农户提供实时诊断的可能。</div>
</details>
</div>
<div class="card">
<div class="title">Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks</div>
<div class="meta-line">Authors: Nishan Rai, Sujan Khatri, Devendra Risal</div>
<div class="meta-line">First: 2025-08-13T21:02:38+00:00 · Latest: 2026-01-05T18:51:53+00:00</div>
<div class="meta-line">Comments: 11 pages, 9 figures, 4 tables. Undergraduate research project report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10196v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10196v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early detection of lung cancer is critical to improving survival outcomes. We present a deep learning framework for automated lung cancer screening from chest computed tomography (CT) images with integrated explainability. Using the IQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes), we evaluate a custom convolutional neural network (CNN) and three fine-tuned transfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are trained with cost-sensitive learning to mitigate class imbalance and evaluated via accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152 achieved the highest accuracy (97.3%), DenseNet121 provided the best overall balance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We further apply Shapley Additive Explanations (SHAP) to visualize evidence contributing to predictions, improving clinical transparency. Results indicate that CNN-based approaches augmented with explainability can provide fast, accurate, and interpretable support for lung cancer screening, particularly in resource-limited settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用卷积神经网络的肺癌检测可解释人工智能技术</div>
<div class="mono" style="margin-top:8px">早期检测肺癌对改善生存结果至关重要。我们提出了一种深度学习框架，用于从胸部计算机断层扫描（CT）图像中自动筛查肺癌，并集成了解释性。使用IQ-OTH/NCCD数据集（包含正常、良性和恶性类别的1197个扫描），我们评估了一个定制的卷积神经网络（CNN）和三个微调的迁移学习骨干网络：DenseNet121、ResNet152和VGG19。模型采用成本敏感学习进行训练，以减轻类别不平衡，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。虽然ResNet152达到了最高的准确率（97.3%），但DenseNet121在精确率、召回率和F1分数上提供了最佳的整体平衡（分别高达92%、90%、91%）。我们进一步应用Shapley加法解释（SHAP）可视化对预测有贡献的证据，提高临床透明度。结果表明，增强了解释性的基于CNN的方法可以为肺癌筛查提供快速、准确和可解释的支持，特别是在资源有限的环境中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance early detection of lung cancer, which is crucial for improving patient survival rates. The authors developed a deep learning framework utilizing a custom convolutional neural network (CNN) and three fine-tuned transfer learning models (DenseNet121, ResNet152, and VGG19) to automate lung cancer screening from chest CT images using the IQ-OTH/NCCD dataset. The findings reveal that while ResNet152 achieved the highest accuracy at 97.3%, DenseNet121 offered the best balance in precision, recall, and F1 scores, reaching up to 92%, 90%, and 91%, respectively, and the use of Shapley Additive Explanations (SHAP) enhanced the interpretability of the model&#x27;s predictions, thereby supporting effective lung cancer screening in resource-limited environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高肺癌的早期检测，这对改善患者生存率至关重要。作者开发了一种深度学习框架，利用自定义卷积神经网络（CNN）和三种微调的迁移学习模型（DenseNet121、ResNet152和VGG19）来分析包含1,197个胸部CT扫描的IQ-OTH/NCCD数据集。实验结果显示，ResNet152的准确率最高，达到97.3%，而DenseNet121在精确度、召回率和F1分数方面表现最佳。此外，使用Shapley加法解释（SHAP）提供了对模型预测的见解，增强了临床透明度，表明可解释的人工智能可以有效支持肺癌筛查，特别是在资源有限的环境中。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2026-01-05T18:45:47+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展开放式推理以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及在不确定性下对未来进行推理。在这项工作中，我们训练语言模型对开放式预测问题进行预测。为了扩大训练数据，我们从每日新闻报道的全球事件中合成新颖的预测问题，使用完全自动化的精心策划方法。我们在我们的数据集OpenForesight上训练Qwen3思维模型。为了防止在训练和评估过程中泄露未来信息，我们使用离线新闻语料库，既用于数据生成，也用于我们预测系统中的检索。在小型验证集的指导下，我们展示了检索的好处，以及改进的强化学习（RL）奖励函数。一旦获得最终的预测系统，我们在2025年5月至8月之间进行保留测试。我们的专用模型OpenForecaster 8B与更大规模的专有模型相匹配，我们的训练提高了预测的准确性、校准和一致性。我们发现，预测训练的校准改进在流行基准上具有普遍性。我们开源了所有模型、代码和数据，以使语言模型预测的研究广泛可及。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance decision-making under uncertainty by training language models for open-ended forecasting. The authors synthesized novel forecasting questions from daily news using an automated curation process and trained their Qwen3 thinking models on the resulting OpenForesight dataset. The key findings indicate that their specialized model, OpenForecaster 8B, achieves comparable performance to larger proprietary models, demonstrating improvements in accuracy, calibration, and consistency of predictions, with calibration enhancements generalizing across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过训练语言模型来提高在不确定性下的决策能力，以应对开放式预测问题。作者使用自动化策划过程从每日新闻中合成新的预测问题，并在生成的数据集OpenForesight上训练Qwen3思维模型。主要发现表明，他们的专用模型OpenForecaster 8B在预测准确性和一致性方面与更大规模的专有模型相当，并且在校准方面的改进能够在流行基准上推广，他们已将所有模型、代码和数据公开，以便进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Action Smoothness for a Cascaded Online Learning Flight Control System</div>
<div class="meta-line">Authors: Yifei Li, Erik-jan van Kampen</div>
<div class="meta-line">First: 2025-07-06T11:19:34+00:00 · Latest: 2026-01-05T18:39:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04346v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.04346v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进级联在线学习飞行控制系统的动作平滑性</div>
<div class="mono" style="margin-top:8px">本文旨在提高级联在线学习飞行控制系统的动作平滑性。尽管级联结构在飞行控制设计中被广泛使用，但其稳定性可能会因振荡控制动作而受到影响，这给实际工程应用带来了挑战。为了解决这个问题，我们引入了一种在线时间平滑技术和低通滤波器，以减少控制动作的幅度和频率。使用快速傅里叶变换（FFT）分析策略在频域中的性能。仿真结果证明了这两种提出的技术所带来的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the action smoothness of a cascaded online learning flight control system, which often suffers from oscillatory control actions that can compromise stability in practical applications. The authors propose an online temporal smoothness technique and a low-pass filter to mitigate the amplitude and frequency of these control actions. Experimental results from simulations indicate that the implementation of these techniques significantly improves the performance of the flight control system in terms of action smoothness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高级联在线学习飞行控制系统的动作平滑性，该系统常常因控制动作的振荡而影响稳定性，给实际应用带来挑战。作者提出了一种在线时间平滑技术和低通滤波器，以减少控制动作的幅度和频率。模拟实验结果表明，采用这些技术后，系统性能显著改善，使用快速傅里叶变换（FFT）在频域中进行了分析。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions</div>
<div class="meta-line">Authors: Tyler E. Maltba, Ben S. Southworth, Jeffrey R. Haack, Marc L. Klasky</div>
<div class="meta-line">First: 2025-09-05T21:39:53+00:00 · Latest: 2026-01-05T18:34:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05510v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.05510v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continued progress in inertial confinement fusion (ICF) requires solving inverse problems relating experimental observations to simulation input parameters, followed by design optimization. However, such high-dimensional dynamic PDE-constrained optimization problems are extremely challenging or even intractable. It has been recently shown that inverse problems can be solved by only considering certain robust features. Here we consider the ICF capsule&#x27;s deuterium-tritium (DT) interface, and construct a causal, dynamic, multifidelity reduced-order surrogate that maps from a time-dependent radiation temperature drive to the interface&#x27;s radius and velocity dynamics. The surrogate targets an ODE embedding of DT interface dynamics, and is constructed by learning a controller for a base analytical model using low- and high-fidelity simulation training data with respect to radiation energy group structure. After demonstrating excellent accuracy of the surrogate interface model, we use machine learning (ML) models with surrogate-generated data to solve inverse problems optimizing radiation temperature drive to reproduce observed interface dynamics. For sparse snapshots in time, the ML model further characterizes the most informative times at which to sample dynamics. Altogether we demonstrate how operator learning, causal architectures, and physical inductive bias can be integrated to accelerate discovery, design, and diagnostics in high-energy-density systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果多保真代理前向和逆向模型用于惯性约束聚变的内爆</div>
<div class="mono" style="margin-top:8px">惯性约束聚变（ICF）的持续进展需要解决将实验观察与模拟输入参数相关联的逆问题，随后进行设计优化。然而，这类高维动态偏微分方程约束的优化问题极具挑战性，甚至难以处理。最近的研究表明，逆问题可以通过仅考虑某些稳健特征来解决。在这里，我们考虑ICF胶囊的氘-氚（DT）界面，并构建一个因果的动态多保真降阶代理，该代理将时间依赖的辐射温度驱动映射到界面的半径和速度动态。该代理针对DT界面动态的常微分方程嵌入，并通过学习一个基础解析模型的控制器来构建，使用与辐射能量组结构相关的低保真和高保真模拟训练数据。在展示了代理界面模型的优异准确性后，我们使用机器学习（ML）模型与代理生成的数据来解决逆问题，优化辐射温度驱动以重现观察到的界面动态。对于时间上的稀疏快照，ML模型进一步表征了采样动态的最有信息的时间。总之，我们展示了如何将算子学习、因果架构和物理归纳偏见结合起来，以加速高能密度系统中的发现、设计和诊断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in solving inverse problems related to inertial confinement fusion (ICF), particularly in linking experimental observations to simulation input parameters for design optimization. The authors developed a causal, dynamic, multifidelity reduced-order surrogate model that connects the time-dependent radiation temperature drive to the dynamics of the deuterium-tritium interface. This model was constructed using low- and high-fidelity simulation data and demonstrated excellent accuracy, enabling the use of machine learning to optimize the radiation temperature drive to match observed dynamics. Additionally, the study identified key time points for sampling dynamics, showcasing the integration of operator learning and physical inductive bias to enhance the understanding and design of high-energy-density systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决与惯性约束聚变（ICF）相关的逆问题的挑战，特别是在基于实验观察优化设计方面。作者开发了一种因果的动态多保真度降阶代理模型，该模型将时间依赖的辐射温度驱动与氘-氚界面的动态联系起来，利用低保真度和高保真度的仿真数据。主要发现表明，该代理模型在预测界面动态方面具有高精度，并且基于代理生成数据训练的机器学习模型能够有效优化辐射温度驱动以匹配观察到的动态，同时识别出稀疏数据收集中最具信息性的采样时间。</div>
</details>
</div>
<div class="card">
<div class="title">SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models</div>
<div class="meta-line">Authors: Eric Xue, Ruiyi Zhang, Pengtao Xie</div>
<div class="meta-line">First: 2025-11-18T09:56:16+00:00 · Latest: 2026-01-05T18:33:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14301v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.14301v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time. Recent work has emphasized stealthy attacks that stress-test data-curation defenses using stylized artifacts or token-level perturbations as triggers, but this focus leaves a more practically relevant threat model underexplored: backdoors tied to naturally occurring semantic concepts. We introduce SteganoBackdoor, an optimization-based framework that constructs SteganoPoisons, steganographic poisoned training examples in which a backdoor payload is distributed across a fluent sentence while exhibiting no representational overlap with the inference-time semantic trigger. Across diverse model architectures, SteganoBackdoor achieves high attack success under constrained poisoning budgets and remains effective under conservative data-level filtering, highlighting a blind spot in existing data-curation defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SteganoBackdoor：对语言模型的隐蔽且数据高效的后门攻击</div>
<div class="mono" style="margin-top:8px">现代语言模型仍然容易受到通过污染数据进行的后门攻击，其中包含触发器的训练输入与目标输出配对，导致模型在推理时每当出现触发器时就重现该行为。最近的研究强调了隐蔽攻击，这些攻击使用风格化伪影或令牌级扰动作为触发器来压力测试数据策划防御，但这种关注使得一个更具实际相关性的威胁模型未被充分探索：与自然发生的语义概念相关的后门。我们引入了SteganoBackdoor，这是一种基于优化的框架，构建SteganoPoisons，即隐写污染的训练示例，其中后门有效载荷分布在流畅的句子中，同时与推理时的语义触发器没有表现上的重叠。在多种模型架构中，SteganoBackdoor在受限的污染预算下实现了高攻击成功率，并在保守的数据级过滤下仍然有效，突显了现有数据策划防御中的盲点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of modern language models to backdoor attacks through poisoned data, particularly focusing on the underexplored threat of backdoors linked to naturally occurring semantic concepts. The authors introduce SteganoBackdoor, an optimization-based framework that creates SteganoPoisons, which are steganographic poisoned training examples that distribute a backdoor payload across a fluent sentence without overlapping with the semantic trigger used during inference. Experimental results demonstrate that SteganoBackdoor achieves high attack success rates even with limited poisoning budgets and remains effective against conservative data-level filtering, revealing a significant gap in current data-curation defenses.</div>
<div class="mono" style="margin-top:8px">本研究关注现代语言模型对通过污染数据进行后门攻击的脆弱性，特别是聚焦于与自然语义概念相关的后门威胁，这一领域尚未得到充分探索。作者提出了SteganoBackdoor，这是一个基于优化的框架，创建了SteganoPoisons，这些是将后门有效载荷分布在流畅句子中的隐写污染训练示例，且在推理时与语义触发器没有重叠。实验结果表明，SteganoBackdoor在各种模型架构中即使在有限的污染预算下也能实现高攻击成功率，并且在保守的数据级过滤下仍然有效，揭示了当前数据策划防御的弱点。</div>
</details>
</div>
<div class="card">
<div class="title">BEDS: Bayesian Emergent Dissipative Structures</div>
<div class="meta-line">Authors: Laurent Caraffa</div>
<div class="meta-line">First: 2026-01-05T18:21:02+00:00 · Latest: 2026-01-05T18:21:02+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02329v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine&#x27;s theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel&#x27;s incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEDS：贝叶斯涌现耗散结构</div>
<div class="mono" style="margin-top:8px">我们提出了BEDS（贝叶斯涌现耗散结构），这是一个统一非平衡热力学、贝叶斯推断、信息几何和机器学习概念的理论框架。中心论点提出，物理、生物和计算系统中的学习本质上是通过熵输出将通量转化为结构。在普里戈金的耗散结构理论基础上，我们建立了热力学过程与贝叶斯更新之间的形式同构，证明可持续学习系统必须遵循耗散模式，其中结晶的后验成为后续涌现层次的先验。
  我们在最小公理下推导出基本数学常数（e，π，φ）作为贝叶斯推断的固定点，表明这些常数必然从任何能够表示和更新不确定性的系统中涌现。此外，我们提出一个猜想，将哥德尔的不完备性定理与热力学约束联系起来，假设形式系统的病态（不完备性、不可判定性）在结构上类似于物理系统中的耗散缺陷。
  作为实际验证，我们展示了一个实现BEDS原则的点对点网络架构，与现有的分布式共识系统相比，能源效率提高了六个数量级，同时实现了持续学习。这项工作架起了基础物理、数学逻辑和实际系统设计之间的桥梁，提供了对学习和计算本质的理论见解，以及通向可持续人工智能的具体路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to unify concepts from various fields to understand learning as a process of converting flux into structure through entropy export. The authors introduce BEDS, a theoretical framework that connects non-equilibrium thermodynamics, Bayesian inference, and machine learning, establishing a formal isomorphism between thermodynamic processes and Bayesian updating. Key experimental findings include the derivation of fundamental mathematical constants as fixed points of Bayesian inference and the development of a peer-to-peer network architecture that demonstrates a six orders of magnitude improvement in energy efficiency compared to existing systems, thereby validating the principles of BEDS in practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要统一来自不同领域的概念，以理解物理、生物和计算系统中的学习本质。作者提出了BEDS（贝叶斯涌现耗散结构），这是一个将非平衡热力学与贝叶斯推理和机器学习相连接的理论框架。主要实验结果包括将基本数学常数推导为贝叶斯推理的固定点，以及展示一种实现BEDS原理的点对点网络架构，该架构在能效上比现有系统提高了六个数量级，同时促进了持续学习。</div>
</details>
</div>
<div class="card">
<div class="title">Diminishing Returns in Self-Supervised Learning</div>
<div class="meta-line">Authors: Oli Bridge, Huey Sun, Botond Branyicskai-Nagy, Charles D&#x27;Ornano, Shomit Basu</div>
<div class="meta-line">First: 2025-12-03T15:11:44+00:00 · Latest: 2026-01-05T18:17:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03862v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based architectures have become a dominant paradigm in vision and language, but their success is often attributed to large model capacity and massive training data. In this work, we examine how self-supervised pre-training, intermediate fine-tuning, and downstream fine-tuning interact in a low-capacity regime, using a 5M-parameter Vision Transformer for semantic segmentation. Across multiple data scales, we find that masked image modeling pre-training and downstream fine-tuning reliably improve performance, but with clear diminishing returns as supervision increases. In contrast, inserting an intermediate classification fine-tuning stage consistently degrades downstream performance, with the largest drops occurring precisely where pre-training is most effective. Through an analysis of patch-level representation geometry, we show that classification-based intermediate supervision actively interferes with representations learned during pre-training by collapsing spatial structure critical for dense prediction. These results indicate that, in small models, the geometry of supervision matters more than the number of training stages: misaligned intermediate objectives can negate the benefits of pre-training rather than amplify them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自监督学习中的收益递减</div>
<div class="mono" style="margin-top:8px">基于Transformer的架构已成为视觉和语言领域的主导范式，但其成功通常归因于大模型容量和海量训练数据。在本研究中，我们考察了自监督预训练、中间微调和下游微调在低容量环境中的相互作用，使用一个500万参数的视觉Transformer进行语义分割。在多个数据规模下，我们发现掩码图像建模预训练和下游微调可靠地提高了性能，但随着监督的增加，收益递减现象明显。相反，插入中间分类微调阶段会持续降低下游性能，最大降幅恰好发生在预训练最有效的地方。通过对块级表示几何的分析，我们表明基于分类的中间监督会主动干扰预训练期间学习到的表示，破坏对密集预测至关重要的空间结构。这些结果表明，在小模型中，监督的几何结构比训练阶段的数量更为重要：不对齐的中间目标可能会抵消预训练的好处，而不是增强它们。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the interactions between self-supervised pre-training, intermediate fine-tuning, and downstream fine-tuning in low-capacity transformer models, motivated by the need to understand performance limitations in smaller architectures. The researchers utilized a 5M-parameter Vision Transformer for semantic segmentation across various data scales. They found that while masked image modeling pre-training and downstream fine-tuning enhance performance, there are diminishing returns as supervision increases, and that introducing an intermediate classification fine-tuning stage consistently degrades downstream performance, particularly where pre-training is most beneficial, indicating that the geometry of supervision is crucial in small models.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自监督预训练、中间微调和下游微调在低容量Transformer模型中的相互作用，特别是针对一个500万参数的视觉Transformer进行语义分割。研究发现，尽管掩蔽图像建模预训练和下游微调能够提高性能，但随着监督的增加，它们的收益呈现出递减趋势。相反，引入中间分类微调阶段会持续降低下游性能，尤其是在预训练最有效的地方。分析表明，这种中间监督干扰了预训练期间学习到的表示的空间结构，表明在小模型中，监督目标的一致性比训练阶段的数量更为重要。</div>
</details>
</div>
<div class="card">
<div class="title">Hunting for &quot;Oddballs&quot; with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders</div>
<div class="meta-line">Authors: Alexander Roman, Emilie Panek, Roy T. Forestano, Eyup B. Unlu, Katia Matcheva, Konstantin T. Matchev</div>
<div class="meta-line">First: 2026-01-05T18:15:53+00:00 · Latest: 2026-01-05T18:15:53+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02324v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02324v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder&#x27;s latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用机器学习寻找“异常体”：使用自编码器对过境光谱的低维表示检测异常系外行星</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于自编码器的机器学习技术在异常检测中的应用，以识别具有非常规化学特征的系外行星大气，使用低维数据表示。我们使用大气大挑战（ABC）数据库，这是一个公开可用的数据集，包含超过100,000个模拟的系外行星光谱，通过将富含CO2的大气定义为异常，将贫CO2大气定义为正常类，构建异常检测场景。我们基准测试了四种不同的异常检测策略：自编码器重构损失、单类支持向量机（1类-SVM）、K均值聚类和局部离群因子（LOF）。每种方法在原始光谱空间和自编码器的潜在空间中使用接收者操作特征（ROC）曲线和曲线下面积（AUC）指标进行评估。为了测试不同方法在现实条件下的性能，我们引入了从10到50 ppm的高斯噪声水平。我们的结果表明，在所有噪声水平下，在潜在空间中进行的异常检测始终更有效。具体而言，潜在空间中的K均值聚类表现出稳定且高效的方法。我们证明这种异常检测方法对高达30 ppm的噪声水平具有鲁棒性（与现实的基于空间的观测一致），并且在利用潜在空间表示时，即使在50 ppm时仍然可行。另一方面，直接在原始光谱空间中应用的异常检测方法的性能随着噪声水平的增加而显著下降。这表明，自编码器驱动的降维为在大规模调查中标记化学异常目标提供了一种稳健的方法，因为全面检索在计算上是不可行的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the use of autoencoder-based machine learning techniques for detecting exoplanets with unusual atmospheric chemical signatures, motivated by the need to identify anomalies in large datasets. The researchers utilized the Atmospheric Big Challenge (ABC) database, which contains over 100,000 simulated exoplanet spectra, to define CO2-rich atmospheres as anomalies and CO2-poor atmospheres as normal. They compared four anomaly detection methods—Autoencoder Reconstruction Loss, One-Class Support Vector Machine, K-means Clustering, and Local Outlier Factor—evaluating their performance in both original and latent spaces under varying noise levels. The findings reveal that K-means clustering in the latent space consistently outperforms other methods, remaining effective up to 30 ppm of noise and still viable at 50 ppm, while performance in the raw spectral space deteriorates significantly with increased noise, highlighting the advantages of autoencoder-driven dimensionality reduction for identifying chemically anomalous exoplanets in extensive surveys.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于自编码器的机器学习技术在检测具有异常大气化学特征的系外行星中的应用，旨在识别大型数据集中存在的异常现象。研究人员利用包含超过100,000个模拟系外行星光谱的气候大挑战（ABC）数据库，将富含CO2的大气定义为异常，将贫CO2大气定义为正常类，并对四种异常检测策略进行了基准测试：自编码器重构损失、一类支持向量机、K均值聚类和局部离群因子。研究结果表明，在自编码器的潜在空间中进行异常检测更为有效，K均值聚类作为一种特别稳定的方法，显示出对高达30 ppm的噪声水平的鲁棒性，并在50 ppm时仍保持可行性，而直接应用于原始光谱数据的方法在噪声水平增加时表现显著下降。</div>
</details>
</div>
<div class="card">
<div class="title">Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction</div>
<div class="meta-line">Authors: Shuozhi Zuo, Yixin Wang</div>
<div class="meta-line">First: 2026-01-05T18:13:02+00:00 · Latest: 2026-01-05T18:13:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02322v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>环境自适应协变量选择：学习何时使用虚假相关性进行分布外预测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）预测通常通过限制模型使用因果或不变的协变量来处理，避免可能在不同环境中不稳定的非因果虚假关联。尽管这一策略在理论上具有吸引力，但在实践中往往表现不如经验风险最小化（ERM）。我们研究了这一差距的来源，并表明，当仅观察到结果的部分真实原因时，这种失败自然会发生。在这些情况下，非因果虚假协变量可以作为未观察到的原因的有用代理，并显著改善预测，除非在破坏这些代理关系的分布变化下。因此，最佳的预测协变量集既不是普遍的，也不一定在所有环境中与结果表现出不变关系，而是依赖于遇到的特定类型的变化。关键是，我们观察到不同的协变量变化在协变量分布中引发不同的、可观察的特征。此外，这些特征可以从目标OOD环境中的未标记数据中提取，并用于评估代理协变量何时保持可靠以及何时失效。基于这一观察，我们提出了一种环境自适应协变量选择（EACS）算法，该算法将环境级协变量摘要映射到特定环境的协变量集，同时允许将先前的因果知识作为约束纳入。通过模拟和应用数据集，EACS在各种分布变化下始终优于静态因果、不变和基于ERM的预测器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional out-of-distribution (OOD) prediction methods that rely on causal or invariant covariates, which often underperform compared to empirical risk minimization (ERM). The authors propose an environment-adaptive covariate selection (EACS) algorithm that identifies when non-causal spurious covariates can serve as useful proxies for unobserved causes, depending on the specific type of distribution shift. Experimental results demonstrate that EACS outperforms static causal, invariant, and ERM-based predictors across various simulations and applied datasets, effectively adapting to different covariate shifts by utilizing observable signatures in the covariate distribution from unlabeled data in the target OOD environment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在分布外（OOD）预测中通常避免使用非因果虚假相关性所带来的局限性，这在实践中可能导致次优表现。作者提出了一种环境自适应协变量选择（EACS）算法，通过分析不同环境中协变量分布的可观察特征，识别何时虚假相关性可能是有益的。实验结果表明，EACS在各种分布转变场景中显著优于传统的静态因果和经验风险最小化方法，突显了根据特定环境背景调整协变量选择的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching</div>
<div class="meta-line">Authors: Roja Sahoo, Anoop Namboodiri</div>
<div class="meta-line">First: 2026-01-05T18:09:27+00:00 · Latest: 2026-01-05T18:09:27+00:00</div>
<div class="meta-line">Comments: 15 pages, 8 figures, 5 tables. Submitted to ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02318v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02318v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fusion2Print：用于无接触指纹匹配的深度闪光-非闪光融合</div>
<div class="mono" style="margin-top:8px">无接触指纹识别提供了一种卫生和便捷的替代方案，能够快速获取而无需潜在指纹、压力伪影或卫生风险。然而，无接触图像由于照明变化、皮肤下色素沉着和镜面反射，通常显示出脊线清晰度下降。闪光捕捉保留脊线细节但引入噪声，而非闪光捕捉减少噪声但降低脊线对比度。我们提出Fusion2Print（F2P），这是第一个系统性捕捉和融合成对闪光-非闪光无接触指纹的框架。我们构建了一个自定义的成对数据集FNF数据库，并进行手动闪光-非闪光减法以隔离保留脊线的信号。一个轻量级的基于注意力的融合网络还整合了两种模式，强调信息通道并抑制噪声，然后一个U-Net增强模块生成一个最佳加权的灰度图像。最后，一个具有跨域兼容性的深度嵌入模型，在统一的嵌入空间中生成可区分和鲁棒的表示，兼容无接触和基于接触的指纹进行验证。F2P增强了脊线清晰度，并在单次捕捉基线（Verifinger，DeepPrint）上实现了优越的识别性能（AUC=0.999，EER=1.12%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve contactless fingerprint recognition, which suffers from issues like degraded ridge clarity due to various factors, while offering a hygienic alternative to traditional methods. The authors introduce Fusion2Print (F2P), a novel framework that systematically captures and fuses paired flash and non-flash contactless fingerprints, utilizing a custom dataset called the FNF Database. Key experimental results demonstrate that F2P significantly enhances ridge clarity and achieves superior recognition performance, with an AUC of 0.999 and an EER of 1.12%, outperforming existing single-capture methods such as Verifinger and DeepPrint.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善无接触指纹识别，但由于照明变化和皮肤变色等因素，导致脊线清晰度下降。作者提出了Fusion2Print（F2P），这是一个新颖的框架，系统地捕获和融合成对的闪光和非闪光无接触指纹。他们创建了一个自定义数据集FNF数据库，并利用轻量级的基于注意力的融合网络来增强脊线清晰度，同时抑制噪声。实验结果表明，F2P的识别性能优越，AUC达到0.999，EER为1.12%，超越了现有的单次捕获方法，如Verifinger和DeepPrint。</div>
</details>
</div>
<div class="card">
<div class="title">BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache</div>
<div class="meta-line">Authors: Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang</div>
<div class="meta-line">First: 2025-03-24T15:22:41+00:00 · Latest: 2026-01-05T18:08:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18773v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.18773v3">PDF</a> · <a href="https://github.com/OpenBitSys/BitDecoding">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.
  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper&#x27;s warpgroup tensor instructions and Blackwell&#x27;s NVFP4 (MXFP4) tensor formats.
  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BitDecoding：为低比特KV缓存解锁长上下文LLM的张量核心</div>
<div class="mono" style="margin-top:8px">长上下文大型语言模型（LLMs）的增长显著增加了自回归解码过程中的内存和带宽压力，因为Key-Value（KV）缓存不断扩展。尽管保持准确性的KV缓存量化（例如，4位或2位）减少了内存占用，但现有系统仅依赖CUDA核心进行解码，未充分利用张量核心这一GPU的主要计算资源。我们提出了BitDecoding，这是第一个通过协同利用CUDA核心和张量核心高效解码低比特KV缓存的推理系统。BitDecoding巧妙地诱导张量核心友好的布局，引入了warp级去量化并行性，并通过查询转换、高性能张量和通道级量化以及支持混合精度执行的软件流水线去量化内核提供统一的系统支持。架构感知优化进一步利用了Hopper的warpgroup张量指令和Blackwell的NVFP4（MXFP4）张量格式。在Blackwell、Hopper和Ampere GPU上评估，BitDecoding在FP16 FlashDecoding-v2上实现了平均7.5倍的解码加速，在Blackwell上使用NVFP4可达8.6倍，在最先进的方法上可达4.3倍。在具有128K上下文的LLaMA-3.1-8B上，BitDecoding将单批解码延迟减少了3倍。BitDecoding已开源，地址为https://github.com/OpenBitSys/BitDecoding。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the increased memory and bandwidth demands of long-context Large Language Models (LLMs) during autoregressive decoding, particularly due to the expanding Key-Value (KV) cache. The authors propose BitDecoding, an inference system that optimally utilizes both CUDA cores and Tensor Cores to decode low-bit KV caches efficiently. Experimental results demonstrate that BitDecoding achieves an average decoding speedup of 7.5 times compared to FP16 FlashDecoding-v2, with peak improvements of up to 8.6 times on Blackwell GPUs using NVFP4, and reduces single-batch decoding latency by three times on LLaMA-3.1-8B with a 128K context.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决长上下文大型语言模型（LLMs）在自回归解码过程中，由于扩展的键值（KV）缓存而导致的内存和带宽需求增加。作者提出了BitDecoding，这是一种新颖的推理系统，能够优化利用CUDA核心和Tensor核心来解码低位KV缓存。实验结果表明，BitDecoding在与FP16 FlashDecoding-v2相比时，平均解码速度提升了7.5倍，在使用NVFP4的Blackwell GPU上最高可达8.6倍，并且在LLaMA-3.1-8B（128K上下文）上将单批解码延迟减少了三倍。</div>
</details>
</div>
<div class="card">
<div class="title">DatBench: Discriminative, Faithful, and Efficient VLM Evaluations</div>
<div class="meta-line">Authors: Siddharth Joshi, Haoli Yin, Rishabh Adiga, Ricardo Monti, Aldo Carranza, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Darren Teh, David Schwab, Fan Pan, Haakon Mongstad, Jack Urbanek, Jason Lee, Jason Telanoff, Josh Wills, Kaleigh Mentzer, Luke Merrick, Parth Doshi, Paul Burstein, Pratyush Maini, Scott Loftin, Spandan Das, Tony Jiang, Vineeth Dorna, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt</div>
<div class="meta-line">First: 2026-01-05T18:07:51+00:00 · Latest: 2026-01-05T18:07:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02316v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02316v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DatBench：具有区分性、可信性和高效性的VLM评估</div>
<div class="mono" style="margin-top:8px">实证评估是指导基础模型研究进展的主要指南。尽管有大量工作集中在训练前沿视觉-语言模型（VLMs）上，但对其评估的方法仍处于初期阶段。为了指导其成熟，我们提出评估应满足的三个愿望：（1）对模态和应用的可信性，（2）对不同质量模型的区分性，以及（3）计算的高效性。从这个角度出发，我们识别出违反可信性和区分性的关键失败模式，错误地表现模型能力：（i）多项选择格式奖励猜测，无法很好地反映下游用例，并且随着模型的改进而早期饱和；（ii）盲目可解的问题，能够在没有图像的情况下回答，占某些评估的70%；（iii）错误标记或模糊样本在某些数据集中妨碍了多达42%的示例。关于效率，评估前沿模型的计算负担已变得不可承受：据一些说法，近20%的开发计算专用于评估。我们并不打算丢弃现有基准，而是通过转换和过滤来策划它们，以最大化保真度和区分性。我们发现将多项选择问题转换为生成任务会导致能力下降高达35%。此外，过滤盲目可解和错误标记的样本提高了区分能力，同时降低了计算成本。我们发布了DatBench-Full，这是一个涵盖九种VLM能力的33个数据集的清理评估套件，以及DatBench，这是一个区分子集，平均加速达到13倍（最高可达50倍），同时与原始数据集的区分能力紧密匹配。我们的工作为VLM持续扩展的评估实践指明了一条既严格又可持续的道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation methods for vision-language models (VLMs), which have seen significant advancements but lack robust assessment frameworks. The authors propose a set of three criteria for effective evaluations: faithfulness, discriminability, and efficiency, and identify common pitfalls in current evaluation practices, such as reliance on multiple-choice formats and the presence of misleading samples. By transforming existing benchmarks and filtering datasets, they demonstrate that converting multiple-choice questions to generative tasks can reveal significant drops in model capabilities, while also enhancing discriminative power and reducing computational costs. The resulting DatBench-Full and DatBench evaluation suites provide a more reliable assessment of VLMs, achieving substantial speed improvements without sacrificing discriminative accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善当前视觉语言模型（VLM）的评估方法，这些方法在模型训练取得显著进展的情况下仍显不足。作者提出了一个基于三个关键标准的框架：对模态的忠实性、模型之间的可区分性和计算效率。他们的研究结果表明，现有评估方法由于多项选择格式鼓励猜测、高比例的盲目可解问题和错误标记样本等问题，常常误导模型能力的表现。通过转换和过滤现有基准，他们创建了DatBench-Full和DatBench，这些工具提高了评估的真实性和可区分性，同时实现了显著的计算效率，DatBench在不牺牲性能准确性的情况下提供了高达13倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</div>
<div class="meta-line">Authors: Saurabh Kaushik, Lalit Maurya, Beth Tellman</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-05T18:07:21+00:00 · Latest: 2026-01-05T18:07:21+00:00</div>
<div class="meta-line">Comments: Accepted at CV4EO Workshop @ WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02315v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02315v1">PDF</a> · <a href="https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE">Code1</a> · <a href="https://github.com/Sk-2103/Prithvi-CAFE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model&#x27;s limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Prithvi-补充自适应融合编码器 (CAFE)：释放洪水淹没映射的全部潜力</div>
<div class="mono" style="margin-top:8px">地理基础模型 (GFM) 在语义分割、分类和回归任务等多种下游应用中已证明有效。然而，在使用 Sen1Flood11 数据集进行洪水映射时，GFM 难以超越基线 U-Net，突显了模型在捕捉关键局部细节方面的局限性。为了解决这个问题，我们提出了 Prithvi-补充自适应融合编码器 (CAFE)，它将 Prithvi GFM 预训练编码器与通过卷积注意模块 (CAM) 增强的并行 CNN 残差分支集成。Prithvi-CAFE 通过 Prithvi 中的适配器实现快速高效的微调，并与 CNN 特征进行多尺度、多层次融合，捕捉关键局部细节，同时保留长距离依赖关系。我们在两个综合洪水映射数据集上取得了最先进的结果：Sen1Flood11 和 FloodPlanet。在 Sen1Flood11 测试数据上，Prithvi-CAFE (IoU 83.41) 超越了原始 Prithvi (IoU 82.50) 和其他主要 GFM (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02)。在保留测试站点的改进更为明显，Prithvi-CAFE 实现了 81.37 的 IoU，而基线 U-Net 为 70.57，原始 Prithvi 为 72.42。在 FloodPlanet 上，Prithvi-CAFE 也超越了基线 U-Net 和其他 GFM，取得了 64.70 的 IoU，而 U-Net 为 60.14，Terramind 为 62.33，DOFA 为 59.15，Prithvi 2.0 为 61.91。我们提出的简单而有效的 Prithvi-CAFE 展示了在多通道和多模态数据提供互补信息且局部细节至关重要的分割任务中改善性能的强大潜力。代码已发布在 \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance flood inundation mapping, as existing Geo-Foundation Models (GFMs) struggle to outperform the baseline U-Net in capturing local nuances using the Sen1Flood11 dataset. The authors introduce the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which combines a pretrained Prithvi GFM encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM) to enable efficient fine-tuning and multi-scale fusion of features. Experimental results demonstrate that Prithvi-CAFE achieves state-of-the-art performance on the Sen1Flood11 and FloodPlanet datasets, with an IoU of 83.41 on Sen1Flood11, surpassing the original Prithvi and other GFMs, and an IoU of 64.70 on FloodPlanet, indicating its effectiveness in improving segmentation tasks that require detailed local information.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于地基模型（GFM）在洪水映射任务中的局限性，尤其是在使用Sen1Flood11数据集时无法超越基线U-Net。为了解决这个问题，作者开发了Prithvi-互补自适应融合编码器（CAFE），该编码器结合了预训练的Prithvi GFM编码器和增强了卷积注意模块（CAM）的并行CNN残差分支。实验结果表明，Prithvi-CAFE在Sen1Flood11和FloodPlanet数据集上均实现了最先进的性能，在Sen1Flood11上达到83.41的交并比（IoU），超越了原始Prithvi和其他GFM，而在FloodPlanet上则实现了64.70的IoU，超越了基线U-Net和其他模型。</div>
</details>
</div>
<div class="card">
<div class="title">Non-omniscient backdoor injection with one poison sample: Proving the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks</div>
<div class="meta-line">Authors: Thorsten Peinemann, Paula Arnold, Sebastian Berndt, Thomas Eisenbarth, Esfandiar Mohammadi</div>
<div class="meta-line">First: 2025-08-07T17:41:33+00:00 · Latest: 2026-01-05T18:07:06+00:00</div>
<div class="meta-line">Comments: Added generalization to 2-layer ReLU neural networks</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05600v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.05600v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Backdoor poisoning attacks are a threat to machine learning models trained on large data collected from untrusted sources; these attacks enable attackers to inject malicious behavior into the model that can be triggered by specially crafted inputs. Prior work has established bounds on the success of backdoor attacks and their impact on the benign learning task, however, an open question is what amount of poison data is needed for a successful backdoor attack. Typical attacks either use few samples but need much information about the data points, or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one poison sample and limited background knowledge can inject a backdoor with zero backdooring-error and without significantly impacting the benign learning task performance. Moreover, we prove the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks. For adversaries that utilize a direction unused by the clean data distribution for the poison sample, we prove for linear classification and linear regression that the resulting model is functionally equivalent to a model where the poison was excluded from training. We build on prior work on statistical backdoor learning to show that in all other cases, the impact on the benign learning task is still limited. We validate our theoretical results experimentally with realistic benchmark data sets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非全知后门注入与一个毒样本：证明线性回归、线性分类和2层ReLU神经网络的一毒假设</div>
<div class="mono" style="margin-top:8px">后门中毒攻击对在不可信来源收集的大数据上训练的机器学习模型构成威胁；这些攻击使攻击者能够将恶意行为注入模型，并通过特制输入触发。先前的工作已建立了后门攻击成功的界限及其对良性学习任务的影响，然而，一个未解的问题是成功的后门攻击需要多少毒数据。典型攻击要么使用少量样本但需要大量关于数据点的信息，要么需要毒化许多数据点。本文中，我们提出了一毒假设：一个拥有一个毒样本和有限背景知识的对手可以在零后门错误的情况下注入后门，而不会显著影响良性学习任务的性能。此外，我们证明了线性回归、线性分类和2层ReLU神经网络的一毒假设。对于利用干净数据分布未使用方向的毒样本的对手，我们证明了线性分类和线性回归的结果模型在功能上等同于一个未将毒样本纳入训练的模型。我们在统计后门学习的先前工作基础上，表明在所有其他情况下，对良性学习任务的影响仍然有限。我们通过现实基准数据集实验验证了我们的理论结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the threat posed by backdoor poisoning attacks on machine learning models, particularly regarding the amount of poison data required for successful attacks. The authors propose the one-poison hypothesis, demonstrating that an adversary can inject a backdoor using just one poison sample with minimal background knowledge, without significantly affecting the model&#x27;s performance on benign tasks. They validate this hypothesis through theoretical proofs for linear regression, linear classification, and 2-layer ReLU neural networks, showing that in certain cases, the model remains functionally equivalent to one trained without the poison, and they support their findings with experimental results using realistic benchmark datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决后门投毒攻击对机器学习模型的威胁，特别是成功攻击所需的毒样本数量。作者提出了单毒假设，认为攻击者可以仅使用一个毒样本和有限的知识成功注入后门，而不会显著影响模型在良性任务上的表现。他们证明了这一假设在线性回归、线性分类和2层ReLU神经网络中的有效性，表明当毒样本的方向未被干净数据分布利用时，模型在功能上等同于未使用毒样本训练的模型。通过使用现实基准数据集进行实验验证，支持了他们的理论发现，表明在大多数情况下，对良性学习任务的影响是有限的。</div>
</details>
</div>
<div class="card">
<div class="title">Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</div>
<div class="meta-line">Authors: Sourena Khanzadeh</div>
<div class="meta-line">First: 2026-01-05T18:05:29+00:00 · Latest: 2026-01-05T18:05:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02314v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02314v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model&#x27;s output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as &quot;Reasoning Theater&quot; while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阿里阿德涅计划：审计大型语言模型代理忠实性的结构因果框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLM）代理越来越多地承担高风险的自主决策任务，它们推理过程的透明性已成为一个关键的安全问题。虽然链式思维（CoT）提示允许代理生成可读的人类推理痕迹，但这些痕迹是否是模型输出的忠实生成驱动因素，或仅仅是事后合理化，仍不清楚。我们引入了阿里阿德涅计划，这是一个新颖的可解释人工智能（XAI）框架，利用结构因果模型（SCM）和反事实逻辑来审计代理推理的因果完整性。与现有的依赖表面文本相似性的可解释性方法不同，阿里阿德涅计划对中间推理节点进行硬干预（$do$-微积分）——系统性地反转逻辑、否定前提和逆转事实声明——以测量终端答案的因果敏感性（$φ$）。我们对最先进模型的实证评估揭示了持续存在的忠实性差距。我们定义并检测到一种广泛的失败模式，称为因果解耦，其中代理在事实和科学领域表现出高达$0.77$的违反密度（$ρ$）。在这些情况下，尽管内部逻辑矛盾，代理却得出相同的结论，证明它们的推理痕迹充当了“推理剧场”，而决策则由潜在的参数先验控制。我们的研究结果表明，当前的代理架构本质上容易产生不忠实的解释，我们提出了阿里阿德涅评分作为对齐声明逻辑与模型行为的新基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing need for transparency in the reasoning processes of Large Language Model (LLM) agents, particularly in high-stakes decision-making contexts. The authors introduce Project Ariadne, a novel explainable artificial intelligence (XAI) framework that employs Structural Causal Models (SCMs) and counterfactual logic to assess the causal integrity of reasoning in LLMs. Their experimental results indicate a significant &#x27;Faithfulness Gap,&#x27; revealing a failure mode called &#x27;Causal Decoupling,&#x27; where agents demonstrate a violation density of up to 0.77 in factual domains, leading to identical conclusions despite contradictory reasoning, thus highlighting the inadequacy of current models in providing faithful explanations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是随着大型语言模型（LLM）代理在高风险决策中日益被使用，透明的推理过程变得至关重要。作者提出了Project Ariadne，这是一个新颖的可解释人工智能（XAI）框架，利用结构因果模型（SCM）和反事实逻辑来评估这些代理产生的推理的因果完整性。实验结果显示出显著的忠实性差距，特征是称为因果解耦的失败模式，在事实和科学领域中，代理的违反密度高达0.77，表明它们的推理轨迹往往与决策过程不一致，因此需要引入Ariadne Score作为评估模型解释忠实度的新基准。</div>
</details>
</div>
<div class="card">
<div class="title">Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning</div>
<div class="meta-line">Authors: Hanzaleh Akbari Nodehi, Viveck R. Cadambe, Mohammad Ali Maddah-Ali</div>
<div class="meta-line">First: 2026-01-05T18:04:32+00:00 · Latest: 2026-01-05T18:04:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02313v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02313v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary&#x27;s strategy is unknown and outline several open problems for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>编码游戏：在理性对手存在下的编码理论，受去中心化机器学习的启发</div>
<div class="mono" style="margin-top:8px">编码理论在实现可靠的通信、存储和计算中发挥着关键作用。经典方法假设最坏情况的对手模型，仅在诚实节点数量超过对手节点数量一定幅度时确保错误纠正和数据恢复。然而，在一些新兴的去中心化应用中，特别是在去中心化机器学习（DeML）中，参与节点因接受的贡献而获得奖励。这种激励结构自然产生了理性对手，他们采取战略性行动，而不是纯粹恶意的行为。
在本文中，我们首先阐述了在理性对手存在下进行编码的必要性，特别是在去中心化系统中的外包计算背景下。我们将这种需求与现有方法进行对比，并强调其局限性。然后，我们引入编码游戏，这是一种新颖的博弈论框架，将编码理论扩展到诚实节点不占多数的信任最小化环境中。我们重点关注重复编码，强调该框架的两个关键特征：（1）即使在对手节点占多数时也能实现非零的数据恢复概率，以及（2）Sybil抗性，即使对手节点数量增加，均衡状态也保持不变。最后，我们探讨了对手策略未知的场景，并概述了未来研究的若干开放问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for reliable communication and computation in decentralized machine learning environments, where rational adversaries can strategically influence outcomes. The authors introduce a game-theoretic framework called the game of coding, which extends traditional coding theory to scenarios where honest nodes are not in the majority. Key experimental findings demonstrate that this framework allows for a non-zero probability of data recovery even when adversarial nodes dominate, and it exhibits Sybil resistance, maintaining equilibrium despite increasing adversarial participation.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于经典编码理论在去中心化机器学习（DeML）中面临的局限性，传统理论假设最坏情况的对手模型，而在DeML中，理性对手可能采取战略性行为。作者提出了一种名为编码博弈的新型博弈论框架，将编码理论适应于诚实节点不占多数的场景。主要实验结果表明，该框架即使在对手节点占主导地位时也能实现非零的数据恢复概率，并且具有抗Sybil攻击的特性，即在对手节点数量增加时，均衡状态保持不变。</div>
</details>
</div>
<div class="card">
<div class="title">Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies</div>
<div class="meta-line">Authors: Deep Pankajbhai Mehta</div>
<div class="meta-line">First: 2026-01-05T18:01:38+00:00 · Latest: 2026-01-05T18:01:38+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02311v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02311v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布式深度学习的放置语义：分析并行策略的系统框架</div>
<div class="mono" style="margin-top:8px">训练大型语言模型需要在多个加速器之间分配计算，但从业者通过反复试验选择并行策略（数据、张量、流水线、ZeRO），因为没有统一的系统框架来预测其行为。我们引入放置语义：每种策略通过如何在设备上放置四种训练状态（参数、优化器、梯度、激活）来指定，使用五种模式（复制、分片、分片与聚合、物化、卸载）。仅从放置出发，在没有实现细节的情况下，我们推导出内存消耗和通信量。我们的预测与已发布结果完全一致：ZeRO-3在1.5倍通信成本下使用的内存比数据并行少8倍，如原始论文所述。我们证明了两个条件（梯度完整性、状态一致性）是分布式训练与单设备结果相匹配的必要且充分条件，并提供了安全组合策略的组合规则。该框架统一了ZeRO阶段1-3、完全分片数据并行（FSDP）、张量并行和流水线并行，作为具有不同放置选择的实例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to provide a systematic framework for analyzing parallelism strategies in distributed deep learning, as current methods rely on trial and error. The authors introduce the concept of placement semantics, which categorizes parallelism strategies based on the distribution of four training states across devices using five modes. Their findings demonstrate that the derived predictions for memory consumption and communication volume align precisely with existing results, notably showing that ZeRO-3 requires 8 times less memory than data parallelism at a 1.5 times communication cost. Additionally, they establish necessary conditions for ensuring gradient integrity and state consistency in distributed training, while also offering rules for safely combining different strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于建立一个系统框架，以分析分布式深度学习中的并行策略，因为目前的实践依赖于试错法，缺乏预测模型。作者引入了放置语义的概念，根据四个训练状态在设备上的分布以及五种模式对并行策略进行分类，从而在不涉及实现细节的情况下推导出内存消耗和通信量。研究结果表明，他们的预测与现有文献完全一致，例如，ZeRO-3的内存使用量比数据并行少8倍，但通信成本增加1.5倍，同时证明了确保分布式训练结果与单设备训练结果一致的必要条件，并提供了安全组合不同策略的规则。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay</div>
<div class="meta-line">Authors: Ahmad Makinde</div>
<div class="meta-line">First: 2026-01-05T17:59:42+00:00 · Latest: 2026-01-05T17:59:42+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests.Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02310v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02310v1">PDF</a> · <a href="https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the &#x27;shape&#x27; of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the &#x27;dead-zones&#x27; being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高频限价单簿预测的时序Kolmogorov-Arnold网络（T-KAN）：效率、可解释性和阿尔法衰减</div>
<div class="mono" style="margin-top:8px">高频交易（HFT）环境的特点是大量的限价单簿（LOB）数据，这些数据以噪声大和非线性著称。阿尔法衰减是一个重大挑战，传统模型如DeepLOB在时间范围（k）增加时预测能力下降。本文使用FI-2010数据集，引入时序Kolmogorov-Arnold网络（T-KAN），用可学习的B样条激活函数替代标准LSTM的固定线性权重。这使得模型能够学习市场信号的“形状”，而不仅仅是其幅度。在k = 100的范围内，F1-score相对提高了19.1%。T-KAN网络的有效性不容小觑，在1.0 bps交易成本下，产生了132.48%的回报，相比之下，DeepLOB的回撤为-82.76%。此外，T-KAN模型的可解释性也相当强，样条中的“死区”清晰可见。T-KAN架构还特别优化了低延迟FPGA实现，通过高级综合（HLS）。本项目实验的代码可以在https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of high-frequency trading (HFT) environments characterized by noisy and non-linear limit order book (LOB) data, particularly the issue of alpha decay in predictive models. The authors introduce Temporal Kolmogorov-Arnold Networks (T-KAN), which utilize learnable B-spline activation functions instead of fixed linear weights in standard LSTMs, allowing the model to capture the shape of market signals. The experimental results demonstrate a 19.1% relative improvement in the F1-score at a time horizon of k = 100, and the T-KAN model achieved a return of 132.48% compared to a -82.76% drawdown from the DeepLOB model under transaction costs of 1.0 bps, while also providing clear interpretability of market signals through visible &#x27;dead-zones&#x27; in the splines.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决高频交易（HFT）环境中的挑战，特别是嘈杂的限价订单簿数据和预测模型中的阿尔法衰减问题。作者提出了时间科尔莫哥罗夫-阿诺德网络（T-KAN），该网络使用可学习的B样条激活函数替代标准LSTM中的固定线性权重，从而使模型能够捕捉市场信号的形状。实验结果表明，在k=100的时间范围内，F1分数提高了19.1%，而与DeepLOB模型相比，回报率显著达到132.48%，而DeepLOB则出现了-82.76%的回撤，同时通过样条中的可见“死区”提供了解释性，并针对低延迟FPGA实现进行了优化。</div>
</details>
</div>
<div class="card">
<div class="title">360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</div>
<div class="meta-line">Authors: Xiaopeng Guo, Yinzhe Xu, Huajian Huang, Sai-Kit Yeung</div>
<div class="meta-line">First: 2026-01-05T17:52:50+00:00 · Latest: 2026-01-05T17:52:50+00:00</div>
<div class="meta-line">Comments: 12 pages. Received by RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02309v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02309v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://chris1004336379.github.io/360DVO-homepage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>360DVO：单目360度相机的深度视觉里程计</div>
<div class="mono" style="margin-top:8px">单目全向视觉里程计（OVO）系统利用360度相机克服透视视觉里程计系统的视野限制。然而，现有方法依赖手工特征或光度目标，往往在激烈运动和变化照明等挑战场景中缺乏鲁棒性。为此，我们提出了360DVO，这是第一个基于深度学习的OVO框架。我们的方法引入了一种抗畸变的球面特征提取器（DAS-Feat），能够自适应地从360度图像中学习抗畸变特征。这些稀疏特征块随后用于在新颖的全向可微束调整（ODBA）模块中建立有效的位姿估计约束。为了便于在现实环境中评估，我们还贡献了一个新的真实世界OVO基准。在该基准和公共合成数据集（TartanAir V2和360VO）上的大量实验表明，360DVO超越了最先进的基线（包括360VO和OpenVSLAM），鲁棒性提高了50%，准确性提高了37.5%。主页：https://chris1004336379.github.io/360DVO-homepage</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the robustness of monocular omnidirectional visual odometry (OVO) systems, which are limited by the field-of-view constraints of traditional perspective visual odometry methods. The authors propose 360DVO, a deep learning-based OVO framework that employs a distortion-aware spherical feature extractor (DAS-Feat) to learn distortion-resistant features from 360-degree images. Experimental results on a new real-world OVO benchmark and public synthetic datasets show that 360DVO outperforms existing state-of-the-art methods, achieving a 50% improvement in robustness and a 37.5% increase in accuracy compared to baselines like 360VO and OpenVSLAM.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强单目全向视觉里程计（OVO）系统的鲁棒性，而现有方法在激烈运动和光照变化等挑战条件下表现不佳。作者提出了360DVO，一个基于深度学习的框架，采用了一个抗畸变球面特征提取器（DAS-Feat），从360度图像中学习抗畸变特征。实验结果表明，在新的真实世界OVO基准和公共合成数据集上，360DVO的表现超过了现有方法，相比于360VO和OpenVSLAM等最先进基线，鲁棒性提高了50%，准确性提高了37.5%。</div>
</details>
</div>
<div class="card">
<div class="title">Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck</div>
<div class="meta-line">Authors: Dina El Zein, James Henderson</div>
<div class="meta-line">First: 2026-01-05T17:49:39+00:00 · Latest: 2026-01-05T17:49:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02307v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02307v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非参数变分信息瓶颈下的文本变换器嵌入的差分隐私</div>
<div class="mono" style="margin-top:8px">我们提出了一种通过共享其变换器嵌入的噪声版本来保护隐私的文本数据共享方法。研究表明，深度模型学习的隐藏表示可以编码输入中的敏感信息，使得对手能够以相当高的准确性恢复输入数据。由于变换器嵌入由多个向量组成（每个标记一个），这一问题在变换器嵌入中更加严重。为减轻这一风险，我们提出了非参数变分差分隐私（NVDP），确保有用的数据共享和强大的隐私保护。我们采用差分隐私的方法，将非参数变分信息瓶颈（NVIB）层集成到变换器架构中，以向其多向量嵌入中注入噪声，从而隐藏信息，并通过Rényi散度及其相应的贝叶斯差分隐私（BDP）保证来衡量隐私保护。训练NVIB层根据效用校准噪声水平。我们在GLUE基准上测试NVDP，结果表明，改变噪声水平可以在隐私和准确性之间实现有用的权衡。在较低的噪声水平下，我们的模型保持高准确性，同时提供强大的隐私保证，有效平衡隐私和效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of protecting sensitive information in transformer embeddings of text data, which can inadvertently reveal input data to adversaries. The authors propose a method called Nonparametric Variational Differential Privacy (NVDP), which integrates a Nonparametric Variational Information Bottleneck layer into the transformer architecture to add noise to the embeddings, thereby enhancing privacy. Experimental results on the GLUE benchmark demonstrate that by adjusting the noise level, the model achieves a balance between privacy and accuracy, maintaining high accuracy with lower noise levels while ensuring strong privacy guarantees.</div>
<div class="mono" style="margin-top:8px">本研究解决了保护文本数据中变换器嵌入的敏感信息的问题，这些嵌入可能会无意中向对手泄露输入数据。作者提出了一种称为非参数变分差分隐私（NVDP）的方法，该方法将非参数变分信息瓶颈层集成到变换器架构中，以向嵌入添加噪声。对GLUE基准的实验结果表明，通过调整噪声水平，该方法在隐私和准确性之间达成了平衡，在较低的噪声水平下保持高准确性，同时确保强隐私保证。</div>
</details>
</div>
<div class="card">
<div class="title">Grounded Test-Time Adaptation for LLM Agents</div>
<div class="meta-line">Authors: Arthur Chen, Zuxin Liu, Jianguo Zhang, Akshara Prabhakar, Zhiwei Liu, Shelby Heinecke, Silvio Savarese, Victor Zhong, Caiming Xiong</div>
<div class="meta-line">First: 2025-11-06T22:24:35+00:00 · Latest: 2026-01-05T17:43:48+00:00</div>
<div class="meta-line">Comments: Our code is available here: https://github.com/r2llab/GTTA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04847v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04847v3">PDF</a> · <a href="https://github.com/r2llab/GTTA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model&#x27;s output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment&#x27;s causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent&#x27;s success rate from 2% to 23%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对LLM代理的基础测试时适应</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLM）的代理在新颖和复杂环境（如未见过的网站或新功能集）中难以泛化，这源于其预训练和测试时条件之间的根本不匹配。这一挑战源于两种不同的失败模式：对环境特定组件（如观察格式）的句法误解，以及对状态转移动态的语义误解，这些在测试时才会显现。为了解决这些问题，我们提出了两种不同且互补的策略，通过利用部署期间可用的环境特定信息来适应LLM代理。首先，一种在线分布适应方法通过学习轻量级适应向量来参数化环境细微差别，从而偏向模型的输出分布，使其快速与环境响应格式对齐。其次，一种部署时动态基础方法采用以角色驱动的探索阶段，在任务执行前系统性地探测和学习环境的因果动态，为代理提供非参数化的世界模型。我们在多种代理基准测试中评估了这些策略，包括函数调用和网页导航。我们的实证结果显示，这两种策略在所有基准测试中都有效，且计算成本极低。我们发现，动态基础在复杂环境中尤其有效，因为不可预测的动态构成了主要障碍，展示了朝着更具泛化能力和能力的基于LLM的代理的稳健路径。例如，在WebArena多站点拆分中，该方法将代理的成功率从2%提高到23%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of large language model (LLM)-based agents in adapting to novel environments, which often leads to poor generalization due to mismatches between pre-training and test-time conditions. The authors propose two complementary adaptation strategies: an online distributional adaptation method that learns a lightweight adaptation vector to align the model&#x27;s output with environment-specific formats, and a deployment-time dynamics grounding method that explores the environment to learn its causal dynamics before task execution. Experimental results demonstrate the effectiveness of both methods across various benchmarks, with the dynamics grounding approach significantly improving performance in complex environments, as evidenced by an increase in success rate from 2% to 23% on the WebArena multi-site split.</div>
<div class="mono" style="margin-top:8px">本研究解决了基于大型语言模型（LLM）的智能体在适应新环境时的局限性，这通常导致由于预训练和测试时条件不匹配而导致的较差泛化能力。作者提出了两种互补的适应策略：一种在线分布适应方法，通过学习轻量级适应向量来使模型输出与环境特定格式对齐，另一种部署时动态基础方法在任务执行前探索并学习环境的因果动态。实验结果表明，这些策略在各种基准测试中的有效性，其中动态基础在复杂环境中显著提高了性能，例如在WebArena多站点分割中成功率从2%提高到23%。</div>
</details>
</div>
<div class="card">
<div class="title">SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting</div>
<div class="meta-line">Authors: Sara Inácio, Hugo Proença, João C. Neves</div>
<div class="meta-line">First: 2026-01-05T17:34:50+00:00 · Latest: 2026-01-05T17:34:50+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02299v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02299v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene&#x27;s hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SortWaste：用于工业废物分类的密集标注数据集</div>
<div class="mono" style="margin-top:8px">由于人口增长，废物生产不断增加，这给有效管理和回收材料带来了挑战。手动废物分类是一种常见做法；然而，对于大规模废物流的处理效率低下，并且对工人存在健康风险。另一方面，现有的自动分类方法在处理真实废物流的高变异性、杂乱和视觉复杂性方面仍然面临困难。缺乏真实世界的废物分类数据集是导致该问题的自动化系统发展滞后的主要原因。因此，我们推出了SortWaste，这是一个从材料回收设施收集的密集标注物体检测数据集。此外，我们通过提出ClutterScore，一个使用一组影响视觉复杂性的代理（例如，物体数量、类别和大小熵、空间重叠）来评估场景难度水平的客观指标，来推动分类线上的废物检测标准化。除了这些贡献外，我们还提供了最先进的物体检测模型的广泛基准，详细说明了它们在所提指标评估的难度水平下的结果。尽管在仅检测塑料的任务中取得了令人鼓舞的结果（mAP为59.7%），但在高度杂乱的场景中性能显著下降。这突显了该主题上需要新颖且更具挑战性的数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing production of waste due to population growth has created challenges in effective waste management and recycling, particularly with manual sorting being inefficient and hazardous for workers. To address this issue, the authors introduce SortWaste, a densely annotated dataset for object detection in industrial waste sorting, collected from a Material Recovery Facility. They also propose ClutterScore, a metric to evaluate the complexity of waste scenes, and benchmark various state-of-the-art object detection models, revealing a mean Average Precision (mAP) of 59.7% for plastic detection, though performance declines significantly in cluttered environments, indicating a need for more challenging datasets in this area.</div>
<div class="mono" style="margin-top:8px">由于人口增长，废物的增加使得更有效的废物管理和回收方法变得必要，因为人工分类效率低且对工人健康构成风险。本研究介绍了SortWaste，一个从材料回收设施收集的工业废物分类的密集注释数据集，并提出了ClutterScore，一个评估场景复杂度的指标。实验结果表明，尽管最先进的目标检测模型在仅塑料检测任务中实现了59.7%的平均精度（mAP），但在杂乱环境中的表现显著下降，这表明该领域需要更具挑战性的数据集。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
