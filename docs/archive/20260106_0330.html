<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-06 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260106_0330</div>
    <div class="row"><div class="card">
<div class="title">Effects of Structural Allocation of Geometric Task Diversity in Linear Meta-Learning Models</div>
<div class="meta-line">Authors: Saptati Datta, Nicolas W. Hengartner, Yulia Pimonova, Natalie E. Klein, Nicholas Lubbers</div>
<div class="meta-line">First: 2025-09-22T19:16:59+00:00 · Latest: 2026-01-02T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18349v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18349v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta-learning aims to leverage information across related tasks to improve prediction on unlabeled data for new tasks when only a small number of labeled observations are available (&quot;few-shot&quot; learning). Increased task diversity is often believed to enhance meta-learning by providing richer information across tasks. However, recent work by Kumar et al. (2022) shows that increasing task diversity, quantified through the overall geometric spread of task representations, can in fact degrade meta-learning prediction performance across a range of models and datasets. In this work, we build on this observation by showing that meta-learning performance is affected not only by the overall geometric variability of task parameters, but also by how this variability is allocated relative to an underlying low-dimensional structure. Similar to Pimonova et al. (2025), we decompose task-specific regression effects into a structurally informative component and an orthogonal, non-informative component. We show theoretically and through simulation that meta-learning prediction degrades when a larger fraction of between-task variability lies in orthogonal, non-informative directions, even when the overall geometric variability of tasks is held fixed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性元学习模型中几何任务多样性的结构分配效应</div>
<div class="mono" style="margin-top:8px">元学习旨在利用相关任务之间的信息，以提高在仅有少量标记观察数据时对新任务的无标记数据的预测（“少样本”学习）。增加任务多样性通常被认为可以通过提供更丰富的任务信息来增强元学习。然而，Kumar等人（2022）的最新研究表明，通过任务表示的整体几何分布量化的任务多样性增加，实际上可能会降低一系列模型和数据集的元学习预测性能。在这项工作中，我们基于这一观察，展示了元学习性能不仅受任务参数的整体几何变异性的影响，还受这种变异相对于潜在低维结构的分配方式的影响。类似于Pimonova等人（2025），我们将任务特定的回归效应分解为一个结构性信息成分和一个正交的、非信息成分。我们通过理论和模拟表明，当任务之间的变异性中有更大比例位于正交的、非信息方向时，即使任务的整体几何变异性保持不变，元学习预测也会下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand how the structural allocation of geometric task diversity impacts meta-learning performance, particularly in few-shot learning scenarios. The authors build on previous findings that increasing task diversity can degrade prediction performance and investigate how the distribution of variability among tasks affects outcomes. Through theoretical analysis and simulations, they demonstrate that meta-learning performance deteriorates when a significant portion of between-task variability is concentrated in orthogonal, non-informative directions, even when the overall geometric variability remains constant.</div>
<div class="mono" style="margin-top:8px">本研究探讨了任务多样性对元学习性能的影响，动机源于观察到增加任务多样性有时会降低预测准确性。作者在先前研究的基础上分析了任务之间几何变异性的分配如何影响元学习结果，特别是区分了任务变异性的结构性信息成分和非信息成分。通过理论分析和模拟，研究表明，当较大比例的变异性是正交且非信息性的时，元学习的预测性能会下降，即使总的几何变异性保持不变。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Sparse Linear Regression under Communication Constraints</div>
<div class="meta-line">Authors: Rodney Fonseca, Boaz Nadler</div>
<div class="meta-line">First: 2023-01-09T08:23:37+00:00 · Latest: 2026-01-02T18:58:26+00:00</div>
<div class="meta-line">Comments: 50 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2301.04022v2">Abs</a> · <a href="https://arxiv.org/pdf/2301.04022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In multiple domains, statistical tasks are performed in distributed settings, with data split among several end machines that are connected to a fusion center. In various applications, the end machines have limited bandwidth and power, and thus a tight communication budget. In this work we focus on distributed learning of a sparse linear regression model, under severe communication constraints. We propose several two round distributed schemes, whose communication per machine is sublinear in the data dimension. In our schemes, individual machines compute debiased lasso estimators, but send to the fusion center only very few values. On the theoretical front, we analyze one of these schemes and prove that with high probability it achieves exact support recovery at low signal to noise ratios, where individual machines fail to recover the support. We show in simulations that our scheme works as well as, and in some cases better, than more communication intensive approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通信约束下的分布式稀疏线性回归</div>
<div class="mono" style="margin-top:8px">在多个领域，统计任务在分布式环境中进行，数据分布在多个连接到融合中心的终端机器上。在各种应用中，终端机器的带宽和功率有限，因此通信预算紧张。在这项工作中，我们专注于在严重通信约束下的稀疏线性回归模型的分布式学习。我们提出了几种两轮分布式方案，每台机器的通信量在数据维度上是次线性的。在我们的方案中，单个机器计算去偏的套索估计量，但仅向融合中心发送极少的值。在理论方面，我们分析了其中一种方案，并证明在低信噪比下，它以高概率实现精确的支持恢复，而单个机器无法恢复支持。我们在模拟中显示，我们的方案在效果上与更高通信强度的方法相当，甚至在某些情况下更好。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of performing statistical tasks in distributed settings with limited communication bandwidth and power. The authors propose several two-round distributed schemes for learning a sparse linear regression model, where each machine computes debiased lasso estimators and transmits only a few values to a fusion center, ensuring communication is sublinear in the data dimension. Theoretical analysis demonstrates that one of these schemes can achieve exact support recovery at low signal-to-noise ratios, outperforming individual machines, and simulations indicate that the proposed method performs comparably or better than more communication-intensive approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在分布式环境中执行统计任务的需求，其中数据分布在多个具有有限通信带宽和功率的机器之间。作者提出了几种在这些通信限制下学习稀疏线性回归模型的两轮分布式方案，使得各个机器能够计算去偏的套索估计量，同时仅向融合中心传输极少量的信息。关键实验结果表明，所提出的其中一种方案在低信噪比下以高概率实现了精确的支持恢复，且在某些情况下优于需要更多通信的传统方法。</div>
</details>
</div>
<div class="card">
<div class="title">Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI</div>
<div class="meta-line">Authors: Wenhui Chu, Nikolaos V. Tsekos</div>
<div class="meta-line">Venue: 2022 12th International Conference on Bioscience, Biochemistry and Bioinformatics (ICBBB &#x27;22), January 7-10, 2022, Tokyo, Japan</div>
<div class="meta-line">First: 2026-01-02T18:56:15+00:00 · Latest: 2026-01-02T18:56:15+00:00</div>
<div class="meta-line">Comments: 7 pages, 5 figures, published in ICBBB 2022</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>两种深度学习方法用于心脏MRI中左心室的自动分割</div>
<div class="mono" style="margin-top:8px">左心室（LV）分割对于心脏图像的临床定量和诊断至关重要。在本研究中，我们提出了两种新颖的深度学习架构，分别称为LNU-Net和IBU-Net，用于从短轴心脏MRI图像中进行左心室分割。LNU-Net源自层归一化（LN）U-Net架构，而IBU-Net源自实例批归一化（IB）U-Net用于医学图像分割。LNU-Net和IBU-Net的架构具有特征提取的下采样路径和精确定位的上采样路径。我们使用原始U-Net作为基本分割方法，并将其与我们提出的架构进行了比较。LNU-Net和IBU-Net均具有左心室分割方法：LNU-Net在每个卷积块中应用层归一化，而IBU-Net在第一个卷积块中结合实例和批归一化，并将其结果传递给下一层。我们的方法结合了仿射变换和弹性变形用于图像数据处理。我们使用包含来自45名患者的805张关于左心室的MRI图像的数据集进行评估。我们实验性地评估了所提方法的结果，发现其在Dice系数和平均垂直距离上优于其他最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of left ventricle segmentation in cine cardiac MRI, which is essential for clinical diagnosis. The authors propose two deep learning architectures, LNU-Net and IBU-Net, which enhance the traditional U-Net model by incorporating layer normalization and instance-batch normalization, respectively. Experimental results demonstrate that both LNU-Net and IBU-Net significantly outperform the original U-Net and other state-of-the-art methods in terms of the dice coefficient and average perpendicular distance, based on a dataset of 805 MRI images from 45 patients.</div>
<div class="mono" style="margin-top:8px">本研究的动机是心脏影像中准确的左心室分割对临床诊断的关键需求。作者提出了两种深度学习架构，LNU-Net和IBU-Net，旨在从短轴心脏MRI图像中分割左心室，其中LNU-Net采用层归一化，IBU-Net则使用实例-批量归一化。实验结果表明，这两种架构在骰子系数和平均垂直距离方面显著优于原始U-Net和其他最先进的方法，基于来自45名患者的805张MRI图像的数据集。</div>
</details>
</div>
<div class="card">
<div class="title">Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning</div>
<div class="meta-line">Authors: Valentin Noël</div>
<div class="meta-line">First: 2026-01-02T18:49:37+00:00 · Latest: 2026-01-02T18:49:37+00:00</div>
<div class="meta-line">Comments: 58 pages, 19 figures, Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen&#x27;s $d = 3.30$ ($p &lt; 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B&#x27;s Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理的几何：有效数学推理的谱特征</div>
<div class="mono" style="margin-top:8px">我们提出了一种无训练的方法，通过对注意力模式的谱分析来检测大型语言模型中的有效数学推理。通过将注意力矩阵视为动态图的邻接矩阵，我们提取了四个可解释的谱诊断，分别是Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，这些特征在有效和无效数学证明之间表现出统计显著差异。对来自四个独立架构家族（Meta Llama、Alibaba Qwen、Microsoft Phi和Mistral AI）的七个变换器模型的实验表明，这种谱特征产生的效应大小高达Cohen的$d = 3.30$（$p &lt; 10^{-116}$），在严格评估下实现85.0--95.6\%的分类准确率，经过校准的阈值在完整数据集上达到93--95\%。该方法不需要训练数据、微调或学习分类器：单一的谱度量阈值足以实现高准确率。通过系统的标签修正，我们发现谱方法检测的是逻辑一致性而非编译器接受，识别出由于技术故障而被正式验证者拒绝的数学有效证明。我们进一步识别出一种架构依赖性：Mistral-7B的滑动窗口注意力将区分信号从HFER转移到后层平滑度（$d = 2.09$，$p_{\text{MW}} = 1.16 \times 10^{-48}$），揭示了注意力机制设计影响哪些谱特征捕捉推理有效性。这些发现确立了谱图分析作为推理验证的原则框架，具有对幻觉检测和AI安全监控的直接应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a method for detecting valid mathematical reasoning in large language models without requiring training data. The authors utilize spectral analysis of attention patterns by treating attention matrices as adjacency matrices of dynamic graphs, extracting four spectral diagnostics that differentiate valid from invalid mathematical proofs. Experiments conducted on seven transformer models show that this method achieves high classification accuracy, with effect sizes indicating significant differences between valid and invalid proofs, and highlights the influence of architectural design on the effectiveness of spectral features in reasoning verification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在大型语言模型中检测有效的数学推理，而无需训练数据。作者提出了一种无训练的方法，通过将注意力矩阵视为动态图的邻接矩阵，采用谱分析注意力模式。关键实验结果表明，包括Fiedler值和高频能量比在内的谱诊断在有效和无效证明之间显示出显著差异，在各种变压器模型中实现了85.0%到95.6%的分类准确率，效应量高达Cohen&#x27;s d = 3.30。此外，该方法揭示其检测的是逻辑一致性而非单纯的编译器接受，并强调了架构设计如何影响与推理有效性相关的谱特征。</div>
</details>
</div>
<div class="card">
<div class="title">Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection</div>
<div class="meta-line">Authors: Shukesh Reddy, Srijan Das, Abhijit Das</div>
<div class="meta-line">First: 2026-01-02T18:47:36+00:00 · Latest: 2026-01-02T18:47:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fusion-SSAT：通过特征融合释放自监督辅助任务在广义深伪检测中的潜力</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们尝试释放自监督学习作为辅助任务的潜力，以优化广义深伪检测的主要任务。为此，我们研究了这些任务的训练方案的不同组合，以寻找最有效的方式。我们的研究结果表明，融合自监督辅助任务的特征表示是解决当前问题的强大特征表示。这种表示可以利用最终潜力，并带来自监督任务和主要任务的独特表示，从而在主要任务中实现更好的性能。我们在包括DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV在内的大量数据集上进行了实验，结果显示与当前最先进的检测器相比，在跨数据集评估中具有更好的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the effectiveness of generalized deepfake detection by leveraging self-supervised learning as an auxiliary task. The authors employed various training schemes to investigate the optimal combinations for feature representation. The key findings indicate that fusing feature representations from self-supervised auxiliary tasks significantly improves performance in deepfake detection, demonstrating better generalizability across multiple datasets compared to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过利用自监督学习作为辅助任务来增强广义深伪检测。研究人员探索了不同的训练方案组合，以优化主要检测任务。实验结果表明，从自监督辅助任务中融合特征表示显著提高了性能，在多个数据集上实现了比现有最先进检测器更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</div>
<div class="meta-line">Authors: Sunny Gupta, Amit Sethi</div>
<div class="meta-line">First: 2026-01-02T18:40:41+00:00 · Latest: 2026-01-02T18:40:41+00:00</div>
<div class="meta-line">Comments: 10 pages, 1 figures, Accepted at AAI&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00785v1">PDF</a> · <a href="http://github.com/sunnyinAI/FedHypeVAE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FedHypeVAE：使用超网络生成条件变分自编码器的联邦学习以实现差分隐私嵌入共享</div>
<div class="mono" style="margin-top:8px">联邦数据共享承诺在不集中原始数据的情况下提供效用，但现有的嵌入级生成器在非独立同分布的客户端异质性下表现不佳，并且对梯度泄漏提供有限的正式保护。我们提出了FedHypeVAE，这是一个差分隐私的、基于超网络的框架，用于在去中心化客户端之间合成嵌入级数据。基于条件变分自编码器的骨干网络，我们用客户端感知解码器和由共享超网络从私有、可训练的客户端代码生成的类条件先验替换了单一的全局解码器和固定的潜在先验。这种双层设计个性化了生成层而不是下游模型，同时将本地数据与传输参数解耦。共享超网络在差分隐私下进行优化，确保仅聚合经过噪声扰动和裁剪的梯度。真实和合成嵌入之间的局部MMD对齐以及对超网络输出的Lipschitz正则化进一步增强了在非独立同分布条件下的稳定性和分布一致性。训练后，中性元代码实现领域无关的合成，而元代码的混合提供可控的多领域覆盖。FedHypeVAE在生成器层面统一了个性化、隐私和分布对齐，为联邦环境中的隐私保护数据合成建立了原则性基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance federated data sharing by addressing the challenges of non-IID client heterogeneity and gradient leakage in embedding-level generators. The authors propose FedHypeVAE, a framework that utilizes a hypernetwork to generate conditional VAEs for synthesizing embedding-level data while ensuring differential privacy. Key experimental findings demonstrate that the bi-level design of FedHypeVAE, which incorporates client-aware decoders and class-conditional priors, effectively personalizes the generative process and maintains stability and distributional coherence, enabling domain-agnostic synthesis and controllable multi-domain coverage.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决非独立同分布（non-IID）客户端异质性和梯度泄漏的问题，来增强联邦数据共享。作者提出了FedHypeVAE框架，利用超网络生成条件变分自编码器（VAEs），以在去中心化客户端之间合成嵌入级数据，同时确保差分隐私。主要实验结果表明，FedHypeVAE的双层设计结合了客户端感知解码器和类条件先验，有效地个性化了生成层，并保持了稳定性和分布一致性，允许进行领域无关的合成和可控的多领域覆盖。</div>
</details>
</div>
<div class="card">
<div class="title">Categorical Reparameterization with Denoising Diffusion models</div>
<div class="meta-line">Authors: Samson Gourevitch, Alain Durmus, Eric Moulines, Jimmy Olsson, Yazid Janati</div>
<div class="meta-line">First: 2026-01-02T18:30:05+00:00 · Latest: 2026-01-02T18:30:05+00:00</div>
<div class="meta-line">Comments: working paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00781v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带有去噪扩散模型的分类重参数化</div>
<div class="mono" style="margin-top:8px">基于梯度的优化通常依赖于得分函数估计器，这些估计器是无偏但噪声较大的，或者依赖于连续松弛，将离散分布替换为允许路径（重参数化）梯度的平滑代理，代价是优化一个有偏的、依赖温度的目标。在本文中，我们通过引入基于扩散的软重参数化来扩展这一松弛家族，适用于分类分布。在这些分布下，经过高斯噪声过程的去噪器具有封闭形式并且可以高效计算，从而产生一个无训练的扩散采样器，通过它我们可以进行反向传播。我们的实验表明，所提出的重参数化技巧在各种基准测试中产生了具有竞争力或改进的优化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve gradient-based optimization methods for categorical variables, which often face challenges due to the noise in score-function estimators and the biases in continuous relaxations. The authors propose a diffusion-based soft reparameterization technique for categorical distributions, leveraging a Gaussian noising process that allows for efficient computation of the denoiser in closed form. Experimental results demonstrate that this new reparameterization method achieves competitive or enhanced optimization performance across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了分类变量的基于梯度的优化中的挑战，这通常依赖于噪声较大的评分函数估计器或有偏的连续松弛。作者提出了一种新颖的基于扩散的软重参数化方法，用于分类分布，该方法允许在高斯噪声过程中高效计算去噪器，从而实现无训练的扩散采样器以进行反向传播。实验结果表明，这种新的重参数化技术在各种基准测试中实现了具有竞争力或更好的优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</div>
<div class="meta-line">Authors: Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</div>
<div class="meta-line">First: 2025-12-28T21:57:42+00:00 · Latest: 2026-01-02T18:25:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23090v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23090v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准成功，临床失败：当强化学习优化基准而非患者时</div>
<div class="mono" style="margin-top:8px">最近针对大型语言模型（LLMs）的强化学习（RL）进展改善了推理任务，但其在资源受限的医学影像应用中仍未得到充分探索。我们介绍了ChexReason，这是一种通过R1风格方法（SFT后跟GRPO）训练的视觉-语言模型，仅使用2000个SFT样本、1000个RL样本和一台A100 GPU。在CheXpert和NIH基准上的评估揭示了一个根本的矛盾：GRPO恢复了分布内性能（在CheXpert上提高23%，宏F1=0.346），但降低了跨数据集的可迁移性（在NIH上下降19%）。这与高资源模型如NV-Reason-CXR-3B相似，表明问题源于RL范式而非规模。我们识别出一个泛化悖论，即SFT检查点在优化前独特地改善了NIH，表明教师引导的推理捕捉了更多与机构无关的特征。此外，跨模型比较显示结构化推理支架有利于通用VLM，但对医学预训练模型的增益有限。因此，精心策划的监督微调可能在需要跨多样人群的临床部署中优于激进的RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the application of reinforcement learning (RL) in medical imaging, specifically addressing the limitations of current methods in optimizing for patient outcomes rather than benchmarks. The authors introduce ChexReason, a vision-language model trained using a combination of supervised fine-tuning (SFT) and gradient policy optimization (GRPO) with a limited dataset and computational resources. The findings reveal a trade-off where GRPO enhances performance on the CheXpert benchmark by 23% but results in a 19% decrease in transferability to the NIH dataset, highlighting a generalization paradox that suggests supervised methods may be more effective for clinical applications requiring robustness across varied patient populations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在医学影像中应用强化学习（RL）的挑战，特别是在大语言模型（LLMs）的背景下。作者介绍了ChexReason，这是一种使用有限数据结合监督微调（SFT）和强化学习训练的视觉-语言模型。实验结果表明，尽管该模型在CheXpert基准上实现了23%的分布内性能提升，但在NIH数据集上的跨数据集迁移能力下降了19%，突显出一种泛化悖论，即在优化之前，SFT在NIH上的性能有所改善。研究结果表明，针对需要在不同人群中保持稳健性的临床应用，监督微调可能比激进的RL更有效。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Anchor Transport: Robust Test-Time Adaptation for Vision-Language Models</div>
<div class="meta-line">Authors: Shambhavi Mishra, Julio Silva-Rodriguez, Ismail Ben Ayed, Marco Pedersoli, Jose Dolz</div>
<div class="meta-line">First: 2024-11-26T00:15:37+00:00 · Latest: 2026-01-02T18:18:27+00:00</div>
<div class="meta-line">Comments: Added additional figures to communicate the algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.17002v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.17002v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large pre-trained vision-language models (VLMs), such as CLIP, have shown unprecedented zero-shot performance across a wide range of tasks. Nevertheless, these models may be unreliable under distributional shifts, as their performance is significantly degraded. In this work, we investigate how to efficiently utilize class text information to mitigate distribution drifts encountered by VLMs during inference. In particular, we propose generating pseudo-labels for the noisy test-time samples by aligning visual embeddings with reliable, text-based semantic anchors. Specifically, to maintain the regular structure of the dataset properly, we formulate the problem as a batch-wise label assignment, which is efficiently solved using Optimal Transport. Our method, Semantic Anchor Transport (SAT), utilizes such pseudo-labels as supervisory signals for test-time adaptation, yielding a principled cross-modal alignment solution. Moreover, SAT further leverages heterogeneous textual clues, with a multi-template distillation approach that replicates multi-view contrastive learning strategies in unsupervised representation learning without incurring additional computational complexity. Extensive experiments on multiple popular test-time adaptation benchmarks presenting diverse complexity empirically show the superiority of SAT, achieving consistent performance gains over recent state-of-the-art methods, yet being computationally efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义锚点传输：视觉语言模型的鲁棒测试时适应</div>
<div class="mono" style="margin-top:8px">大型预训练视觉语言模型（VLMs），如CLIP，在广泛任务中展现了前所未有的零样本性能。然而，这些模型在分布变化下可能不可靠，其性能显著下降。在本研究中，我们探讨如何有效利用类别文本信息来减轻VLMs在推理过程中遇到的分布漂移。特别地，我们提出通过将视觉嵌入与可靠的基于文本的语义锚点对齐，为噪声测试样本生成伪标签。具体而言，为了正确维护数据集的常规结构，我们将问题表述为批量标签分配，并使用最优传输高效求解。我们的方法，语义锚点传输（SAT），利用这些伪标签作为测试时适应的监督信号，提供了一个原则性的跨模态对齐解决方案。此外，SAT进一步利用异构文本线索，采用多模板蒸馏方法，在无监督表示学习中复制多视角对比学习策略，而不增加额外的计算复杂性。在多个流行的测试时适应基准上进行的广泛实验，展示了SAT的优越性，取得了相对于最近的最先进方法的一致性能提升，同时计算效率高。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation of large pre-trained vision-language models (VLMs) like CLIP under distributional shifts during inference. The authors propose a method called Semantic Anchor Transport (SAT), which generates pseudo-labels for noisy test-time samples by aligning visual embeddings with reliable text-based semantic anchors, formulated as a batch-wise label assignment problem solved using Optimal Transport. Experimental results demonstrate that SAT achieves consistent performance improvements over state-of-the-art methods on various test-time adaptation benchmarks while maintaining computational efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型预训练视觉-语言模型（VLMs），如CLIP，在分布变化下的不可靠性，这显著降低了它们的性能。作者提出了一种名为语义锚运输（SAT）的方法，通过将视觉嵌入与可靠的基于文本的语义锚对齐，为嘈杂的测试时间样本生成伪标签，该问题被表述为批量标签分配问题，并通过最优运输方法求解。实验结果表明，SAT提供了一种合理的跨模态对齐解决方案，并在各种测试时间适应基准上实现了相对于最新方法的一致性能提升，同时保持了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection</div>
<div class="meta-line">Authors: Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall</div>
<div class="meta-line">First: 2026-01-02T18:17:22+00:00 · Latest: 2026-01-02T18:17:22+00:00</div>
<div class="meta-line">Comments: Accepted at IJCB 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00777v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00777v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在音频深度伪造检测中使用多模态大型语言模型的可行性研究</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型（VLMs）和多模态大型语言模型（MLLMs）在检测图像和视频深度伪造方面表现出强大的泛化能力，但它们在音频深度伪造检测中的应用仍然未被充分探索。本研究旨在探讨MLLMs在音频深度伪造检测中的潜力。通过将音频输入与一系列文本提示结合，作为查询以了解MLLMs在音频深度伪造检测中学习跨模态的稳健表示的可行性。因此，我们尝试探索基于文本的、上下文丰富的、基于问题-回答的提示与二元决策。我们假设这种特征引导的推理将有助于促进更深层次的多模态理解，并使音频深度伪造检测的特征学习更加稳健。我们在两种评估模式下评估了两个MLLMs的性能： (a)零样本和(b)微调。我们的实验表明，将音频与多提示方法结合可能是音频深度伪造检测的可行途径。实验结果显示，模型在没有特定任务训练的情况下表现不佳，并且难以对域外数据进行泛化。然而，它们在域内数据上以最小的监督获得了良好的性能，表明音频深度伪造检测的潜力可期。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the underutilized potential of multi-modal large language models (MLLMs) for audio deepfake detection, an area that has not been extensively studied compared to image and video deepfakes. The study employs a method that combines audio inputs with various text prompts to assess the ability of MLLMs to learn robust representations for this task. The key findings indicate that while the models struggle with generalization to out-of-domain data and perform poorly without specific training, they demonstrate good performance on in-domain data with minimal supervision, suggesting a viable approach for audio deepfake detection using a multi-prompt strategy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨多模态大语言模型（MLLMs）在音频深度伪造检测中的潜力，这是一个相较于图像和视频深度伪造尚未被广泛研究的领域。研究采用的方法是将音频输入与文本提示结合，以评估MLLMs在此任务中学习稳健表示的能力。关键实验结果表明，尽管模型在没有特定任务训练的情况下表现不佳，并且在域外数据上难以泛化，但在域内数据上以最小监督获得良好表现，表明使用多提示策略进行音频深度伪造检测是一种可行的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected</div>
<div class="meta-line">Authors: Yingtao Zhang, Diego Cerretti, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci</div>
<div class="meta-line">First: 2025-01-31T13:04:37+00:00 · Latest: 2026-01-02T18:15:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.19107v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.19107v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏神经网络的脑网络科学建模使变换器和大型语言模型能够表现为全连接</div>
<div class="mono" style="margin-top:8px">动态稀疏训练（DST）可以减少人工神经网络的计算需求，但在高稀疏水平下保持峰值性能面临困难。Cannistraci-Hebb训练（CHT）是一种受脑启发的方法，用于在DST中增加连接性。CHT利用无梯度、基于拓扑的链接再生长，与全连接网络相比，在各种任务中显示出超稀疏（连接性低于1%）的优势。然而，CHT存在两个主要缺点：（i）其时间复杂度为$O(Nd^3)$ - N为节点网络大小，d为节点度 - 限制其在超稀疏状态下的应用。（ii）它选择最高的链接预测分数，这在网络呈现不可靠连接的早期训练阶段是不合适的。在这里，我们设计了第一个受脑启发的网络模型 - 称为二分接收场（BRF） - 用于初始化稀疏人工神经网络的连接性。我们进一步引入了一种GPU友好的基于矩阵的CH链接预测近似，将复杂度降低到$O(N^3)$。我们引入了Cannistraci-Hebb训练软规则（CHTs），采用灵活的策略在链接移除和再生长中进行连接采样，平衡网络拓扑的探索与利用。此外，我们将CHTs与sigmoid渐进密度衰减（CHTss）结合。实证结果表明，BRF在性能上优于以前的网络科学模型。在使用1%连接的情况下，CHTs在图像分类任务中的多层感知器架构上优于全连接网络，将某些网络压缩到不到30%的节点。在使用5%连接的情况下，CHTss在两个基于变换器的机器翻译任务中优于全连接网络。最后，在30%连接性下，CHTs和CHTss在语言建模任务中均优于其他DST方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to reduce computational demands in artificial neural networks (ANNs) while maintaining performance at high sparsity levels. The authors propose a brain-inspired model called bipartite receptive field (BRF) and introduce a GPU-friendly matrix-based approximation of Cannistraci-Hebb training (CHT) to address the limitations of existing methods. Experimental results demonstrate that BRF, combined with the Cannistraci-Hebb training soft rule (CHTs) and sigmoid gradual density decay (CHTss), achieves superior performance in various tasks, outperforming fully connected networks with only 1% to 5% of connections in image classification and machine translation tasks, and showing advantages over other dynamic sparse training methods in language modeling at 30% connectivity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过动态稀疏训练（DST）在保持低计算需求的同时提高人工神经网络（ANNs）的性能。作者提出了一种新的脑启发模型，称为双分支感受野（BRF），并引入了一种基于GPU友好的矩阵近似的Cannistraci-Hebb训练（CHT），以解决现有方法的局限性。实验结果表明，BRF结合Cannistraci-Hebb训练软规则（CHTs）和Sigmoid逐渐密度衰减（CHTss），在各种任务中显著提高了性能，在图像分类和机器翻译任务中以仅1%到5%的连接率超越全连接网络，并在30%连接率下超越其他DST方法的语言建模表现。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization</div>
<div class="meta-line">Authors: Simon Paquette-Greenbaum, Jiangbo Yu</div>
<div class="meta-line">First: 2026-01-02T18:02:13+00:00 · Latest: 2026-01-02T18:02:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00770v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00770v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>组合有效前沿的LLM代理：投资组合优化</div>
<div class="mono" style="margin-top:8px">投资组合优化是所有主要金融机构进行的任务。基数约束均值-方差投资组合优化（CCPO）问题的表述在投资组合优化中无处不在。这种类型的投资组合优化的挑战是一个混合整数二次规划（MIQP）问题，源于精确求解器的解的不可处理性，因此使用启发式算法来寻找近似投资组合解决方案。CCPO涉及许多繁琐复杂的工作流程，并且还需要大量的启发式算法开发工作，其中汇集的启发式解决方案的组合导致有效前沿的改善。因此，常见的方法是开发许多启发式算法。代理框架作为组合优化中许多问题的有前景的候选者出现，因为它们在自动化大型工作流程方面被证明同样高效，并且在算法开发方面表现出色，有时超过人类水平的表现。本研究为CCPO实施了一种新颖的代理框架，并探索了几种具体架构。在基准问题中，实施的代理框架与最先进的算法相匹配。此外，复杂的工作流程和算法开发工作得到了缓解，而在最坏的情况下，报告了较低但可接受的误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO), which is a common yet complex task in financial institutions due to its intractability when using exact solvers. The study implements a novel agentic framework to tackle the CCPO problem, exploring various architectures to enhance the efficiency of portfolio optimization. The experimental results demonstrate that the proposed framework matches the performance of state-of-the-art algorithms on benchmark problems, while also simplifying complex workflows and reducing the effort required for algorithm development, achieving acceptable error rates even in the worst-case scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决卡尔迪纳尔约束均值-方差投资组合优化（CCPO）所面临的挑战，这在金融机构中是一个普遍存在的问题，由于混合整数二次规划解的不可解性，通常需要大量的启发式算法开发。该研究引入了一种新颖的代理框架，旨在自动化和增强投资组合优化过程，并探索该框架内的各种架构。实验结果表明，所提出的代理框架在基准问题上实现了与最先进算法相当的性能，显著降低了工作流程和算法开发的复杂性，同时在最坏情况下保持了可接受的误差率。</div>
</details>
</div>
<div class="card">
<div class="title">uGMM-NN: Univariate Gaussian Mixture Model Neural Network</div>
<div class="meta-line">Authors: Zakeria Sharif Ali</div>
<div class="meta-line">First: 2025-09-09T10:13:37+00:00 · Latest: 2026-01-02T17:57:38+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07569v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07569v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed non-linearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feed-forward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>uGMM-NN：单变量高斯混合模型神经网络</div>
<div class="mono" style="margin-top:8px">本文介绍了单变量高斯混合模型神经网络（uGMM-NN），这是一种新颖的神经架构，将概率推理直接嵌入深度网络的计算单元中。与传统神经元不同，后者应用加权和并随后施加固定非线性，每个uGMM-NN节点将其激活参数化为单变量高斯混合，具有可学习的均值、方差和混合系数。这种设计通过在单个神经元层面捕捉多模态性和不确定性，能够实现更丰富的表示，同时保留标准前馈网络的可扩展性。我们证明uGMM-NN可以实现与传统多层感知器相媲美的判别性能，同时还提供了激活的概率解释。所提出的框架为将不确定性感知组件集成到现代神经架构中提供了基础，为判别和生成建模开辟了新的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the representation capabilities of neural networks by incorporating probabilistic reasoning at the neuron level. The authors propose a novel architecture called the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), where each node parameterizes its activations as a univariate Gaussian mixture with learnable parameters. Experimental results show that uGMM-NN achieves competitive performance in discriminative tasks compared to traditional multilayer perceptrons while providing a probabilistic interpretation of neuron activations, thus paving the way for integrating uncertainty-aware components into neural architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过在单个神经元层面引入概率推理来增强神经网络的表示能力。作者提出了单变量高斯混合模型神经网络（uGMM-NN），其中每个节点将其激活参数化为具有可学习参数的单变量高斯混合。实验结果表明，uGMM-NN在判别任务中与传统的多层感知器相比表现出竞争力，同时提供了神经元激活的概率解释，从而为神经架构中的不确定性感知组件铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Clustering by Denoising: Latent plug-and-play diffusion for single-cell data</div>
<div class="meta-line">Authors: Dominik Meier, Shixing Yu, Sagnik Nandy, Promit Ghosal, Kyra Gan</div>
<div class="meta-line">First: 2025-10-26T21:03:56+00:00 · Latest: 2026-01-02T17:32:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22835v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22835v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-cell RNA sequencing (scRNA-seq) enables the study of cellular heterogeneity. Yet, clustering accuracy, and with it downstream analyses based on cell labels, remain challenging due to measurement noise and biological variability. In standard latent spaces (e.g., obtained through PCA), data from different cell types can be projected close together, making accurate clustering difficult. We introduce a latent plug-and-play diffusion framework that separates the observation and denoising space. This separation is operationalized through a novel Gibbs sampling procedure: the learned diffusion prior is applied in a low-dimensional latent space to perform denoising, while to steer this process, noise is reintroduced into the original high-dimensional observation space. This unique &quot;input-space steering&quot; ensures the denoising trajectory remains faithful to the original data structure. Our approach offers three key advantages: (1) adaptive noise handling via a tunable balance between prior and observed data; (2) uncertainty quantification through principled uncertainty estimates for downstream analysis; and (3) generalizable denoising by leveraging clean reference data to denoise noisier datasets, and via averaging, improve quality beyond the training set. We evaluate robustness on both synthetic and real single-cell genomics data. Our method improves clustering accuracy on synthetic data across varied noise levels and dataset shifts. On real-world single-cell data, our method demonstrates improved biological coherence in the resulting cell clusters, with cluster boundaries that better align with known cell type markers and developmental trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去噪聚类：用于单细胞数据的潜在即插即用扩散</div>
<div class="mono" style="margin-top:8px">单细胞RNA测序（scRNA-seq）使得细胞异质性的研究成为可能。然而，由于测量噪声和生物变异性，聚类准确性及其基于细胞标签的下游分析仍然具有挑战性。在标准潜在空间（例如，通过PCA获得）中，不同细胞类型的数据可能被投影得很接近，从而使准确聚类变得困难。我们引入了一种潜在即插即用扩散框架，分离观察和去噪空间。这种分离通过一种新颖的Gibbs采样程序实现：学习到的扩散先验在低维潜在空间中进行去噪，而为了引导这一过程，噪声被重新引入到原始高维观察空间。这种独特的“输入空间引导”确保去噪轨迹忠实于原始数据结构。我们的方法提供了三个关键优势：（1）通过可调的先验和观察数据之间的平衡进行自适应噪声处理；（2）通过原则性的不确定性估计进行不确定性量化，以便于下游分析；（3）通过利用干净的参考数据去噪更嘈杂的数据集，并通过平均提高超出训练集的质量，实现可推广的去噪。我们在合成和真实单细胞基因组数据上评估了鲁棒性。我们的方法在不同噪声水平和数据集变化的合成数据上提高了聚类准确性。在真实单细胞数据上，我们的方法在生成的细胞簇中展示了更好的生物一致性，聚类边界更好地与已知细胞类型标记和发育轨迹对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance clustering accuracy in single-cell RNA sequencing (scRNA-seq) data, which is often compromised by measurement noise and biological variability. The authors propose a latent plug-and-play diffusion framework that employs a novel Gibbs sampling procedure to separate the observation and denoising spaces, allowing for effective denoising while maintaining the integrity of the original data structure. Experimental results show that this method significantly improves clustering accuracy on synthetic datasets across various noise levels and dataset shifts, and on real-world single-cell data, it yields cell clusters that exhibit better biological coherence and alignment with known cell type markers and developmental trajectories.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高单细胞RNA测序（scRNA-seq）数据的聚类准确性，而这一准确性常常受到测量噪声和生物变异性的影响。作者提出了一种潜在的即插即用扩散框架，采用新颖的吉布斯采样程序来分离观察空间和去噪空间，从而有效去噪，同时保持原始数据结构的完整性。实验结果表明，该方法在各种噪声水平和数据集变化下提高了合成数据集的聚类准确性，并且在真实的单细胞数据上，生成的细胞簇在生物学上更具一致性，更好地与已知的细胞类型标记和发育轨迹对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</div>
<div class="meta-line">Authors: Akash Samanta, Sheldon Williamson</div>
<div class="meta-line">First: 2025-12-30T19:57:52+00:00 · Latest: 2026-01-02T17:32:09+00:00</div>
<div class="meta-line">Comments: This preprint focuses on the theoretical framework and diagnostic behavior. Comprehensive experimental validation in application-specific settings is deferred to a companion experimental study</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24445v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference (TD) error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Within this framework, we introduce three diagnostic-driven instantiations: the Human-inspired Supervised Adaptive Optimizer (HSAO), Hybrid Error-Diagnostic Reinforcement Learning (HED-RL) for actor-critic methods, and the Meta-Learned Learning Policy (MLLP). Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to TD error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏差-噪声-对齐诊断的自适应学习</div>
<div class="mono" style="margin-top:8px">在非平稳和安全关键环境中部署的学习系统，通常在学习动态随时间演变时遭受不稳定、收敛缓慢或脆弱适应的问题。虽然现代优化、强化学习和元学习方法适应于梯度统计，但它们在很大程度上忽视了误差信号本身的时间结构。本文提出了一种基于诊断驱动的自适应学习框架，明确通过偏差、噪声和对齐的原则性分解来建模误差演变。偏差捕捉持续漂移；噪声捕捉随机变异；对齐捕捉导致超调的重复方向激励。这些诊断是从轻量级的损失或时间差（TD）误差轨迹的统计数据中在线计算的，并且与模型架构或任务领域无关。我们展示了所提出的偏差-噪声-对齐分解为监督优化、演员-评论家强化学习和学习优化器提供了统一的控制基础。在此框架内，我们引入了三种基于诊断的实例：人类启发的监督自适应优化器（HSAO）、用于演员-评论家方法的混合误差诊断强化学习（HED-RL）和元学习学习策略（MLLP）。在标准平滑性假设下，我们为所有情况建立了有界有效更新和稳定性特性。演员-评论家学习中的代表性诊断插图突显了所提出的信号如何调节对TD误差结构的适应。总体而言，这项工作将误差演变提升为自适应学习中的一类重要对象，并为动态环境中的可靠学习提供了可解释的、轻量级的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by learning systems in nonstationary and safety-critical environments, which often experience instability and slow convergence. The authors propose a diagnostic-driven adaptive learning framework that models error evolution through a decomposition into bias, noise, and alignment, computed online from lightweight statistics of loss or temporal-difference error trajectories. The key findings demonstrate that this bias-noise-alignment decomposition serves as a unifying control backbone for various learning methods, with representative illustrations showing how these diagnostics effectively modulate adaptation in response to the structure of temporal-difference errors, thereby enhancing reliability in dynamic environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决学习系统在非平稳和安全关键环境中面临的不稳定性和缓慢收敛等挑战。作者提出了一种基于诊断的自适应学习框架，通过对偏差、噪声和对齐的分解来建模误差演变，这些分解是通过对损失或时间差错误轨迹的轻量统计在线计算得出的。主要发现表明，这种偏差-噪声-对齐分解为各种学习方法提供了统一的控制基础，而提出的实例，包括人类启发的监督自适应优化器和混合误差诊断强化学习，显示出在响应时间差错误结构方面的改进适应性，从而增强了动态环境中的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Memory Bank Compression for Continual Adaptation of Large Language Models</div>
<div class="meta-line">Authors: Thomas Katraouras, Dimitrios Rafailidis</div>
<div class="meta-line">First: 2026-01-02T17:22:34+00:00 · Latest: 2026-01-02T17:22:34+00:00</div>
<div class="meta-line">Comments: Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00756v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00756v1">PDF</a> · <a href="https://github.com/Thomkat/MBC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型的持续适应记忆库压缩</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已成为许多日常应用的主流。然而，随着数据的演变，它们的知识很快变得过时。持续学习旨在在不抹去先前获得知识的情况下，用新信息更新LLMs。尽管全量微调等方法可以纳入新数据，但它们计算成本高且容易导致灾难性遗忘，即先前知识被覆盖。增强记忆的方法通过为LLMs配备记忆库来解决这个问题，记忆库是一个存储未来使用信息的外部内存模块。然而，这些方法面临一个关键限制，特别是在大规模数据流到来时，记忆库在现实世界场景中不断增长。在本文中，我们提出了MBC，一个通过代码本优化策略在在线适应学习过程中压缩记忆库的模型。为了确保稳定学习，我们还引入了一种在线重置机制，以防止代码本崩溃。此外，我们在LLM的注意力层中采用了键值低秩适应，能够高效利用压缩的记忆表示。与最具竞争力的基线相比，基准问答数据集的实验表明，MBC将记忆库大小减少到0.3%，同时在在线适应学习中保持高保留准确性。我们的代码可在https://github.com/Thomkat/MBC上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of keeping Large Language Models (LLMs) updated with new information without losing previously acquired knowledge, as traditional methods like full fine-tuning are computationally expensive and lead to catastrophic forgetting. The authors propose a method called Memory Bank Compression (MBC), which employs a codebook optimization strategy to compress the memory bank during online adaptation learning, along with an online resetting mechanism to prevent codebook collapse. Experimental results on benchmark question-answering datasets show that MBC can reduce the memory bank size to 0.3% compared to the most competitive baseline while maintaining high retention accuracy during the adaptation process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决如何在不丢失先前知识的情况下，保持大型语言模型（LLMs）与新信息同步更新的问题，而这一问题在传统微调方法中由于计算开销大和灾难性遗忘的风险而更加严重。作者提出了一种名为记忆库压缩（MBC）的新方法，该方法利用代码本优化策略在在线适应学习过程中压缩记忆库，并引入在线重置机制以保持稳定学习并防止代码本崩溃。实验结果表明，与领先基线相比，MBC能够将记忆库的大小减少到仅为原始大小的0.3%，同时在适应过程中保持高保留准确性。</div>
</details>
</div>
<div class="card">
<div class="title">A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football</div>
<div class="meta-line">Authors: Sean Groom, Shuo Wang, Francisco Belo, Axl Rice, Liam Anderson</div>
<div class="meta-line">First: 2026-01-02T17:10:36+00:00 · Latest: 2026-01-02T17:10:36+00:00</div>
<div class="meta-line">Comments: 40 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00748v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00748v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating &quot;average&quot; behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>足球中无球防守角色与表现评估的机器学习框架</div>
<div class="mono" style="margin-top:8px">评估足球中的无球防守表现具有挑战性，因为传统指标无法捕捉限制对手行动选择和成功概率的细微协调动作。尽管广泛使用的控球价值模型在评估持球动作方面表现出色，但其在防守中的应用仍然有限。现有的反事实方法，如幽灵模型，帮助扩展这些分析，但通常依赖于缺乏战术背景的“平均”行为模拟。为了解决这个问题，我们引入了一种针对角球的协变量依赖隐马尔可夫模型（CDHMM），这是足球比赛中高度结构化的一个方面。我们的无标签模型直接从球员跟踪数据中推断时间分辨的盯人和区域分配。我们利用这些分配提出了一种新的防守信用归属框架和一种基于角色的幽灵方法，用于无球防守表现的反事实分析。我们展示了这些贡献如何提供对防守贡献的可解释评估，针对上下文感知的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of off-ball defensive performance in football, as traditional metrics fail to capture the complex movements that influence opponent actions. The authors developed a covariate-dependent Hidden Markov Model (CDHMM) specifically for analyzing corner kicks, allowing for the inference of man-marking and zonal assignments from player tracking data without the need for labels. The experimental results demonstrate that this new framework enables a more interpretable assessment of defensive contributions, providing insights that are more context-aware compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善足球中无球防守表现的评估，因为传统指标无法捕捉影响对手动作的复杂运动。作者开发了一种专门针对角球的协变量依赖隐马尔可夫模型（CDHMM），该模型能够从球员跟踪数据中推断出无标签的盯人和区域分配。主要发现表明，该模型能够更清晰地评估防守贡献，提供防守信用归属的框架，并增强与现有方法相比的反事实分析。</div>
</details>
</div>
<div class="card">
<div class="title">The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving</div>
<div class="meta-line">Authors: Max Ruiz Luyten, Mihaela van der Schaar</div>
<div class="meta-line">First: 2026-01-02T17:10:31+00:00 · Latest: 2026-01-02T17:10:31+00:00</div>
<div class="meta-line">Comments: 56 pages, 9 figures, submitted to Twenty-Ninth Annual Conference on Artificial Intelligence and Statistics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00747v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00747v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model&#x27;s distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理与创造力的权衡：朝向创造力驱动的问题解决</div>
<div class="mono" style="margin-top:8px">最先进的大型语言模型（LLM）管道依赖于自举推理循环：采样多样的思维链并强化得分最高的链，主要优化正确性。我们分析了这一设计选择如何对模型在推理路径上的分布崩溃敏感，削减语义熵并削弱创造性问题解决。为分析这一失败，我们引入了分布式创造性推理（DCR），这是一个统一的变分目标，将训练视为通过解决方案轨迹上的概率测度进行的梯度流。STaR、GRPO和DPO，以及熵奖励和其他方法，都是同一损失的特例。该框架提供了三个核心结果：（i）多样性衰退定理，描述了基于正确性的目标如何导致STaR、GRPO和DPO的多样性衰退模式不同；（ii）确保收敛到稳定和多样化策略的设计，有效防止崩溃；（iii）在实践中实现这一目标的简单可行的方案。因此，DCR提供了第一个原则性方案，使LLM既保持正确性又具创造力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the limitations of current large language model (LLM) pipelines that prioritize correctness over creativity, leading to a collapse in the diversity of reasoning paths. The authors propose a new framework called Distributional Creative Reasoning (DCR), which formulates training as gradient flow through probability measures on solution traces. Key findings include the diversity decay theorem, which illustrates how correctness-driven objectives reduce diversity, the development of designs that promote stable and diverse policy convergence, and practical strategies to implement these designs, ultimately enabling LLMs to maintain both correctness and creativity.</div>
<div class="mono" style="margin-top:8px">本研究探讨了当前大型语言模型（LLM）管道在优先考虑正确性而非创造力时的局限性，这导致推理路径的多样性崩溃。作者提出了一种新框架，称为分布式创造性推理（DCR），将训练重新表述为通过解决方案轨迹上的概率测度进行梯度流动。主要发现包括多样性衰退定理，说明以正确性驱动的目标如何对多样性产生负面影响，确保稳定和多样化政策收敛的策略，以及在LLM中保持正确性和创造力的实用指南。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs</div>
<div class="meta-line">Authors: Jing Yang Lee, Kong-Aik Lee, Woon-Seng Gan</div>
<div class="meta-line">First: 2025-06-18T04:19:33+00:00 · Latest: 2026-01-02T17:03:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15131v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.15131v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用大型语言模型建模开放域对话中的一对多属性</div>
<div class="mono" style="margin-top:8px">开放域对话（OD）表现出一对多（o2m）属性，即对于单一对话上下文存在多个合适的响应。尽管先前的研究表明建模这一属性可以提高响应多样性，但大多数现代基于大型语言模型（LLM）的对话代理并未明确做到这一点。在本研究中，我们通过将OD生成分解为两个关键任务：多响应生成（MRG）和基于偏好的选择（PS），来建模LLM中的o2m属性。这两个任务分别涉及为给定对话上下文生成一组n个语义和词汇多样的高质量响应，然后根据人类偏好选择单一响应。为了促进MRG和PS，我们引入了o2mDial，这是一个专门设计的对话语料库，旨在通过为每个上下文提供多个合理的响应来捕捉o2m属性。利用o2mDial，我们提出了新的上下文学习和指令调优策略，以及针对MRG的新评估指标，并提出了一种基于模型的PS方法。实证结果表明，将所提出的两阶段框架应用于较小的LLM进行OD生成，可以提高整体响应多样性，同时保持上下文一致性，响应质量提高了多达90%，使其更接近于较大模型的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the one-to-many (o2m) property in open-domain dialogue, which allows for multiple suitable responses to a single context, a feature often overlooked by current LLM-based dialogue agents. The authors propose a two-stage framework that includes Multi-Response Generation (MRG) and Preference-based Selection (PS), utilizing a newly introduced dialogue corpus called o2mDial, designed to provide multiple plausible responses for each context. Experimental results indicate that this framework significantly enhances response diversity and quality in smaller LLMs, achieving up to a 90% improvement in response quality while maintaining contextual coherence, thus approaching the performance of larger models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决开放域对话中的一对多（o2m）特性，该特性允许对单一上下文产生多个合适的响应，而这一特性在当前的基于LLM的对话系统中常常被忽视。作者提出了一种两阶段的方法，包括多响应生成（MRG）以创建多样化的响应和基于偏好的选择（PS）以根据人类偏好选择最佳响应，利用新引入的对话语料库o2mDial。实验结果表明，该框架显著提高了响应的多样性和质量，在保持上下文一致性的同时，响应质量提高了多达90%，即使在较小的LLM上应用也能取得良好效果。</div>
</details>
</div>
<div class="card">
<div class="title">An Agentic Framework for Neuro-Symbolic Programming</div>
<div class="meta-line">Authors: Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi</div>
<div class="meta-line">First: 2026-01-02T16:59:39+00:00 · Latest: 2026-01-02T16:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00743v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library&#x27;s specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种代理框架用于神经符号编程</div>
<div class="mono" style="margin-top:8px">将符号约束整合到深度学习模型中可以使其更具鲁棒性、可解释性和数据效率。然而，这仍然是一项耗时且具有挑战性的任务。现有框架如DomiKnowS通过提供高级声明性编程接口来帮助这一整合，但仍假设用户熟悉库的特定语法。我们提出了AgenticDomiKnowS (ADS)以消除这一依赖。ADS使用代理工作流将自由形式的任务描述转换为完整的DomiKnowS程序，逐个创建和测试每个DomiKnowS组件。该工作流支持可选的人机协作干预，使熟悉DomiKnowS的用户能够优化中间输出。我们展示了ADS如何使经验丰富的DomiKnowS用户和非用户快速构建神经符号程序，将开发时间从数小时缩短到10-15分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the integration of symbolic constraints into deep learning models to improve their robustness, interpretability, and data efficiency, while addressing the challenges associated with existing frameworks that require users to be proficient in specific syntaxes. The authors propose a new framework called AgenticDomiKnowS (ADS), which translates free-form task descriptions into complete DomiKnowS programs through an agentic workflow that allows for the separate creation and testing of components, with optional human intervention for refinement. Experimental results demonstrate that ADS significantly reduces the time required to construct neuro-symbolic programs from hours to just 10-15 minutes for both experienced and novice users of DomiKnowS.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强符号约束与深度学习模型的整合，以提高模型的鲁棒性、可解释性和数据效率，同时解决现有框架对特定语法知识的依赖问题。作者提出了一种新框架AgenticDomiKnowS（ADS），该框架通过一种代理工作流程将自由形式的任务描述转换为完整的DomiKnowS程序，允许对组件进行单独创建和测试，并支持人类干预以进行细化。实验结果表明，ADS显著缩短了构建神经符号程序所需的时间，使经验丰富的用户和新手用户都能在10-15分钟内完成这一过程，而不是几个小时。</div>
</details>
</div>
<div class="card">
<div class="title">QUITE: A Query Rewrite System Beyond Rules with LLM Agents</div>
<div class="meta-line">Authors: Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang</div>
<div class="meta-line">First: 2025-06-09T11:51:27+00:00 · Latest: 2026-01-02T16:51:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07675v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.07675v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QUITE：超越规则的查询重写系统与LLM代理</div>
<div class="mono" style="margin-top:8px">查询重写将SQL查询转换为语义等价的形式，以更高效地运行。现有方法主要依赖预定义的重写规则，但它们只能处理有限的查询子集，并可能导致性能回退。这一限制源于基于规则的查询重写的三个挑战：（1）发现和验证新规则困难，（2）固定重写规则无法推广到新的查询模式，以及（3）某些重写技术无法用固定规则表达。由于人类专家表现出显著更好的重写能力但面临可扩展性问题，而大型语言模型（LLMs）已展示出接近人类水平的语义和推理能力，我们提出了一种新的方法，利用LLMs超越规则重写SQL查询。由于LLMs中的幻觉问题，直接应用LLMs往往导致不等价和次优的查询。为了解决这个问题，我们提出了QUITE（查询重写），这是一个基于LLM代理的无训练和反馈感知系统，将SQL查询重写为语义等价的形式，性能显著优于基于规则的方法，覆盖更广泛的查询模式和重写策略。首先，我们设计了一个由有限状态机（FSM）控制的多代理框架，使LLMs能够使用外部工具，并通过实时数据库反馈增强重写过程。其次，我们开发了一个重写中间件，以增强LLMs生成优化查询等价物的能力。最后，我们采用了一种新颖的提示注入技术，以改善重写查询的执行计划。大量实验表明，QUITE将查询执行时间减少了高达35.8%，并比现有最先进的方法产生了24.1%更多的重写，覆盖了早期系统未处理的查询案例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing rule-based SQL query rewrite systems, which struggle with discovering new rules and generalizing to diverse query patterns, often leading to performance regressions. The authors propose QUITE, a novel system that leverages Large Language Models (LLMs) to rewrite SQL queries without relying on fixed rules. Through a multi-agent framework and a rewrite middleware, QUITE effectively utilizes real-time database feedback and a hint injection technique, resulting in a significant reduction in query execution time by up to 35.8% and generating 24.1% more rewrites than previous methods, thus addressing a wider range of query cases.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高SQL查询重写的效率，传统方法主要依赖于预定义的规则，这些规则在范围上有限且可能导致性能问题。作者提出了QUITE，这是一种利用大型语言模型（LLMs）进行SQL查询重写的新系统，解决了发现新重写策略和推广到多样化查询模式的挑战。实验结果表明，QUITE相比现有方法在查询执行时间上减少了多达35.8%，并生成了24.1%更多的重写，有效处理了更广泛的查询案例。</div>
</details>
</div>
<div class="card">
<div class="title">Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation</div>
<div class="meta-line">Authors: Lameya Sabrin, Md. Sanaullah Chowdhury, Salauddin Tapu, Noyon Kumar Sarkar, Ferdous Bin Ali</div>
<div class="meta-line">First: 2025-04-20T19:04:43+00:00 · Latest: 2026-01-02T16:47:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.14715v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.14715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and efficient medical image segmentation is crucial for advancing clinical diagnostics and surgical planning, yet remains a complex challenge due to the variability in anatomical structures and the demand for low-complexity models. In this paper, we introduced Med-2D SegNet, a novel and highly efficient segmentation architecture that delivers outstanding accuracy while maintaining a minimal computational footprint. Med-2D SegNet achieves state-of-the-art performance across multiple benchmark datasets, including KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient (DSC) of 89.77% across 20 diverse datasets. Central to its success is the compact Med Block, a specialized encoder design that incorporates dimension expansion and parameter reduction, enabling precise feature extraction while keeping model parameters to a low count of just 2.07 million. Med-2D SegNet excels in cross-dataset generalization, particularly in polyp segmentation, where it was trained on KVASIR-SEG and showed strong performance on unseen datasets, demonstrating its robustness in zero-shot learning scenarios, even though we acknowledge that further improvements are possible. With top-tier performance in both binary and multi-class segmentation, Med-2D SegNet redefines the balance between accuracy and efficiency, setting a new benchmark for medical image analysis. This work paves the way for developing accessible, high-performance diagnostic tools suitable for clinical environments and resource-constrained settings, making it a step forward in the democratization of advanced medical technology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Med-2D SegNet：一种轻量级深度神经网络用于医学2D图像分割</div>
<div class="mono" style="margin-top:8px">准确高效的医学图像分割对推动临床诊断和手术规划至关重要，但由于解剖结构的变异性和对低复杂度模型的需求，这仍然是一个复杂的挑战。本文介绍了Med-2D SegNet，一种新颖且高效的分割架构，能够在保持最小计算负担的同时提供卓越的准确性。Med-2D SegNet在多个基准数据集上实现了最先进的性能，包括KVASIR-SEG、PH2、EndoVis和GLAS，在20个不同数据集上的平均Dice相似系数（DSC）为89.77%。其成功的核心是紧凑的Med Block，这是一种专门的编码器设计，结合了维度扩展和参数减少，能够精确提取特征，同时将模型参数控制在仅207万的低数量。Med-2D SegNet在跨数据集泛化方面表现出色，特别是在息肉分割中，在KVASIR-SEG上训练后，在未见数据集上表现强劲，展示了其在零样本学习场景中的鲁棒性，尽管我们承认仍有进一步改进的可能。Med-2D SegNet在二分类和多分类分割中均表现出顶级性能，重新定义了准确性与效率之间的平衡，为医学图像分析设定了新的基准。这项工作为开发适合临床环境和资源受限设置的可访问高性能诊断工具铺平了道路，是推动先进医疗技术民主化的一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the accuracy and efficiency of medical image segmentation, which is essential for clinical diagnostics and surgical planning but is complicated by anatomical variability and the need for low-complexity models. The authors introduced Med-2D SegNet, a lightweight deep neural network architecture designed for medical 2D image segmentation, which utilizes a compact Med Block for effective feature extraction while maintaining a low parameter count of 2.07 million. Experimental results demonstrate that Med-2D SegNet achieves a state-of-the-art average Dice similarity coefficient of 89.77% across 20 diverse datasets, excelling particularly in polyp segmentation and showcasing strong cross-dataset generalization and robustness in zero-shot learning scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高医学图像分割的准确性和效率，这对临床诊断和手术规划至关重要，但由于解剖结构的变异性和对低复杂度模型的需求而变得复杂。作者提出了Med-2D SegNet，这是一种轻量级深度神经网络架构，采用紧凑的Med Block进行有效的特征提取，同时保持仅2.07百万的低参数量。实验结果表明，Med-2D SegNet在20个不同数据集上实现了89.77%的平均Dice相似系数，尤其在息肉分割方面表现出色，并展示了强大的跨数据集泛化能力，从而为医学图像分析建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty</div>
<div class="meta-line">Authors: Uğurcan Özalp</div>
<div class="meta-line">First: 2026-01-02T16:33:17+00:00 · Latest: 2026-01-02T16:33:17+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic&#x27;s epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机演员-评论家：通过时间随机不确定性减轻高估</div>
<div class="mono" style="margin-top:8px">强化学习中的离策略演员-评论家方法通过时间差分更新训练评论家，并将其作为策略（演员）的学习信号。这种设计通常比纯粹的在策略方法实现更高的样本效率。然而，评论网络往往系统性地高估价值估计。通常通过引入基于不确定性估计的悲观偏差来解决这个问题。目前的方法采用集成来量化评论家的认知不确定性——由于数据有限和模型模糊性导致的不确定性——以缩放悲观更新。在本研究中，我们提出了一种新算法，称为随机演员-评论家（STAC），它结合了时间（一步）随机不确定性——源于随机转移、奖励和策略引起的贝尔曼目标变异性——以缩放时间差分更新中的悲观偏差，而不是依赖于认知不确定性。STAC使用单一的分布式评论网络来建模时间回报不确定性，并对评论和演员网络应用dropout以进行正则化。我们的结果表明，仅基于分布式评论的悲观主义足以减轻高估，并自然导致在随机环境中的风险厌恶行为。引入dropout通过正则化进一步提高了训练的稳定性和性能。通过这种设计，STAC使用单一的分布式评论网络实现了更高的计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the systematic overestimation of value estimates in off-policy actor-critic methods in reinforcement learning, which can hinder sample efficiency. The authors propose a novel algorithm called Stochastic Actor-Critic (STAC) that utilizes temporal aleatoric uncertainty to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty from ensembling. Experimental results demonstrate that STAC effectively mitigates overestimation through a single distributional critic network, leading to risk-averse behavior in stochastic environments, while dropout regularization enhances training stability and overall performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决强化学习中离线策略演员-评论家方法中价值估计的系统性高估，这可能会影响样本效率。作者提出了一种新算法，称为随机演员-评论家（STAC），该算法利用时间上的随机不确定性来调整时间差分更新中的悲观偏差，而不是依赖于来自集成的认知不确定性。实验结果表明，STAC通过单一的分布式评论家网络有效地减轻了高估现象，导致在随机环境中表现出风险规避行为，同时，dropout正则化增强了训练的稳定性和整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the Performance of Large Language Models on Subjective Span Identification Tasks</div>
<div class="meta-line">Authors: Alphaeus Dmonte, Roland Oruche, Tharindu Ranasinghe, Marcos Zampieri, Prasad Calyam</div>
<div class="meta-line">First: 2026-01-02T16:30:14+00:00 · Latest: 2026-01-02T16:30:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00736v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00736v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索大型语言模型在主观文本跨度识别任务中的表现</div>
<div class="mono" style="margin-top:8px">识别相关文本跨度对自然语言处理中的多个下游任务至关重要，因为它有助于模型的可解释性。虽然大多数跨度识别方法依赖于相对较小的预训练语言模型，如BERT，但一些最新的方法已利用最新一代大型语言模型（LLMs）来完成该任务。目前的工作主要集中在显式跨度识别，如命名实体识别（NER），而在基于方面的情感分析（ABSA）等任务中使用LLMs进行更主观的跨度识别尚未得到充分探索。本文通过评估各种LLMs在情感分析、攻击性语言识别和声明验证等三个热门任务中的文本跨度识别表现，填补了这一重要空白。我们探索了多种LLM策略，如指令调优、上下文学习和思维链。我们的结果表明，文本中的潜在关系有助于LLMs识别精确的文本跨度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance model explainability in natural language processing (NLP) by improving the identification of relevant text spans, particularly in subjective tasks that have been less explored. The authors evaluate the performance of various large language models (LLMs) on text span identification across three tasks: sentiment analysis, offensive language identification, and claim verification, employing methods such as instruction tuning, in-context learning, and chain of thought. The key findings reveal that LLMs benefit from underlying relationships within the text, which significantly aids in the accurate identification of text spans.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改善主观跨度识别来增强自然语言处理（NLP）中的模型可解释性，而这一领域相比于显式跨度识别任务探索较少。作者评估了多种大型语言模型（LLMs）在主观跨度识别中的表现，涵盖了情感分析、攻击性语言识别和主张验证三个任务，采用了指令调优、上下文学习和思维链等方法。主要发现表明，LLMs能够利用文本中的潜在关系来准确识别相关跨度，从而展示了它们在主观跨度识别任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining</div>
<div class="meta-line">Authors: Jewel Rana Palit, Vijayalakshmi K Kumarasamy, Osama A. Osman</div>
<div class="meta-line">First: 2025-12-27T13:30:07+00:00 · Latest: 2026-01-02T16:28:22+00:00</div>
<div class="meta-line">Comments: 7 tables, 7 figures, 23 pages including references</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22589v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22589v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology&#x27;s safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于数据驱动的SAE 2级和4级自动驾驶车辆碰撞模式分析：K均值聚类与关联规则挖掘</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆（AV）有潜力减少或消除人为驾驶错误，提高交通安全，并支持可持续出行。最近，碰撞数据越来越多地显示，AV行为可能偏离预期的安全结果，引发了对该技术在混合交通环境中安全性和操作可靠性的担忧。尽管过去的研究调查了AV碰撞，但大多数研究依赖于以加利福尼亚为中心的小规模数据集，且对不同SAE自动化级别的碰撞趋势理解有限。本研究分析了来自美国国家公路交通安全管理局（NHTSA）的2500多条AV碰撞记录，涵盖SAE 2级和4级，以揭示潜在的碰撞动态。研究开发了一个两阶段数据挖掘框架。首先应用K均值聚类将碰撞记录根据时间、空间和环境因素划分为4个不同的行为集群。然后，使用关联规则挖掘（ARM）提取每个集群中碰撞模式与碰撞因素（包括光照条件、路面状况、车辆动态和环境条件）之间的可解释的多变量关系。这些见解为AV开发者、安全监管者和政策制定者在制定AV部署策略和最小化碰撞风险方面提供了可行的指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the safety concerns associated with automated vehicles (AVs), particularly their unexpected crash behaviors in mixed traffic environments. The researchers employed a two-stage data mining framework, utilizing K-means clustering to categorize over 2,500 AV crash records from the NHTSA into four distinct behavioral clusters based on various factors, followed by Association Rule Mining to identify relationships between crash patterns and contributing factors. The findings reveal critical insights into the dynamics of AV crashes, highlighting the influence of conditions such as lighting, surface quality, and vehicle dynamics, which can inform AV developers and policymakers in enhancing safety measures and deployment strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决与自动驾驶汽车（AV）相关的安全问题，特别是因为事故数据表明其安全结果与预期存在偏差。研究人员采用了两阶段数据挖掘框架，利用K均值聚类将来自NHTSA的2500多条AV事故记录根据各种因素分类为四个不同的行为聚类，随后使用关联规则挖掘识别事故模式与影响因素之间的关系。研究结果揭示了AV事故动态的重要见解，这些见解可以为开发者、监管者和政策制定者提供指导，以提高AV安全性并减少事故风险。</div>
</details>
</div>
<div class="card">
<div class="title">The Curse of Depth in Large Language Models</div>
<div class="meta-line">Authors: Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-09T07:03:36+00:00 · Latest: 2026-01-02T16:15:39+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.05795v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.05795v3">PDF</a> · <a href="https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling">Code1</a> · <a href="https://github.com/lmsdss/LayerNorm-Scaling">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling (LNS), which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Across a wide range of model sizes (130M to 7B), our experiments show that LNS consistently outperforms previous normalization and scaling techniques in enhancing LLM pre-training performance. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at \href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型中的深度诅咒</div>
<div class="mono" style="margin-top:8px">在本文中，我们引入了深度诅咒这一概念，强调、解释并解决了现代大型语言模型（LLMs）中最近观察到的现象，即近一半的层效果低于预期。我们首先确认这一现象在Llama、Mistral、DeepSeek和Qwen等最流行的LLM家族中广泛存在。我们的理论和实证分析表明，LLM中深层无效的根本原因是广泛使用的预层归一化（Pre-LN）。虽然Pre-LN稳定了Transformer LLM的训练，但其输出方差随着模型深度呈指数增长，这不利地导致深层Transformer块的导数成为单位矩阵，因此几乎不对训练产生贡献。为了解决这一训练陷阱，我们提出了层归一化缩放（LNS），它通过深度的平方根反向缩放层归一化的输出方差。这一简单的修改减轻了深层Transformer层的输出方差爆炸，提高了它们的贡献。在广泛的模型规模（130M到7B）中，我们的实验表明，LNS在提升LLM预训练性能方面始终优于之前的归一化和缩放技术。此外，这一改进无缝地延续到监督微调中。所有这些收益都归因于层归一化缩放使得深层在训练期间能够更有效地贡献。我们的代码可在\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that nearly half of the layers in modern Large Language Models (LLMs) are less effective than anticipated, a phenomenon termed the Curse of Depth. The authors confirm this issue across popular LLM families and identify that the widespread use of Pre-Layer Normalization (Pre-LN) is responsible for the ineffectiveness of deep layers, as it leads to an exponential increase in output variance with model depth. To address this, they propose LayerNorm Scaling (LNS), which adjusts the variance of layer normalization output based on depth, resulting in improved training contributions from deeper layers. Experimental results demonstrate that LNS consistently enhances pre-training performance across various model sizes, with benefits also observed during supervised fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）中的深度诅咒现象，即许多层的效果低于预期。作者确认了这一问题在多种流行的LLM家族中普遍存在，并指出使用预层归一化（Pre-LN）是关键因素，因为它导致输出方差随着模型深度增加，从而使得深层的训练贡献无效。为了解决这个问题，他们提出了层归一化缩放（LNS），该方法根据深度调整层归一化的方差。实验结果表明，LNS显著提高了不同模型规模的预训练性能，并且通过使深层在训练中更有效地贡献，也有利于监督微调。</div>
</details>
</div>
<div class="card">
<div class="title">Designing an Optimal Sensor Network via Minimizing Information Loss</div>
<div class="meta-line">Authors: Daniel Waxman, Fernando Llorente, Katia Lamer, Petar M. Djurić</div>
<div class="meta-line">First: 2025-12-05T18:38:30+00:00 · Latest: 2026-01-02T16:14:47+00:00</div>
<div class="meta-line">Comments: 37 pages, 15 figures. Camera-ready version; accepted to Bayesian Analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05940v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05940v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that &quot;minimize information loss&quot; from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过最小化信息损失设计最佳传感器网络</div>
<div class="mono" style="margin-top:8px">最优实验设计是统计学中的经典主题，涉及许多研究良好的问题、应用和解决方案。我们研究的设计问题是传感器的布置，以监测时空过程，在建模和优化中明确考虑时间维度。我们观察到，计算科学的最新进展通常基于物理模拟生成大数据集，而这些数据在实验设计中很少被利用。我们引入了一种新颖的基于模型的传感器布置标准，以及一种高效的优化算法，该算法结合了基于物理的模拟和贝叶斯实验设计原则，以识别“最小化模拟数据中的信息损失”的传感器网络。我们的方法依赖于稀疏变分推断和（可分离的）高斯-马尔可夫先验，因此可以适应许多贝叶斯实验设计中的技术。我们通过一个案例研究验证了我们的方法，该研究监测了亚利桑那州凤凰城的空气温度，使用了最先进的基于物理的模拟。我们的结果表明，我们的框架优于随机或准随机采样，特别是在传感器数量有限的情况下。最后，我们讨论了我们框架的实际考虑和影响，包括更复杂的建模工具和现实世界的部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance optimal experimental design by effectively placing sensors to monitor spatiotemporal processes while considering the temporal dimension. The authors introduce a novel model-based sensor placement criterion and a highly-efficient optimization algorithm that combines physics-based simulations with Bayesian experimental design principles to minimize information loss. Experimental results from a case study on air temperature monitoring in Phoenix, Arizona, demonstrate that the proposed framework outperforms random or quasi-random sampling methods, especially when the number of sensors is limited.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于有效传感器布置的需求，以监测时空过程，同时最小化来自物理基础模拟生成的大型数据集的信息损失。作者提出了一种新颖的基于模型的传感器布置标准和高效的优化算法，结合了物理基础模拟和贝叶斯实验设计原则。来自亚利桑那州凤凰城空气温度监测案例研究的实验结果表明，所提出的框架优于随机或准随机采样方法，特别是在传感器数量有限的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">Grading Handwritten Engineering Exams with Multimodal Large Language Models</div>
<div class="meta-line">Authors: Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec</div>
<div class="meta-line">First: 2026-01-02T16:10:08+00:00 · Latest: 2026-01-02T16:10:08+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 2 tables. Supplementary material available at https://lmi.fe.uni-lj.si/en/janez-pers-2/supplementary-material/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00730v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用多模态大型语言模型评分手写工程考试</div>
<div class="mono" style="margin-top:8px">手写STEM考试捕捉开放式推理和图表，但人工评分速度慢且难以扩展。我们提出了一种端到端的工作流程，用于评分扫描的手写工程测验，采用多模态大型语言模型（LLMs），保持标准考试流程（A4纸，不受限制的学生手写）。讲师仅提供手写参考答案（100%）和一套简短的评分规则；参考答案被转换为仅文本的摘要，以指导评分而不暴露参考扫描。通过多阶段设计实现可靠性，包括格式/存在性检查以防止评分空白答案、独立评分者的集成、监督聚合，以及具有确定性验证的严格模板，以生成可审计的、机器可解析的报告。我们在斯洛文尼亚的一门真实课程测验上，采用干净室协议评估冻结的管道，包括手绘电路原理图。使用最先进的后端（GPT-5.2和Gemini-3 Pro），完整管道实现了与讲师评分的平均绝对差异约为8分，偏差低，估计的人工审核触发率约为17%（D_{\max}=40）。消融实验表明，简单提示和去除参考答案会显著降低准确性并引入系统性过度评分，确认结构化提示和参考基础是必不可少的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of slow and difficult manual grading of handwritten STEM exams, which often include open-ended reasoning and diagrams. The authors propose an end-to-end workflow utilizing multimodal large language models (LLMs) to grade scanned handwritten engineering quizzes, relying on a handwritten reference solution and a set of grading rules. Experimental results demonstrate that the pipeline, evaluated on a real course quiz in Slovenian, achieves an approximate 8-point mean absolute difference from lecturer grades with low bias, while also indicating a manual-review trigger rate of about 17%, highlighting the importance of structured prompting and reference grounding for maintaining grading accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高手写STEM考试的评分效率和可扩展性，因为传统的人工评分过程既缓慢又难以评估。作者提出了一种利用多模态大型语言模型（LLMs）对扫描的手写工程测验进行评分的端到端工作流程，该流程依赖于讲师提供的手写参考答案和一套评分规则。实验结果表明，该评分管道在真实课程测验上的评估与讲师评分的平均绝对差约为8分，偏差较低，手动审核触发率约为17%，突显了结构化提示和参考基础在保持评分准确性方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL</div>
<div class="meta-line">Authors: Erin Carson, Xinye Chen</div>
<div class="meta-line">First: 2026-01-02T15:59:42+00:00 · Latest: 2026-01-02T15:59:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00728v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00728v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于上下文赌博机的强化学习线性求解器精度自调节</div>
<div class="mono" style="margin-top:8px">我们提出了一种用于线性求解器自适应精度调节的强化学习（RL）框架，并可扩展到一般算法。该框架被表述为一个上下文赌博机问题，并通过增量动作值估计与离散状态空间相结合来选择计算步骤的最佳精度配置，以平衡精度和计算效率。为了验证其有效性，我们将该框架应用于线性系统 $Ax = b$ 的迭代精化。在此应用中，我们的方法根据系统计算的特征动态选择精度。具体而言，Q表将离散特征（例如，近似条件数和矩阵范数）映射到动作（特定步骤的选择精度配置），通过epsilon-greedy策略优化，以最大化平衡准确性和计算成本的多目标奖励。实证结果表明有效的精度选择，降低计算成本，同时保持与双精度基线相当的准确性。该框架可推广到多样的样本外数据，并提供了利用RL精度选择用于其他数值算法的见解，推动了科学计算中混合精度数值方法的发展。据我们所知，这是首个基于RL的精度自调节工作，并在未见数据集上进行了验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for adaptive precision tuning in linear solvers to improve computational efficiency without sacrificing accuracy. The authors propose a reinforcement learning framework formulated as a contextual bandit problem, utilizing incremental action-value estimation to select optimal precision configurations based on system features. Experimental results show that this approach effectively reduces computational costs while maintaining accuracy comparable to double-precision baselines, and it generalizes well to diverse out-of-sample data, indicating its potential for broader applications in numerical algorithms.</div>
<div class="mono" style="margin-top:8px">本研究解决了线性求解器中自适应精度调优的挑战，以提高计算效率同时保持准确性。作者提出了一种基于上下文赌博机问题的强化学习框架，利用增量动作值估计来优化计算步骤中的精度配置。实验结果表明，该方法有效选择精度水平，降低了计算成本，同时实现了与传统双精度方法相当的准确性，并且在各种样本外数据中表现出广泛的适应性，暗示其在其他数值算法中的潜在应用。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection</div>
<div class="meta-line">Authors: Johannes C. Bauer, Paul Geng, Stephan Trattnig, Petr Dokládal, Rüdiger Daub</div>
<div class="meta-line">First: 2026-01-02T15:50:32+00:00 · Latest: 2026-01-02T15:50:32+00:00</div>
<div class="meta-line">Comments: Accepted at the 2025 IEEE 13th International Conference on Control, Mechatronics and Automation (ICCMA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00725v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于视觉质量检测的持续学习的多级特征融合</div>
<div class="mono" style="margin-top:8px">深度神经网络在自动化制造中的各种视觉质量检测任务中显示出巨大的潜力。然而，它们在更不稳定的场景中适用性有限，例如再制造，其中被检测的产品和缺陷模式经常变化。在这种情况下，部署的模型需要频繁适应新条件，实际上构成了一个持续学习问题。为了实现快速适应，必要的训练过程必须在计算上高效，同时避免灾难性遗忘等影响。本研究提出了一种多级特征融合（MLFF）方法，旨在通过利用预训练网络不同深度的表示来同时改善这两个方面。我们展示了我们的方法能够在使用显著更少的可训练参数的情况下，匹配不同质量检测问题的端到端训练性能。此外，它减少了灾难性遗忘，并提高了对新产品类型或缺陷的泛化鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the adaptability of deep neural networks in visual quality inspection tasks, particularly in dynamic environments like remanufacturing where product and defect patterns frequently change. The authors propose a multi-level feature fusion (MLFF) method that leverages representations from various depths of a pretrained network to achieve efficient training while mitigating catastrophic forgetting. Experimental results demonstrate that MLFF matches the performance of traditional end-to-end training across different quality inspection scenarios, while significantly reducing the number of trainable parameters and improving generalization to new product types and defects.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高深度神经网络在动态制造环境中进行视觉质量检查的适应性，特别是在产品和缺陷模式频繁变化的再制造过程中。作者提出了一种多级特征融合（MLFF）方法，利用预训练网络不同深度的表示来实现高效的持续学习。实验结果表明，MLFF在不同质量检查任务中能够与传统的端到端训练相媲美，同时显著减少可训练参数的数量，减轻灾难性遗忘，并提高对新产品类型和缺陷的泛化能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
