<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 03:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0346</div>
    <div class="row"><div class="card">
<div class="title">Reinforced Attention Learning</div>
<div class="meta-line">Authors: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng</div>
<div class="meta-line">First: 2026-02-04T18:59:52+00:00 · Latest: 2026-02-04T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04884v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化注意力学习</div>
<div class="mono" style="margin-top:8px">通过强化学习（RL）进行后训练显著改善了大型语言模型（LLMs）的推理能力，尤其是在测试时扩展。然而，通过冗长的推理将这一范式扩展到多模态LLMs（MLLMs）对感知的提升有限，甚至可能导致性能下降。我们提出了强化注意力学习（RAL），这是一种策略梯度框架，直接优化内部注意力分布而非输出令牌序列。通过将优化从生成内容转向关注位置，RAL促进了有效的信息分配和在复杂多模态输入中的改进基础。针对多种图像和视频基准的实验显示，RAL在GRPO和其他基准上 consistently 提升了性能。我们进一步引入了在线策略注意力蒸馏，证明转移潜在注意力行为比标准知识蒸馏产生更强的跨模态对齐。我们的结果将注意力策略定位为多模态后训练的原则性和通用替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance reasoning in Multimodal Large Language Models (MLLMs) through improved attention mechanisms, as traditional post-training methods yield limited benefits and can degrade performance. The authors propose a novel framework called Reinforced Attention Learning (RAL), which employs a policy-gradient approach to optimize internal attention distributions instead of focusing solely on output sequences. Experimental results across various image and video benchmarks demonstrate that RAL consistently outperforms existing methods, including GRPO, and the introduction of On-Policy Attention Distillation shows that transferring latent attention behaviors leads to better cross-modal alignment compared to conventional knowledge distillation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改进注意力机制来增强多模态大型语言模型（MLLMs）的推理能力，因为传统的后训练方法效果有限。作者提出了强化注意力学习（RAL），这是一种优化内部注意力分布的策略梯度框架，而不是仅关注输出序列。各种图像和视频基准测试的实验结果表明，RAL在性能上始终优于现有方法如GRPO，并且引入的在线策略注意力蒸馏表明，转移潜在的注意力行为比标准知识蒸馏能实现更好的跨模态对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Protein Autoregressive Modeling via Multiscale Structure Generation</div>
<div class="meta-line">Authors: Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu</div>
<div class="meta-line">First: 2026-02-04T18:59:49+00:00 · Latest: 2026-02-04T18:59:49+00:00</div>
<div class="meta-line">Comments: ByteDance Seed Tech Report; Page: https://par-protein.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04883v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04883v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://par-protein.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多尺度结构生成的蛋白质自回归建模</div>
<div class="mono" style="margin-top:8px">我们提出了蛋白质自回归建模（PAR），这是第一个用于蛋白质主链生成的多尺度自回归框架，通过粗到细的下一尺度预测。利用蛋白质的层次特性，PAR生成的结构类似于雕刻雕像，形成粗略的拓扑结构，并在不同尺度上细化结构细节。为此，PAR由三个关键组件组成：（i）多尺度下采样操作，在训练过程中表示蛋白质结构的多个尺度；（ii）编码多尺度信息并生成条件嵌入以指导结构生成的自回归变换器；（iii）基于流的主链解码器，根据这些嵌入生成主链原子。此外，自回归模型受到曝光偏差的影响，这种偏差是由训练和生成过程的不匹配引起的，显著降低了结构生成质量。我们通过采用噪声上下文学习和计划采样有效缓解了这个问题，从而实现了稳健的主链生成。值得注意的是，PAR表现出强大的零样本泛化能力，支持灵活的人类提示条件生成和基序支架，而无需微调。在无条件生成基准上，PAR有效学习蛋白质分布，生成高设计质量的主链，并表现出良好的扩展性。这些特性共同确立了PAR作为蛋白质结构生成的有前景框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve protein backbone generation through a novel multi-scale autoregressive framework called Protein Autoregressive Modeling (PAR). The method involves a hierarchical approach that includes multi-scale downsampling, an autoregressive transformer for encoding information, and a flow-based decoder for generating backbone atoms. The key findings indicate that PAR effectively addresses exposure bias, enhances structure generation quality, and demonstrates strong zero-shot generalization capabilities, producing high-quality protein backbones without the need for fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要改进蛋白质结构生成方法，以有效捕捉蛋白质的层次特性。作者提出了蛋白质自回归建模（PAR），这是一种多尺度自回归框架，通过粗到细的下一尺度预测来生成蛋白质主链。主要发现表明，PAR通过噪声上下文学习和计划采样有效缓解了曝光偏差，导致主链生成稳健，具有强大的零样本泛化能力，并在无条件生成基准中展现出高质量的设计。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Continual Learning for Model Adaptability in Internet of Things</div>
<div class="meta-line">Authors: Ajesh Koyatan Chathoth</div>
<div class="meta-line">First: 2026-02-04T18:59:14+00:00 · Latest: 2026-02-04T18:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04881v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物联网中模型适应性的对比持续学习</div>
<div class="mono" style="margin-top:8px">物联网（IoT）部署在非平稳的动态环境中运行，其中传感器漂移、用户行为演变和异构用户隐私要求等因素可能影响应用效用。持续学习（CL）通过随时间调整模型而不发生灾难性遗忘来解决这个问题。同时，对比学习作为一种强大的表示学习范式，已在自监督方式中提高了鲁棒性和样本效率。本文回顾了对比持续学习（CCL）在物联网中的应用，将算法设计（重放、正则化、蒸馏、提示）与物联网系统现实（TinyML约束、间歇性连接、隐私）相连接。我们提出了一个统一的问题表述，推导出结合对比损失和蒸馏损失的共同目标，提出了一个面向物联网的参考架构，用于设备端、边缘和基于云的CCL，并提供了评估协议和指标的指导。最后，我们强调了物联网领域的独特开放挑战，例如跨表格和流式物联网数据、概念漂移、联邦设置和能量感知训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance model adaptability in Internet of Things (IoT) environments, which are characterized by nonstationary conditions and varying user requirements. The authors propose a method called contrastive continual learning (CCL), which integrates contrastive learning with continual learning techniques to address challenges such as sensor drift and evolving user behavior. Key experimental findings indicate that the proposed CCL framework effectively improves model robustness and sample efficiency while accommodating the specific constraints of IoT systems, including privacy concerns and intermittent connectivity.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高物联网（IoT）环境中模型的适应性，这些环境具有非平稳特征和各种挑战，如传感器漂移和用户行为的变化。作者采用了一种称为对比持续学习（CCL）的方法，该方法将重放和蒸馏等算法设计元素与物联网系统的实际限制相结合。主要实验结果表明，CCL能够有效提高模型的鲁棒性和样本效率，同时解决物联网中的独特挑战，包括概念漂移和隐私要求。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Trust Region in LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</div>
<div class="meta-line">First: 2026-02-04T18:59:04+00:00 · Latest: 2026-02-04T18:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考LLM强化学习中的信任区域</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已成为微调大型语言模型（LLM）的基石，近端策略优化（PPO）作为事实上的标准算法。尽管其普遍存在，我们认为PPO中的核心比率裁剪机制在结构上不适合LLM固有的大词汇量。PPO基于采样令牌的概率比来限制策略更新，这作为真实策略发散的嘈杂单样本蒙特卡洛估计。这造成了次优的学习动态：低概率令牌的更新被过度惩罚，而高概率令牌的潜在灾难性变化则受到约束不足，导致训练效率低下和不稳定。为了解决这个问题，我们提出了发散近端策略优化（DPPO），用基于策略发散的直接估计（例如，总变差或KL）替代启发式裁剪。为了避免巨大的内存占用，我们引入了高效的二进制和Top-K近似，以在可忽略的开销下捕捉基本的发散。广泛的实证评估表明，DPPO在训练稳定性和效率上优于现有方法，为基于RL的LLM微调提供了更稳健的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and stability of reinforcement learning (RL) in fine-tuning Large Language Models (LLMs), as the standard Proximal Policy Optimization (PPO) algorithm is found to be inadequate due to its core ratio clipping mechanism. The authors propose a new method called Divergence Proximal Policy Optimization (DPPO), which replaces the heuristic clipping with a principled constraint based on direct estimates of policy divergence, such as Total Variation or KL divergence, while also introducing efficient Binary and Top-K approximations to minimize memory usage. Experimental results show that DPPO significantly enhances training stability and efficiency compared to existing methods, providing a more effective approach for RL-based fine-tuning of LLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善使用强化学习（RL）对大型语言模型（LLMs）进行微调的过程，特别是针对普遍使用但在结构上不适合LLMs的近端策略优化（PPO）算法的局限性。作者提出了一种新方法，称为发散近端策略优化（DPPO），该方法用基于政策发散的直接估计的原则约束替代PPO中的启发式剪切，同时引入高效的二进制和Top-K近似，以最小化内存使用。实验结果表明，DPPO在训练稳定性和效率方面显著优于传统方法，为基于RL的LLM微调提供了更有效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">CoWTracker: Tracking by Warping instead of Correlation</div>
<div class="meta-line">Authors: Zihang Lai, Eldar Insafutdinov, Edgar Sucar, Andrea Vedaldi</div>
<div class="meta-line">First: 2026-02-04T18:58:59+00:00 · Latest: 2026-02-04T18:58:59+00:00</div>
<div class="meta-line">Comments: Project website: cowtracker.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04877v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoWTracker：通过变形而非相关性进行跟踪</div>
<div class="mono" style="margin-top:8px">密集点跟踪是计算机视觉中的一个基本问题，应用范围从视频分析到机器人操作。最先进的跟踪器通常依赖于成本体积在帧之间匹配特征，但这种方法在空间分辨率上会导致二次复杂性，限制了可扩展性和效率。本文提出了\method，一种新颖的密集点跟踪器，摒弃了成本体积，采用变形方法。受近期光流研究进展的启发，我们的方法通过根据当前估计将目标帧的特征变形到查询帧，迭代地细化跟踪估计。结合执行所有轨迹的联合时空推理的变换器架构，我们的设计在不计算特征相关性的情况下建立了长距离对应关系。我们的模型简单，并在标准密集点跟踪基准测试中实现了最先进的性能，包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP。值得注意的是，该模型在光流方面也表现出色，有时在Sintel、KITTI和Spring基准测试中超越了专门的方法。这些结果表明，基于变形的架构可以统一密集点跟踪和光流估计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of state-of-the-art dense point trackers that rely on cost volumes, which lead to quadratic complexity and hinder scalability. The authors propose CoWTracker, a novel dense point tracker that utilizes a warping method instead of cost volumes, refining track estimates by warping features from the target frame to the query frame based on current estimates. Experimental results demonstrate that CoWTracker achieves state-of-the-art performance on various dense point tracking benchmarks and excels in optical flow tasks, indicating that warping-based architectures can effectively unify dense point tracking and optical flow estimation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有密集点跟踪方法的局限性，这些方法通常依赖于代价体积，导致二次复杂度。作者提出了CoWTracker，这是一种新颖的方法，利用变形而非相关性来跟踪帧间特征，灵感来自光流的进展。实验结果表明，CoWTracker在多个密集点跟踪基准上实现了最先进的性能，并在光流任务中表现出色，表明基于变形的架构可以有效地统一这两个研究领域。</div>
</details>
</div>
<div class="card">
<div class="title">PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation</div>
<div class="meta-line">Authors: Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu</div>
<div class="meta-line">First: 2026-02-04T18:58:55+00:00 · Latest: 2026-02-04T18:58:55+00:00</div>
<div class="meta-line">Comments: Project website: https://johnzhan2023.github.io/PerpetualWonder/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04876v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://johnzhan2023.github.io/PerpetualWonder/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PerpetualWonder：长时间视角下的动作条件4D场景生成</div>
<div class="mono" style="margin-top:8px">我们介绍了PerpetualWonder，这是一种混合生成模拟器，能够从单张图像生成长时间视角下的动作条件4D场景。目前的研究在这一任务上失败，因为它们的物理状态与视觉表现脱钩，阻碍了生成性改进更新后续交互的基础物理。PerpetualWonder通过引入第一个真正的闭环系统来解决这个问题。它具有一种新颖的统一表示，创建了物理状态与视觉原语之间的双向链接，使生成性改进能够同时纠正动态和外观。它还引入了一种强大的更新机制，从多个视角收集监督以解决优化歧义。实验表明，PerpetualWonder能够从单张图像成功模拟复杂的多步骤交互，保持物理合理性和视觉一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to address the limitations of existing methods in long-horizon, action-conditioned 4D scene generation, which struggle due to the disconnection between physical states and visual representations. The authors developed PerpetualWonder, a hybrid generative simulator that employs a closed-loop system with a unified representation linking physical states and visual primitives, allowing for generative refinements that enhance both dynamics and appearance. Experimental results show that PerpetualWonder can effectively simulate complex, multi-step interactions from a single image while maintaining physical plausibility and visual consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有方法在长时间范围内的动作条件4D场景生成中的局限性，这些方法因物理状态与视觉表示的解耦而面临困难。作者提出了PerpetualWonder，这是一种混合生成模拟器，采用闭环系统，使用统一表示将物理状态与视觉原语连接起来。主要实验结果表明，PerpetualWonder能够有效地从单张图像中模拟复杂的多步骤交互，同时保持物理的合理性和视觉的一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Laminating Representation Autoencoders for Efficient Diffusion</div>
<div class="meta-line">Authors: Ramón Calvo-González, François Fleuret</div>
<div class="meta-line">First: 2026-02-04T18:57:33+00:00 · Latest: 2026-02-04T18:57:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04873v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04873v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于高效扩散的层压表示自编码器</div>
<div class="mono" style="margin-top:8px">最近的研究表明，扩散模型可以通过直接操作SSL补丁特征而非像素空间潜变量生成高质量图像。然而，像DINOv2这样的编码器产生的密集补丁网格包含显著的冗余，使得扩散变得不必要地昂贵。我们引入了FlatDINO，这是一种变分自编码器，将这种表示压缩为仅32个连续标记的一维序列——序列长度减少8倍，总维度压缩48倍。在ImageNet 256x256上，基于FlatDINO潜变量训练的DiT-XL在无分类器引导下实现了1.80的gFID，同时每次前向传播所需的FLOPs减少8倍，每个训练步骤所需的FLOPs减少最多4.5倍，相较于在未压缩的DINOv2特征上进行的扩散。这些是初步结果，相关工作仍在进行中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of diffusion models in generating high-quality images by addressing the redundancy in dense patch grids from encoders like DINOv2. The authors propose FlatDINO, a variational autoencoder that compresses the representation into a one-dimensional sequence of 32 continuous tokens, achieving an 8x reduction in sequence length and a 48x reduction in total dimensionality. Experimental results on ImageNet 256x256 demonstrate that a DiT-XL model trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to using uncompressed DINOv2 features.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决使用像DINOv2这样的编码器生成高质量图像的扩散模型中的低效问题，这些编码器的密集补丁网格包含显著的冗余。作者提出了FlatDINO，这是一种变分自编码器，将这些表示压缩为一维序列，仅包含32个连续标记，实现了序列长度的8倍缩减和总维度的48倍压缩。在ImageNet 256x256上的实验结果表明，基于FlatDINO潜在变量训练的DiT-XL模型在无分类器引导下达到了1.80的gFID，同时每次前向传递所需的FLOPs减少了8倍，训练步骤所需的FLOPs减少了最多4.5倍，相比于使用未压缩的DINOv2特征。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning</div>
<div class="meta-line">Authors: Nicholas Barnfield, Subhabrata Sen, Pragya Sur</div>
<div class="meta-line">First: 2026-02-04T18:57:30+00:00 · Latest: 2026-02-04T18:57:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04872v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多层交叉注意力在多模态上下文学习中是可证明的最优</div>
<div class="mono" style="margin-top:8px">最近的进展迅速推动了我们对现代基于注意力的神经网络中上下文学习机制的理解。然而，现有结果仅专注于单模态数据；相比之下，多模态数据的上下文学习的理论基础仍然不够清晰。我们引入了一个数学上可处理的框架来研究多模态学习，并探讨了何时变换器类架构能够在上下文中恢复贝叶斯最优性能。为了建模多模态问题，我们假设观察到的数据来自潜在因子模型。我们的第一个结果是对表达能力的负面看法：我们证明单层线性自注意力无法在任务分布上均匀恢复贝叶斯最优预测器。为了解决这一限制，我们引入了一种新颖的线性交叉注意力机制，并在交叉注意力层数和上下文长度都很大的情况下进行研究。我们证明，当使用梯度流进行优化时，这种交叉注意力机制是可证明的贝叶斯最优。我们的结果强调了深度对上下文学习的好处，并确立了交叉注意力在多模态分布中的可证明效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of understanding regarding in-context learning for multi-modal data, as existing studies have primarily focused on unimodal data. The authors introduce a mathematically tractable framework to investigate multi-modal learning and determine the conditions under which transformer-like architectures achieve Bayes-optimal performance. Their key findings reveal that single-layer, linear self-attention is insufficient for recovering the Bayes-optimal predictor across task distributions, while a novel linearized cross-attention mechanism demonstrates provable Bayes optimality when optimized with gradient flow, highlighting the advantages of increased depth in in-context learning for multi-modal distributions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强对多模态数据中上下文学习机制的理解，因为现有研究主要集中在单模态数据上。作者引入了一个数学上可处理的框架来分析多模态学习，并研究了变换器架构在何种条件下能够实现贝叶斯最优性能。主要发现表明，单层线性自注意力不足以在任务分布中恢复贝叶斯最优预测器，而一种新颖的线性交叉注意力机制在使用梯度流优化时显示出可证明的贝叶斯最优性，突显了深度在多模态分布上下文学习中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism</div>
<div class="meta-line">Authors: Chenwei Cui, Rockwell Jackson, Benjamin Joseph Herrera, Ana María Tárano, Hannah Kerner</div>
<div class="meta-line">First: 2026-02-04T18:57:19+00:00 · Latest: 2026-02-04T18:57:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04870v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04870v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多头潜在专家模型与头并行：高效通信和确定性专家模型并行性</div>
<div class="mono" style="margin-top:8px">大型语言模型已改变许多应用，但训练成本仍然很高。稀疏专家混合模型（MoE）通过条件计算解决了这一问题，专家并行（EP）是标准的分布式训练方法。然而，EP有三个局限性：通信成本随着激活专家数量$k$线性增长，负载不平衡影响延迟和内存使用，数据依赖的通信需要元数据交换。我们提出了多头潜在专家模型和头并行（HP），一种新架构和并行性，实现了$O(1)$的通信成本，无论$k$如何，完全平衡的流量和确定性通信，同时与EP兼容。为了加速多头潜在专家模型，我们提出了IO感知路由和专家计算。与使用EP的MoE相比，使用HP的多头潜在专家模型训练速度提高了最多$1.61\times$，同时性能相同。通过加倍粒度，它实现了更高的整体性能，同时仍然快$1.11\times$。我们的方法使得数十亿参数的基础模型研究变得更加可及。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of training large language models, which are costly and resource-intensive. The authors introduce Multi-Head LatentMoE and Head Parallel (HP) as a novel architecture that addresses the limitations of the standard Expert Parallel (EP) method, specifically its linear communication costs, load imbalance, and data-dependent communication issues. Experimental results demonstrate that the proposed method achieves up to 1.61 times faster training compared to MoE with EP while maintaining identical performance, and with increased granularity, it further improves overall performance while being 1.11 times faster.</div>
<div class="mono" style="margin-top:8px">本研究的动机是大型语言模型的高训练成本以及稀疏专家混合（MoE）中标准专家并行（EP）方法的局限性，包括通信成本增加、负载不平衡和数据依赖通信。作者提出了一种新的架构，称为多头潜在MoE与头并行（HP），该架构实现了恒定的通信成本、平衡的流量和确定性的通信，同时与EP兼容。实验结果表明，多头潜在MoE与HP的训练速度比MoE与EP快1.61倍，同时保持相同的性能，并且在粒度加倍的情况下，整体性能更高，同时速度快1.11倍。</div>
</details>
</div>
<div class="card">
<div class="title">CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</div>
<div class="meta-line">Authors: Yannick Denker, Alexander Gepperth</div>
<div class="meta-line">First: 2026-02-04T18:54:26+00:00 · Latest: 2026-02-04T18:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRoSS：一个用于可扩展强化学习的持续机器人仿真套件，具有高任务多样性和真实物理仿真</div>
<div class="mono" style="margin-top:8px">持续强化学习（CRL）要求智能体从一系列任务中学习，而不遗忘先前获得的策略。在本研究中，我们引入了一个基于Gazebo仿真器中真实模拟机器人的新基准套件。我们的持续机器人仿真套件（CRoSS）基准依赖于两个机器人平台：一个配备激光雷达、相机和碰撞传感器的两轮差动驱动机器人，以及一个具有七个关节的机器人手臂。前者代表在跟踪线路和推物体场景中的智能体，其中视觉和结构参数的变化产生大量不同的任务，而后者用于两个目标到达场景，具有高层次的笛卡尔手部位置控制（基于持续世界基准建模）和基于关节角度的低层次控制。对于机器人手臂基准，我们提供额外的仅运动学变体，绕过物理仿真的需求（只要不需要传感器读数），并且可以以两个数量级更快的速度运行。CRoSS旨在易于扩展，并能够在具有高物理真实感的机器人环境中进行持续强化学习的受控研究，特别是允许使用几乎任意的模拟传感器。为了确保可重复性和易用性，我们提供了一个开箱即用的容器化设置（Apptainer），并报告标准强化学习算法的性能，包括深度Q网络（DQN）和策略梯度方法。这突显了其作为可扩展和可重复基准的适用性，适用于CRL研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance continual reinforcement learning (CRL) by providing a benchmark suite that allows agents to learn from a sequence of tasks without forgetting previous policies. The authors developed the Continual Robotic Simulation Suite (CRoSS) using the Gazebo simulator, featuring two robotic platforms: a two-wheeled robot for line-following and object-pushing tasks, and a robotic arm for goal-reaching scenarios. Key findings demonstrate that CRoSS supports a diverse range of tasks and can run both physical and kinematics-only simulations, facilitating controlled studies of CRL while ensuring reproducibility through a containerized setup that showcases the performance of standard RL algorithms like Deep Q-Networks and policy gradient methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过提供一个基准套件来推动持续强化学习（CRL），使智能体能够从一系列任务中学习而不遗忘先前获得的策略。作者使用Gazebo模拟器开发了持续机器人模拟套件（CRoSS），该套件包含两个机器人平台：一个用于多样化的跟踪和物体推动任务的双轮机器人，以及一个用于目标达成场景的机器人手臂。主要发现表明，CRoSS支持高任务多样性和真实的物理模拟，能够进行CRL的控制研究，并展示了标准强化学习算法（如深度Q网络和策略梯度方法）的性能，从而确立了其作为CRL研究的可扩展和可重复基准。</div>
</details>
</div>
<div class="card">
<div class="title">When LLaVA Meets Objects: Token Composition for Vision-Language-Models</div>
<div class="meta-line">Authors: Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当LLaVA遇见物体：视觉语言模型的标记组合</div>
<div class="mono" style="margin-top:8px">当前的自回归视觉语言模型（VLMs）通常依赖大量视觉标记来表示图像，这在推理时需要更多的计算资源。为了解决这个问题，我们提出了Mask-LLaVA，一个利用不同层次视觉特征创建紧凑且信息丰富的视觉表示的框架，适用于自回归VLMs。具体而言，我们将基于掩码的物体表示与全局标记和局部补丁标记结合起来。虽然在训练期间使用所有标记，但结果表明，所得到的模型在测试时可以灵活地减少基于掩码的物体标记的数量，从而在推理过程中适应标记数量，而无需重新训练模型且性能没有显著下降。我们在一系列标准基准上评估了所提出的方法，显示出与当前标记高效方法的竞争力，并且在仅使用少量视觉标记的情况下与原始LLaVA基线相当。我们的分析表明，结合多层次特征能够在使用更少标记的情况下实现高效学习，同时允许在测试时进行动态标记选择以获得良好性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to reduce the computational demands of autoregressive Vision Language Models (VLMs) that rely on numerous visual tokens for image representation. The authors propose Mask-LLaVA, a framework that integrates various levels of visual features, including mask-based object representations, global tokens, and local patch tokens, to create a more compact visual representation. Experimental results indicate that the model can effectively decrease the number of mask-based object tokens during inference without significant performance loss, achieving competitive results on standard benchmarks while using fewer visual tokens compared to existing methods and the original LLaVA baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机是减少自回归视觉语言模型（VLMs）在图像表示中对大量视觉标记的计算需求。作者提出了Mask-LLaVA框架，该框架结合了不同层次的视觉特征，包括基于掩码的对象表示、全局标记和局部补丁标记，以创建更紧凑且信息丰富的视觉表示。实验结果表明，该模型能够在推理过程中有效减少基于掩码的对象标记数量，而不会显著降低性能，在标准基准测试中取得了与现有方法和原始LLaVA基线相竞争的结果，同时使用的视觉标记数量更少。</div>
</details>
</div>
<div class="card">
<div class="title">Subliminal Effects in Your Data: A General Mechanism via Log-Linearity</div>
<div class="meta-line">Authors: Ishaq Aden-Ali, Noah Golowich, Allen Liu, Abhishek Shetty, Ankur Moitra, Nika Haghtalab</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/ishaqadenali/logit-linear-selection</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04863v1">PDF</a> · <a href="https://github.com/ishaqadenali/logit-linear-selection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model&#x27;s properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据中的潜意识效应：通过对数线性的一般机制</div>
<div class="mono" style="margin-top:8px">训练现代大型语言模型（LLMs）已成为一个真正的算法和数据集自助餐，旨在引发特定行为，因此开发理解数据集对模型属性影响的技术至关重要。最近的实验表明，数据集可以传递从单个数据点无法直接观察到的信号，这加剧了对以数据集为中心的LLM训练理解的概念挑战，并暗示缺乏对这种现象的基本解释。为了理解这些效应，受最近关于LLMs线性结构的研究启发，我们揭示了一种通过通用数据集产生隐藏潜台词的一般机制。我们引入了对数线性选择（LLS）方法，规定如何选择通用偏好数据集的子集，以引发广泛的隐藏效应。我们应用LLS发现现实世界数据集的子集，使得在其上训练的模型表现出从具有特定偏好到以数据集中不存在的不同语言响应提示，再到呈现不同人格的行为。关键是，这种效应在所选子集上持续存在，跨越不同架构的模型，支持其普遍性和通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand how datasets influence the properties of large language models (LLMs), particularly in light of evidence that datasets can convey hidden signals not evident from individual data points. The authors introduce a method called Logit-Linear-Selection (LLS), which allows for the selection of specific subsets of preference datasets to reveal various hidden effects. Experimental results demonstrate that models trained on these selected subsets can exhibit diverse behaviors, including specific preferences, responses in different languages, and altered personas, indicating the method&#x27;s broad applicability across different model architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解数据集如何影响大型语言模型（LLMs）的特性，特别是考虑到数据集可以传递从单个数据点中无法直接观察到的隐藏信号。作者提出了一种名为Logit-Linear-Selection（LLS）的方法，该方法允许从通用偏好数据集中选择特定子集，以揭示各种隐藏效果。实验结果表明，在这些选定子集上训练的模型可以表现出多样化的行为，例如显示特定偏好、以数据集中未包含的语言作出响应以及采用不同的人格，这表明该方法在不同模型架构中的普遍性。</div>
</details>
</div>
<div class="card">
<div class="title">From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures</div>
<div class="meta-line">Authors: Ryan Liu, Eric Qu, Tobias Kreiman, Samuel M. Blau, Aditi S. Krishnapriyan</div>
<div class="meta-line">First: 2026-02-04T18:50:10+00:00 · Latest: 2026-02-04T18:50:10+00:00</div>
<div class="meta-line">Comments: 13 pages main text, 10 pages reference &amp; appendix, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04861v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an &quot;in-the-loop&quot; model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从评估到设计：利用潜能面光滑度指标指导机器学习原子间势架构</div>
<div class="mono" style="margin-top:8px">机器学习原子间势（MLIPs）有时无法再现量子潜能能量面（PES）的物理光滑性，导致下游模拟中出现错误行为，而标准的能量和力回归评估可能无法发现这些问题。现有评估，如微正则分子动力学（MD），计算成本高且主要探测近平衡状态。为了改进MLIPs的评估指标，我们引入了键光滑度表征测试（BSCT）。该高效基准通过控制键变形探测PES，并检测非光滑性，包括不连续性、人工极小值和虚假力，无论是在平衡附近还是远离平衡。我们展示了BSCT与MD稳定性之间的强相关性，同时所需成本仅为MD的一小部分。为了演示BSCT如何指导迭代模型设计，我们利用无约束的Transformer骨干作为测试平台，说明如何通过新的可微分$k$-最近邻算法和温度控制注意力等改进减少我们指标识别的伪影。通过基于BSCT系统地优化模型设计，最终得到的MLIP同时实现了低的传统E/F回归误差、稳定的MD模拟和可靠的原子性质预测。我们的结果确立了BSCT作为验证指标和“在环”模型设计代理，提醒MLIP开发者关注当前MLIP基准无法高效评估的物理挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the shortcomings of Machine Learning Interatomic Potentials (MLIPs) in accurately reproducing the physical smoothness of quantum potential energy surfaces, which can lead to errors in simulations. The authors introduce the Bond Smoothness Characterization Test (BSCT) as a new evaluation metric that efficiently assesses the potential energy surface through controlled bond deformations, identifying issues such as non-smoothness and spurious forces. Experimental results demonstrate that BSCT correlates well with the stability of microcanonical molecular dynamics while being significantly less computationally intensive, and the application of BSCT in model design led to improvements in MLIPs, achieving lower regression errors and more stable simulations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器学习原子势（MLIPs）在准确再现量子势能面物理光滑性方面的不足，这可能导致模拟中的错误。作者提出了键光滑性表征测试（BSCT），作为一种更高效的评估指标，通过控制键变形来评估势能面，识别不连续性和虚假力等问题。实验结果表明，BSCT与微正则分子动力学的稳定性高度相关，同时计算成本显著降低，BSCT在模型设计中的应用改善了MLIP的性能，实现了低回归误差和稳定的模拟。</div>
</details>
</div>
<div class="card">
<div class="title">Combining Residual U-Net and Data Augmentation for Dense Temporal Segmentation of Spike Wave Discharges in Single-Channel EEG</div>
<div class="meta-line">Authors: Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown</div>
<div class="meta-line">First: 2026-01-01T19:58:20+00:00 · Latest: 2026-02-04T18:43:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00459v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00459v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manual annotation of spike-wave discharges (SWDs), the electrographic hallmark of absence seizures, is labor-intensive for long-term electroencephalography (EEG) monitoring studies. While machine learning approaches show promise for automated detection, they often struggle with cross-subject generalization due to high inter-individual variability in seizure morphology and signal characteristics. In this study we compare the performance of 15 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs and find that a 1D U-Net performs the best. We then improve its performance by employing residual connections and data augmentation strategies combining amplitude scaling, Gaussian noise injection, and signal inversion during training to enhance cross-subject generalization. We also compare our method, named AugUNet1D, to a recently published time- and frequency-based algorithmic approach called &quot;Twin Peaks&quot; and show that AugUNet1D performs better on our dataset. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for other users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结合残差U-Net和数据增强进行单通道EEG中尖波放电的密集时间分割</div>
<div class="mono" style="margin-top:8px">尖波放电（SWDs）的手动标注是缺失性癫痫的电图特征，对于长期脑电图（EEG）监测研究来说劳动强度大。尽管机器学习方法在自动检测方面显示出前景，但由于癫痫形态和信号特征的个体差异性高，它们在跨个体泛化方面常常面临挑战。在本研究中，我们比较了15种机器学习分类器在我们自己手动标注的961小时EEG记录数据集上的表现，该数据集来自C3H/HeJ小鼠，包括22,637个标记的SWDs，结果发现1D U-Net表现最佳。然后，我们通过在训练过程中采用残差连接和数据增强策略（结合幅度缩放、高斯噪声注入和信号反转）来提高其性能，以增强跨个体泛化能力。我们还将我们的方法（命名为AugUNet1D）与最近发布的基于时间和频率的算法方法“Twin Peaks”进行了比较，结果显示AugUNet1D在我们的数据集上表现更好。AugUNet1D，无论是预训练于我们的手动标注数据还是未训练，均向其他用户公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the labor-intensive nature of manually annotating spike-wave discharges (SWDs) in long-term EEG monitoring, which is crucial for studying absence seizures. The authors employed a 1D U-Net architecture enhanced with residual connections and data augmentation techniques, including amplitude scaling, Gaussian noise injection, and signal inversion, to improve the model&#x27;s performance and cross-subject generalization. The results demonstrated that the proposed method, AugUNet1D, outperformed 15 other machine learning classifiers and a competing algorithm, Twin Peaks, on a dataset of 961 hours of EEG recordings from C3H/HeJ mice, comprising 22,637 labeled SWDs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决长期脑电图监测中手动标注尖波放电（SWD）所需的劳动强度，这对研究缺失性癫痫至关重要。研究采用了1D U-Net架构，并通过残差连接和数据增强技术（包括幅度缩放、高斯噪声注入和信号反转）来提高模型的性能和跨个体的泛化能力。实验结果表明，所提出的方法AugUNet1D在961小时的脑电图记录数据集上（包含22,637个标记的SWD）优于15种其他机器学习分类器和竞争算法&#x27;Twin Peaks&#x27;，因此为自动SWD检测提供了更有效的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations</div>
<div class="meta-line">Authors: Aroon Sankoh, Victor Wickerhauser</div>
<div class="meta-line">First: 2025-05-06T21:07:53+00:00 · Latest: 2026-02-04T18:42:42+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03980v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.03980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stochastic differential equations such as the Ornstein-Uhlenbeck process have long been used to model realworld probablistic events such as stock prices and temperature fluctuations. While statistical methods such as Maximum Likelihood Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have historically been used to estimate the parameters of stochastic differential equations, the recent explosion of deep learning technology suggests that models such as a Recurrent Neural Network (RNN) could produce more precise estimators. We present a series of experiments that compare the estimation accuracy and computational expensiveness of a statistical method (MLE) with a deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>比较统计与深度学习技术在连续时间随机微分方程参数估计中的应用</div>
<div class="mono" style="margin-top:8px">随机微分方程，如奥恩斯坦-乌伦贝克过程，长期以来被用于建模现实世界的概率事件，如股票价格和温度波动。虽然统计方法如最大似然估计（MLE）、卡尔曼滤波、逆变量法等历史上用于估计随机微分方程的参数，但最近深度学习技术的爆炸性发展表明，循环神经网络（RNN）等模型可能产生更精确的估计器。我们展示了一系列实验，比较统计方法（MLE）与深度学习模型（RNN）在奥恩斯坦-乌伦贝克过程参数估计中的准确性和计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the effectiveness of deep learning techniques compared to traditional statistical methods for estimating parameters in continuous-time stochastic differential equations, specifically the Ornstein-Uhlenbeck process, which is commonly used to model real-world phenomena. The study employs a series of experiments to compare the estimation accuracy and computational efficiency of Maximum Likelihood Estimation (MLE) against a Recurrent Neural Network (RNN). The findings indicate that while both methods have their merits, the RNN demonstrates superior estimation accuracy, suggesting a potential advantage of deep learning approaches in this context.</div>
<div class="mono" style="margin-top:8px">本研究探讨了深度学习技术与传统统计方法在连续时间随机微分方程参数估计中的有效性，特别是奥恩斯坦-乌伦贝克过程。研究通过一系列实验比较了最大似然估计（MLE）与递归神经网络（RNN）的估计准确性和计算效率。结果表明，RNN模型提供的估计器比MLE更精确，突显了深度学习在这一领域的潜在优势。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization</div>
<div class="meta-line">Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli</div>
<div class="meta-line">First: 2026-01-30T16:58:40+00:00 · Latest: 2026-02-04T18:42:12+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23174v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23174v2">PDF</a> · <a href="https://github.com/lucadellalib/dycast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越固定帧：动态字符对齐语音标记化</div>
<div class="mono" style="margin-top:8px">神经音频编解码器是现代对话语音技术的核心，将连续语音转换为可由大型语言模型处理的离散标记序列。然而，现有编解码器通常以固定帧率运行，均匀分配时间上的标记，导致不必要的长序列。在这项工作中，我们介绍了DyCAST，一种动态字符对齐语音标记器，通过软字符级对齐和显式持续时间建模实现可变帧率标记化。DyCAST在训练期间学习将标记与字符级语言单位关联，并支持无对齐推理，在解码时直接控制标记持续时间。为了提高低帧率下的语音重合成质量，我们进一步引入了一种增强重建保真度的检索增强解码机制，而不增加比特率。实验表明，DyCAST在使用显著更少的标记的情况下，达到了具有竞争力的语音重合成质量和下游性能。代码和检查点将公开发布在https://github.com/lucadellalib/dycast。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of neural audio codecs used in conversational speech technologies, which currently rely on fixed frame rates that lead to unnecessarily long token sequences. The authors propose DyCAST, a Dynamic Character-Aligned Speech Tokenizer that utilizes soft character-level alignment and explicit duration modeling to enable variable-frame-rate tokenization. Experimental results demonstrate that DyCAST achieves competitive speech resynthesis quality and downstream performance while significantly reducing the number of tokens used compared to traditional fixed-frame-rate codecs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高神经音频编解码器中语音标记化的效率，传统上使用固定帧率导致不必要的长标记序列。作者提出了DyCAST，一种动态字符对齐语音标记器，采用软字符级对齐和显式持续时间建模，实现可变帧率标记化。实验结果表明，DyCAST在语音重合成质量和下游性能方面具有竞争力，同时使用的标记数量显著少于固定帧率编解码器。</div>
</details>
</div>
<div class="card">
<div class="title">The Key to State Reduction in Linear Attention: A Rank-based Perspective</div>
<div class="meta-line">Authors: Philipp Nazari, T. Konstantin Rusch</div>
<div class="meta-line">First: 2026-02-04T18:39:38+00:00 · Latest: 2026-02-04T18:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04852v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04852v1">PDF</a> · <a href="https://github.com/camail-official/LinearAttentionPruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性注意力中状态减少的关键：基于秩的视角</div>
<div class="mono" style="margin-top:8px">线性注意力提供了一种计算效率高但表达能力强的替代方案，取代了softmax注意力。然而，最近的实证结果表明，训练后的线性注意力模型的状态通常表现出低秩结构，暗示这些模型在实践中未能充分利用其能力。为了阐明这一现象，我们提供了对线性注意力中秩作用的理论分析，揭示低有效秩会通过放大查询噪声影响检索误差。除了这些理论见解外，我们推测低秩状态在训练后可以显著减少，且仅会导致轻微的性能下降，从而产生更快且更节省内存的模型。为此，我们提出了一种新颖的硬件感知方法，结构性地修剪关键和查询矩阵，减少状态大小，同时保持与现有CUDA内核的兼容性。我们调整了几种现有的修剪策略以适应我们的框架，并基于我们的理论分析，提出了一种基于秩揭示QR分解的新型结构化修剪方法。我们的实证结果在不同大小的模型和各种下游任务上进行了评估，证明了我们的状态减少框架的有效性。我们强调，该框架能够在仅轻微增加困惑度的情况下，移除50%的查询和关键通道。该项目的代码可以在https://github.com/camail-official/LinearAttentionPruning找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the low-rank structure observed in trained linear attention models, which indicates an underutilization of their capacity. The authors conduct a theoretical analysis to show how low effective rank can increase retrieval error by amplifying query noise. They propose a hardware-aware approach for structurally pruning key and query matrices, which allows for significant state reduction with minimal performance loss. Experimental results demonstrate that their framework can remove 50% of the query and key channels while only slightly increasing perplexity, confirming its effectiveness across various model sizes and downstream tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于观察到线性注意力模型虽然计算效率高，但通常表现出低秩结构，表明其能力未得到充分利用。作者进行理论分析，探讨低有效秩如何通过放大查询噪声影响检索误差，并提出了一种硬件感知的方法，通过对关键和查询矩阵的结构性修剪实现训练后状态的减少。实验结果表明，他们的框架能够通过去除50%的查询和关键通道，仅以轻微增加困惑度的代价，减少状态大小，证明了其在各种模型和任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PDF-HR: Pose Distance Fields for Humanoid Robots</div>
<div class="meta-line">Authors: Yi Gu, Yukang Gao, Yangchen Zhou, Xingyu Chen, Yixiao Feng, Mingle Zhao, Yunyang Mo, Zhaorui Wang, Lixin Xu, Renjing Xu</div>
<div class="meta-line">First: 2026-02-04T18:38:51+00:00 · Latest: 2026-02-04T18:38:51+00:00</div>
<div class="meta-line">Comments: \href{https://gaoyukang33.github.io/PDF-HR/}{Project page}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04851v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gaoyukang33.github.io/PDF-HR/}{Project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PDF-HR：用于类人机器人姿态距离场</div>
<div class="mono" style="margin-top:8px">姿态和运动先验在类人机器人中起着至关重要的作用。尽管这些先验在人体运动恢复（HMR）领域得到了广泛研究，并有多种模型，但在类人机器人中的应用仍然有限，主要是由于高质量类人运动数据的稀缺。在这项工作中，我们引入了类人机器人姿态距离场（PDF-HR），这是一种轻量级先验，将机器人姿态分布表示为连续且可微的流形。给定任意姿态，PDF-HR预测其与大量重新定向的机器人姿态的距离，从而产生适合优化和控制的平滑姿态合理性度量。PDF-HR可以作为奖励塑形项、正则化项或独立的合理性评分器集成到各种管道中。我们在多种类人任务上评估PDF-HR，包括单轨迹运动跟踪、一般运动跟踪、基于风格的运动模仿和一般运动重新定向。实验表明，这种即插即用的先验始终显著增强了强基线。代码和模型将会发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the application of pose and motion priors in humanoid robotics, which have been limited due to the lack of high-quality humanoid motion data. The authors introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that models the robot pose distribution as a continuous and differentiable manifold, allowing for the prediction of distance to a large set of retargeted robot poses. Experimental results demonstrate that PDF-HR significantly improves performance across various humanoid tasks, including motion tracking and style-based motion mimicry, thereby strengthening existing baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强姿态和运动先验在类人机器人中的应用，因缺乏高质量的类人运动数据而未得到充分利用。作者提出了类人机器人姿态距离场（PDF-HR），这是一种轻量级的先验，能够将机器人姿态分布建模为连续且可微的流形，从而预测与大量重定向机器人姿态的距离。实验结果表明，PDF-HR在各种类人任务中显著提高了性能，包括运动跟踪和基于风格的运动模仿，从而增强了现有基线的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">El Agente Quntur: A research collaborator agent for quantum chemistry</div>
<div class="meta-line">Authors: Juan B. Pérez-Sánchez, Yunheng Zou, Jorge A. Campos-Gonzalez-Angulo, Marcel Müller, Ignacio Gustin, Andrew Wang, Han Hao, Tsz Wai Ko, Changhyeok Choi, Eric S. Isbrandt, Mohammad Ghazi Vakili, Hanyong Xu, Chris Crebolder, Varinia Bernales, Alán Aspuru-Guzik</div>
<div class="meta-line">First: 2026-02-04T18:38:50+00:00 · Latest: 2026-02-04T18:38:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04850v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software&#x27;s internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Quntur代理：量子化学研究协作代理</div>
<div class="mono" style="margin-top:8px">量子化学是化学、材料科学、计算生物学等领域的基础性工具。尽管其强大，量子化学模拟的实际应用仍掌握在合格专家手中，原因在于方法复杂性、软件异质性以及对结果的知情解读需求。为了缩小这些工具的可及性差距，并将其推广给背景更广泛的化学家，我们推出了Quntur代理，一个分层的多智能体AI系统，旨在不仅作为自动化工具，而是作为计算量子化学的研究协作伙伴。Quntur的设计遵循三项主要策略：i) 消除硬编码的程序政策，转而采用基于推理的决策；ii) 构建通用和可组合的行动，以促进泛化和效率；iii) 实施引导深度研究，以整合跨子学科的抽象量子化学推理以及对软件内部逻辑和语法的详细理解。尽管在ORCA中实现，这些设计原则普遍适用于研究代理，并且可以轻松扩展到其他量子化学软件包及更广泛的领域。Quntur支持ORCA 6.0中可用的全范围计算，并根据最佳实践推理软件文档和科学文献，以规划、执行、调整和分析计算化学实验。我们讨论了在计算化学研究层面上运行的代理系统的进展和当前瓶颈，并概述了通向完全自主的端到端计算化学研究代理的路线图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the accessibility of quantum chemistry simulations, which are typically limited to qualified experts due to their complexity and the need for informed interpretation. The authors developed El Agente Quntur, a hierarchical multi-agent AI system that acts as a research collaborator rather than just an automation tool, employing strategies such as reasoning-driven decisions, generalizable actions, and guided deep research. The key findings indicate that Quntur can support a wide range of calculations in ORCA 6.0 and effectively reason over software documentation and scientific literature to facilitate in silico chemistry experiments, while also identifying current challenges and proposing a roadmap for future advancements in autonomous computational chemistry agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高量子化学模拟的可及性，这些模拟通常因其复杂性和对知情解释的需求而仅限于合格的专家。作者开发了El Agente Quntur，这是一个多智能体人工智能系统，作为研究合作者而不仅仅是自动化工具，采用了推理驱动决策、可推广的行动和引导深度研究等策略，以整合量子化学推理。主要发现表明，Quntur能够支持ORCA 6.0中的广泛计算，并有效地推理软件文档和科学文献，以促进计算化学实验的规划、执行、适应和分析，同时也解决了计算化学中智能系统的当前挑战。</div>
</details>
</div>
<div class="card">
<div class="title">El Agente Estructural: An Artificially Intelligent Molecular Editor</div>
<div class="meta-line">Authors: Changhyeok Choi, Yunheng Zou, Marcel Müller, Han Hao, Yeonghun Kang, Juan B. Pérez-Sánchez, Ignacio Gustin, Hanyong Xu, Mohammad Ghazi Vakili, Chris Crebolder, Alán Aspuru-Guzik, Varinia Bernales</div>
<div class="meta-line">First: 2026-02-04T18:38:48+00:00 · Latest: 2026-02-04T18:38:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04849v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构代理：一种人工智能分子编辑器</div>
<div class="mono" style="margin-top:8px">我们介绍了结构代理，这是一个多模态、自然语言驱动的几何生成和操控代理，用于自主化学和分子建模。与通过生成模型进行的分子生成或编辑不同，结构代理模拟人类专家如何在三维中直接操控分子系统，通过整合一套全面的领域知识工具和视觉语言模型。这种设计使得在不需要重建广泛核心分子框架的情况下，能够精确控制原子或功能团的替换、原子连接性和立体化学。通过一系列代表性案例研究，我们展示了结构代理能够在广泛的现实场景中实现化学上有意义的几何操控。这些场景包括位点选择性功能化、配体结合、配体交换、立体化学控制的结构构建、异构体互变、片段级结构分析、从示意反应机制生成结构的图像引导，以及基于机制的几何生成和修改。这些例子说明了多模态推理与专业几何感知工具结合时，如何支持超越结构生成的交互式和上下文感知的分子建模。展望未来，将结构代理整合到结构代理Quntur，一个自主多代理量子化学平台中，通过增加复杂的三维结构生成和编辑工具，增强其能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the development of El Agente Estructural is to create an intelligent molecular editor that allows for precise manipulation of molecular structures in a manner similar to human experts. The method involves a multimodal, natural-language-driven approach that integrates domain-informed tools and vision-language models to facilitate geometry generation and manipulation without extensive rebuilding of molecular frameworks. Key experimental findings demonstrate that Estructural effectively enables meaningful geometry manipulation in various real-world scenarios, including site-selective functionalization and stereochemically controlled structure construction, showcasing its potential for interactive and context-aware molecular modeling.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一种更直观和精确的分子编辑和几何操作方法，以模仿人类专家的技术。作者开发了El Agente Estructural，这是一种多模态代理，集成了领域知识工具和视觉语言模型，允许在三维中直接操作分子系统。关键实验结果表明，Estructural有效地促进了多种具有化学意义的几何操作，包括位点选择性功能化和立体化学控制的结构构建，展示了其在现实场景中进行交互式分子建模的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Fluid Representations in Reasoning Models</div>
<div class="meta-line">Authors: Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin</div>
<div class="meta-line">First: 2026-02-04T18:34:50+00:00 · Latest: 2026-02-04T18:34:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04843v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04843v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理模型中的流体表征</div>
<div class="mono" style="margin-top:8px">推理语言模型在生成长链思维方面显著优于非推理语言模型，尤其在抽象问题上。然而，导致这种优越性能的内部模型机制仍然不甚了解。我们对QwQ-32B模型进行了机械分析，该模型专门训练以生成广泛的推理痕迹，处理抽象结构信息。在神秘积木世界这一语义模糊的规划领域中，我们发现QwQ-32B在推理过程中逐渐改善其对动作和概念的内部表征。该模型发展出关注结构而非具体动作名称的抽象编码。通过引导实验，我们建立了这些适应性改善问题解决的因果证据：从成功痕迹中注入精细表征提高了准确性，而符号表征可以以最小的性能损失替代许多模糊编码。我们发现，推动推理模型性能的因素之一是上下文中对标记表征的精细化，我们称之为流体推理表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand the internal mechanisms that enable reasoning language models, like QwQ-32B, to outperform non-reasoning models on abstract problems. The authors conducted a mechanistic analysis of QwQ-32B&#x27;s processing of abstract structural information in the Mystery Blocksworld planning domain. The key findings indicate that QwQ-32B enhances its internal representations of actions and concepts during reasoning, developing abstract encodings that prioritize structure over specific action names, and that refining these representations significantly improves problem-solving accuracy while allowing for the replacement of obfuscated encodings with symbolic representations with minimal performance loss.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解推理语言模型（如QwQ-32B）在抽象问题上优于非推理模型的内部机制。作者对QwQ-32B在神秘积木世界领域处理抽象结构信息进行了机械分析。关键发现表明，该模型在推理过程中增强了对动作和概念的内部表征，关注于与结构相关的抽象编码，而非具体动作。此外，研究表明，从成功的推理轨迹中提炼的表征可以提高问题解决的准确性，而符号表征可以有效替代许多模糊编码，且性能损失最小，突显了上下文中对标记表征的精细化的重要性，这被称为流体推理表征。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过人机协作的贝叶斯优化进行个性化图像生成</div>
<div class="mono" style="margin-top:8px">想象一下，爱丽丝心中有一幅特定的图像 $x^\ast$，比如她童年时成长的街道景象。为了生成那幅确切的图像，她通过多轮提示引导生成模型，最终得到了图像 $x^{p*}$。尽管 $x^{p*}$ 与 $x^\ast$ 相对接近，但爱丽丝发现使用语言提示很难缩小这个差距。本文旨在通过观察即使在语言达到极限后，人类仍能判断新图像 $x^+$ 是否比 $x^{p*}$ 更接近 $x^\ast$ 来缩小这一差距。基于这一观察，我们开发了 MultiBO（多选偏好贝叶斯优化），该方法根据 $x^{p*}$ 精心生成 $K$ 幅新图像，获取用户的偏好反馈，利用反馈指导扩散模型，最终生成一组新的 $K$ 幅图像。我们展示了在 $B$ 轮用户反馈内，即使生成模型对 $x^\ast$ 没有信息，也能更接近 $x^\ast$。来自 $30$ 位用户的定性评分，以及与 $5$ 个基线的定量指标比较，显示出良好的结果，表明人类的多选反馈可以有效地用于个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve personalized image generation by addressing the limitations of language prompts in conveying specific visual concepts. The authors propose a method called MultiBO (Multi-Choice Preferential Bayesian Optimization), which generates multiple images based on user feedback to iteratively refine the output. Experimental results demonstrate that after several rounds of user feedback, the generated images can significantly closer match the user&#x27;s desired image, even without direct information about the target image, as evidenced by qualitative assessments from 30 users and quantitative comparisons against five baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决语言提示在指导生成模型方面的局限性来改善个性化图像生成。作者提出了一种名为MultiBO（多选偏好贝叶斯优化）的方法，该方法根据用户反馈生成多个图像以优化输出。实验结果表明，在经过几轮用户反馈后，生成的图像可以显著接近用户所期望的图像，即使生成模型没有目标图像的先前知识，这一点通过30名用户的定性评估和与五种基线方法的定量比较得到了证实。</div>
</details>
</div>
<div class="card">
<div class="title">OverThink: Slowdown Attacks on Reasoning LLMs</div>
<div class="meta-line">Authors: Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian</div>
<div class="meta-line">First: 2025-02-04T18:12:41+00:00 · Latest: 2026-02-04T18:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02542v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.02542v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most flagship language models generate explicit reasoning chains, enabling inference-time scaling. However, producing these reasoning chains increases token usage (i.e., reasoning tokens), which in turn increases latency and costs. Our OverThink attack increases overhead for applications that rely on reasoning language models (RLMs) and external context by forcing them to spend substantially more reasoning tokens while still producing contextually correct answers. An adversary mounts an attack by injecting decoy reasoning problems into public content that is consumed by RLM at inference time. Because our decoys (e.g., Markov decision processes, Sudokus, etc.) are benign, they evade safety filters. We evaluate OverThink on both closed-source and open-source reasoning models across the FreshQA, SQuAD, and MuSR datasets. We also explore the attack in multi-modal settings by creating images that cause excessive reasoning. We show that the resulting slowdown transfers across models. Finally, we explore both LLM-based and systems-level defenses, and discuss the societal, financial, and energy implications of the OverThink attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过度思考：对推理大型语言模型的减速攻击</div>
<div class="mono" style="margin-top:8px">大多数旗舰语言模型生成明确的推理链，从而实现推理时的扩展。然而，生成这些推理链会增加令牌使用（即推理令牌），进而增加延迟和成本。我们的过度思考攻击通过迫使依赖推理语言模型（RLM）和外部上下文的应用程序消耗更多的推理令牌，同时仍然生成上下文正确的答案，增加了开销。对手通过在推理时被RLM消费的公共内容中注入诱饵推理问题来发起攻击。由于我们的诱饵（例如，马尔可夫决策过程、数独等）是良性的，因此它们能够逃避安全过滤器。我们在FreshQA、SQuAD和MuSR数据集上评估了过度思考对闭源和开源推理模型的影响。我们还通过创建导致过度推理的图像来探索多模态环境中的攻击。我们展示了结果减速在模型之间的转移。最后，我们探讨了基于LLM和系统级的防御，并讨论了过度思考攻击的社会、财务和能源影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the inefficiencies in reasoning language models (RLMs) that generate explicit reasoning chains, which increase token usage and latency. The authors introduce the OverThink attack, which involves injecting benign decoy reasoning problems into public content to force RLMs to use more reasoning tokens while still providing correct answers. Experimental results demonstrate that this attack effectively increases overhead across various closed-source and open-source models evaluated on datasets like FreshQA, SQuAD, and MuSR, and also in multi-modal settings, revealing that the slowdown effect transfers across different models and discussing potential defenses and broader implications of the attack.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于推理语言模型（RLM）因其显式推理链而导致的延迟和成本增加。作者提出了OverThink攻击，通过将无害的诱饵推理问题注入公共内容，迫使RLM在生成上下文正确的答案时使用更多的推理令牌。实验结果表明，该攻击有效地增加了闭源和开源推理模型的开销，评估数据集包括FreshQA、SQuAD和MuSR，并且显示减速效应在不同模型和模态之间转移，突显了重大的社会、经济和能源影响。</div>
</details>
</div>
<div class="card">
<div class="title">Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing</div>
<div class="meta-line">Authors: Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-04T18:29:36+00:00 · Latest: 2026-02-04T18:29:36+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04837v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04837v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体进化代理：通过经验共享实现开放式自我改进</div>
<div class="mono" style="margin-top:8px">开放式自我改进代理能够自主修改自身结构设计，以提升其能力并克服预定义架构的限制，从而减少对人类干预的依赖。我们介绍了群体进化代理（GEA），一种新的开放式自我改进范式，将一组代理视为基本的进化单元，允许在进化过程中在组内显式共享和重用经验。与现有的采用树状结构进化的开放式自我进化范式不同，GEA克服了因孤立进化分支导致的探索多样性利用效率低下的限制。我们在具有挑战性的编码基准上评估了GEA，其显著优于最先进的自我进化方法（在SWE-bench Verified上71.0%对56.7%，在Polyglot上88.3%对68.3%），并在两个基准上与顶级人类设计的代理框架相匹配或超越（分别为71.8%和52.0%）。分析表明，GEA更有效地将早期探索多样性转化为持续的长期进展，在相同数量的进化代理下实现更强的性能。此外，GEA在不同编码模型之间表现出一致的可转移性和更大的鲁棒性，平均在1.4次迭代中修复框架级错误，而自我进化方法则为5次。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop open-ended self-improving agents that can autonomously enhance their capabilities without heavy reliance on human intervention. The authors introduce Group-Evolving Agents (GEA), a novel approach that treats a group of agents as the primary unit of evolution, allowing for explicit experience sharing and reuse, which addresses inefficiencies found in traditional tree-structured evolution. Experimental results demonstrate that GEA significantly outperforms existing self-evolving methods on various coding benchmarks, achieving scores of 71.0% and 88.3% compared to 56.7% and 68.3% respectively, and shows comparable or superior performance to top human-designed frameworks, while also exhibiting better transferability and robustness in fixing bugs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发能够进行开放式自我改进的智能体，使其能够自主增强设计和能力，而不需要过多依赖人类输入。作者提出了群体进化智能体（GEA），这是一种新方法，将智能体群体视为进化的基本单元，促进智能体之间的明确经验共享和重用。实验结果表明，GEA在各种编码基准测试中显著优于现有的自我进化方法，准确率达到71.0%，而最先进的方法为56.7%，并且在不同编码模型之间的知识转移方面表现更好，同时在修复错误时表现出更大的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Grammatical Error Correction for Low-Resource Languages: The Case of Zarma</div>
<div class="meta-line">Authors: Mamadou K. Keita, Adwoa Bremang, Huy Le, Dennis Owusu, Christopher Homan, Marcos Zampieri</div>
<div class="meta-line">First: 2024-10-20T23:51:36+00:00 · Latest: 2026-02-04T18:29:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15539v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.15539v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grammatical error correction (GEC) aims to improve text quality and readability. Previous work on the task focused primarily on high-resource languages, while low-resource languages lack robust tools. To address this shortcoming, we present a study on GEC for Zarma, a language spoken by over five million people in West Africa. We compare three approaches: rule-based methods, machine translation (MT) models, and large language models (LLMs). We evaluated GEC models using a dataset of more than 250,000 examples, including synthetic and human-annotated data. Our results showed that the MT-based approach using M2M100 outperforms others, with a detection rate of 95.82% and a suggestion accuracy of 78.90% in automatic evaluations (AE) and an average score of 3.0 out of 5.0 in manual evaluation (ME) from native speakers for grammar and logical corrections. The rule-based method was effective for spelling errors but failed on complex context-level errors. LLMs -- Gemma 2b and MT5-small -- showed moderate performance. Our work supports use of MT models to enhance GEC in low-resource settings, and we validated these results with Bambara, another West African language.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低资源语言的语法错误纠正：扎尔马语的案例</div>
<div class="mono" style="margin-top:8px">语法错误纠正（GEC）旨在提高文本质量和可读性。以往的研究主要集中在高资源语言上，而低资源语言缺乏强大的工具。为了解决这一问题，我们对扎尔马语的GEC进行了研究，这是一种在西非有超过五百万人使用的语言。我们比较了三种方法：基于规则的方法、机器翻译（MT）模型和大型语言模型（LLMs）。我们使用超过250,000个示例的数据集评估了GEC模型，包括合成数据和人工标注数据。我们的结果显示，基于MT的方法使用M2M100的检测率为95.82%，建议准确率为78.90%（自动评估AE），并且在母语者的手动评估（ME）中，语法和逻辑纠正的平均得分为5分中的3.0。基于规则的方法对拼写错误有效，但在复杂的上下文错误上失败。LLMs -- Gemma 2b和MT5-small -- 显示出中等的表现。我们的工作支持在低资源环境中使用MT模型来增强GEC，并且我们用另一种西非语言班巴拉语验证了这些结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to improve grammatical error correction (GEC) tools for low-resource languages, specifically Zarma, which is spoken by over five million people in West Africa. The researchers compared three approaches: rule-based methods, machine translation (MT) models, and large language models (LLMs), using a dataset of over 250,000 examples that included both synthetic and human-annotated data. The findings revealed that the MT-based approach using M2M100 outperformed the other methods, achieving a detection rate of 95.82% and a suggestion accuracy of 78.90% in automatic evaluations, along with an average score of 3.0 out of 5.0 in manual evaluations from native speakers, while rule-based methods were limited to spelling corrections and LLMs demonstrated moderate performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善低资源语言的语法错误纠正（GEC），特别是扎尔马语，该语言在西非有超过五百万人使用。作者比较了三种方法：基于规则的方法、机器翻译（MT）模型和大型语言模型（LLMs），使用了一个包含超过250,000个示例的数据集，其中包括合成和人工标注的数据。研究结果表明，使用M2M100的MT方法优于其他方法，在自动评估中实现了95.82%的检测率和78.90%的建议准确率，在母语者的手动评估中平均得分为3.0（满分5.0），而基于规则的方法仅限于拼写错误，LLMs表现中等。</div>
</details>
</div>
<div class="card">
<div class="title">Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</div>
<div class="meta-line">Authors: Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, Xiao-Yu Zhang</div>
<div class="meta-line">First: 2025-10-10T04:39:57+00:00 · Latest: 2026-02-04T18:29:24+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09712v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09712v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving threat landscape necessitates detection systems that simultaneously prioritize predictive accuracy and structural robustness. However, current detectors often fail to generalize across diverse and novel comment attack patterns. To bridge this gap, we propose AdComment, an adaptive adversarial training framework for robustness enhancement against diverse malicious comments. Based on cognitive psychology, we categorize adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and leverage LLMs to synthesize diverse, category-specific perturbations. Central to our framework is an InfoDirichlet Resampling (IDR) mechanism that dynamically adjusts malicious comment proportions during training, thereby steering optimization toward the model&#x27;s most susceptible regions. Experimental results demonstrate that our approach achieves state-of-the-art performance on three benchmark datasets, improving the F1 scores by 17.9%, 14.5% and 9.0%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对恶意评论的鲁棒假新闻检测的群体自适应对抗学习</div>
<div class="mono" style="margin-top:8px">在线假新闻深刻扭曲公众判断，侵蚀社交平台的信任。尽管现有检测器在基准数据集上表现出色，但它们仍然对专门设计的恶意评论特别脆弱，这些评论旨在诱导错误分类。这种不断演变的威胁环境需要同时优先考虑预测准确性和结构鲁棒性的检测系统。然而，当前的检测器往往无法在多样化和新颖的评论攻击模式中进行泛化。为了解决这一问题，我们提出了AdComment，一种针对多样恶意评论的鲁棒性增强的自适应对抗训练框架。基于认知心理学，我们将对抗评论分类为事实扭曲、逻辑混淆和情感操控，并利用大型语言模型合成多样的、特定类别的扰动。我们框架的核心是信息Dirichlet重采样（IDR）机制，该机制在训练过程中动态调整恶意评论的比例，从而引导优化朝向模型最易受攻击的区域。实验结果表明，我们的方法在三个基准数据集上实现了最先进的性能，F1分数分别提高了17.9%、14.5%和9.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing vulnerability of fake news detection systems to malicious comments that can lead to misclassification, which undermines public trust in social platforms. The authors propose AdComment, an adaptive adversarial training framework that enhances robustness against various types of malicious comments categorized into Fact Distortion, Logical Confusion, and Emotional Manipulation, utilizing large language models to generate specific perturbations. The experimental results indicate that this method achieves state-of-the-art performance on three benchmark datasets, with improvements in F1 scores of 17.9%, 14.5%, and 9.0%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机是在线假新闻的日益普遍以及现有检测系统对恶意评论的脆弱性，这些评论可能导致错误分类。作者提出了AdComment，这是一种自适应对抗训练框架，旨在增强对各种恶意评论的鲁棒性，通过将其分类为事实扭曲、逻辑混淆和情感操控，并利用大型语言模型生成特定的扰动。关键实验结果表明，该方法在三个基准数据集上实现了最先进的性能，F1分数分别提高了17.9%、14.5%和9.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</div>
<div class="meta-line">Authors: Haosen Ge, Hamsa Bastani, Osbert Bastani</div>
<div class="meta-line">First: 2026-02-04T18:28:49+00:00 · Latest: 2026-02-04T18:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04836v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04836v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation &amp; Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能能力是否在指数增长？一个竞争假设</div>
<div class="mono" style="margin-top:8px">快速增长的人工智能能力对现实世界产生了重大影响，从人工智能安全问题到劳动市场后果不一。模型评估与威胁研究（METR）报告认为，自2019年以来，人工智能能力表现出指数增长。在这篇文章中，我们认为数据并不支持指数增长，即使在短期内也是如此。虽然METR研究声称拟合S型/逻辑曲线会导致未来很远的拐点，但我们对他们当前的数据拟合了S型曲线，发现拐点已经过去。此外，我们提出了一个更复杂的模型，将人工智能能力分解为基础能力和推理能力，展现各自的改进速率。我们证明该模型支持我们的假设，即人工智能能力将在不久的将来表现出拐点。我们的目标不是建立我们自己的严格预测，而是强调现有指数增长预测的脆弱性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to critically assess the claim that AI capabilities have been growing exponentially, as suggested by the Model Evaluation &amp; Threat Research (METR) report, which has implications for AI safety and labor markets. The authors employed a sigmoid curve fitting approach to analyze the METR data and found that the predicted inflection point for exponential growth has already occurred, contrary to METR&#x27;s assertions. Additionally, they introduced a more nuanced model that separates AI capabilities into base and reasoning components, each with distinct improvement rates, supporting their hypothesis that AI capabilities will reach an inflection point soon, thereby emphasizing the uncertainty in current exponential growth forecasts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于批判性地评估人工智能能力是否正在经历指数增长的说法，这对人工智能安全和劳动市场具有重要影响。作者采用模型拟合方法，特别是对模型评估与威胁研究（METR）报告中的数据应用了S型曲线，并提出了一种更复杂的模型，将基础能力和推理能力分开。其主要发现表明，人工智能能力的拐点已经到达，这与METR报告中对未来拐点的主张相矛盾，并强调了当前对人工智能能力指数增长预测的不确定性。</div>
</details>
</div>
<div class="card">
<div class="title">It&#x27;s not a Lottery, it&#x27;s a Race: Understanding How Gradient Descent Adapts the Network&#x27;s Capacity to the Task</div>
<div class="meta-line">Authors: Hannah Pinson</div>
<div class="meta-line">First: 2026-02-04T18:22:40+00:00 · Latest: 2026-02-04T18:22:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>这不是彩票，而是一场比赛：理解梯度下降如何将网络的容量适应于任务</div>
<div class="mono" style="margin-top:8px">我们对神经网络的理论理解滞后于其经验成功。一个重要的未解现象是，在使用梯度下降进行训练的过程中，神经网络的理论容量为何以及如何被减少到适合任务的有效容量。我们通过分析单隐层ReLU网络中单个神经元的学习动态，研究了梯度下降实现这一目标的机制。我们识别出三个动态原则——相互对齐、解锁和竞赛——它们共同解释了为什么我们在训练后通常能够通过合并等效神经元或修剪低范数权重成功减少容量。我们特别解释了彩票票据猜想背后的机制，即为什么某些神经元的特定、有益的初始条件使它们获得更高的权重范数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the theoretical understanding of neural networks, particularly regarding how gradient descent reduces their theoretical capacity to an effective capacity suited for specific tasks. The authors employ an analysis of learning dynamics at the level of individual neurons in single hidden layer ReLU networks to investigate this mechanism. They identify three key dynamical principles—mutual alignment, unlocking, and racing—that explain the successful reduction of capacity post-training, including the merging of equivalent neurons and the pruning of low norm weights, while also elucidating the mechanism behind the lottery ticket conjecture related to beneficial initial conditions of certain neurons.</div>
<div class="mono" style="margin-top:8px">本研究解决了神经网络理论理解的不足，特别是梯度下降如何将其理论容量降低到适合特定任务的有效容量。作者分析了单隐藏层ReLU网络中单个神经元的学习动态，以揭示其中的机制。他们识别出三个关键的动态原则——相互对齐、解锁和竞赛，解释了训练后成功降低容量的原因，包括合并等效神经元和修剪低范数权重，同时阐明了彩票票据猜想，即某些神经元的有利初始条件导致更高的权重范数。</div>
</details>
</div>
<div class="card">
<div class="title">Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</div>
<div class="meta-line">Authors: Deepit Sapru</div>
<div class="meta-line">First: 2025-12-22T19:02:09+00:00 · Latest: 2026-02-04T18:18:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19805v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19805v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a marketing decision framework that optimizes customer targeting by integrating heterogeneous treatment effect estimation with explicit business guardrails. The objective is to maximize revenue and retention while adhering to constraints such as budget, revenue protection, and customer experience. The framework first estimates Conditional Average Treatment Effects (CATE) using uplift learners, then solves a constrained allocation problem to decide whom to target and which offer to deploy. It supports decisions in retention messaging, event rewards, and spend-threshold assignment. Validated through offline simulations and online A/B tests, the approach consistently outperforms propensity and static baselines, offering a reusable playbook for causal targeting at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有护栏的提升目标定位：营销策略的因果优化手册</div>
<div class="mono" style="margin-top:8px">本文介绍了一种营销决策框架，通过将异质性处理效应估计与明确的业务护栏相结合，优化客户定位。其目标是在遵循预算、收入保护和客户体验等约束的同时，最大化收入和客户保留。该框架首先使用提升学习器估计条件平均处理效应（CATE），然后解决一个受限分配问题，以决定目标客户和部署的优惠。它支持在客户保留信息、事件奖励和消费阈值分配中的决策。通过离线模拟和在线A/B测试验证，该方法始终优于倾向性和静态基线，为大规模因果定位提供了可重用的手册。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a marketing decision framework that effectively optimizes customer targeting while adhering to business constraints. The authors propose a method that integrates heterogeneous treatment effect estimation with explicit guardrails, estimating Conditional Average Treatment Effects (CATE) using uplift learners and solving a constrained allocation problem for targeting decisions. Experimental results from offline simulations and online A/B tests demonstrate that this approach consistently outperforms traditional propensity and static baselines, providing a scalable playbook for causal targeting in marketing strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过优化客户定位来增强营销策略，同时遵循特定的业务约束。作者开发了一个决策框架，该框架利用提升学习器估计条件平均处理效应（CATE），并解决一个受限分配问题，以确定目标受众和适当的优惠。离线模拟和在线A/B测试的实验结果表明，该方法始终优于传统的倾向性和静态基线，为营销中的有效因果定位提供了可扩展的操作手册。</div>
</details>
</div>
<div class="card">
<div class="title">Domain Generalization Under Posterior Drift</div>
<div class="meta-line">Authors: Yilun Zhu, Naihao Deng, Naichen Shi, Aditya Gangrade, Clayton Scott</div>
<div class="meta-line">First: 2025-10-06T02:17:12+00:00 · Latest: 2026-02-04T18:18:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04441v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. For the prevailing benchmark datasets in DG, there exists a single classifier that performs well across all domains.
  In this work, we study a fundamentally different regime where the domains satisfy a \emph{posterior drift} assumption, in which the optimal classifier might vary substantially with domain. We establish a decision-theoretic framework for DG under posterior drift, and investigate the practical implications of this framework through experiments on language and vision tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后验漂移下的领域泛化</div>
<div class="mono" style="margin-top:8px">领域泛化（DG）是从多个分布（或领域）中进行泛化的问题，这些分布有标记的训练数据，而新的测试领域没有标记数据。对于DG中流行的基准数据集，存在一个在所有领域中表现良好的单一分类器。本文研究了一种根本不同的情况，其中领域满足\emph{后验漂移}假设，最优分类器可能因领域而有显著变化。我们建立了一个后验漂移下的DG决策理论框架，并通过语言和视觉任务的实验研究该框架的实际意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of domain generalization (DG) in scenarios where the optimal classifier varies significantly across different domains due to a posterior drift assumption. The authors establish a decision-theoretic framework for DG under this assumption and conduct experiments on language and vision tasks to explore its practical implications. The key findings reveal that the performance of classifiers can differ markedly across domains, highlighting the need for tailored approaches in DG settings where posterior drift is present.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在后验漂移情况下，最优分类器在不同领域之间显著变化的领域泛化（DG）挑战。作者建立了一个决策理论框架来分析在这一假设下的DG，并在语言和视觉任务上进行实验以探讨其实际意义。主要发现表明，该框架为不同领域中分类器的性能提供了见解，突显了后验漂移在DG场景中引入的复杂性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
