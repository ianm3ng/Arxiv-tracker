<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 03:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0335</div>
    <div class="row"><div class="card">
<div class="title">Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models</div>
<div class="meta-line">Authors: Zahraa Al Sahili, Ioannis Patras, Matthew Purver</div>
<div class="meta-line">First: 2025-01-22T21:08:30+00:00 · Latest: 2026-01-23T15:12:35+00:00</div>
<div class="meta-line">Comments: Published at TMLR; updated version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.13223v7">Abs</a> · <a href="https://arxiv.org/pdf/2501.13223v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors -- model size, training-data scale, and training-data source -- by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M/2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies -- Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据至关重要：对比视觉语言模型中的社会偏见审计</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在零样本识别中表现强劲，但经常从训练数据中继承社会偏见。我们通过比较CLIP和OpenCLIP这两个共享相同对比目标但在编码器宽度和预训练图像-文本语料库（400M专有对比400M/2B LAION）上存在差异的模型，系统性地解开三个设计因素——模型大小、训练数据规模和训练数据来源。在平衡的面部分析基准测试中，增大编码器在CLIP中减少了性别偏差，但在OpenCLIP中加剧了性别和种族偏差；将LAION语料库从400M增加到2B进一步增加了OpenCLIP的偏见。在匹配的模型和数据预算下，用LAION替代专有数据提高了性别公平性，同时增加了种族偏差，强调数据来源是偏见模式的主要驱动因素。我们还评估了三种后期测试去偏见策略——偏见提示、提示数组和SANER。去偏见减少了但并未消除伤害，其有效性依赖于来源和规模：在较小模型规模下，偏见提示最有效地减少了CLIP中的性别偏差，而提示数组和SANER更可靠地减少了OpenCLIP中的种族偏差；扩展LAION重新配置了最公平的方法。综合来看，这些发现挑战了更大模型或数据集自动更公平的假设，并突出了训练数据来源作为偏见和缓解有效性的关键决定因素。我们发布了代码和评估脚本，以便对未来的VLM进行透明、可重复的审计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the social biases present in vision-language models (VLMs), motivated by their strong zero-shot recognition capabilities yet problematic bias inheritance from training data. The study systematically examines the effects of model size, training-data scale, and training-data source by comparing CLIP and OpenCLIP, which differ in encoder width and training datasets. Key findings reveal that increasing encoder size in CLIP reduces gender bias but exacerbates both gender and racial bias in OpenCLIP, while expanding the LAION dataset from 400M to 2B worsens bias. Additionally, three debiasing strategies were tested, showing that while they can reduce bias, their effectiveness varies based on the model and data source, highlighting the importance of training data in determining bias and mitigation outcomes.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLM）中存在的社会偏见，动机在于这些模型在零样本识别方面表现出色，但常常从训练数据中继承偏见。研究系统地分析了模型大小、训练数据规模和训练数据来源的影响，通过比较CLIP和OpenCLIP这两个在编码器宽度和预训练数据集上存在差异的模型。研究结果表明，在CLIP中增加编码器大小可以减少性别偏见，但在OpenCLIP中则会加剧性别和种族偏见，而将LAION数据集从4亿扩展到20亿则进一步加重了OpenCLIP的偏见。此外，评估了多种去偏见策略，结果显示虽然这些策略可以减少偏见，但其有效性因模型和数据来源而异，强调了训练数据在决定偏见和缓解效果中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis</div>
<div class="meta-line">Authors: Yinqi Cai, Jichang Li, Zhaolun Li, Weikai Chen, Rushi Lan, Xi Xie, Xiaonan Luo, Guanbin Li</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-10-29T07:35:29+00:00 · Latest: 2026-01-23T14:32:04+00:00</div>
<div class="meta-line">Comments: ICCV 2025. Code is available at https://github.com/lijichang/DeepShield</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25237v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25237v2">PDF</a> · <a href="https://github.com/lijichang/DeepShield">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks. Code is available at https://github.com/lijichang/DeepShield.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepShield：通过局部和全局伪造分析强化深度伪造视频检测</div>
<div class="mono" style="margin-top:8px">最近深度生成模型的进展使得操纵人脸视频变得更加容易，这引发了对其潜在用于欺诈和错误信息的重大担忧。现有检测器在领域内场景中表现良好，但由于依赖于特定伪造伪影，无法在多样化的操纵技术中进行泛化。在本研究中，我们介绍了DeepShield，这是一种新颖的深度伪造检测框架，平衡局部敏感性和全局泛化，以提高对未见伪造的鲁棒性。DeepShield通过两个关键组件增强CLIP-ViT编码器：局部补丁引导（LPG）和全局伪造多样化（GFD）。LPG应用时空伪影建模和补丁级监督，以捕捉全球模型常常忽视的细粒度不一致性。GFD引入领域特征增强，利用领域桥接和边界扩展特征生成合成多样化伪造，减轻过拟合并增强跨领域适应性。通过整合新颖的局部和全局分析进行深度伪造检测，DeepShield在跨数据集和跨操纵评估中超越了最先进的方法，实现了对未见深度伪造攻击的卓越鲁棒性。代码可在 https://github.com/lijichang/DeepShield 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing ease of manipulating face videos using deep generative models, which raises concerns about their misuse in fraud and misinformation. The authors propose DeepShield, a deepfake detection framework that enhances robustness by integrating local sensitivity and global generalization. The method includes Local Patch Guidance (LPG) for capturing fine-grained inconsistencies and Global Forgery Diversification (GFD) for synthesizing diverse forgeries, which improves adaptability across different manipulation techniques. Experimental results demonstrate that DeepShield outperforms existing state-of-the-art methods in cross-dataset and cross-manipulation evaluations, showing superior robustness against unseen deepfake attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是深度生成模型使得操纵人脸视频变得更加容易，这对欺诈和错误信息构成了风险。作者提出了DeepShield，一个通过结合局部敏感性和全局泛化来增强鲁棒性的深度伪造检测框架。关键实验结果表明，DeepShield通过其局部补丁引导和全局伪造多样化组件，在跨数据集和跨操纵评估中显著优于现有的最先进方法，显示出对未见深度伪造攻击的更好适应性。</div>
</details>
</div>
<div class="card">
<div class="title">LATTLE: LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains</div>
<div class="meta-line">Authors: Ibna Kowsar, Kazi F. Akhter, Manar D. Samad</div>
<div class="meta-line">First: 2025-11-08T23:05:31+00:00 · Latest: 2026-01-23T13:15:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06161v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06161v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transfer learning on tabular data is challenging due to disparate feature spaces across domains, in contrast to the homogeneous structures of image and text. Large language models (LLMs) offer a knowledge base to improve the limited effectiveness of cross-domain transfer learning for tabular data. However, LLM performance often stagnates due to subjective text prompts and the computational limitations of in-context learning. We present a novel language-to-tabular context-learning method that uses attention-specific transformer weights, enabling seamless transfer learning across disparate tabular data sets. The LLM attention transplant mechanism facilitates a domain-agnostic transfer learning, eliminating the need for shared features between tables, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of disjoint source-target data sets and 12 baseline methods demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and models trained on thousands to billions of tabular samples. The proposed cross-domain attention transfer demonstrates an effective solution for adapting LLMs to learning non-text tabular data in a low-resource environment. The source code of the LATTLE implementation is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LATTLE：用于跨不同领域表格数据迁移学习的LLM注意力移植</div>
<div class="mono" style="margin-top:8px">由于不同领域之间特征空间的差异，表格数据的迁移学习面临挑战，这与图像和文本的同质结构形成对比。大型语言模型（LLMs）提供了一个知识基础，以提高跨领域表格数据迁移学习的有限效果。然而，由于主观文本提示和上下文学习的计算限制，LLM的性能往往停滞不前。我们提出了一种新颖的语言到表格上下文学习方法，使用特定于注意力的变换器权重，实现跨不同表格数据集的无缝迁移学习。LLM注意力移植机制促进了领域无关的迁移学习，消除了表格之间共享特征、LLM提示工程和大规模预训练模型的需求。我们使用十对不相交的源-目标数据集和12种基线方法的实验表明，所提出的LLM注意力移植迁移学习（LATTLE）方法优于传统机器学习模型、最先进的深度表格架构以及在数千到数十亿个表格样本上训练的模型。所提出的跨领域注意力转移展示了在低资源环境中将LLM适应于学习非文本表格数据的有效解决方案。LATTLE实现的源代码已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of transfer learning on tabular data, which often involves disparate feature spaces across different domains, unlike the more uniform structures found in image and text data. The authors introduce a novel method called LATTLE, which employs attention-specific transformer weights to facilitate domain-agnostic transfer learning without the need for shared features or extensive prompt engineering. Experimental results show that LATTLE significantly outperforms traditional machine learning models and state-of-the-art deep tabular architectures across ten pairs of disjoint source-target datasets, demonstrating its effectiveness in adapting large language models to tabular data in low-resource settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决表格数据迁移学习中的挑战，这通常涉及不同领域之间的特征空间差异，而与图像和文本数据中更统一的结构不同。作者提出了一种名为LATTLE的新方法，该方法利用特定于注意力的变换器权重来促进领域无关的迁移学习，无需共享特征或广泛的提示工程。实验结果表明，LATTLE在十对不相交的源-目标数据集上优于传统机器学习模型和最先进的深度表格架构，证明了其在低资源环境中将大型语言模型适应于非文本表格数据的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</div>
<div class="meta-line">Authors: Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-23T11:06:51+00:00 · Latest: 2026-01-23T11:06:51+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16645v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16645v1">PDF</a> · <a href="https://github.com/gongms00/SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model&#x27;s generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过具有新颖结构保留损失的扩散模型进行边缘感知图像处理</div>
<div class="mono" style="margin-top:8px">最近的图像编辑进展利用潜在扩散模型（LDMs）进行多样化、基于文本提示的编辑。然而，保持像素级边缘结构——对于如照片真实感风格转移或图像色调调整等任务至关重要——仍然是潜在扩散编辑的一大挑战。为克服这一限制，我们提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异。我们的无训练方法将SPL直接集成到扩散模型的生成过程中，以确保结构的保真性。该核心机制通过后处理步骤来减轻LDM解码失真、精确编辑定位的掩蔽策略以及保留未编辑区域色调的颜色保留损失进行补充。实验确认SPL增强了结构保真性，在基于潜在扩散的图像编辑中实现了最先进的性能。我们的代码将在https://github.com/gongms00/SPL公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of maintaining pixel-level edge structures in image editing using latent diffusion models (LDMs), which is essential for tasks like photorealistic style transfer. The authors propose a novel Structure Preservation Loss (SPL) that quantifies structural differences between input and edited images using local linear models, integrating this loss into the generative process of the diffusion model without requiring additional training. Experimental results demonstrate that the SPL significantly enhances structural fidelity in image editing, achieving state-of-the-art performance in latent-diffusion-based methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用潜在扩散模型（LDM）进行图像编辑时保持像素级边缘结构的挑战，这对于如逼真风格转移等任务至关重要。作者提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异，并将该损失集成到扩散模型的生成过程中，无需训练。实验结果表明，SPL显著增强了结构保真度，在基于潜在扩散的图像编辑任务中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval</div>
<div class="meta-line">Authors: Hongyu Guo, Xiangzhao Hao, Jiarui Guo, Haiyun Guo, Jinqiao Wang, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-08-06T07:02:39+00:00 · Latest: 2026-01-23T10:16:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04136v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.04136v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniFGVC：通过属性感知多模态检索实现通用无训练少样本细粒度视觉分类</div>
<div class="mono" style="margin-top:8px">少样本细粒度视觉分类（FGVC）旨在利用有限数据使模型能够区分微妙的类别差异。近期的研究大多对预训练的视觉语言模型进行了微调以提高性能，但却遭遇了过拟合和弱泛化的问题。为了解决这个问题，我们提出了UniFGVC，一个通用的无训练框架，将少样本FGVC重新表述为多模态检索。首先，我们提出了类别区分视觉描述生成器（CDV-Captioner），利用多模态大型语言模型（MLLMs）的开放世界知识生成结构化文本描述，以捕捉区分密切相关类别的细粒度属性特征。CDV-Captioner使用思维链提示和视觉相似的参考图像来减少幻觉并增强生成描述的区分性。通过它，我们可以将每个图像转换为图像-描述对，从而实现更全面的特征表示，并使用少样本构建多模态类别模板以供后续检索管道使用。然后，现成的视觉和文本编码器嵌入查询和模板对，FGVC通过在联合空间中检索最近的模板来完成。UniFGVC确保与多样的MLLMs和编码器广泛兼容，提供可靠的泛化和适应性，适用于少样本FGVC场景。在12个FGVC基准上的广泛实验表明，它在性能上始终优于先前的少样本基于CLIP的方法，甚至优于一些完全监督的基于MLLMs的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve few-shot fine-grained visual classification (FGVC) by addressing issues of overfitting and weak generalization in existing methods that rely on fine-tuning pre-trained visual language models. The authors introduce UniFGVC, a training-free framework that reformulates FGVC as multimodal retrieval, utilizing a Category-Discriminative Visual Captioner (CDV-Captioner) to generate structured text descriptions that highlight distinguishing attributes of closely related classes. Experimental results across 12 FGVC benchmarks indicate that UniFGVC consistently outperforms previous few-shot CLIP-based methods and several fully-supervised MLLMs-based approaches, demonstrating its effectiveness and adaptability in various few-shot scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决现有微调预训练视觉语言模型方法中的过拟合和弱泛化问题，来改善少样本细粒度视觉分类（FGVC）。作者提出了UniFGVC，这是一个无训练的框架，将FGVC重新构建为多模态检索，利用类别区分视觉描述生成器（CDV-Captioner）生成结构化文本描述，突出密切相关类别的区分属性。通过在12个FGVC基准上的实验结果表明，UniFGVC在性能上始终优于先前的少样本CLIP方法和几种完全监督的多模态语言模型方法，展示了其在各种少样本场景中的有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding</div>
<div class="meta-line">Authors: Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu, Yifei Dong, Shuyuan Tu, Qiyu Hu, Huiting Huang, Yuxiang Lin, Jun-Yan He, Kai Wang, Zheng Lian, Zhi-Qi Cheng</div>
<div class="meta-line">First: 2026-01-23T05:02:43+00:00 · Latest: 2026-01-23T05:02:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16449v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Emotion-LLaMAv2 和 MMEVerse：多模态情感理解的新框架和基准</div>
<div class="mono" style="margin-top:8px">从多模态信号理解人类情感在情感计算和人机交互中面临重大挑战。尽管多模态大型语言模型（MLLMs）在一般视觉-语言任务中表现出色，但它们在情感推理方面的能力仍然有限。该领域目前缺乏高质量、描述性情感注释的大规模数据集，并且缺乏标准化的评估基准。我们的初步框架Emotion-LLaMA开创了情感推理的指令调优多模态学习，但受到显式人脸检测、隐式融合策略和低质量训练数据（规模有限）的限制。为了解决这些局限性，我们提出了Emotion-LLaMAv2和MMEVerse基准，建立了一个端到端的管道以及情感识别和推理的标准化评估设置。Emotion-LLaMAv2引入了三个关键进展。首先，端到端的多视角编码器消除了外部人脸检测，通过更丰富的空间和时间多视角标记捕捉细微的情感线索。其次，设计了一个卷积注意力预融合模块，以便在LLM主干外实现局部和全局多模态特征的同时交互。第三，在LLaMA2主干内的感知到认知课程指令调优方案统一了情感识别和自由形式的情感推理。为了支持大规模训练和可重复评估，MMEVerse将包括IEMOCAP、MELD、DFEW和MAFW在内的十二个公开可用的情感数据集聚合为统一的多模态指令格式。数据通过涉及Qwen2 Audio、Qwen2.5 VL和GPT 4o的多代理管道重新注释，生成了130k个训练片段和36k个测试片段，涵盖18个评估基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the understanding of human emotions from multimodal signals, addressing the limitations of existing multimodal large language models in emotional reasoning and the lack of high-quality datasets and standardized benchmarks. The authors developed Emotion-LLaMAv2 and the MMEVerse benchmark, which includes an end-to-end pipeline for emotion recognition and reasoning, featuring a multiview encoder that captures emotional cues without external face detection, a Conv Attention pre-fusion module for multimodal feature interactions, and a curriculum instruction tuning scheme for unified emotion processing. Key findings indicate that the new framework supports large-scale training with 130k training clips and 36k testing clips, significantly improving the evaluation of emotional understanding across 18 benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了从多模态信号理解人类情感的挑战，这对情感计算和人机交互至关重要。作者开发了Emotion-LLaMAv2和MMEVerse基准，以增强多模态大语言模型（MLLMs）在情感推理方面的能力，提出了端到端的多视角编码器、卷积注意力预融合模块和感知到认知的课程指令调优方案。实验结果表明，这些创新的有效性，得益于130,000个训练片段和36,000个测试片段的大规模数据集，使得在18个基准上进行标准化评估成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose</div>
<div class="meta-line">Authors: Jongmin Yu, Hyeontaek Oh, Zhongtian Sun, Angelica I Aviles-Rivero, Moongu Jeon, Jinhong Yang</div>
<div class="meta-line">First: 2026-01-23T04:01:49+00:00 · Latest: 2026-01-23T04:01:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16429v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16429v1">PDF</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git">Code1</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git&amp;#39">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git&#x27;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaFace：高保真且实时的面部交换器，能够适应面部姿态</div>
<div class="mono" style="margin-top:8px">现有的面部交换方法在受限环境中通常能提供竞争性结果，但在处理极端面部姿态时质量显著下降。为了提高面部姿态的鲁棒性，应用了显式几何特征，但这种方法仍然存在问题，因为它引入了额外的依赖关系并增加了计算成本。基于扩散的方法取得了显著成果；然而，它们在实时处理方面不切实际。我们引入了AlphaFace，它利用开源的视觉-语言模型和CLIP图像与文本嵌入，应用新颖的视觉和文本语义对比损失。AlphaFace实现了更强的身份表示和更精确的属性保留，同时保持实时性能。在FF++、MPIE和LPFF上的全面实验表明，AlphaFace在姿态挑战案例中超越了最先进的方法。该项目在`https://github.com/andrewyu90/Alphaface_Official.git`上公开可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to address the limitations of existing face-swapping methods that struggle with extreme facial poses, leading to quality degradation. The authors introduce AlphaFace, which utilizes an open-source vision-language model and CLIP image and text embeddings, applying novel visual and textual semantic contrastive losses to enhance identity representation and attribute preservation while ensuring real-time performance. Experimental results across datasets such as FF++, MPIE, and LPFF indicate that AlphaFace outperforms state-of-the-art methods in scenarios involving challenging poses.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有换脸方法在面对极端面部姿势时质量下降的问题。作者提出了AlphaFace，这是一种利用开源视觉-语言模型以及CLIP图像和文本嵌入的方法，实施新颖的视觉和文本语义对比损失。实验结果表明，AlphaFace在具有挑战性的姿势场景中优于最先进的技术，同时保持实时性能，这通过在FF++、MPIE和LPFF等数据集上的全面测试得以证明。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction</div>
<div class="meta-line">Authors: Shuang Wu, Meijie Wang, Lun Yu</div>
<div class="meta-line">First: 2026-01-23T03:51:19+00:00 · Latest: 2026-01-23T03:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16426v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model&#x27;s out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **&quot;safe multitask&quot;** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全的多任务分子图网络用于蒸汽压和气味阈值预测</div>
<div class="mono" style="margin-top:8px">我们研究了与气味相关的属性建模中的两个重要任务：蒸汽压（VP）和气味阈值（OP）。为了评估模型的分布外（OOD）能力，我们采用了Bemis-Murcko支架分割。在特征方面，我们引入了丰富的A20/E17分子图特征（20维原子特征 + 17维键特征），并系统地比较了GINE和PNA骨干网络。结果显示：对于VP，PNA与简单回归头的验证均方误差约为0.21（归一化空间）；对于在相同支架分割下的OP单任务，使用A20/E17和稳健训练（Huber/winsor）实现了验证均方误差约为0.60-0.61。对于多任务训练，我们提出了一种**“安全多任务”**方法：将VP作为主要任务，将OP作为辅助任务，使用延迟激活 + 梯度裁剪 + 小权重，避免对主要任务造成损害，同时实现最佳的VP泛化性能。本文提供了完整的可重复实验、消融研究和误差相似性分析，同时讨论了数据噪声和方法限制的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges in modeling odor-related properties, specifically Vapor Pressure (VP) and Odor Threshold (OP). The authors utilize a novel approach that incorporates A20/E17 molecular graph features and compare the performance of GINE and PNA network architectures, employing the Bemis-Murcko scaffold split to assess out-of-distribution capabilities. Key findings indicate that the PNA model with a simple regression head achieves a validation mean squared error of approximately 0.21 for VP, while the OP task reaches a validation mean squared error of about 0.60-0.61 using robust training methods. Additionally, a &#x27;safe multitask&#x27; training strategy is proposed, which prioritizes VP while effectively incorporating OP, leading to improved generalization performance for VP without compromising the primary task.</div>
<div class="mono" style="margin-top:8px">本研究针对建模气味相关属性中的挑战，特别是蒸汽压（VP）和气味阈值（OP），通过使用Bemis-Murcko支架划分评估模型的分布外能力。研究引入了一套全面的分子图特征，并比较了GINE和PNA架构的性能。结果表明，PNA模型在简单回归头下对VP的验证均方误差（MSE）约为0.21，而在使用稳健训练技术的情况下，OP任务的验证MSE约为0.60-0.61。此外，提出了一种“安全多任务”方法，将VP作为主要任务，从而在不影响主要任务准确性的情况下提高了泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍然是盲点。目前的VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器。训练方案通常将图像展平为一维补丁序列，忽略了进行空间推理所需的二维结构。我们认为，这种缺乏空间意识是VLM设计中的一个缺失维度，也是需要空间基础的应用（如机器人技术和具身人工智能）的瓶颈。为了解决这个问题，我们研究了（i）使用替代目标训练的图像编码器和（ii）二维位置编码。我们的实验表明，这些架构选择可以在多个基准上改善空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of vision-language models (VLMs) in capturing spatial relationships, which is crucial for applications like robotics and embodied AI. The authors propose a solution by exploring alternative training objectives for image encoders and incorporating 2D positional encodings to enhance spatial awareness. Experimental results demonstrate that these modifications significantly improve spatial reasoning performance across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLMs）在捕捉空间关系方面的局限性，这对机器人技术和具身人工智能等应用至关重要。作者提出研究图像编码器的替代训练目标以及引入二维位置编码，以增强空间意识。实验结果表明，这些架构修改显著提高了多个基准测试中的空间推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</div>
<div class="meta-line">Authors: Zequn Xie, Xin Liu, Boyun Zhang, Yuxiao Lin, Sihang Cai, Tao Jin</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-22T17:57:42+00:00 · Latest: 2026-01-22T17:57:42+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16155v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &quot;blind&quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HVD：基于人类视觉驱动的视频表示学习用于文本-视频检索</div>
<div class="mono" style="margin-top:8px">CLIP的成功推动了文本-视频检索的重大进展。然而，当前的方法往往遭受“盲目”特征交互的困扰，模型难以从背景噪声中辨别关键信息，因为文本查询的稀疏性。为了解决这一问题，我们从人类认知行为中获得灵感，提出了人类视觉驱动（HVD）模型。我们的框架建立了一个粗到细的对齐机制，包括两个关键组件：帧特征选择模块（FFSM）和补丁特征压缩模块（PFCM）。FFSM通过选择关键帧来消除时间冗余，模拟人类的宏观感知能力。随后，PFCM通过先进的注意机制将补丁特征聚合成显著的视觉实体，模拟微观感知，实现精确的实体级匹配。在五个基准上的大量实验表明，HVD不仅捕捉到类人视觉焦点，还达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of current text-video retrieval methods, which often fail to effectively differentiate important visual information from background noise due to sparse textual queries. To address this issue, the authors propose the Human Vision-Driven (HVD) model, which incorporates a coarse-to-fine alignment mechanism featuring a Frame Features Selection Module (FFSM) and a Patch Features Compression Module (PFCM). FFSM selects key frames to reduce temporal redundancy, while PFCM aggregates patch features into salient visual entities using an advanced attention mechanism. Experimental results across five benchmarks indicate that HVD successfully mimics human visual focus and achieves state-of-the-art performance in text-video retrieval tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善当前在文本-视频检索中由于文本查询稀疏而导致的特征交互无效的问题。作者提出了人类视觉驱动（HVD）模型，该模型结合了粗到细的对齐机制，包含一个帧特征选择模块（FFSM），通过选择关键帧来减少时间冗余，以及一个补丁特征压缩模块（PFCM），利用先进的注意力机制将补丁特征聚合为显著的视觉实体。五个基准测试的实验结果表明，HVD有效模拟了人类的视觉聚焦，并在文本-视频检索任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap</div>
<div class="meta-line">Authors: Elisabeth Jüttner, Janelle Pfeifer, Leona Krath, Stefan Korfhage, Hannah Dröge, Matthias B. Hullin, Markus Plack</div>
<div class="meta-line">First: 2025-10-27T16:28:55+00:00 · Latest: 2026-01-22T13:52:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23494v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23494v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yesnt：扩散重光照模型是否准备好用于捕捉阶段合成？一种弥合差距的混合替代方案</div>
<div class="mono" style="margin-top:8px">体积视频重光照对于将捕捉的表演带入虚拟世界至关重要，但当前的方法在提供时间稳定、适合生产的结果方面存在困难。基于扩散的内在分解方法在单帧上显示出潜力，但在扩展到序列时受到随机噪声和不稳定性的影响，而视频扩散模型则受到内存和规模的限制。我们提出了一种混合重光照框架，将扩散衍生的材料先验与时间正则化和物理驱动的渲染相结合。我们的方法将每帧材料属性的多个随机估计聚合为时间一致的阴影组件，使用光流引导的正则化。对于阴影和反射等间接效果，我们从高斯不透明度场中提取网格代理，并在标准图形管线中进行渲染。对真实和合成捕捉的实验表明，这种混合策略在序列中实现了比仅使用扩散的基线更稳定的重光照，同时超越了视频扩散可行的剪辑长度。这些结果表明，平衡学习先验与物理约束的混合方法是实现适合生产的体积视频重光照的实际步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the stability and quality of volumetric video relighting for virtual environments, as current methods struggle with temporal consistency and production readiness. The authors propose a hybrid relighting framework that integrates diffusion-derived material priors with temporal regularization and physically motivated rendering techniques. Experimental results demonstrate that this approach significantly enhances the stability of relighting across sequences compared to diffusion-only methods, while also allowing for longer clip lengths than those manageable by video diffusion models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善虚拟环境中体积视频重光照的稳定性和质量，解决当前方法在时间一致性方面的局限性。作者提出了一种混合重光照框架，将基于扩散的材料先验与时间正则化和物理驱动的渲染技术相结合。实验结果表明，该方法显著提高了视频序列中重光照的稳定性，相较于仅使用扩散的方法，同时也支持比现有视频扩散模型更长的剪辑长度。</div>
</details>
</div>
<div class="card">
<div class="title">The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars</div>
<div class="meta-line">Authors: Yarin Benyamin</div>
<div class="meta-line">First: 2026-01-22T12:44:12+00:00 · Latest: 2026-01-22T12:44:12+00:00</div>
<div class="meta-line">Comments: Technical Report benchmarking off-the-shelf CV latencies on commodity CPU hardware for therapeutic VR applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15914v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a &quot;Latency Wall&quot; exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (&lt;23%) or speed (&gt;150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>延迟壁垒：基于现成情感识别的实时虚拟化身基准测试</div>
<div class="mono" style="margin-top:8px">在虚拟现实（VR）和人机交互（HCI）领域，实时情感识别在支持自闭症谱系障碍（ASD）个体提高社交技能方面展现出潜力。该任务需要严格的延迟-准确性权衡，运动到光子（MTP）延迟需保持在140毫秒以下以维持应急性。然而，大多数现成的深度学习模型优先考虑准确性，而忽视了商品硬件的严格时间限制。作为可访问VR治疗的第一步，我们基准测试了在虚拟角色上使用UIBVFED数据集的零样本面部表情识别（FER）的最先进（SOTA）模型。我们评估了YOLO（v8、v11和v12）的中型和纳米变体用于人脸检测，以及包括CLIP、SigLIP和ViT-FER在内的通用视觉变换器。我们在仅使用CPU的推理结果表明，尽管在风格化化身上的人脸检测是稳健的（100%准确率），但在分类阶段存在“延迟壁垒”。YOLOv11n架构在检测方面提供了最佳平衡（约54毫秒）。然而，像CLIP和SigLIP这样的通用变换器未能在实时循环中实现可行的准确性（&lt;23%）或速度（&gt;150毫秒）。本研究强调了轻量级、特定领域架构的必要性，以便在治疗环境中实现可访问的实时AI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for real-time emotion recognition in Virtual Reality (VR) to assist individuals with Autism Spectrum Disorder (ASD) in enhancing their social skills, requiring a strict latency-accuracy trade-off. The study benchmarks various State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) using the UIBVFED dataset, evaluating both Medium and Nano variants of YOLO and general-purpose Vision Transformers. The findings reveal that while face detection on stylized avatars achieves 100% accuracy, a significant &#x27;Latency Wall&#x27; is encountered during classification, with YOLOv11n providing the best detection speed (~54 ms), while general-purpose Transformers like CLIP and SigLIP fail to meet the necessary accuracy and speed for real-time applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强虚拟现实（VR）中的实时情感识别，以帮助自闭症谱系障碍（ASD）个体发展社交技能，这需要严格的延迟-准确性权衡。作者使用UIBVFED数据集对多种最新的零样本面部表情识别（FER）模型进行基准测试，评估了YOLO的中型和纳米变体以及通用视觉变换器。研究结果表明，尽管在风格化虚拟角色上的面部检测达到了100%的准确率，但在分类阶段遇到了显著的“延迟墙”，其中YOLOv11n提供了最佳的检测速度（约54毫秒），而通用变换器如CLIP和SigLIP在实时应用中在准确性和速度上均未能达到可行水平。</div>
</details>
</div>
<div class="card">
<div class="title">VideoPro: Adaptive Program Reasoning for Long Video Understanding</div>
<div class="meta-line">Authors: Chenglin Li, Feng Han, Yikun Wang, Ruilin Li, Shuai Dong, Haowen Hou, Haitao Li, Qianglong Chen, Feng Tao, Jingqi Tong, Yin Zhang, Jiaqi Wang</div>
<div class="meta-line">First: 2025-09-22T13:06:17+00:00 · Latest: 2026-01-22T10:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17743v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.17743v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models&#x27; ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoPro：用于长视频理解的自适应程序推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生成视觉任务的程序工作流方面显示出潜力。然而，以前的方法往往依赖于闭源模型，缺乏系统推理，并且在长视频问答（videoQA）中表现不佳。为了解决这些挑战，我们引入了FS-VisPR框架，这是一种自适应视觉程序推理方法，平衡了简单查询的快速推理与困难查询的慢速推理。首先，我们设计了高效的视觉模块（例如，关键片段检索和字幕检索）以支持长视频任务。然后，我们构建了一个多样化且高质量的快慢推理数据集，结合强大的LLM，以对齐开源语言模型生成视觉程序工作流的能力，称为FS-LLM。接下来，我们设计了一个快慢推理框架与FS-LLM：简单查询由VideoLLMs直接解决，而困难查询则调用视觉程序推理，受到类人推理过程的启发。在此过程中，低置信度的快速思考答案将触发第二阶段的慢推理过程，如果程序执行失败，则激活回退机制以进行快速推理。此外，我们通过在训练和推理期间的参数搜索来改进视觉程序。通过调整程序中视觉模块的参数，生成多个变体：在训练期间，选择产生正确答案的程序，而在推理期间，应用置信度最高的程序。实验表明，FS-VisPR提高了视觉程序工作流的效率和可靠性。在LVBench上取得了50.4%的准确率，超越了GPT-4o，匹配了Qwen2.5VL-72B在VideoMME上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing approaches in long-form video question answering (videoQA) by introducing the FS-VisPR framework, which combines fast and slow reasoning for visual program workflows. The method involves designing efficient visual modules for tasks like key clip and subtitle retrieval, and creating a diverse dataset to train an open-source language model (FS-LLM) for generating visual programs. Experimental results demonstrate that FS-VisPR enhances both efficiency and reliability, achieving 50.4% accuracy on the LVBench dataset, outperforming GPT-4o and matching Qwen2.5VL-72B on VideoMME.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决以往依赖闭源模型和缺乏系统推理的局限性，来增强长视频理解和问答能力。作者提出了FS-VisPR框架，该框架结合了针对查询复杂性的快速和慢速推理策略，利用高效的视觉模块进行关键片段和字幕检索等任务。实验结果表明，FS-VisPR在LVBench数据集上实现了50.4%的准确率，超越了GPT-4o，并与Qwen2.5VL-72B在VideoMME上的表现相匹配，从而提高了视觉程序工作流的效率和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">WavLink: Compact Audio-Text Embeddings with a Global Whisper Token</div>
<div class="meta-line">Authors: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:55:58+00:00 · Latest: 2026-01-22T08:55:20+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15118v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WavLink：带有全局Whisper令牌的紧凑音频-文本嵌入</div>
<div class="mono" style="margin-top:8px">Whisper已成为大型音频语言模型中提取通用音频特征的事实标准编码器，其中30秒的音频片段通常由1500个帧特征表示并投影到LLM中。相比之下，像基于CLAP的音频-文本嵌入模型在很大程度上依赖于替代音频编码器（例如HTS-AT、PaSST），并未有效利用Whisper。我们提出了WavLink，这是一种紧凑的音频-文本嵌入模型，通过可学习的全局令牌增强Whisper编码器，并与文本编码器共同训练。通过对设计选择的系统研究，包括预训练文本编码器、损失函数、训练模式和数据混合，我们识别出能够实现最先进检索性能的配置。我们在三种模型规模上采用的两阶段训练方案，结合马特ryoshka风格的监督，提高了可扩展性，使得嵌入体积缩小8倍且性能损失最小。WavLink在AIR-Bench的多项选择题和零样本分类中也表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of audio-text embedding models by leveraging the capabilities of the Whisper encoder, which has not been fully utilized in existing models. The authors introduce WavLink, a compact audio-text embedding model that incorporates a learnable global token and is trained jointly with a text encoder. Their systematic exploration of various design choices and training configurations leads to the identification of optimal setups that achieve state-of-the-art retrieval performance, resulting in embeddings that are 8 times smaller with minimal performance degradation, while also demonstrating competitive results on the AIR-Bench benchmark for multiple-choice questions and zero-shot classification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用Whisper编码器的能力来增强音频-文本嵌入模型的有效性，而Whisper在这一背景下尚未得到充分利用。作者提出了WavLink，这是一种紧凑的音频-文本嵌入模型，结合了可学习的全局标记与Whisper编码器，并与文本编码器共同训练。关键实验结果表明，通过系统地探索设计选择和两阶段训练方法，WavLink在检索性能上达到了最先进的水平，同时生成的嵌入体积缩小了8倍，性能下降极小，并且在AIR-Bench基准测试中对多项选择题和零样本分类任务也表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework</div>
<div class="meta-line">Authors: Shubham Shukla, Kunal Sonalkar</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-22T07:33:41+00:00 · Latest: 2026-01-22T07:33:41+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, &quot;outer fabric&quot; is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn&#x27;t exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的零样本产品属性标注：三层评估框架</div>
<div class="mono" style="margin-top:8px">细粒度属性预测对于时尚零售应用至关重要，包括目录丰富、视觉搜索和推荐系统。视觉语言模型（VLMs）提供零样本预测，无需特定任务训练，但其在多属性时尚任务上的系统评估仍未得到充分探索。一个关键挑战是时尚属性通常是有条件的。例如，当没有外衣可见时，“外层面料”是未定义的。这要求模型在尝试分类之前检测属性的适用性。我们引入了一个三层评估框架，分解了这一挑战：（1）所有属性的整体任务性能（包括NA类：表示属性不适用），（2）属性适用性检测，以及（3）当属性可确定时的细粒度分类。使用DeepFashion-MultiModal，该数据集在属性标签空间中明确定义NA（表示属性不存在或不可见），我们对九个VLM进行了基准测试，涵盖旗舰（GPT-5，Gemini 2.5 Pro）、高效（GPT-5 Mini，Gemini 2.5 Flash）和超高效（GPT-5 Nano，Gemini 2.5 Flash-Lite）层次，针对在18个属性上对5,000张图像进行预训练的Fashion-CLIP嵌入的分类器。我们的发现表明：（1）零样本VLM的宏观F1达到64.0%，比在预训练Fashion-CLIP嵌入上进行的逻辑回归提高了三倍；（2）VLM在细粒度分类（第3层：70.8% F1）方面表现出色，但在适用性检测（第2层：34.1% NA-F1）方面存在困难，识别出一个关键瓶颈；（3）高效模型在成本较低的情况下实现了超过90%的旗舰性能，提供了实际部署路径。该诊断框架使从业者能够确定错误是源于可见性检测还是分类，从而指导生产系统的针对性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance fine-grained attribute prediction in fashion retail applications, which is crucial for tasks such as catalog enrichment and visual search. The authors propose a three-tier evaluation framework to systematically assess Vision-Language Models (VLMs) on multi-attribute fashion tasks, addressing the challenge of conditional fashion attributes. Using the DeepFashion-MultiModal dataset, they benchmark nine VLMs against classifiers trained on Fashion-CLIP embeddings, revealing that zero-shot VLMs achieve a macro-F1 score of 64.0%, significantly outperforming logistic regression. While VLMs excel in fine-grained classification with a 70.8% F1 score, they face challenges in applicability detection, achieving only 34.1% NA-F1, indicating a critical area for improvement. Efficient models demonstrate over 90% of flagship performance at a lower cost, suggesting viable deployment options.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高时尚零售应用中的细粒度属性预测，这对目录丰富和视觉搜索等任务至关重要。作者提出了一个三层评估框架，以系统地评估视觉语言模型（VLM）在多属性时尚任务上的表现，解决了条件时尚属性的挑战。通过在5000张图像的数据集上对九个VLM进行基准测试，研究发现零样本VLM的宏F1得分为64.0%，显著优于逻辑回归，而细粒度分类的F1得分为70.8%，但适用性检测的挑战仍然存在，NA-F1得分为34.1%。高效模型展示了超过旗舰性能的90%，为实际部署提供了可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans</div>
<div class="meta-line">Authors: Shubhankar Borse, Seokeon Choi, Sunghyun Park, Jeongho Kim, Shreya Kadambi, Risheek Garrepalli, Sungrack Yun, Munawar Hayat, Fatih Porikli</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-25T23:00:57+00:00 · Latest: 2026-01-22T06:59:37+00:00</div>
<div class="meta-line">Comments: Accepted at the NeurIPS 2025 D&amp;B Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20879v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.20879v4">PDF</a> · <a href="https://github.com/Qualcomm-AI-research/MultiHuman-Testbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1,800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation. The dataset and evaluation codes will be available at https://github.com/Qualcomm-AI-research/MultiHuman-Testbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MultiHuman-Testbench：多人的图像生成基准测试</div>
<div class="mono" style="margin-top:8px">生成包含多个人类、执行复杂动作的图像，同时保持其面部身份，是一个重大挑战。造成这一问题的主要因素是缺乏专门的基准。为了解决这个问题，我们引入了MultiHuman-Testbench，这是一个用于严格评估多人人物生成的生成模型的新基准。该基准包含1800个样本，包括精心策划的文本提示，描述从简单到复杂的人类动作。这些提示与总共5550个独特的人脸图像相匹配，均匀抽样以确保在年龄、种族背景和性别上的多样性。除了标题外，我们还提供了人类选择的姿势条件图像，这些图像与提示准确匹配。我们提出了一个多方面的评估套件，采用四个关键指标来量化面部数量、身份相似性、提示对齐和动作检测。我们对一组多样化的模型进行了全面评估，包括零样本方法和基于训练的方法，带有和不带有区域先验。我们还提出了新技术，通过人类分割和匈牙利匹配来结合图像和区域隔离，显著提高身份相似性。我们提出的基准和关键发现为推动多人人物图像生成的研究提供了有价值的见解和标准化工具。数据集和评估代码将可在https://github.com/Qualcomm-AI-research/MultiHuman-Testbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in generating images of multiple humans performing complex actions while maintaining their facial identities, which is hindered by the absence of a dedicated benchmark. The authors introduce MultiHuman-Testbench, a benchmark consisting of 1,800 samples with curated text prompts and 5,550 unique human face images to evaluate generative models for multi-human generation. Key findings from the evaluation of various models reveal significant improvements in ID similarity through novel techniques that incorporate image and region isolation, providing a standardized tool for advancing research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成多个人类在执行复杂动作时保持面部身份的图像的挑战，而这一挑战受到缺乏专门基准的阻碍。作者提出了MultiHuman-Testbench，一个包含1800个样本的基准，配有精心策划的文本提示和5550个独特的人脸图像，以评估多人体生成的生成模型。评估采用四个指标来评估面部数量、ID相似性、提示对齐和动作检测，结果表明，图像和区域隔离的新技术显著提高了ID相似性，从而为该领域的研究提供了标准化工具。</div>
</details>
</div>
<div class="card">
<div class="title">Controllable Layered Image Generation for Real-World Editing</div>
<div class="meta-line">Authors: Jinrui Yang, Qing Liu, Yijun Li, Mengwei Ren, Letian Zhang, Zhe Lin, Cihang Xie, Yuyin Zhou</div>
<div class="meta-line">First: 2026-01-21T22:29:33+00:00 · Latest: 2026-01-21T22:29:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15507v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15507v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rayjryang.github.io/LASAGNA-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可控分层图像生成用于现实世界编辑</div>
<div class="mono" style="margin-top:8px">最近的图像生成模型取得了显著进展，但在用户尝试编辑现有图像中的特定元素时，往往难以产生可控且一致的结果。分层表示使灵活的用户驱动内容创作成为可能，但现有方法通常未能生成具有连贯合成关系的层，其对象层通常缺乏真实的视觉效果，如阴影和反射。为克服这些限制，我们提出了LASAGNA，一个新颖的统一框架，能够与其组成层共同生成图像——一个逼真的背景和一个具有引人注目的视觉效果的高质量透明前景。与之前的工作不同，LASAGNA有效地从广泛的条件输入（文本提示、前景、背景和位置掩码）中学习正确的图像合成，为现实世界应用提供更大的可控性。为此，我们引入了LASAGNA-48K，一个由干净背景和具有物理基础视觉效果的RGBA前景组成的新数据集。我们还提出了LASAGNABENCH，这是第一个用于层编辑的基准。我们展示了LASAGNA在同时生成多个图像层时能够产生高度一致和连贯的结果，从而支持多样的后期编辑应用，准确保留身份和视觉效果。LASAGNA-48K和LASAGNABENCH将公开发布，以促进社区的开放研究。项目页面是https://rayjryang.github.io/LASAGNA-Page/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing image generation models that struggle with controllable and consistent results during specific element edits in images. The authors propose LASAGNA, a unified framework that generates images along with their compositional layers, including a photorealistic background and a high-quality transparent foreground with realistic visual effects. Experimental results show that LASAGNA achieves high consistency and coherence across multiple image layers, enabling effective post-editing applications while preserving identity and visual effects, supported by the introduction of the LASAGNA-48K dataset and the LASAGNABENCH benchmark for layer editing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有图像生成模型在特定元素编辑时难以实现可控和一致结果的局限性。作者提出了LASAGNA，一个统一框架，能够生成图像及其组成层，包括逼真的背景和高质量的透明前景，具有真实的视觉效果。实验结果表明，LASAGNA在多个图像层之间实现了高度一致和连贯的生成，能够有效支持后期编辑应用，同时保持身份和视觉效果，并引入了LASAGNA-48K数据集和LASAGNABENCH层编辑基准。</div>
</details>
</div>
<div class="card">
<div class="title">DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection</div>
<div class="meta-line">Authors: Morteza Poudineh, Marc Lalonde</div>
<div class="meta-line">First: 2026-01-21T20:35:51+00:00 · Latest: 2026-01-21T20:35:51+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DevPrompt：基于偏差的提示学习用于单正常样本图像异常检测</div>
<div class="mono" style="margin-top:8px">少量正常样本异常检测（FNSAD）旨在仅使用少量正常训练样本检测图像中的异常区域，由于监督有限和潜在缺陷的多样性，这一任务极具挑战性。最近的方法利用视觉-语言模型，如CLIP，通过基于提示的学习来对齐图像和文本特征。然而，现有方法通常在正常和异常提示之间表现出较弱的可区分性，并缺乏针对补丁级异常的原则性评分机制。我们提出了一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差的评分的统计可靠性相结合。具体而言，我们用可学习的上下文向量替换固定的提示前缀，这些向量在正常和异常提示之间共享，而特定于异常的后缀标记则实现类感知对齐。为了增强可分离性，我们引入了带有Top-K多实例学习（MIL）的偏差损失，将补丁级特征建模为来自正态分布的高斯偏差。这使得网络能够为具有统计显著偏差的补丁分配更高的异常分数，从而改善定位和可解释性。在MVTecAD和VISA基准上的实验表明，与PromptAD和其他基线相比，像素级检测性能更优。消融研究进一步验证了可学习提示、基于偏差的评分和Top-K MIL策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of few-normal shot anomaly detection (FNSAD), which involves identifying abnormal regions in images with limited normal training samples. The authors propose a deviation-guided prompt learning framework that combines vision-language models with a deviation-based scoring mechanism. Key experimental findings indicate that this approach significantly enhances pixel-level detection performance on the MVTecAD and VISA benchmarks compared to existing methods, with ablation studies confirming the effectiveness of learnable prompts, deviation-based scoring, and the Top-K Multiple Instance Learning strategy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决少量正常样本异常检测（FNSAD）中的挑战，该任务涉及在有限的正常训练样本下识别图像中的异常区域。作者提出了一种基于偏差的提示学习框架，通过利用可学习的上下文向量和特定于异常的后缀标记，结合使用Top-K多实例学习（MIL）的偏差损失来增强提示的可区分性，并将补丁级特征建模为高斯偏差。实验结果表明，该方法在MVTecAD和VISA基准测试中显著提高了像素级检测性能，相较于现有方法如PromptAD，消融研究进一步证实了所提技术的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考具身世界的视频生成模型</div>
<div class="mono" style="margin-top:8px">视频生成模型在具身智能方面取得了显著进展，为生成捕捉物理世界中感知、推理和行动的多样化机器人数据开辟了新可能性。然而，合成准确反映现实世界机器人交互的高质量视频仍然具有挑战性，缺乏标准化基准限制了公平比较和进展。为了解决这一问题，我们引入了一个全面的机器人基准RBench，旨在评估五个任务领域和四种不同具身形式的机器人导向视频生成。它通过可重复的子指标评估任务级正确性和视觉保真度，包括结构一致性、物理合理性和行动完整性。对25个代表性模型的评估突显了生成物理现实机器人行为的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。虽然RBench提供了识别这些不足的必要视角，但实现物理现实需要超越评估，解决高质量训练数据的严重短缺。基于这些见解，我们引入了一个精细的四阶段数据管道，最终形成RoVid-X，这是用于视频生成的最大开源机器人数据集，包含400万个带注释的视频片段，涵盖数千个任务，并配有全面的物理属性注释。总体而言，这一评估和数据的协同生态系统为视频模型的严格评估和可扩展训练奠定了坚实基础，加速了具身人工智能向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video generation models for embodied intelligence, particularly in generating realistic robot data that reflects real-world interactions. The authors introduce RBench, a comprehensive benchmark designed to evaluate robot-oriented video generation across various tasks and embodiments, focusing on metrics such as structural consistency and physical plausibility. Their evaluation of 25 models reveals significant shortcomings in generating realistic robot behaviors, while RBench demonstrates a high correlation with human evaluations, and the introduction of RoVid-X, a large open-source dataset with 4 million annotated clips, aims to address the lack of high-quality training data necessary for improving physical realism in video generation models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升具身智能的视频生成模型，特别是在生成准确反映机器人在物理世界中交互的高质量视频方面。作者介绍了RBench，这是一个综合基准，旨在评估五个任务领域和四种具身形式的机器人导向视频生成，重点关注结构一致性和物理 plausibility等指标。实验结果显示，25个评估模型在生成真实机器人行为方面存在显著不足，而RBench与人类评估的相关性很高，Spearman系数达到0.96。为了应对这些不足，作者提出了RoVid-X，这是一个包含400万条注释视频片段的广泛开源数据集，旨在改善视频生成模型可用的训练数据。</div>
</details>
</div>
<div class="card">
<div class="title">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</div>
<div class="meta-line">Authors: Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Ziwei Liu, Chenyang Si</div>
<div class="meta-line">First: 2026-01-21T18:59:02+00:00 · Latest: 2026-01-21T18:59:02+00:00</div>
<div class="meta-line">Comments: 17 pages, 21 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StableWorld：朝着稳定和一致的长时交互视频生成</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中被忽视的稳定性和时间一致性挑战，该生成通过相机移动和文本提示等交互行为合成动态和可控的视频世界。尽管在世界建模方面取得了显著进展，但当前方法仍然面临严重的不稳定性和时间退化，常常导致长时间交互中的空间漂移和场景崩溃。为了更好地理解这个问题，我们最初调查了不稳定性的根本原因，并确定主要的误差积累来源于同一场景，其中生成的帧逐渐偏离初始干净状态，并将误差传播到后续帧。基于这一观察，我们提出了一种简单而有效的方法，\textbf{StableWorld}，动态帧驱逐机制。通过持续过滤掉退化帧，同时保留几何一致的帧，StableWorld有效地防止了在源头的累积漂移，从而提高了交互生成的稳定性和时间一致性。在多个交互视频模型上（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）取得的良好结果表明，StableWorld是模型无关的，可以应用于不同的交互视频生成框架，以显著提高稳定性、时间一致性和在多样化交互场景中的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of stability and temporal consistency in interactive video generation, which is crucial for synthesizing dynamic video worlds through user interactions. The authors investigate the causes of instability, identifying that error accumulation primarily occurs within the same scene, leading to spatial drift and scene collapse. To mitigate this issue, they propose StableWorld, a Dynamic Frame Eviction Mechanism that filters out degraded frames while preserving geometrically consistent ones, resulting in improved stability and temporal consistency across various interactive video models such as Matrix-Game, Open-Oasis, and Hunyuan-GameCraft.</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中稳定性和时间一致性的问题，这对于通过用户交互创建动态视频世界至关重要。作者研究了不稳定性的原因，发现错误积累主要发生在同一场景中，导致空间漂移和场景崩溃。他们提出了一种名为StableWorld的方法，采用动态帧驱逐机制，过滤掉退化帧，同时保留几何一致的帧。针对多个交互视频模型（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）的实验结果表明，StableWorld显著提高了稳定性和时间一致性，证明了其在不同框架中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</div>
<div class="meta-line">Authors: Letian Zhang, Sucheng Ren, Yanqing Liu, Xianhang Li, Zeyu Wang, Yuyin Zhou, Huaxiu Yao, Zeyu Zheng, Weili Nie, Guilin Liu, Zhiding Yu, Cihang Xie</div>
<div class="meta-line">First: 2026-01-21T18:47:12+00:00 · Latest: 2026-01-21T18:47:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15369v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15369v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenVision 3：统一视觉编码器家族，用于理解和生成</div>
<div class="mono" style="margin-top:8px">本文提出了一种先进的视觉编码器家族，名为OpenVision 3，学习一种单一的统一视觉表示，既可用于图像理解，也可用于图像生成。我们的核心架构简单：我们将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持两种互补角色。首先，编码器输出传递给ViT-VAE解码器以重建原始图像，鼓励表示捕捉生成结构。其次，使用对比学习和图像-标题目标优化相同的表示，增强语义特征。通过在共享潜在空间中联合优化重建和语义驱动信号，编码器学习到的表示在两种模式下协同并良好泛化。我们通过广泛的下游评估验证了这种统一设计，编码器保持冻结。对于多模态理解，我们将编码器插入LLaVA-1.5框架：其性能与标准CLIP视觉编码器相当（例如，SeedBench上62.4对62.2，POPE上83.7对82.9）。对于生成，我们在RAE框架下进行测试：我们的表现大幅超越标准CLIP基础编码器（例如，ImageNet上的gFID：1.89对2.54）。我们希望这项工作能激发未来对统一建模的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a unified visual encoder that can effectively handle both image understanding and generation tasks. The authors propose OpenVision 3, which utilizes a simple architecture where VAE-compressed image latents are fed into a ViT encoder, enabling the model to learn a shared representation optimized for both reconstructing images and enhancing semantic features through contrastive learning and image-captioning objectives. Experimental results demonstrate that OpenVision 3 performs comparably to standard CLIP vision encoders in multimodal understanding tasks and significantly outperforms them in image generation tasks, indicating its effectiveness in unifying these two domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的视觉编码器，有效支持图像理解和生成任务。作者介绍了OpenVision 3，采用简单的架构，将VAE压缩的图像潜变量输入ViT编码器，使其能够执行双重角色：重建原始图像和通过对比学习和图像-标题目标优化语义特征。实验结果表明，该编码器在LLaVA-1.5框架中集成时，与标准CLIP视觉编码器相比表现出竞争力，并在RAE框架下的生成任务中显著超越，表明统一设计在增强多模态能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</div>
<div class="meta-line">Authors: Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-28T17:57:05+00:00 · Latest: 2026-01-21T16:16:08+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24709v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object binding, the brain&#x27;s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型预训练视觉变换器中对象绑定是否自然出现？</div>
<div class="mono" style="margin-top:8px">对象绑定是大脑将共同代表一个对象的多个特征结合成一个连贯整体的能力，这对人类认知至关重要。它将低级感知特征分组为高级对象表示，能够高效且组合地存储这些对象，并支持人类对单个对象实例的推理。尽管之前的研究通常明确施加以对象为中心的注意力（例如，插槽注意力）来探讨这些好处，但尚不清楚这种能力是否在预训练的视觉变换器（ViTs）中自然出现。直观上，它们可能会：识别哪些补丁属于同一对象对于下游预测应该是有用的，从而引导注意力。基于自注意力的二次特性，我们假设ViTs表示两个补丁是否属于同一对象，我们称之为IsSameObject。我们使用二次相似性探针从ViT层的补丁嵌入中解码IsSameObject，准确率超过90%。重要的是，这种对象绑定能力在DINO、CLIP和ImageNet监督的ViTs中可靠地出现，但在MAE中明显较弱，这表明绑定不是一个简单的架构伪影，而是通过特定预训练目标获得的能力。我们进一步发现IsSameObject在对象特征之上的低维子空间中编码，并且该信号积极引导注意力。从模型激活中消除IsSameObject会降低下游性能，并与学习目标相悖，这意味着新兴的对象绑定自然服务于预训练目标。我们的发现挑战了ViTs缺乏对象绑定的观点，并强调了“哪些部分属于一起”的符号知识如何在连接主义系统中自然出现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates whether object binding, the cognitive ability to integrate various features into coherent object representations, naturally emerges in large pretrained Vision Transformers (ViTs). The authors hypothesize that ViTs can represent the relationship between patches belonging to the same object, referred to as IsSameObject, and they employ a quadratic similarity probe to decode this property from patch embeddings across ViT layers, achieving over 90% accuracy. The results indicate that this object-binding capability is consistently present in DINO, CLIP, and ImageNet-supervised ViTs, but significantly weaker in MAE, suggesting that effective object binding is linked to specific pretraining objectives rather than being an inherent architectural feature, and that it plays a crucial role in guiding attention and enhancing downstream performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了对象绑定，即将各种特征整合为一致的对象表征的认知能力，是否在大型预训练视觉变换器（ViTs）中自然出现。作者假设ViTs能够表示属于同一对象的补丁之间的关系，称为IsSameObject，并使用二次相似性探针从补丁嵌入中解码这一属性，准确率超过90%。结果表明，这种对象绑定能力在DINO、CLIP和ImageNet监督的ViTs中始终存在，但在MAE中显著较弱，表明有效的对象绑定与特定的预训练目标相关，而不是固有的架构特征。</div>
</details>
</div>
<div class="card">
<div class="title">BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation</div>
<div class="meta-line">Authors: Andrey Moskalenko, Danil Kuznetsov, Irina Dudko, Anastasiia Iasakova, Nikita Boldyrev, Denis Shepelev, Andrei Spiridonov, Andrey Kuznetsov, Vlad Shakhuro</div>
<div class="meta-line">First: 2026-01-21T16:02:21+00:00 · Latest: 2026-01-21T16:02:21+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15123v1">PDF</a> · <a href="https://github.com/emb-ai/BREPS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BREPS：可提示分割的边界框鲁棒性评估</div>
<div class="mono" style="margin-top:8px">可提示分割模型如SAM建立了一种强大的范式，使其能够在最小用户输入（包括点、边界框和文本提示）的情况下，对未见对象和领域进行强泛化。其中，边界框特别有效，通常优于点，同时显著降低注释成本。然而，目前的训练和评估协议通常依赖于通过简单启发式生成的合成提示，提供的对现实世界鲁棒性的洞察有限。本文研究了可提示分割模型对边界框提示自然变化的鲁棒性。首先，我们进行了一项受控用户研究，收集了数千个真实的边界框注释。我们的分析显示，对于相同模型和实例，不同用户的分割质量存在显著变异，表明类似SAM的模型对自然提示噪声高度敏感。然后，由于对所有可能用户输入进行全面测试在计算上是不可行的，我们将鲁棒性评估重新表述为边界框提示空间上的白盒优化问题。我们引入了BREPS，一种生成对抗性边界框的方法，旨在最小化或最大化分割错误，同时遵循自然性约束。最后，我们在10个数据集上基准测试了最先进的模型，涵盖日常场景到医学成像。代码 - https://github.com/emb-ai/BREPS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the robustness of promptable segmentation models, particularly in relation to bounding box prompts, which are effective yet often assessed using synthetic heuristics that do not reflect real-world scenarios. The authors conducted a controlled user study to gather thousands of real bounding box annotations, revealing significant variability in segmentation quality across different users, indicating sensitivity to natural prompt noise. To address the computational challenges of exhaustive testing, they developed BREPS, a method that formulates robustness evaluation as a white-box optimization problem, generating adversarial bounding boxes to either minimize or maximize segmentation errors while maintaining naturalness, and benchmarked this approach across ten diverse datasets, including everyday scenes and medical imaging.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估可提示分割模型的鲁棒性，特别是与边界框提示相关的鲁棒性，这些提示有效但通常使用合成启发式方法进行评估，无法反映真实世界的情况。作者进行了受控用户研究，收集了数千个真实的边界框注释，揭示了用户之间在相同模型和实例上的分割质量存在显著差异，表明像SAM这样的模型对自然提示噪声高度敏感。为了应对全面测试的挑战，他们将鲁棒性评估重新表述为一个白盒优化问题，并引入了BREPS，这是一种生成对抗性边界框的方法，可以在保持自然性的同时最小化或最大化分割错误，最终在包括日常场景和医学成像在内的十个不同数据集上对最先进的模型进行了基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background</div>
<div class="meta-line">Authors: Tianyu Li, Songyue Cai, Zongqian Wu, Ping Hu, Xiaofeng Zhu</div>
<div class="meta-line">Venue: arXiv preprint arXiv:2601.15065 (2026)</div>
<div class="meta-line">First: 2026-01-21T15:12:11+00:00 · Latest: 2026-01-21T15:12:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15065v1">PDF</a> · <a href="https://github.com/lounwb/FoBoR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过前景和背景的细化增强少样本分布外检测</div>
<div class="mono" style="margin-top:8px">基于CLIP的前景-背景（FG-BG）分解方法在提高少样本分布外（OOD）检测性能方面表现出显著的有效性。然而，现有方法仍然存在一些局限性。对于从分解中获得的背景区域，现有方法对所有补丁采用统一的抑制策略，忽视了不同补丁对预测的不同贡献。对于前景区域，现有方法未能充分考虑某些局部补丁可能与其他类别表现出外观或语义相似性，这可能误导训练过程。为了解决这些问题，我们提出了一种新的即插即用框架。该框架由三个核心组件组成：（1）前景-背景分解模块，遵循以前的FG-BG方法将图像分为前景和背景区域；（2）自适应背景抑制模块，自适应地加权补丁分类熵；（3）可混淆前景校正模块，识别并校正可混淆的前景补丁。大量实验结果表明，所提出的即插即用框架显著提高了现有FG-BG分解方法的性能。代码可在：https://github.com/lounwb/FoBoR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance few-shot out-of-distribution (OOD) detection performance, which is currently limited by existing foreground-background decomposition methods. The authors propose a new framework that includes a Foreground-Background Decomposition module, an Adaptive Background Suppression module, and a Confusable Foreground Rectification module to address the shortcomings of uniform suppression strategies and misclassification of local patches. Experimental results show that this framework significantly improves the performance of existing FG-BG decomposition methods, indicating its effectiveness in refining OOD detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高少样本异常检测的性能，特别是解决现有基于CLIP的前景-背景分解方法中的局限性。作者提出了一个新框架，包括前景-背景分解模块、自适应背景抑制模块和混淆前景修正模块，以改善分类过程。实验结果表明，该框架显著提升了当前前景-背景分解技术的性能，表明其在处理图像补丁的不同贡献和修正混淆前景方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Leakage with Generative Flow Matching Denoiser</div>
<div class="meta-line">Authors: Isaac Baglin, Xiatian Zhu, Simon Hadfield</div>
<div class="meta-line">First: 2026-01-21T14:51:01+00:00 · Latest: 2026-01-21T14:51:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15049v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度泄漏与生成流匹配去噪器</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）已成为一种强大的去中心化模型训练范式，但仍然容易受到深度泄漏（DL）攻击，这些攻击通过共享模型更新重建私有客户端数据。尽管先前的DL方法已显示出不同程度的成功，但它们通常在现实FL环境下存在不稳定、保真度有限或鲁棒性差的问题。我们提出了一种新的DL攻击，将生成流匹配（FM）先验整合到重建过程中。通过引导优化朝向真实图像的分布（由流匹配基础模型表示），我们的方法在不需要私有数据知识的情况下增强了重建保真度。在多个数据集和目标模型上的广泛实验表明，我们的方法在像素级、感知和基于特征的相似性度量上始终优于最先进的攻击。重要的是，该方法在不同的训练周期、更大的客户端批量大小以及在常见防御措施（如噪声注入、裁剪和稀疏化）下仍然有效。我们的发现呼吁开发新的防御策略，明确考虑配备强大生成先验的对手。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerability of Federated Learning (FL) to deep leakage attacks, which can reconstruct private client data from shared model updates. The authors propose a novel deep leakage attack that utilizes a generative Flow Matching prior to improve the fidelity of data reconstruction without needing access to the private data itself. Experimental results across various datasets and target models show that this method consistently outperforms existing state-of-the-art attacks in terms of pixel-level, perceptual, and feature-based similarity metrics, while also maintaining effectiveness under different training conditions and common defense mechanisms, highlighting the need for new defense strategies against such advanced adversarial techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决联邦学习（FL）在深度泄露攻击下的脆弱性，这种攻击可以从共享模型更新中重建私有客户端数据。作者提出了一种新颖的深度泄露攻击，利用生成的流匹配先验来提高数据重建的保真度，而无需访问私有数据。跨多个数据集和目标模型的实验结果表明，该方法在像素级、感知和特征相似性指标方面显著优于现有的最先进攻击，同时在噪声注入和剪辑等常见防御下仍保持有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</div>
<div class="meta-line">Authors: Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers</div>
<div class="meta-line">First: 2026-01-21T14:42:33+00:00 · Latest: 2026-01-21T14:42:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于课程的深度强化学习框架用于电动车路径规划问题</div>
<div class="mono" style="margin-top:8px">带时间窗的电动车路径规划问题（EVRPTW）是可持续物流中的复杂优化问题，路径决策必须在满足严格客户时间约束的同时，最小化总旅行距离、车队规模和电池使用。尽管深度强化学习（DRL）作为经典启发式和精确求解器的替代方案显示出巨大潜力，但现有的DRL模型在约束密集时往往难以保持训练稳定性，无法收敛或泛化。在本研究中，我们提出了一种基于课程的深度强化学习（CB-DRL）框架，旨在解决这种不稳定性。该框架利用结构化的三阶段课程，逐步增加问题复杂性：代理首先学习距离和车队优化（阶段A），然后是电池管理（阶段B），最后是完整的EVRPTW（阶段C）。为了确保各阶段的稳定学习，该框架采用了修改后的近端策略优化算法，具有阶段特定的超参数、价值和优势裁剪，以及自适应学习率调度。策略网络基于异构图注意力编码器构建，增强了全局-局部注意力和特征线性调制。这种专门的架构明确捕捉了仓库、客户和充电站的不同特性。模型仅在N=10客户的小实例上训练，显示出对N=5到N=100的未见实例的强泛化能力，在中等规模问题上显著优于标准基线。实验结果确认，这种课程引导的方法在标准DRL基线失败的分布外实例上实现了高可行性率和竞争性解质量，有效弥合了神经速度与操作可靠性之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of the electric vehicle routing problem with time windows (EVRPTW), which involves optimizing travel distance, fleet size, and battery usage under strict time constraints. The authors propose a curriculum-based deep reinforcement learning (CB-DRL) framework that employs a structured three-phase curriculum to enhance training stability and generalization. The experimental results indicate that the CB-DRL framework, trained on small instances, successfully generalizes to larger, unseen instances, outperforming standard baselines and achieving high feasibility rates and competitive solution quality, particularly in scenarios where traditional DRL methods struggle.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决带时间窗口的电动车路径规划问题（EVRPTW），该问题涉及在遵守严格的客户时间约束的同时优化旅行距离、车队规模和电池使用。作者提出了一种基于课程的深度强化学习（CB-DRL）框架，该框架采用结构化的三阶段课程来增强训练的稳定性和泛化能力。实验结果表明，CB-DRL框架在小规模实例上训练后，能够有效地推广到更大且未见过的实例，在中等规模问题上超越标准基线，特别是在传统DRL方法面临挑战的情况下，达到了较高的可行性率。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：用于遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有的视觉语言模型（VLMs）主要在网络抓取的嘈杂图像-文本数据上训练，缺乏对遥感（RS）专业领域的充分接触。这一缺陷导致在RS特定任务上的表现不佳，因为常用数据集往往缺乏详细、科学准确的文本描述，而仅强调日期和位置等属性。为弥补这一关键缺口，我们推出了GAIA，一个为多尺度、多传感器和多模态RS图像分析而设计的新数据集。GAIA包含201,005对精心策划的RS图像-文本对，代表了与不同空间分辨率相关的多样化RS模态。与现有的RS视觉语言数据集不同，GAIA特别关注捕捉多样化的RS应用，提供有关环境变化、自然灾害和其他各种动态现象的独特信息。该数据集提供了空间和时间上平衡的分布，覆盖全球，涵盖过去25年，观察的时间分布也很平衡。GAIA的构建涉及两个阶段的过程：（1）从信誉良好的RS相关来源有针对性地抓取图像及其伴随文本，和（2）为每张图像生成五个高质量、科学基础的合成标题，使用精心设计的提示，利用GPT-4o的先进视觉语言能力。我们的广泛实验，包括对CLIP和BLIP2模型的微调，表明GAIA显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。我们在项目的GitHub仓库上公开了我们的数据集、自动处理框架和微调模型权重：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing Vision-Language Models (VLMs) that are primarily trained on noisy web-scraped data, which inadequately supports remote sensing (RS) tasks due to a lack of scientifically accurate textual descriptions. The authors introduce GAIA, a comprehensive dataset consisting of 201,005 curated RS image-text pairs, designed for multi-scale and multi-modal analysis, created through targeted web-scraping and the generation of high-quality synthetic captions using GPT-4o. Experimental results indicate that fine-tuning models like CLIP and BLIP2 on GAIA leads to significant improvements in RS image classification, cross-modal retrieval, and image captioning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视觉语言模型（VLM）在遥感（RS）任务中表现不佳的问题，这主要是由于它们在噪声较大的网络抓取数据上训练，缺乏科学准确的文本描述。作者提出了GAIA，一个包含201,005个精心策划的RS图像-文本对的数据集，该数据集通过针对性网络抓取和使用GPT-4o生成高质量合成标题的两阶段过程创建。实验结果表明，在GAIA上微调CLIP和BLIP2等模型显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</div>
<div class="meta-line">Authors: Kangcheng Zhou, Jun Jiang, Qing Zhang, Shuang Zheng, Qingli Li, Shugong Xu</div>
<div class="meta-line">First: 2026-01-21T08:21:35+00:00 · Latest: 2026-01-21T08:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14757v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14757v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReinPath：一种用于病理学的多模态强化学习方法</div>
<div class="mono" style="margin-top:8px">可解释性在计算病理学中至关重要，促使从组织病理图像和相应文本数据中开发多模态信息集成。然而，现有的多模态方法由于缺乏支持明确推理和推断的高质量数据集以及简单的推理过程，导致可解释性有限。为了解决上述问题，我们引入了一种具有强大推理能力的新型多模态病理大语言模型。为了提高准确且上下文相关的文本描述的生成，我们设计了一种与组相对策略优化相结合的语义奖励策略。我们构建了一个高质量的病理视觉问答（VQA）数据集，专门设计用于支持复杂推理任务。在该数据集上进行的全面实验表明，我们的方法在性能上优于最先进的方法，即使仅用20%的数据进行训练。我们的方法在下游零样本图像分类任务中也与CLIP相比表现出可比的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance interpretability in computational pathology by integrating multimodal information from histopathological images and corresponding text data. The authors propose a novel multimodal pathology large language model that incorporates a semantic reward strategy and group relative policy optimization to improve the generation of accurate textual descriptions. Experimental results show that this approach outperforms existing state-of-the-art methods on a newly constructed high-quality visual question answering dataset, achieving superior performance even with only 20% of the training data and demonstrating comparable results in downstream zero-shot image classification tasks against CLIP.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过整合来自组织病理图像和相应文本数据的多模态信息来增强计算病理学中的可解释性。作者提出了一种新颖的多模态病理大语言模型，该模型结合了语义奖励策略和组相对策略优化，以改善准确文本描述的生成。实验结果表明，该方法在新构建的高质量视觉问答数据集上优于现有的最先进方法，即使仅使用20%的训练数据也能取得优越的性能，并且在与CLIP的下游零样本图像分类任务中表现出可比的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</div>
<div class="meta-line">Authors: Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</div>
<div class="meta-line">First: 2025-12-19T01:39:43+00:00 · Latest: 2026-01-21T07:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17160v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成图像能否作为有效且高效的类别原型？</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在零样本图像分类任务中表现出色。然而，现有方法，包括对比语言-图像预训练（CLIP），都依赖于注释的文本-图像对来对齐视觉和文本模态。这种依赖引入了准备高质量数据集的巨大成本和准确性要求。同时，从两种模式处理数据也需要双塔编码器，这也阻碍了它们的轻量化。为了解决这些限制，我们提出了“基于大型语言模型生成的对比语言-图像预训练（LGCLIP）”框架。LGCLIP利用大型语言模型（LLM）生成特定类别的提示，引导扩散模型合成参考图像。随后，这些生成的图像作为视觉原型，真实图像的视觉特征被提取并与这些原型的视觉特征进行比较，以实现比较预测。通过优化LLM的提示生成并仅使用视觉编码器，LGCLIP保持轻量和高效。关键是，我们的框架在整个实验过程中仅需要类别标签作为输入，消除了对手动注释的图像-文本对和额外预处理的需求。实验结果验证了LGCLIP的可行性和效率，在零样本分类任务中表现出色，并建立了分类的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and effectiveness of zero-shot image classification by eliminating the reliance on annotated text-to-image pairs, which are costly and time-consuming to prepare. The authors propose a new framework called LGCLIP, which utilizes a Large Language Model to generate class-specific prompts that guide a diffusion model in synthesizing reference images, allowing for the extraction and comparison of visual features without the need for dual-tower encoders. Experimental results demonstrate that LGCLIP achieves strong performance in zero-shot classification tasks while remaining lightweight and requiring only class labels as input, thus establishing a novel approach to classification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法依赖于注释的图像-文本对的局限性，来提高零-shot图像分类的效率和有效性。作者提出了一种新颖的框架LGCLIP，该框架利用大型语言模型生成特定类别的提示，指导扩散模型合成参考图像，从而使用这些合成图像作为视觉原型。实验结果表明，LGCLIP在零-shot分类任务中表现出色，同时保持轻量和高效，因为它只需要类别标签，而无需手动注释的图像-文本对。</div>
</details>
</div>
<div class="card">
<div class="title">T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs</div>
<div class="meta-line">Authors: Shao-Jun Xia, Huixin Zhang, Zhengzhong Tu</div>
<div class="meta-line">First: 2025-11-20T07:02:06+00:00 · Latest: 2026-01-21T06:18:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16107v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16107v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across twelve cross-task scenarios and second-tier performance in nine additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T2T-VICL：通过隐式文本驱动的视觉语言模型解锁跨任务视觉上下文学习的边界</div>
<div class="mono" style="margin-top:8px">在大型语言模型（LLM）中，上下文学习（ICL）是指通过输入上下文中提供的小示例来执行新任务。最近在视觉上下文学习（VICL）方面的进展展示了统一视觉语言模型（VLM）在解决下游任务方面的良好能力。当视觉提示和目标图像来自不同的视觉任务时，VLM是否仍能实现VICL？在本文中，我们提出了一种完全协作的管道，即T2T-VICL，以研究跨任务VICL的潜力。从根本上讲，我们设计了一种机制来生成和选择最佳隐式描述两个不同低级视觉任务之间差异的文本提示，并构建了第一个跨任务VICL数据集。在此基础上，我们提出了一种新颖的推理框架，将基于感知分数的推理与传统评估指标相结合，以执行跨任务VICL。我们的方法在十二个跨任务场景中取得了顶级结果，在另外九个场景中取得了二级性能，解锁了VLM中跨任务VICL的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the capabilities of visual in-context learning (VICL) in unified vision-language models (VLMs) when dealing with tasks from different visual domains. The authors propose a collaborative pipeline called T2T-VICL, which includes a mechanism for generating and selecting text prompts that effectively highlight the differences between distinct low-level vision tasks, along with the creation of the first cross-task VICL dataset. The experimental results demonstrate that their approach achieves top-tier performance across twelve cross-task scenarios and second-tier performance in nine additional scenarios, thereby expanding the potential of cross-task VICL within VLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索大型语言模型（LLM）在处理跨任务场景时的视觉上下文学习（VICL）能力。作者提出了一种名为T2T-VICL的协作管道，其中包括生成和选择文本提示的机制，以有效突出两个不同低级视觉任务之间的差异，并创建了第一个跨任务VICL数据集。实验结果表明，该方法在十二个跨任务场景中实现了顶级性能，在另外九个场景中实现了第二级性能，从而扩展了视觉语言模型（VLM）中VICL的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
