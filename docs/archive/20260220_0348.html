<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-20 03:48</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260220_0348</div>
    <div class="row"><div class="card">
<div class="title">TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos</div>
<div class="meta-line">Authors: Namitha Padmanabhan, Matthew Gwilliam, Abhinav Shrivastava</div>
<div class="meta-line">First: 2026-02-18T18:59:55+00:00 · Latest: 2026-02-18T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16711v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://namithap10.github.io/teconerv/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TeCoNeRV：利用时间一致性压缩视频的神经表示</div>
<div class="mono" style="margin-top:8px">隐式神经表示（INRs）最近在视频压缩方面表现出色。然而，由于每个视频必须单独过拟合一个INR，因此在保持编码效率的同时扩展到高分辨率视频仍然是一个重大挑战。基于超网络的方法以高速度预测未见视频的INR权重（超网络），但在高分辨率下质量低、压缩大小大且内存需求高。我们通过三个关键贡献解决这些基本限制：（1）一种空间和时间上分解权重预测任务的方法，通过将短视频片段分解为补丁管道，减少预训练内存开销20倍；（2）一种基于残差的存储方案，仅捕获连续片段表示之间的差异，显著减少比特流大小；（3）一种时间一致性正则化框架，鼓励权重空间的变化与视频内容相关。我们提出的方法TeCoNeRV在UVG上在480p和720p的PSNR上分别比基线提高了2.47dB和5.35dB，具有36%的更低比特率和1.5-3倍的更快编码速度。凭借我们的低内存使用，我们是第一个在UVG、HEVC和MCL-JCV上展示480p、720p和1080p结果的超网络方法。我们的项目页面可在https://namithap10.github.io/teconerv/找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of video compression using Implicit Neural Representations (INRs), which face challenges in scaling to high-resolution videos. The authors propose a method called TeCoNeRV that spatially and temporally decomposes the weight prediction task by segmenting videos into patch tubelets, reducing pretraining memory requirements by 20 times. Key experimental results show that TeCoNeRV achieves significant improvements in PSNR of 2.47dB and 5.35dB over the baseline at 480p and 720p resolutions, respectively, while also reducing bitrates by 36% and increasing encoding speeds by 1.5 to 3 times compared to previous hypernetwork approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视频压缩的效率，特别是在高分辨率视频中，现有方法在质量和内存限制方面面临挑战。作者提出了TeCoNeRV，采用了一种新颖的方法，将权重预测任务在空间和时间上分解为补丁管段，利用基于残差的存储方案来最小化比特流大小，并引入时间一致性正则化框架，以使权重变化与视频内容对齐。实验结果表明，TeCoNeRV在480p和720p下的PSNR分别提高了2.47dB和5.35dB，同时比特率降低了36%，编码速度提高了1.5-3倍，证明了其在不同分辨率和数据集上的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Chunking and the Entropy of Natural Language</div>
<div class="meta-line">Authors: Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks</div>
<div class="meta-line">First: 2026-02-13T18:58:10+00:00 · Latest: 2026-02-18T18:59:22+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures; typos fixed</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13194v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13194v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义分块与自然语言的熵</div>
<div class="mono" style="margin-top:8px">印刷英语的熵率被著名地估计为每个字符约一比特，这是现代大型语言模型（LLMs）最近才接近的基准。这一熵率意味着英语相对于随机文本预期的每个字符五比特，几乎包含80%的冗余。我们引入了一种统计模型，试图捕捉自然语言复杂的多尺度结构，为这一冗余水平提供了从第一原理出发的解释。我们的模型描述了一种自相似地将文本分割成语义连贯块的过程，直到单词级别。文本的语义结构可以层次分解，从而允许进行分析处理。与现代LLMs和开放数据集的数值实验表明，我们的模型定量捕捉了真实文本在不同语义层次上的结构。我们模型预测的熵率与印刷英语的估计熵率一致。此外，我们的理论进一步揭示，自然语言的熵率并非固定，而应随着语料库的语义复杂性系统性增加，这一复杂性由我们模型中唯一的自由参数捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the redundancy in natural language, specifically the estimated entropy rate of printed English, which suggests a significant level of redundancy. The authors introduce a statistical model that segments text into semantically coherent chunks, allowing for a hierarchical decomposition of its structure. Experimental results indicate that this model effectively captures the multi-scale structure of real texts and predicts an entropy rate that aligns with existing estimates, while also suggesting that the entropy rate increases with the semantic complexity of the text corpus.</div>
<div class="mono" style="margin-top:8px">本研究探讨了印刷英语的熵率冗余，估计约为每个字符一比特，表明与随机文本相比存在显著的冗余水平。作者提出了一种统计模型，将文本分割成语义一致的块，从而允许对其语义结构进行分层分解。实验结果表明，该模型有效捕捉了真实文本的结构，并与印刷英语的估计熵率一致，同时还表明熵率会随着文本语料的语义复杂性而增加。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Embedded Latent Projection for Robust Representation Learning</div>
<div class="meta-line">Authors: Weijing Tang, Ming Yuan, Zongqi Xia, Tianxi Cai</div>
<div class="meta-line">First: 2026-02-18T18:58:16+00:00 · Latest: 2026-02-18T18:58:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16709v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识嵌入的潜在投影用于鲁棒表示学习</div>
<div class="mono" style="margin-top:8px">潜在空间模型广泛用于分析高维离散数据矩阵，例如电子健康记录（EHR）中的患者特征矩阵，通过低维嵌入捕捉复杂的依赖结构。然而，在一个矩阵维度远大于另一个维度的失衡情况下，估计变得具有挑战性。在EHR应用中，队列大小通常受到疾病流行率或数据可用性的限制，而特征空间由于医疗编码系统的广度而保持极大。受到外部语义嵌入（如EHR中临床概念的预训练嵌入）日益可用的启发，我们提出了一种知识嵌入的潜在投影模型，利用语义侧信息来规范化表示学习。具体而言，我们通过在再生核希尔伯特空间中的映射，将列嵌入建模为语义嵌入的平滑函数。我们开发了一种计算效率高的两步估计程序，将通过核主成分分析的语义引导子空间构建与可扩展的投影梯度下降相结合。我们建立了估计误差界限，表征统计误差与核投影引起的近似误差之间的权衡。此外，我们为我们的非凸优化程序提供了局部收敛保证。广泛的模拟研究和一个真实世界的EHR应用证明了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenges of analyzing high-dimensional discrete data matrices, particularly in electronic health records (EHRs) where imbalanced data can complicate estimation. The authors propose a knowledge-embedded latent projection model that utilizes external semantic embeddings to enhance representation learning, employing a two-step estimation procedure that integrates kernel principal component analysis with projected gradient descent. Experimental results from extensive simulations and a real-world EHR application indicate that the proposed method effectively addresses the issues of statistical and approximation errors, demonstrating its robustness in representation learning under imbalanced conditions.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于分析高维离散数据矩阵（特别是在电子健康记录EHR中）时，维度不平衡带来的估计挑战。作者提出了一种知识嵌入的潜在投影模型，利用外部语义嵌入来增强表示学习，通过在再生核希尔伯特空间中的映射，将列嵌入建模为这些语义嵌入的平滑函数。该方法结合了核主成分分析和投影梯度下降，在广泛的模拟和实际EHR应用中显示出有效性，并通过建立的估计误差界限和优化过程的局部收敛保证提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Compiler for Secure Agentic Systems</div>
<div class="meta-line">Authors: Nils Palumbo, Sarthak Choudhary, Jihye Choi, Prasad Chalasani, Mihai Christodorescu, Somesh Jha</div>
<div class="meta-line">First: 2026-02-18T18:57:12+00:00 · Latest: 2026-02-18T18:57:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16708v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全代理系统的政策编译器</div>
<div class="mono" style="margin-top:8px">基于大型语言模型的代理越来越多地在需要复杂授权政策的环境中部署：客户服务协议、审批工作流、数据访问限制和合规性。将这些政策嵌入提示中无法提供强制执行保证。我们提出了PCAS，一个用于代理系统的政策编译器，提供确定性的政策执行。
 执行这些政策需要跟踪代理之间的信息流，而线性消息历史无法捕捉。相反，PCAS将代理系统状态建模为捕捉事件之间因果关系的依赖图，如工具调用、工具结果和消息。政策以Datalog派生语言表达，作为声明性规则，考虑传递性信息流和跨代理来源。参考监视器拦截所有操作，并在执行前阻止违规，提供独立于模型推理的确定性执行。
 PCAS接受现有的代理实现和政策规范，并将其编译成一个在构建时即符合政策的仪器化系统，无需安全特定的重构。我们在三个案例研究中评估PCAS：用于提示注入防御的信息流政策、多代理药物监测系统中的审批工作流，以及客户服务的组织政策。在客户服务任务中，PCAS将政策合规性从48%提高到93%，在仪器化运行中没有政策违规。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for secure enforcement of complex authorization policies in LLM-based agents used in various applications, such as customer service and regulatory compliance. The authors introduce PCAS, a Policy Compiler for Agentic Systems, which utilizes a dependency graph to model the state of the agentic system and enforce policies expressed in a Datalog-derived language. Experimental results demonstrate that PCAS significantly enhances policy compliance in customer service tasks, improving it from 48% to 93% across different models, while ensuring no policy violations occur during instrumented runs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要在用于客户服务和合规等各种场景中的基于LLM的代理中安全地执行复杂的授权政策。作者提出了PCAS，一个代理系统的政策编译器，它利用依赖图来建模代理系统状态，并捕捉事件之间的因果关系，从而实现确定性的政策执行。实验结果表明，PCAS在客户服务任务中的政策合规性显著提高，从48%提升至93%，在仪器化运行中没有观察到政策违规。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习类人机器人末端执行器控制以实现开放词汇视觉运动操控</div>
<div class="mono" style="margin-top:8px">在野外使用类人机器人对任意物体进行视觉运动操控需要准确的末端执行器（EE）控制和通过视觉输入（例如，RGB-D图像）对场景的可推广理解。现有方法基于真实世界的模仿学习，由于收集大规模训练数据集的困难，表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人对象运动操控，结合了大型视觉模型的强泛化和开放词汇理解与模拟训练的强控制性能。我们通过设计一种准确的残差感知EE跟踪策略来实现这一目标。该EE跟踪策略结合了经典机器人技术与机器学习。它使用a) 逆向运动学将残差末端执行器目标转换为参考轨迹，b) 学习的神经前向模型以实现准确的前向运动学，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差降低了3.2倍。我们使用这个准确的末端执行器跟踪器构建了一个模块化的运动操控系统，在该系统中，我们使用开放词汇的大型视觉模型实现强大的视觉泛化。我们的系统能够在多样的真实环境中操作，从办公室到咖啡店，机器人能够可靠地操控各种日常物体（例如，杯子、苹果、玩具），在高度范围为43cm到92cm的表面上。系统在模拟和现实世界中的系统模块化和端到端测试证明了我们提出设计的有效性。我们相信本文的进展可以开辟类人机器人与日常物体互动的新训练方式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the visual loco-manipulation capabilities of humanoid robots for interacting with arbitrary objects in diverse real-world environments, addressing the limitations of existing imitation learning methods. The authors introduce a novel paradigm called HERO, which integrates large vision models for generalization with a robust end-effector (EE) control policy that employs inverse kinematics, a learned neural forward model, goal adjustment, and replanning. Experimental results indicate that this approach reduces end-effector tracking error by 3.2 times and enables the robot to effectively manipulate various everyday objects across different heights in real-world settings, demonstrating significant improvements in both simulation and practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强类人机器人在多样化真实环境中与任意物体进行视觉运动操控的能力，解决现有模仿学习方法的局限性。作者提出了一种名为HERO的新范式，该范式将大型视觉模型的泛化能力与强大的末端执行器跟踪策略相结合，该策略采用逆向运动学、学习的神经前向模型、目标调整和重新规划。实验结果表明，该方法将末端执行器跟踪误差降低了3.2倍，并使机器人能够在不同表面和环境中有效操控各种日常物品，通过模拟和现实世界测试展示了系统的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology</div>
<div class="meta-line">Authors: Shen Zhou Hong, Alex Kleinman, Alyssa Mathiowetz, Adam Howes, Julian Cohen, Suveer Ganta, Alex Letizia, Dora Liao, Deepika Pahari, Xavier Roberts-Gaal, Luca Righetti, Joe Torres</div>
<div class="meta-line">First: 2026-02-18T18:51:28+00:00 · Latest: 2026-02-18T18:51:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16703v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a &quot;typical&quot; reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>测量2025年中期大型语言模型对生物学新手表现的辅助作用</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生物学基准测试中表现强劲，引发了它们可能帮助新手获得双用途实验室技能的担忧。然而，这是否转化为在实际实验室中的人类表现提升仍不清楚。为此，我们进行了一个预注册、研究者盲法、随机对照试验（2025年6月至8月；n = 153），评估LLMs是否改善新手在模拟病毒逆转录遗传学工作流程的任务中的表现。我们观察到工作流程完成的主要终点（LLM为5.2%，互联网为6.6%；P = 0.759）以及各个任务的成功率没有显著差异。然而，LLM组在五个任务中的四个任务的成功率数值上更高，尤其是在细胞培养任务中（LLM为68.8%，互联网为55.3%；P = 0.059）。后验贝叶斯建模的汇总数据估计，在LLM辅助下，“典型”逆转录遗传学任务的成功率约增加1.4倍（95% CrI 0.74-2.62）。序数回归建模表明，LLM组的参与者在所有任务中更可能通过中间步骤（正效应的后验概率：81%-96%）。总体而言，2025年中期的LLMs并未显著提高新手完成复杂实验室程序的能力，但与适度的表现提升相关。这些结果揭示了计算基准与现实世界效用之间的差距，强调了随着模型能力和用户熟练度的发展，物理世界验证AI生物安全评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the potential of large language models (LLMs) to enhance novice performance in laboratory tasks, motivated by concerns over their ability to impart dual-use skills. A pre-registered, investigator-blinded, randomized controlled trial was conducted with 153 participants to assess the impact of LLMs on completing a viral reverse genetics workflow. The findings indicated no significant difference in overall workflow completion rates between the LLM and Internet groups, though the LLM group showed higher success rates in four out of five tasks, particularly in cell culture. Bayesian modeling suggested a modest performance improvement with LLM assistance, highlighting a discrepancy between LLM capabilities and their practical application in laboratory settings.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在生物实验室任务中提升新手表现的潜力，动机源于其在生物基准测试中的强大表现。研究团队进行了一个预注册、盲法、随机对照试验，涉及153名参与者，评估LLM辅助对完成病毒逆转录遗传学工作流程的影响。结果显示，LLM组与互联网组在整体工作流程完成率上没有显著差异，尽管LLM组在五个任务中的四个任务中表现出更高的成功率，尤其是在细胞培养任务中。贝叶斯建模表明，在LLM辅助下存在适度的表现提升，突显了LLM能力与其在实验室实际应用之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</div>
<div class="meta-line">Authors: Mingjia Shi, Yinhan He, Yaochen Zhu, Jundong Li</div>
<div class="meta-line">First: 2026-02-18T18:49:56+00:00 · Latest: 2026-02-18T18:49:56+00:00</div>
<div class="meta-line">Comments: preprint 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16702v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关注显著性的多路径思维：重访视觉-语言推理</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）旨在通过联合利用视觉和文本模态进行推理。尽管在大型语言模型（LLMs）中分配额外的推理时间计算已被证明有效，但在VLMs中实现类似的扩展仍然具有挑战性。一个关键障碍是视觉输入通常仅在生成开始时提供一次，而文本推理（例如，早期视觉摘要）是自回归生成的，导致推理越来越以文本为主，并使早期视觉基础错误积累。此外，推理过程中对视觉基础的普通指导通常粗糙且嘈杂，使得在长文本中引导推理变得困难。为了解决这些挑战，我们提出了关注显著性原则（SAP）选择。SAP基于高层推理原则而非标记级轨迹操作，这使得在嘈杂反馈下对离散生成进行稳定控制，同时允许后续推理步骤在需要重新基础时重新咨询视觉证据。此外，SAP支持多路径推理，能够并行探索多样的推理行为。SAP是模型无关和无数据的，不需要额外的训练。实证结果表明，SAP在可比的标记生成预算下实现了竞争性能，特别是在减少对象幻觉方面，同时比CoT风格的长序列推理提供了更稳定的推理和更低的响应延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges faced by vision-language models (VLMs) in effectively integrating visual and textual modalities for reasoning, particularly the limitations of static visual inputs and noisy guidance during inference. The authors propose the Saliency-Aware Principle (SAP) selection method, which focuses on high-level reasoning principles to enhance control over generation and allows for multi-route inference, enabling diverse reasoning behaviors without requiring additional training. Experimental results demonstrate that SAP significantly reduces object hallucination and improves reasoning stability and response latency compared to traditional long sequential reasoning methods, all while maintaining competitive performance within similar token-generation budgets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉语言模型中的视觉推理，这些模型由于文本生成的顺序特性而在视觉定位方面面临挑战。作者提出了基于显著性意识原则（SAP）选择的方法，该方法侧重于高层次的推理原则，而非基于标记的轨迹，从而更好地控制生成过程，并在需要时能够重新审视视觉证据。实验结果表明，SAP显著减少了物体幻觉，并在响应延迟更低的情况下提供了更稳定的推理，相较于传统的长序列推理方法，且无需额外训练。</div>
</details>
</div>
<div class="card">
<div class="title">Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents</div>
<div class="meta-line">Authors: Wenxuan Ding, Nicholas Tomlin, Greg Durrett</div>
<div class="meta-line">First: 2026-02-18T18:46:14+00:00 · Latest: 2026-02-18T18:46:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>校准后行动：LLM代理中的成本感知探索</div>
<div class="mono" style="margin-top:8px">LLM越来越多地用于复杂问题，这些问题不一定在单次响应中解决，而是需要与环境互动以获取信息。在这些场景中，LLM必须考虑何时停止探索并承诺答案时固有的成本-不确定性权衡。例如，在编程任务中，如果LLM对生成的代码片段的正确性不确定，则应测试该代码；编写测试的成本是非零的，但通常低于犯错的成本。在这项工作中，我们展示了如何引导LLM明确考虑平衡这些成本-不确定性权衡，然后进行更优的环境探索。我们将多个任务，包括信息检索和编码，形式化为不确定性下的序列决策问题。每个问题都有潜在的环境状态，可以通过传递给LLM代理的先验进行推理。我们引入了一个名为校准后行动（CTA）的框架，在该框架中，我们向LLM提供额外的上下文，以使其能够更优化地行动。即使在基线和CTA的强化学习训练下，这种改进也得以保留。我们在信息寻求问答和简化编码任务上的结果表明，通过CTA明确成本-收益权衡可以帮助代理发现更优的决策策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the decision-making capabilities of large language models (LLMs) in complex environments where they must balance cost and uncertainty when exploring options. The authors propose a framework called Calibrate-Then-Act (CTA), which allows LLMs to reason about cost-uncertainty tradeoffs by providing them with additional contextual information. Experimental results demonstrate that LLMs using the CTA framework perform better in tasks such as information retrieval and coding, leading to more optimal exploration strategies and improved decision-making outcomes, even when subjected to reinforcement learning training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型语言模型（LLMs）在需要与环境互动的复杂问题解决场景中的决策能力。作者提出了一种名为Calibrate-Then-Act（CTA）的方法，使LLMs能够在决定是否进一步探索或承诺答案时考虑成本和不确定性之间的权衡。实验结果表明，使用CTA框架的LLMs在信息检索和编码等任务中表现更佳，通过明确成本效益权衡，导致更优化的探索策略，即使在强化学习训练下也能保持这种改进。</div>
</details>
</div>
<div class="card">
<div class="title">Causality is Key for Interpretability Claims to Generalise</div>
<div class="meta-line">Authors: Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar</div>
<div class="meta-line">First: 2026-02-18T18:45:04+00:00 · Latest: 2026-02-18T18:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#x27;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果关系是可解释性主张普遍化的关键</div>
<div class="mono" style="margin-top:8px">对大型语言模型（LLMs）的可解释性研究提供了对模型行为的重要见解，但仍存在反复出现的陷阱：无法普遍化的发现和超出证据的因果解释。我们的观点是，因果推断明确了什么构成了从模型激活到不变高层结构的有效映射，以及实现这一点所需的数据或假设，以及它可以支持的推断。具体而言，Pearl的因果层次结构阐明了可解释性研究可以证明的内容。观察建立了模型行为与内部组件之间的关联。干预（例如，消融或激活修补）支持关于这些编辑如何影响行为指标（例如，令牌概率的平均变化）在一组提示上的主张。然而，反事实主张——即询问在未观察到的干预下，同一提示的模型输出会是什么——在没有控制监督的情况下仍然大部分无法验证。我们展示了因果表示学习（CRL）如何将这一层次结构操作化，明确哪些变量可以从激活中恢复以及在什么假设下。总之，这些动机促成了一个诊断框架，帮助从业者选择与主张和证据相匹配的方法和评估，以便发现能够普遍化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations in interpretability studies of large language models (LLMs), particularly the issues of non-generalizable findings and unsupported causal interpretations. The authors propose using causal inference to establish valid mappings from model activations to high-level structures, employing Pearl&#x27;s causal hierarchy to clarify the justifications of interpretability studies. The key experimental findings indicate that while associations between model behavior and internal components can be observed, counterfactual claims remain largely unverifiable without controlled supervision, and the introduction of causal representation learning (CRL) provides a framework for identifying recoverable variables from activations, thereby enhancing the generalizability of interpretability claims.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）可解释性研究中的局限性，特别是非可推广的发现和缺乏支持的因果解释问题。作者建议使用因果推断来建立模型激活与高层结构之间的有效映射，遵循Pearl的因果层次结构。他们的研究结果表明，尽管可以观察到模型行为与内部组件之间的关联，但关于未观察干预下模型输出的反事实声明仍然在很大程度上无法验证，这突显了需要一个诊断框架，以将方法和评估与证据对齐，从而增强结果的可推广性。</div>
</details>
</div>
<div class="card">
<div class="title">Protecting the Undeleted in Machine Unlearning</div>
<div class="meta-line">Authors: Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer</div>
<div class="meta-line">First: 2026-02-18T18:44:21+00:00 · Latest: 2026-02-18T18:44:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>保护未删除数据的机器遗忘</div>
<div class="mono" style="margin-top:8px">机器遗忘旨在从训练模型中移除特定数据点，通常努力模拟“完美重训练”，即生成如果未包含被删除数据时将获得的模型。我们证明这种方法及其支持的安全定义对剩余（未删除）数据点存在显著的隐私风险。我们展示了一种重构攻击，表明对于某些任务，这些任务可以在没有删除的情况下安全计算，遵循完美重训练的机制允许一个仅控制$ω(1)$个数据点的对手通过发出删除请求重构几乎整个数据集。我们调查了现有的机器遗忘定义，表明它们要么易受此类攻击，要么过于严格以支持基本功能，如精确求和。为了解决这个问题，我们提出了一种新的安全定义，专门保护未删除数据免受其他点删除引起的泄露。我们展示了我们的定义允许几种基本功能，如公告板、求和和统计学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the privacy risks associated with machine unlearning, particularly concerning the remaining undeleted data points after specific data points are removed from a trained model. The authors conducted a reconstruction attack to demonstrate that existing methods, which aim for perfect retraining, can expose undeleted data to adversaries who control a minimal number of data points. They found that current definitions of machine unlearning are either vulnerable to such attacks or too restrictive to allow basic functionalities. To mitigate these issues, the authors propose a new security definition that protects undeleted data from leakage while enabling essential functionalities like bulletin boards and statistical learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决机器遗忘中与未删除数据点相关的隐私风险，特别是在从训练模型中移除特定数据时。作者展示了一种重构攻击，揭示了即使仅控制少量数据点，攻击者也可以通过发出删除请求重构出数据集的很大一部分。为了减轻这些风险，他们提出了一种新的安全定义，旨在保护未删除数据免受泄露，同时仍允许基本功能，如公告板、求和和统计学习。</div>
</details>
</div>
<div class="card">
<div class="title">Parameter-free representations outperform single-cell foundation models on downstream benchmarks</div>
<div class="meta-line">Authors: Huan Souza, Pankaj Mehta</div>
<div class="meta-line">First: 2026-02-18T18:42:29+00:00 · Latest: 2026-02-18T18:42:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16696v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无参数表示在下游基准测试中优于单细胞基础模型</div>
<div class="mono" style="margin-top:8px">单细胞RNA测序（scRNA-seq）数据表现出强烈且可重复的统计结构。这促使了大规模基础模型的发展，如TranscriptFormer，它使用基于变换器的架构通过将基因嵌入到潜在向量空间中来学习基因表达的生成模型。这些嵌入已被用于在下游任务中获得最先进（SOTA）的性能，如细胞类型分类、疾病状态预测和跨物种学习。在这里，我们探讨是否可以在不利用计算密集型深度学习表示的情况下实现类似的性能。通过使用依赖于仔细归一化和线性方法的简单、可解释的流程，我们在多个常用的基准测试中获得了SOTA或接近SOTA的性能，包括在涉及训练数据中缺失的新细胞类型和生物的分布外任务中超越基础模型。我们的发现强调了严格基准测试的必要性，并表明细胞身份的生物学可以通过单细胞基因表达数据的简单线性表示来捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore whether high performance in analyzing single-cell RNA sequencing (scRNA-seq) data can be achieved without the complexity of deep learning-based models. The authors employed simple, interpretable pipelines that focus on normalization and linear methods to analyze the data. The key findings indicate that these parameter-free representations can achieve state-of-the-art or near state-of-the-art performance on various benchmarks, even outperforming complex foundation models on out-of-distribution tasks involving novel cell types and organisms not included in the training data.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在分析单细胞RNA测序数据时，是否可以在不使用复杂深度学习模型的情况下实现高性能。作者采用基于归一化和线性技术的简单可解释方法来评估其有效性。结果表明，这些无参数表示在多个基准测试中实现了最先进或接近最先进的性能，甚至在涉及训练数据中不存在的新细胞类型和生物体的任务中超越了传统基础模型。</div>
</details>
</div>
<div class="card">
<div class="title">EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents</div>
<div class="meta-line">Authors: Sara Fish, Julia Shephard, Minkai Li, Ran I. Shorrer, Yannai A. Gonczarowski</div>
<div class="meta-line">First: 2025-03-24T16:06:04+00:00 · Latest: 2026-02-18T18:37:52+00:00</div>
<div class="meta-line">Comments: v3 was a major revision with updated experiments and analysis; v4 consists of minor edits</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18825v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.18825v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM&#x27;s ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM&#x27;s choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM&#x27;s tradeoff response, a reliability score, which measures the coherence of an LLM&#x27;s choice behavior, and a competency score, which measures an LLM&#x27;s capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs&#x27; choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EconEvals：LLM代理的经济决策基准和试金石测试</div>
<div class="mono" style="margin-top:8px">我们开发了评估方法，以测量LLM的经济决策能力和倾向。首先，我们开发了基于经济学关键问题（如采购、调度和定价）的基准，测试LLM在上下文中从环境中学习的能力。其次，我们开发了试金石测试框架，这些评估量化了LLM在具有多个冲突目标的风格化决策任务中的选择行为。每个试金石测试输出一个试金石分数，量化LLM的权衡反应，一个可靠性分数，测量LLM选择行为的一致性，以及一个能力分数，测量LLM在冲突目标被单一、明确目标替代时在同一任务上的能力。通过评估广泛的前沿LLM，我们（1）研究LLM能力和倾向随时间的变化，（2）从LLM的选择行为和思维链中得出经济上有意义的见解，（3）通过测试自我一致性、稳健性和可推广性来验证我们的试金石测试框架。总体而言，这项工作为评估LLM代理在进一步融入经济决策中提供了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to establish evaluation methods for assessing the economic decision-making abilities of large language models (LLMs). The authors developed benchmarks based on key economic problems such as procurement, scheduling, and pricing, and introduced a framework of litmus tests to quantify LLM choice behavior in decision-making tasks with conflicting objectives. The experimental findings reveal changes in LLM capabilities over time, provide economically relevant insights from LLM choice behavior, and validate the litmus test framework through assessments of self-consistency, robustness, and generalizability, thereby laying a foundation for the integration of LLMs into economic decision-making processes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是建立评估大型语言模型（LLMs）经济决策能力的方法。作者基于采购、调度和定价等关键经济问题开发了基准，并引入了量化LLM在具有冲突目标的决策任务中选择行为的试金石测试框架。实验结果显示LLM能力随时间变化，从LLM选择行为中提供了经济上有意义的见解，并通过自一致性、稳健性和普适性的评估验证了试金石测试框架，从而为将LLM整合到经济决策过程中奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Random Scaling of Emergent Capabilities</div>
<div class="meta-line">Authors: Rosie Zhao, Tian Qin, David Alvarez-Melis, Sham Kakade, Naomi Saphra</div>
<div class="meta-line">First: 2025-02-24T17:34:45+00:00 · Latest: 2026-02-18T18:37:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17356v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.17356v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models famously improve under a smooth scaling law, but some specific capabilities exhibit sudden breakthroughs in performance. Advocates of &quot;emergence&quot; view these capabilities as unlocked at a specific scale, but others attribute breakthroughs to superficial metric thresholding effects. We propose that breakthroughs are instead driven by continuous changes in the probability distribution of training outcomes when performance is bimodally distributed across random seeds. we show that different random seeds can produce either smooth or emergent scaling trends in synthetic length generalization tasks, multiple choice question answering, and grammatical generalization. We reveal that sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds. These distributions may become abruptly bimodal at a capacity threshold but this threshold appears at scales well before most seeds achieve breakthrough. Our observations hold true even under continuous loss metrics, confirming that random variation must be considered when predicting a model&#x27;s performance from its scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>新兴能力的随机缩放</div>
<div class="mono" style="margin-top:8px">语言模型在平滑缩放法则下显著提升，但某些特定能力表现出突发的性能突破。&quot;新兴&quot;的支持者认为这些能力在特定规模下被解锁，而其他人则将突破归因于表面指标阈值效应。我们提出，突破是由训练结果概率分布的连续变化驱动的，当性能在随机种子之间呈双峰分布时。我们展示了不同的随机种子可以在合成长度泛化任务、多项选择问答和语法泛化中产生平滑或新兴的缩放趋势。我们揭示，指标的急剧突破是由其在种子间分布的潜在连续变化产生的。这些分布可能在容量阈值处突然变为双峰，但这一阈值出现在大多数种子实现突破之前的规模。我们的观察在连续损失指标下依然成立，确认在根据模型的规模预测性能时必须考虑随机变异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the phenomenon of sudden performance breakthroughs in language models, which are often attributed to either emergence at specific scales or superficial metric thresholding effects. The authors propose that these breakthroughs are actually driven by continuous changes in the probability distribution of training outcomes influenced by random seeds. Through experiments on synthetic length generalization tasks, multiple choice question answering, and grammatical generalization, they demonstrate that different random seeds can lead to either smooth or emergent scaling trends, revealing that sharp metric breakthroughs are linked to underlying continuous distribution changes across seeds, with bimodal distributions appearing at capacity thresholds that are reached before most seeds achieve significant performance improvements.</div>
<div class="mono" style="margin-top:8px">本研究探讨了语言模型中突然性能突破的现象，这种现象通常归因于特定规模的涌现或表面指标阈值效应。作者提出，这些突破是由随机种子影响下训练结果概率分布的连续变化驱动的。通过对合成长度泛化任务、多项选择问答和语法泛化的实验，他们证明不同的随机种子可以产生平滑或涌现的缩放趋势，揭示出尖锐的指标突破与种子间的连续分布变化相关，双峰分布在容量阈值处出现，而这一阈值出现在大多数种子达到突破性能之前。</div>
</details>
</div>
<div class="card">
<div class="title">Synthetic-Powered Multiple Testing with FDR Control</div>
<div class="meta-line">Authors: Yonghoon Lee, Meshi Bashari, Edgar Dobriban, Yaniv Romano</div>
<div class="meta-line">First: 2026-02-18T18:36:24+00:00 · Latest: 2026-02-18T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成驱动的多重检验与FDR控制</div>
<div class="mono" style="margin-top:8px">带有假发现率（FDR）控制的多重假设检验是统计推断中的一个基本问题，广泛应用于基因组学、药物筛选和异常值检测。在许多这样的场景中，研究人员不仅可以访问真实的实验观察数据，还可以访问辅助或合成数据——来自过去相关实验或由生成模型生成的数据——这些数据可以为感兴趣的假设提供额外证据。我们引入了SynthBH，这是一种安全利用合成数据的合成驱动多重检验程序。我们证明了在温和的PRDS型正依赖条件下，SynthBH保证有限样本、分布无关的FDR控制，而无需要求合并数据的p值在原假设下有效。该方法适应合成数据的（未知）质量：当合成数据质量高时，它提高了样本效率并可能增强检验能力，同时在用户指定的水平上控制FDR，无论其质量如何。我们在表格异常值检测基准和药物-癌症敏感性关联的基因组分析中展示了SynthBH的实证性能，并通过对模拟数据的控制实验进一步研究其性质。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to improve multiple hypothesis testing with false discovery rate (FDR) control, particularly in fields like genomics and drug screening, where synthetic data can provide additional insights. The authors introduce SynthBH, a novel procedure that incorporates synthetic data to enhance testing efficiency while ensuring FDR control under specific conditions. Experimental results show that SynthBH performs effectively on outlier detection benchmarks and genomic analyses, demonstrating improved sample efficiency and power when high-quality synthetic data is available, while maintaining FDR control regardless of data quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善多重假设检验中的假发现率（FDR）控制，这在基因组学和药物筛选等领域至关重要。作者提出了SynthBH，这是一种利用真实数据和合成数据的程序，以提高检验效率和能力，同时保持FDR控制。实验结果表明，SynthBH有效适应合成数据的质量，在异常值检测基准和药物-癌症敏感性关联的基因组分析中表现出改善的性能，同时确保在指定水平下的FDR控制，无论数据质量如何。</div>
</details>
</div>
<div class="card">
<div class="title">Are Object-Centric Representations Better At Compositional Generalization?</div>
<div class="meta-line">Authors: Ferdinand Kapl, Amir Mohammad Karimi Mamaghan, Maximilian Seitzer, Karl Henrik Johansson, Carsten Marr, Stefan Bauer, Andrea Dittadi</div>
<div class="meta-line">First: 2026-02-18T18:34:07+00:00 · Latest: 2026-02-18T18:34:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以对象为中心的表示在组合泛化方面更好吗？</div>
<div class="mono" style="margin-top:8px">组合泛化，即对熟悉概念的新组合进行推理的能力，是人类认知的基础，也是机器学习面临的关键挑战。以对象为中心（OC）的表示将场景编码为一组对象，通常被认为支持这种泛化，但在视觉丰富的环境中系统证据有限。我们引入了一个视觉问答基准，涵盖三个受控视觉世界（CLEVRTex、Super-CLEVR 和 MOVi-C），以测量具有和不具有对象中心偏见的视觉编码器在未见对象属性组合上的泛化能力。为了确保公平和全面的比较，我们仔细考虑了训练数据的多样性、样本大小、表示大小、下游模型能力和计算。我们使用 DINOv2 和 SigLIP2，这两个广泛使用的视觉编码器，作为基础模型及其 OC 对应模型。我们的主要发现揭示了（1）OC 方法在更困难的组合泛化设置中表现优越；（2）原始密集表示仅在较简单的设置中超越 OC，通常需要显著更多的下游计算；（3）OC 模型在样本效率上更高，以更少的图像实现更强的泛化，而密集编码器只有在数据和多样性充足时才能追赶或超越它们。总体而言，当数据集大小、训练数据多样性或下游计算受到限制时，以对象为中心的表示提供了更强的组合泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the effectiveness of object-centric (OC) representations in achieving compositional generalization, a crucial aspect of human cognition and a challenge in machine learning. The authors developed a Visual Question Answering benchmark across three visual worlds to evaluate the generalization capabilities of vision encoders with and without OC biases. The findings indicate that OC approaches excel in challenging compositional generalization tasks, while dense representations perform better only in simpler scenarios but require more computational resources. Additionally, OC models demonstrate greater sample efficiency, achieving better generalization with fewer images compared to dense encoders, which need larger datasets to perform comparably.</div>
<div class="mono" style="margin-top:8px">本研究探讨了物体中心（OC）表示在实现组合泛化方面的有效性，这是人类认知的一个关键方面，也是机器学习中的一个挑战。作者引入了一个跨三个视觉环境的视觉问答基准，以评估具有和不具有OC偏见的视觉编码器在新颖物体属性组合上的泛化能力。研究结果表明，OC方法在更复杂的组合泛化场景中表现优越，而传统的密集表示仅在简单情况下表现更好，但需要更多的计算资源。此外，OC模型表现出更高的样本效率，在较少的图像下实现更好的泛化，而密集编码器则需要更大的数据集才能达到类似的表现。</div>
</details>
</div>
<div class="card">
<div class="title">On the Hardness of Approximation of the Fair k-Center Problem</div>
<div class="meta-line">Authors: Suhas Thejaswi</div>
<div class="meta-line">First: 2026-02-18T18:33:27+00:00 · Latest: 2026-02-18T18:33:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16688v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $ε&gt;0$, achieving a $(3-ε)$-approximation is NP-hard, assuming $\text{P} \neq \text{NP}$.
  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于公平 k-中心问题的近似难度</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们研究了公平 $k$-中心问题的近似难度。这里数据点被划分为组，任务是从每组中选择预定数量的数据点，称为中心，同时最小化任何点到其最近中心的最大距离。尽管在一般度量下已知该问题有多项式时间的 $3$-近似，但尚不清楚该近似保证是否是紧的或是否可以进一步改进，特别是因为无约束的 $k$-中心问题允许多项式时间的因子-$2$ 近似。我们通过证明，对于每个 $ε&gt;0$，实现 $(3-ε)$-近似是 NP-困难的，假设 $\text{P} \neq \text{NP}$，来解决这个悬而未决的问题。我们的不可近似性结果即使在只有两个不相交的组存在且每组必须选择至少一个中心的情况下也成立。此外，它扩展到具有 $k$-组的典型每组一个的设置（对于任意 $k$），其中必须从每组中选择一个中心。因此，公平 $k$-中心在一般度量空间中的因子-$3$ 障碍是固有的，即使在这些受限的情况下，现有的 $3$-近似算法也是最优的，直到低阶项。这一结果与 $k$-供应商的表述形成鲜明对比，在该表述中，无约束和公平变体都允许在多项式时间内实现因子-$3$ 近似。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the complexity of approximating the fair k-center problem, where data points are divided into groups and centers are selected to minimize the maximum distance to the nearest center. The authors investigate whether the known polynomial-time 3-approximation is optimal or can be improved, particularly in light of the unconstrained k-center problem&#x27;s factor-2 approximation. They demonstrate that achieving a (3-ε)-approximation is NP-hard for any ε&gt;0, even with just two groups, establishing that the 3-approximation barrier is inherent in fair k-center problems across general metric spaces and confirming the optimality of existing algorithms within these constraints.</div>
<div class="mono" style="margin-top:8px">本研究探讨了公平k-中心问题的近似难度，该问题涉及从分组数据中选择中心，同时最小化到最近中心的最大距离。作者证明，在假设P ≠ NP的情况下，实现(3-ε)-近似是NP难的，从而解决了已知的多项式时间3-近似是否紧的这一问题。这种不可近似性即使在只有两个组的情况下也成立，并扩展到需要从k个组中选择一个中心的场景，表明在一般度量空间中公平k-中心问题的因子3障碍是固有的，现有算法在低阶项上是最优的。</div>
</details>
</div>
<div class="card">
<div class="title">MC-LLaVA: Multi-Concept Personalized Vision-Language Model</div>
<div class="meta-line">Authors: Ruichuan An, Sihan Yang, Renrui Zhang, Ming Lu, Tianyi Jiang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang</div>
<div class="meta-line">First: 2024-11-18T16:33:52+00:00 · Latest: 2026-02-18T18:33:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11706v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.11706v4">PDF</a> · <a href="https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA">Code1</a> · <a href="https://github.com/arctanxarc/MC-LLaVA">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MC-LLaVA：多概念个性化视觉语言模型</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型（VLMs）在多种任务中表现出色，例如视觉问答。为了提升用户体验，近期研究探讨了VLM个性化，以理解用户提供的概念。然而，它们主要集中于单一概念，忽视了多个概念的存在和相互作用，这限制了其在现实世界中的适用性。本文提出了MC-LLaVA，一个多概念个性化范式。具体而言，MC-LLaVA采用多概念指令调优策略，在单个训练步骤中有效整合多个概念。为了降低训练成本，我们提出了一种个性化文本提示，利用视觉标记信息初始化概念标记。此外，我们在推理过程中引入个性化视觉提示，聚合位置图以增强识别和定位能力。为了进一步提升性能上限，我们引入了可选的辅助损失，更好地增强所提出的个性化提示。为了丰富VLM个性化研究，我们贡献了一个高质量的数据集。我们仔细收集了来自电影的多角色和物体的图像，并手动创建了多概念场景的问题-答案样本，具有优越的多样性。全面的实验表明，MC-LLaVA实现了令人印象深刻的多概念个性化响应，为VLM成为更好的用户助手铺平了道路。代码和数据集将发布在\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the personalization of vision-language models (VLMs) by addressing the limitations of existing approaches that focus on single concepts. The authors propose MC-LLaVA, a multi-concept personalization paradigm that utilizes a multi-concept instruction tuning strategy to integrate multiple concepts in a single training step. Experimental results show that MC-LLaVA significantly enhances the ability to generate personalized responses by employing personalized textual and visual prompts, leading to improved recognition and grounding capabilities, thus advancing the functionality of VLMs as user assistants.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法仅关注单一概念的局限性，来改善视觉语言模型（VLM）的个性化，从而增强其在现实世界中的适用性。作者提出了MC-LLaVA，一个多概念个性化范式，利用多概念指令调优策略在单次训练步骤中整合多个概念，并通过个性化文本和视觉提示来优化训练成本和提高识别能力。实验结果表明，MC-LLaVA在生成多概念个性化响应方面取得了显著进展，表明其在增强用户与VLM互动方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition</div>
<div class="meta-line">Authors: Bo Pan, Peter Zhiping Zhang, Hao-Wei Pang, Alex Zhu, Xiang Yu, Liying Zhang, Liang Zhao</div>
<div class="meta-line">First: 2026-02-18T18:27:21+00:00 · Latest: 2026-02-18T18:27:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16684v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于匹配分子对转化的检索增强基础模型以重现药物化学直觉</div>
<div class="mono" style="margin-top:8px">匹配分子对（MMPs）捕捉药物化学家在设计类似物时常用的局部化学编辑，但现有的机器学习方法要么在整体分子水平上操作，编辑可控性有限，要么在受限环境和小模型中学习MMP风格的编辑。我们提出了一种变量到变量的类生成公式，并在大规模MMP转化（MMPTs）上训练基础模型，以生成基于输入变量的多样化变量。为了实现实际控制，我们开发了提示机制，让用户在生成过程中指定首选的转化模式。我们进一步引入MMPT-RAG，这是一种检索增强框架，利用外部参考类似物作为上下文指导来引导生成，并从项目特定系列中进行泛化。在一般化学语料库和专利特定数据集上的实验表明，改进了多样性、新颖性和可控性，并显示我们的方法在实际发现场景中恢复了现实的类似物结构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for improved methods in generating matched molecular pairs (MMPs) that reflect the intuitive chemical edits used by medicinal chemists. The authors propose a variable-to-variable formulation for analog generation, training a foundation model on large-scale MMP transformations (MMPTs) and implementing prompting mechanisms for user-defined transformation patterns. Experimental results indicate that their retrieval-augmented framework, MMPT-RAG, enhances diversity, novelty, and controllability in generated analogs, successfully recovering realistic structures in practical drug discovery contexts.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有机器学习方法在生成匹配分子对（MMPs）方面的局限性，这对药物化学至关重要。作者提出了一种变量到变量的类比生成形式，并在大规模MMP转化上训练了基础模型，结合了用户定义转化模式的提示机制。对多种化学数据集的实验结果表明，所提出的检索增强框架（MMPT-RAG）显著提高了生成类比的多样性、新颖性和可控性，有效恢复了药物发现中实际应用的真实结构。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Situated Awareness in the Real World</div>
<div class="meta-line">Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, Ming-Hsuan Yang, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-18T18:22:52+00:00 · Latest: 2026-02-18T18:22:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16682v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent&#x27;s viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model&#x27;s observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在现实世界中学习情境意识</div>
<div class="mono" style="margin-top:8px">人类感知的核心方面是情境意识，即将自己与周围物理环境联系起来并在上下文中推理可能的行动的能力。然而，现有的多模态基础模型（MFM）基准大多强调以环境为中心的空间关系（场景中物体之间的关系），而在很大程度上忽视了需要相对于代理的视角、姿态和运动进行推理的观察者中心关系。为了解决这一问题，我们引入了SAW-Bench（现实世界中的情境意识），这是一个用于评估以自我为中心的情境意识的新基准，使用真实世界的视频。SAW-Bench包含786个使用Ray-Ban Meta（第二代）智能眼镜自录制的视频，涵盖多种室内和室外环境，以及超过2071对人类注释的问题-答案对。它通过六个不同的意识任务探测模型的观察者中心理解。我们的综合评估揭示了即使是表现最佳的MFM，Gemini 3 Flash，人与模型之间的性能差距也达到了37.66%。除了这一差距，我们的深入分析还发现了一些显著的发现；例如，尽管模型可以利用自我中心视频中的部分几何线索，但它们往往无法推断出一致的相机几何形状，导致系统性的空间推理错误。我们将SAW-Bench定位为情境空间智能的基准，超越被动观察，理解物理基础的观察者中心动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in evaluating situated awareness in multimodal foundation models (MFMs), which typically focus on environment-centric spatial relations while neglecting observer-centric perspectives. To tackle this issue, the authors introduce SAW-Bench, a benchmark designed to assess egocentric situated awareness through 786 self-recorded videos captured with smart glasses, accompanied by over 2,071 human-annotated question-answer pairs across various environments. The findings reveal a significant performance gap of 37.66% between human and model responses, even with the top-performing model, Gemini 3 Flash, highlighting that while models can utilize some geometric cues, they struggle with coherent camera geometry, resulting in systematic spatial reasoning errors.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态基础模型（MFM）基准的局限性，这些基准主要关注环境中心的空间关系，而忽视了理解情境意识所需的观察者中心关系。作者引入了SAW-Bench，这是一个新颖的基准，旨在通过786个来自不同环境的自录视频和超过2071个人工标注的问题-答案对来评估自我中心的情境意识，评估模型在六个意识任务中的表现。研究结果显示，即使是表现最好的MFM，Gemini 3 Flash，人与模型之间的性能差距也达到37.66%，并强调虽然模型能够利用部分几何线索，但在一致的相机几何推断上存在困难，导致系统性的空间推理错误。</div>
</details>
</div>
<div class="card">
<div class="title">VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection</div>
<div class="meta-line">Authors: Yingyuan Yang, Tian Lan, Yifei Gao, Yimeng Lu, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang</div>
<div class="meta-line">First: 2026-02-18T18:22:22+00:00 · Latest: 2026-02-18T18:22:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16681v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16681v1">PDF</a> · <a href="https://github.com/yyyangcoder/VETime">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VETime：视觉增强的零-shot时间序列异常检测</div>
<div class="mono" style="margin-top:8px">时间序列异常检测（TSAD）需要识别即时的点异常和长期的上下文异常。然而，现有的基础模型面临一个根本性的权衡：一维时间模型提供细粒度的点位定位，但缺乏全局上下文视角，而二维基于视觉的模型捕捉全局模式，但由于缺乏时间对齐和粗粒度的点位检测而遭受信息瓶颈。为了解决这一困境，我们提出了VETime，这是第一个通过细粒度视觉-时间对齐和动态融合统一时间和视觉模态的TSAD框架。VETime引入了可逆图像转换和补丁级时间对齐模块，以建立共享的视觉-时间时间线，保留区分性细节，同时保持时间敏感性。此外，我们设计了一种异常窗口对比学习机制和任务自适应多模态融合，以自适应地整合两种模态的互补感知优势。大量实验表明，VETime在零-shot场景中显著优于最先进的模型，以更低的计算开销实现更高的定位精度。代码可在：https://github.com/yyyangcoder/VETime获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing time-series anomaly detection (TSAD) models, which struggle to balance fine-grained point anomaly localization with global contextual understanding. The authors propose VETime, a novel framework that integrates temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. Experimental results show that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving better localization precision while reducing computational overhead compared to current vision-based methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有时间序列异常检测（TSAD）模型的局限性，这些模型在细粒度点异常定位与全局上下文理解之间难以平衡。作者提出了VETime，这是一种新颖的框架，通过细粒度的视觉-时间对齐和动态融合整合了时间和视觉模态，利用可逆图像转换和补丁级时间对齐模块。实验结果表明，VETime在零样本场景中显著优于现有的最先进模型，实现了更高的定位精度，同时减少了与当前基于视觉的方法相比的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective</div>
<div class="meta-line">Authors: Feilong Liu</div>
<div class="meta-line">First: 2026-01-09T23:07:14+00:00 · Latest: 2026-02-18T18:17:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11616v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11616v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) architectures are widely used for efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly understood. We study MoEs through a geometric lens, interpreting routing as soft partitioning into overlapping expert-local charts. We introduce a Dual Jacobian-PCA spectral probe that analyzes local function geometry via Jacobian singular value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting with exact Jacobian computation, we compare dense, Top-k, and fully soft routing under matched capacity. Across random seeds, MoE routing consistently reduces local sensitivity: expert-local Jacobians show smaller leading singular values and faster spectral decay than dense baselines. Weighted PCA reveals that expert-local representations distribute variance across more principal directions, indicating higher effective rank. We further observe low alignment among expert Jacobians, suggesting decomposition into low-overlap expert-specific transformations. Routing sharpness modulates these effects: Top-k routing yields more concentrated, lower-rank expert structure, while fully soft routing produces broader, higher-rank representations. Experiments on a 3-layer transformer with WikiText confirm curvature reduction on natural language and show lower cross-expert alignment for Top-k routing. These findings support interpreting MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance, yielding testable predictions for expert scaling, hallucination reduction, and ensemble diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>专家混合模型作为软聚类：双雅可比-PCA谱几何视角</div>
<div class="mono" style="margin-top:8px">专家混合模型（MoE）架构因其效率和条件计算而被广泛使用，但它们对学习函数和表示的几何形状的影响仍然不甚了解。我们通过几何视角研究MoE，将路由解释为重叠专家局部图的软划分。我们引入了一种双雅可比-PCA谱探针，通过雅可比奇异值谱分析局部函数几何，并通过路由隐藏状态的加权PCA分析表示几何。使用具有精确雅可比计算的受控MLP-MoE设置，我们在匹配容量下比较了稠密、Top-k和完全软路由。在随机种子下，MoE路由始终减少局部敏感性：专家局部雅可比显示出较小的主奇异值和更快的谱衰减，优于稠密基线。加权PCA揭示专家局部表示在更多主方向上分配方差，表明有效秩更高。我们进一步观察到专家雅可比之间的低对齐，表明可以分解为低重叠的专家特定变换。路由的锐度调节这些效应：Top-k路由产生更集中、低秩的专家结构，而完全软路由则产生更广泛、高秩的表示。在WikiText上的3层变换器实验确认了自然语言的曲率降低，并显示Top-k路由的跨专家对齐较低。这些发现支持将MoE解释为函数空间的软划分，平坦局部曲率，同时重新分配表示方差，为专家扩展、幻觉减少和集成多样性提供可测试的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the geometric implications of Mixture-of-Experts (MoE) architectures, which are known for their efficiency but lack understanding regarding their impact on learned function geometry. The authors employ a Dual Jacobian-PCA spectral analysis to examine local function geometry and representation geometry in a controlled MLP-MoE setting, comparing different routing methods. The findings reveal that MoE routing reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay compared to dense baselines, while weighted PCA indicates a higher effective rank in expert-local representations, suggesting low alignment among expert Jacobians and varying effects based on routing sharpness, particularly in a transformer model with natural language data.</div>
<div class="mono" style="margin-top:8px">本研究探讨了混合专家（MoE）架构的几何意义，这些架构因其效率和条件计算能力而被广泛应用。作者引入了一种双雅可比-PCA谱探针，通过分析雅可比奇异值谱和对路由隐藏状态进行加权PCA，来研究MoE的局部函数几何和表示几何。研究结果表明，MoE路由始终降低局部敏感性，专家局部雅可比的主奇异值较小，谱衰减速度快于稠密基线，同时专家局部表示的有效秩更高，专家雅可比之间的低对齐度表明受路由锐度影响的专家特定变换。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees</div>
<div class="meta-line">Authors: Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano</div>
<div class="meta-line">First: 2025-09-24T17:37:14+00:00 · Latest: 2026-02-18T18:13:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20345v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.20345v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用无分布假设保证的合成数据进行统计推断</div>
<div class="mono" style="margin-top:8px">高质量合成数据的快速普及——由先进的人工智能模型生成或作为相关任务的辅助数据收集——为统计推断带来了机遇和挑战。本文介绍了一种通用合成驱动推断（GESPI）框架，该框架包裹在任何统计推断程序周围，通过结合合成数据和真实数据安全地增强样本效率。我们的框架利用高质量合成数据来提高统计功效，但在合成数据质量较低时自适应地默认使用仅真实数据的标准推断方法。我们的方法的误差保持在用户指定的界限以下，而不对合成数据做任何分布假设，并且随着合成数据质量的提高而降低。这种灵活性使得与保形预测、风险控制、假设检验和多重检验程序的无缝集成成为可能，且无需修改基础推断方法。我们在有限标记数据的挑战性任务上展示了我们方法的优势，包括AlphaFold蛋白质结构预测，以及在复杂数学问题上比较大型推理模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges and opportunities presented by the increasing availability of high-quality synthetic data for statistical inference. The authors propose the GEneral Synthetic-Powered Inference (GESPI) framework, which enhances sample efficiency by integrating synthetic and real data while ensuring that the error remains below a user-specified bound without requiring distributional assumptions on the synthetic data. Experimental results show that GESPI effectively boosts statistical power and adapts to the quality of synthetic data, demonstrating its utility in tasks with limited labeled data, such as AlphaFold protein structure prediction and complex math problem reasoning comparisons.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决高质量合成数据在统计推断中带来的挑战和机遇。作者提出了GEneral Synthetic-Powered Inference (GESPI)框架，该框架通过整合合成数据和真实数据来提高样本效率，同时保持标准推断方法的完整性。主要实验结果表明，GESPI框架能够有效提高统计能力，即使在合成数据质量变化的情况下，也能在不违反用户指定的误差界限的前提下运行，并成功应用于AlphaFold蛋白质结构预测和复杂数学推理问题等任务。</div>
</details>
</div>
<div class="card">
<div class="title">Neighborhood Stability as a Measure of Nearest Neighbor Searchability</div>
<div class="meta-line">Authors: Thomas Vecchiato, Sebastian Bruch</div>
<div class="meta-line">First: 2026-02-18T18:09:47+00:00 · Latest: 2026-02-18T18:09:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16673v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call &quot;searchability.&quot; To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邻域稳定性作为最近邻搜索性的度量</div>
<div class="mono" style="margin-top:8px">基于聚类的近似最近邻搜索（ANNS）将一组点组织成分区，仅搜索其中少数点以找到查询的最近邻。尽管这种方法很受欢迎，但几乎没有分析工具来确定基于聚类的ANNS对给定数据集的适用性——我们称之为“搜索性”。为了解决这个问题，我们提出了两个用于高维欧几里得空间平面聚类的度量。第一个是聚类邻域稳定性度量（clustering-NSM），这是聚类质量的内部度量——数据集聚类的函数——我们证明它可以预测ANNS的准确性。第二个是点邻域稳定性度量（point-NSM），这是聚类能力的度量——数据集本身的函数——可以预测聚类-NSM。这两者结合使我们能够仅根据数据点确定数据集是否可以通过基于聚类的ANNS进行搜索。重要的是，这两者都是点之间最近邻关系的函数，而不是距离，使其适用于各种距离函数，包括内积。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the lack of analytical tools to evaluate the suitability of clustering-based Approximate Nearest Neighbor Search (ANNS) for specific datasets, referred to as &#x27;searchability.&#x27; The authors introduce two measures: the Clustering-Neighborhood Stability Measure (clustering-NSM), which assesses clustering quality and predicts ANNS accuracy, and the Point-Neighborhood Stability Measure (point-NSM), which evaluates the clusterability of the dataset and predicts clustering-NSM. Experimental results demonstrate that these measures effectively determine the searchability of datasets for clustering-based ANNS by relying on nearest neighbor relationships rather than distances, making them versatile across different distance functions.</div>
<div class="mono" style="margin-top:8px">本研究解决了缺乏分析工具来评估基于聚类的近似最近邻搜索（ANNS）对特定数据集适用性的问题，称为“可搜索性”。作者提出了两个度量：聚类邻域稳定性度量（clustering-NSM），用于评估聚类质量并预测ANNS准确性，以及点邻域稳定性度量（point-NSM），用于评估数据集的聚类能力并预测clustering-NSM。实验结果表明，这些度量能够有效地通过依赖最近邻关系而非距离来确定数据集在基于聚类的ANNS中的可搜索性，使其在各种距离函数中具有广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics</div>
<div class="meta-line">Authors: Jonathan Skaggs, Jacob W. Crandall</div>
<div class="meta-line">First: 2025-05-01T18:13:20+00:00 · Latest: 2026-02-18T18:09:33+00:00</div>
<div class="meta-line">Comments: In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems, Paphos, Cyprus, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03795v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.03795v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG) [39]. These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior matching vs. community-aware behavior) and the moments they model (mean vs. distribution). Results show that the highest-performing method, called hCAB, models the distribution of human behavior rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies, the hCAB model closely mirrors the population dynamics of human groups (with notable differences). Additionally, in a user study, human participants had difficulty distinguishing hCAB agents from other humans, thus illustrating that the hCAB model also produces plausible (individual) behavior in this strategic network game.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在复杂群体动态的战略网络游戏中建模人类行为</div>
<div class="mono" style="margin-top:8px">人类网络对重要社会结果有重大影响，包括财富和健康不平等、贫困和欺凌。因此，理解人类网络对于学习如何促进有利的社会结果至关重要。为了更好地理解人类网络，我们比较和对比了几种在名为初中游戏（JHG）的战略网络游戏中学习人类行为模型的方法。这些建模方法在参数化人类行为时使用的假设（行为匹配与社区感知行为）和建模的时刻（均值与分布）上有所不同。结果表明，表现最佳的方法称为hCAB，它建模人类行为的分布而非均值，并假设人类使用社区感知行为而非行为匹配。当应用于小型社会时，hCAB模型与人类群体的人口动态密切相符（有显著差异）。此外，在用户研究中，人类参与者难以区分hCAB代理与其他人类，从而表明hCAB模型在这个战略网络游戏中也产生了合理的（个体）行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the significant impact of human networks on societal outcomes such as inequality and bullying, highlighting the need for better understanding of these networks. The study compares various modeling methods for human behavior in a strategic network game known as the Junior High Game, focusing on differences in assumptions about behavior and the statistical moments modeled. The findings indicate that the hCAB method, which models the distribution of behavior and incorporates community-aware behavior, outperforms others by closely reflecting population dynamics in small societies and producing behavior that is indistinguishable from real human participants in a user study.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于人际网络对社会结果（如不平等和欺凌）的重大影响，强调了更好理解这些网络的必要性。研究比较了几种在名为初中游戏的战略网络游戏中模拟人类行为的方法，重点关注不同的行为假设和统计时刻如何影响模型。研究结果表明，hCAB方法在模拟社区意识行为的分布方面优于其他方法，准确反映了小型人类社会的动态，同时在用户研究中产生了与真实人类无明显区别的行为。</div>
</details>
</div>
<div class="card">
<div class="title">SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation</div>
<div class="meta-line">Authors: Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand</div>
<div class="meta-line">First: 2026-02-18T18:09:03+00:00 · Latest: 2026-02-18T18:09:03+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16671v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARC：自动化C单元测试生成的场景规划与推理</div>
<div class="mono" style="margin-top:8px">由于高层程序意图与指针运算和手动内存管理的严格语法约束之间的语义差距，自动化C单元测试生成仍然是一个巨大的挑战。尽管大型语言模型（LLMs）表现出强大的生成能力，但直接的意图到代码合成常常遭遇跳跃到代码的失败模式，模型在没有基于程序结构、约束和语义的基础上过早地生成代码。这将导致不可编译的测试、虚构的函数签名、低分支覆盖率和无法正确捕捉错误的语义无关断言。我们提出了SPARC，一个神经符号的基于场景的框架，通过四个阶段弥合这一差距：（1）控制流图（CFG）分析，（2）将LLM推理与经过验证的实用助手相结合的操作图，（3）路径目标测试合成，以及（4）使用编译器和运行时反馈的迭代自我修正验证循环。我们在59个真实世界和算法主题上评估SPARC，结果在行覆盖率上比基础的提示生成基线提高了31.36%，在分支覆盖率上提高了26.01%，在变异分数上提高了20.78%，在复杂主题上与符号执行工具KLEE相匹配或超越。SPARC通过迭代修复保留了94.3%的测试，并生成了开发者评分的可读性和可维护性显著更高的代码。通过将LLM推理与程序结构对齐，SPARC为工业级遗留C代码库的测试提供了可扩展的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of automated unit test generation for C, particularly due to the semantic gap between high-level program intent and the complexities of pointer arithmetic and manual memory management. The authors introduce SPARC, a neuro-symbolic framework that employs a four-stage method including Control Flow Graph analysis, an Operation Map for grounding LLM reasoning, path-targeted test synthesis, and an iterative validation loop. Experimental results demonstrate that SPARC outperforms traditional prompt generation by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, while also achieving high readability and maintainability in the generated code, thus providing a scalable solution for testing legacy C codebases.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决C语言自动单元测试生成中的挑战，特别是高层程序意图与指针算术和内存管理复杂性之间的语义差距。作者提出了SPARC，一个神经符号框架，分为四个阶段：分析控制流图，利用操作映射为大型语言模型（LLM）推理提供基础，合成路径目标测试，以及实施自我修正的迭代验证循环。实验结果表明，SPARC在59个主题上显著优于传统的提示生成方法，行覆盖率提高了31.36%，分支覆盖率提高了26.01%，变异评分提高了20.78%，同时保持了高水平的测试可读性和可维护性。</div>
</details>
</div>
<div class="card">
<div class="title">PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction</div>
<div class="meta-line">Authors: Bo Lang, Nirav Savaliya, Zhihao Zheng, Jinglun Feng, Zheng-Hang Yeh, Mooi Choo Chuah</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-18T18:08:26+00:00 · Latest: 2026-02-18T18:08:26+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16669v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16669v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PredMapNet：一致的在线高清矢量地图构建的未来与历史推理</div>
<div class="mono" style="margin-top:8px">高清（HD）地图对自动驾驶至关重要，提供道路元素的结构化表示以支持导航和规划。然而，现有的基于查询的方法通常采用随机查询初始化，并依赖隐式时间建模，这导致在构建全球地图时出现时间不一致和不稳定。为了解决这些挑战，我们提出了一种新颖的端到端框架，用于一致的在线高清矢量地图构建，该框架联合执行地图实例跟踪和短期预测。首先，我们提出了一种语义感知查询生成器，用空间对齐的语义掩码初始化查询，以全局捕捉场景级上下文。接下来，我们设计了一种历史光栅化地图内存，用于存储每个跟踪实例的细粒度实例级地图，从而实现显式的历史先验。然后，历史地图引导模块将光栅化地图信息集成到跟踪查询中，提高时间连续性。最后，我们提出了一种短期未来引导模块，根据存储的历史轨迹预测地图实例的即时运动。这些预测的未来位置为跟踪实例提供线索，以进一步避免不合理的预测并保持时间一致性。在nuScenes和Argoverse2数据集上的大量实验表明，我们提出的方法在效率上优于最先进（SOTA）的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the temporal inconsistencies and instabilities in high-definition (HD) map construction for autonomous driving, which are often caused by existing query-based methods that rely on random initialization and implicit temporal modeling. The authors propose an end-to-end framework called PredMapNet that integrates map instance tracking and short-term prediction, utilizing a Semantic-Aware Query Generator for initializing queries with spatially aligned semantic masks, a History Rasterized Map Memory for storing detailed instance-level maps, and a History-Map Guidance Module to enhance temporal continuity. Experimental results on the nuScenes and Argoverse2 datasets indicate that PredMapNet significantly outperforms state-of-the-art methods in terms of efficiency and consistency in HD map construction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决高精度（HD）地图构建中存在的时间不一致性和不稳定性，这些问题通常源于依赖随机初始化和隐式时间建模的现有查询方法。作者提出了一种名为PredMapNet的端到端框架，该框架集成了地图实例跟踪和短期预测，利用语义感知查询生成器通过空间对齐的语义掩码初始化查询，历史栅格化地图存储器存储详细的实例级地图，以及历史地图引导模块以增强时间连续性。在nuScenes和Argoverse2数据集上的实验结果表明，PredMapNet显著优于现有最先进的方法，展示了在HD地图构建中的效率和一致性改善。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Science of AI Agent Reliability</div>
<div class="meta-line">Authors: Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</div>
<div class="meta-line">First: 2026-02-18T18:05:44+00:00 · Latest: 2026-02-18T18:05:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16666v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向人工智能代理可靠性的科学</div>
<div class="mono" style="margin-top:8px">人工智能代理越来越多地被部署来执行重要任务。尽管在标准基准上的准确性得分上升表明了快速进展，但许多代理在实践中仍然继续失败。这种差异突显了当前评估的一个基本局限性：将代理行为压缩为单一成功指标掩盖了关键的操作缺陷。特别是，它忽视了代理在多次运行中是否表现一致、是否能抵御扰动、是否可预测地失败或错误严重性是否有限。基于安全关键工程，我们通过提出十二个具体指标来提供全面的性能概况，这些指标沿着一致性、鲁棒性、可预测性和安全性四个关键维度分解代理可靠性。在两个互补基准上评估14个代理模型，我们发现最近的能力提升仅带来了可靠性的小幅改善。通过揭示这些持续的局限性，我们的指标补充了传统评估，同时提供了推理代理如何表现、退化和失败的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap between the rising accuracy of AI agents on benchmarks and their practical failures in real-world tasks. The authors propose a holistic performance profile by introducing twelve metrics that assess agent reliability across four dimensions: consistency, robustness, predictability, and safety. Through evaluating 14 agentic models on two benchmarks, the study finds that recent advancements in AI capabilities have resulted in only marginal improvements in reliability, highlighting the need for better evaluation methods to understand agent performance and failures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决AI代理在标准基准上高准确率与其在实际应用中表现不佳之间的差距。作者通过提出十二个指标，评估代理在一致性、鲁棒性、可预测性和安全性四个维度上的可靠性，从而提供了一个整体性能概况。他们对14个代理模型在两个基准上的评估显示，近期能力的提升仅带来了可靠性方面的微小改善，突显了当前AI评估的持续局限性，并提供了理解代理性能和失败模式的新工具。</div>
</details>
</div>
<div class="card">
<div class="title">Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge</div>
<div class="meta-line">Authors: Jiaming Liu, Felix Petersen, Yunhe Gao, Yabin Zhang, Hyojin Kim, Akshay S. Chaudhari, Yu Sun, Stefano Ermon, Sergios Gatidis</div>
<div class="meta-line">First: 2026-02-18T18:05:00+00:00 · Latest: 2026-02-18T18:05:00+00:00</div>
<div class="meta-line">Comments: 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16664v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16664v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自监督语义桥进行无配对图像到图像的转换</div>
<div class="mono" style="margin-top:8px">对抗扩散和扩散反演方法推动了无配对图像到图像的转换，但各自面临关键限制。对抗方法在训练期间需要目标领域的对抗损失，这可能限制对未见数据的泛化，而扩散反演方法由于对噪声潜在表示的不完美反演，往往产生低保真度的转换。在本研究中，我们提出了自监督语义桥（SSB），这是一个多功能框架，将外部语义先验集成到扩散桥模型中，以实现无需跨域监督的空间忠实转换。我们的关键思想是利用自监督视觉编码器学习对外观变化不变但捕捉几何结构的表示，形成一个共享的潜在空间，以调节扩散桥。大量实验表明，SSB在具有挑战性的医学图像合成中，在域内和域外设置下均优于强先前方法，并且可以轻松扩展到高质量的文本引导编辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing unpaired image-to-image translation methods, specifically adversarial and diffusion-inversion techniques, which struggle with generalization and fidelity. The authors propose a Self-Supervised Semantic Bridge (SSB) framework that incorporates external semantic priors into diffusion models, allowing for spatially accurate translations without the need for cross-domain supervision. Experimental results demonstrate that SSB significantly outperforms prior methods in medical image synthesis tasks, both within the same domain and across different domains, while also facilitating high-quality text-guided editing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有无配对图像到图像翻译方法的局限性，特别是对抗性方法的限制和扩散反演技术的低保真度。作者提出了一种自监督语义桥（SSB）框架，将外部语义先验整合到扩散桥模型中，使得在不需要跨域监督的情况下实现空间准确的翻译。实验结果表明，SSB在医学图像合成任务中显著优于先前的方法，无论是在同一领域还是跨不同领域，并且还促进了高质量的文本引导编辑。</div>
</details>
</div>
<div class="card">
<div class="title">Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment</div>
<div class="meta-line">Authors: Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T18:01:23+00:00 · Latest: 2026-02-18T18:01:23+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16660v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一次对齐，多语言受益：加强大型语言模型安全对齐的多语言一致性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在语言社区的广泛部署需要可靠的多语言安全对齐。然而，最近将对齐扩展到其他语言的努力通常需要大量资源，要么通过目标语言的大规模高质量监督，要么通过与高资源语言的成对对齐，这限制了可扩展性。在这项工作中，我们提出了一种提高多语言安全对齐的资源高效方法。我们引入了一种即插即用的多语言一致性（MLC）损失，可以集成到现有的单语言对齐管道中。通过改善多语言表示向量之间的共线性，我们的方法在单次更新中鼓励多语言语义层面的方向一致性。这使得可以仅使用多语言提示变体在多个语言之间同时对齐，而无需在低资源语言中额外的响应级监督。我们在不同的模型架构和对齐范式中验证了所提出的方法，并展示了其在增强多语言安全性方面的有效性，对一般模型效用的影响有限。对不同语言和任务的进一步评估表明，跨语言泛化能力有所提高，建议所提出的方法作为在有限监督下实现多语言一致性对齐的实用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for reliable multilingual safety alignment in large language models (LLMs) due to their widespread use across different linguistic communities. The authors propose a resource-efficient method that introduces a Multi-Lingual Consistency (MLC) loss, which can be integrated into existing monolingual alignment pipelines to enhance multilingual safety alignment. Experimental results show that this method improves collinearity between multilingual representation vectors, achieving directional consistency across multiple languages with minimal additional supervision, thereby enhancing cross-lingual generalization while maintaining overall model utility.</div>
<div class="mono" style="margin-top:8px">本研究的动机是由于大型语言模型（LLMs）在不同语言社区的广泛使用，需要可靠的多语言安全对齐。作者提出了一种资源高效的方法，引入了多语言一致性（MLC）损失，可以集成到现有的单语对齐管道中，以增强多语言安全对齐。实验结果表明，该方法改善了多语言表示向量之间的共线性，并在一次更新中实现了多个语言之间的方向一致性，从而在总体模型效用影响最小的情况下，提升了跨语言泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Closing the Distribution Gap in Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Chengzhi Hu, Jonas Dornbusch, David Lüdke, Stephan Günnemann, Leo Schwinn</div>
<div class="meta-line">First: 2026-02-16T22:34:52+00:00 · Latest: 2026-02-18T17:57:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15238v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15238v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩小大规模语言模型对抗训练中的分布差距</div>
<div class="mono" style="margin-top:8px">大规模语言模型的对抗训练是可靠提高对抗者鲁棒性的最有前景的方法之一。然而，尽管取得了显著进展，模型仍然容易受到简单的内部分布攻击，例如将提示重写为过去时或翻译成其他语言。我们认为，这种持续的脆弱性源于当前对抗训练算法的一个基本限制：它们在训练集上最小化对抗损失，但对数据分布的覆盖不足，导致对看似简单的攻击脆弱。为了解决这个问题，我们提出了分布对抗训练（DAT）。我们利用扩散大规模语言模型来近似提示和响应的真实联合分布，从而生成多样化的高可能性样本，以解决泛化失败。通过结合扩散模型提供的数据分布的优化与持续的对抗训练，DAT实现了比以前的方法显著更高的对抗鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerabilities of large language models (LLMs) to simple in-distribution adversarial attacks, which persist despite advancements in adversarial training. The authors propose a novel method called Distributional Adversarial Training (DAT), which utilizes Diffusion LLMs to better approximate the true joint distribution of prompts and responses, thereby generating diverse and high-likelihood samples. Experimental results demonstrate that DAT significantly enhances adversarial robustness compared to existing adversarial training methods, effectively mitigating the identified fragility in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型语言模型（LLMs）对对抗攻击的鲁棒性，尽管对抗训练已有显著进展，但仍面临重大挑战。作者提出了一种新方法，称为分布对抗训练（DAT），利用扩散LLMs更好地近似提示和响应的真实联合分布，从而生成多样化和高可能性的样本。实验结果表明，DAT显著提高了对抗鲁棒性，相较于现有方法有效解决了传统对抗训练算法在数据分布覆盖不足方面的局限性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260219_0357.html">20260219_0357</a>
<a href="archive/20260218_0356.html">20260218_0356</a>
<a href="archive/20260217_0341.html">20260217_0341</a>
<a href="archive/20260216_0336.html">20260216_0336</a>
<a href="archive/20260215_0334.html">20260215_0334</a>
<a href="archive/20260213_0357.html">20260213_0357</a>
<a href="archive/20260212_0403.html">20260212_0403</a>
<a href="archive/20260210_0412.html">20260210_0412</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0346.html">20260206_0346</a>
<a href="archive/20260205_0348.html">20260205_0348</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260203_1224.html">20260203_1224</a>
<a href="archive/20260202_0334.html">20260202_0334</a>
<a href="archive/20260201_0330.html">20260201_0330</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0342.html">20260130_0342</a>
<a href="archive/20260129_0342.html">20260129_0342</a>
<a href="archive/20260128_0340.html">20260128_0340</a>
<a href="archive/20260127_0335.html">20260127_0335</a>
<a href="archive/20260126_0328.html">20260126_0328</a>
<a href="archive/20260125_0326.html">20260125_0326</a>
<a href="archive/20260124_0335.html">20260124_0335</a>
<a href="archive/20260123_0336.html">20260123_0336</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0325.html">20260118_0325</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0336.html">20260116_0336</a>
<a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
