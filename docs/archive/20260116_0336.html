<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-16 03:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260116_0336</div>
    <div class="row"><div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适应于稀有类别</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型如CLIP在零-shot识别中表现出色，但在预训练期间很少见的类别（包括新出现的实体和文化特定类别）上表现不佳。我们介绍了LiteEmbed，这是一个轻量级框架，用于CLIP的少量样本个性化，使得可以在不重新训练编码器的情况下添加新类别。LiteEmbed在CLIP的词汇中对文本嵌入进行子空间引导优化，利用基于PCA的分解，将粗略语义方向与细粒度变化分离。两个互补目标，粗对齐和细分离，共同保持全局语义一致性，同时增强视觉相似类别之间的可区分性。一旦优化，嵌入可以即插即用，顺利替代CLIP的原始文本特征，适用于分类、检索、分割和检测任务。大量实验表明，相较于先前的方法，LiteEmbed在适应于代表性不足、稀有或未见类别方面取得了显著提升，确立了其作为有效方法的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of large-scale vision-language models like CLIP on rare classes that are not well-represented during pretraining. The authors introduce LiteEmbed, a lightweight framework that allows for few-shot personalization of CLIP by optimizing text embeddings without the need for retraining the model&#x27;s encoders. Experimental results show that LiteEmbed significantly enhances the model&#x27;s ability to recognize and classify underrepresented classes, achieving better performance compared to previous methods across various tasks such as classification, retrieval, segmentation, and detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高大型视觉语言模型（如CLIP）在预训练期间未充分代表的稀有类别上的表现。作者提出了LiteEmbed，这是一种轻量级框架，允许在不需要重新训练编码器的情况下对CLIP进行少量样本个性化，利用基于PCA的分解进行子空间引导的文本嵌入优化。实验结果表明，LiteEmbed显著增强了模型识别未充分代表、稀有或未见类别的能力，在分类、检索、分割和检测等各种任务中优于先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：用于科学复合图的视觉条件面板检测与标题生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个标记面板组合成单一图像，但在实际流程中，标题往往缺失或仅提供图形级摘要，使得面板级理解变得困难。本文提出了FigEx2，一种视觉条件框架，能够从复合图中定位面板并直接生成面板级标题。为了减轻开放式标题生成中多样化措辞的影响，我们引入了一种噪声感知门控融合模块，能够自适应过滤标记级特征，以稳定检测查询空间。此外，我们采用了一种分阶段优化策略，将监督学习与强化学习（RL）相结合，利用基于CLIP的对齐和基于BERTScore的语义奖励来强制执行严格的多模态一致性。为了支持高质量的监督，我们策划了BioSci-Fig-Cap，这是一个针对面板级定位的精细基准，同时还包括物理和化学领域的跨学科测试套件。实验结果表明，FigEx2在检测方面达到了优越的0.726 mAP@0.5:0.95，并在METEOR上比Qwen3-VL-8B高出0.51，在BERTScore上高出0.24。值得注意的是，FigEx2在没有任何微调的情况下，展现出显著的零-shot迁移能力，能够适应分布外的科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the understanding of scientific compound figures, which often lack detailed panel-level captions. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates specific captions for each panel using a noise-aware gated fusion module to enhance token-level feature stability. Experimental results show that FigEx2 achieves a mean average precision of 0.726 for detection and outperforms the Qwen3-VL-8B model by 0.51 in METEOR and 0.24 in BERTScore, demonstrating strong zero-shot transferability to other scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善科学复合图的理解，因为这些图通常缺乏详细的面板级标题。作者提出了FigEx2，这是一种视觉条件框架，能够定位面板并为每个面板生成特定的标题，使用噪声感知门控融合模块来增强特征的稳定性。实验结果表明，FigEx2在检测中达到了0.726的平均精度，并在标题生成指标上超越了Qwen3-VL-8B模型，展示了在不同科学领域的强大零样本迁移能力，无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">Differentially private federated learning for localized control of infectious disease dynamics</div>
<div class="meta-line">Authors: Raouf Kerkouche, Henrik Zunker, Mario Fritz, Martin J. Kühn</div>
<div class="meta-line">First: 2025-09-17T14:28:04+00:00 · Latest: 2026-01-14T15:45:09+00:00</div>
<div class="meta-line">Comments: 26 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14024v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: R2 around 0.94 (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; R2 around 0.88 (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于传染病动态局部控制的差分隐私联邦学习</div>
<div class="mono" style="margin-top:8px">在流行病时期，迅速反应是减缓疫情传播的必要措施。局部方法在这方面具有多种优势，能够限制所需资源并减少干预对更大范围的影响。然而，由于可用数据有限，在地方范围内训练单独的机器学习（ML）模型通常不可行。集中数据也面临挑战，因为其高度敏感性和隐私限制。在本研究中，我们考虑基于德国县和社区的局部策略，由相关地方卫生机构（LHA）管理。为了在不妨碍详细情境数据可用性的情况下保护隐私，我们提出了一种隐私保护的预测方法，可以协助公共卫生专家和决策者。使用联邦学习（FL）的ML方法在不集中原始数据的情况下训练共享模型。将县、社区或LHA视为客户端，并在效用和隐私之间找到平衡，我们研究了具有客户端差分隐私（DP）的FL框架。我们在最近病例数的滑动窗口上训练共享的多层感知器，以预测病例数量，同时客户端仅交换规范裁剪的更新，服务器则聚合带有DP噪声的更新。我们在两个阶段对县级COVID-19数据评估该方法。如预期，严格的隐私会导致不稳定且不可用的预测。在适度强的隐私水平下，DP模型与非DP模型非常接近：2020年11月R2约为0.94（对比0.95），平均绝对百分比误差（MAPE）为26%；2022年3月R2约为0.88（对比0.93），MAPE为21%。总体而言，客户端级DP-FL可以提供有用的县级预测，并具有强隐私保障，且可行的隐私预算取决于流行病阶段，允许卫生机构之间进行隐私合规的合作以进行地方预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to enhance localized responses to epidemics while preserving data privacy, as traditional methods often struggle with limited data and privacy concerns. The authors propose a federated learning framework that incorporates client-level differential privacy to train a shared multilayer perceptron model on COVID-19 case data from German counties, allowing local health authorities to forecast case numbers without centralizing sensitive data. The experimental results indicate that while very strict privacy leads to unstable forecasts, a moderately strong level of differential privacy yields predictions that are nearly as accurate as those from non-private models, achieving R2 values of approximately 0.94 and 0.88 in different phases of the pandemic, thus demonstrating the feasibility of privacy-preserving local forecasting for public health decision-making.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种在疫情期间进行传染病动态预测的本地化方法，同时确保数据隐私。作者提出了一种使用联邦学习（FL）的隐私保护预测方法，使地方卫生部门能够在不集中敏感数据的情况下训练共享机器学习模型。实验结果表明，虽然非常严格的隐私水平会导致不稳定的预测，但适度强的差分隐私水平可以产生与非隐私模型几乎相当的预测，R2值分别约为0.94和0.88，平均绝对百分比误差分别为26%和21%，在两阶段COVID-19数据评估中取得了良好效果。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity</div>
<div class="meta-line">Authors: Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal</div>
<div class="meta-line">First: 2026-01-14T14:03:11+00:00 · Latest: 2026-01-14T14:03:11+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09497v1">PDF</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr">Code1</a> · <a href="https://github.com/Ritabrata04/cdod-icpr.git">Code2</a> · <a href="https://github.com/Ritabrata04/cdod-icpr">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对领域特异性下的鲁棒跨数据集目标检测泛化</div>
<div class="mono" style="margin-top:8px">目标检测器在分布内通常表现良好，但在不同基准上会急剧下降。我们通过设置特异性的视角研究跨数据集目标检测（CD-OD）。我们将基准分组为具有多样日常场景的设置无关数据集和与狭窄环境相关的设置特定数据集，并在所有训练-测试对上评估标准检测器系列。这揭示了CD-OD中的明确结构：在相同设置类型内的迁移相对稳定，而跨设置类型的迁移显著下降且通常是不对称的。最严重的崩溃发生在从特定源迁移到无关目标时，并且在开放标签对齐后仍然存在，表明领域转移在最困难的情况下占主导地位。为了将领域转移与标签不匹配区分开，我们比较了闭合标签迁移与开放标签协议，后者使用CLIP相似性将预测类别映射到最近的目标标签。开放标签评估产生了一致但有限的增益，许多修正案例对应于图像证据支持的语义近失。总体而言，我们提供了在设置特异性下CD-OD的原则性特征描述，并为在分布转移下评估检测器提供了实用指导。代码将发布在\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the challenges of cross-dataset object detection (CD-OD), particularly how object detectors perform poorly when evaluated on different benchmarks. The authors categorize datasets into setting-agnostic and setting-specific groups and assess a standard detector across various train-test pairs. The findings indicate that while transfer within the same setting type is stable, transferring across different setting types leads to significant performance drops, especially when moving from specific to agnostic targets, highlighting the impact of domain shift. Additionally, the study compares closed-label and open-label transfer methods, revealing that open-label evaluation provides consistent improvements, particularly in cases of semantic near-misses, thus offering insights into evaluating detectors amidst distribution shifts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决物体检测器在不同基准测试中评估时性能下降的问题，特别关注跨数据集物体检测（CD-OD）和设置特异性的影响。作者将数据集分为设置无关和设置特定两类，并在各种训练-测试对上评估标准检测器家族。研究结果表明，同一设置类型内的迁移相对稳定，而跨设置类型的迁移则导致显著的性能下降，尤其是在从特定设置转移到无关设置时，表明领域转移是一个关键挑战。该研究还比较了闭标签和开放标签的迁移方法，发现开放标签评估提供了一致的改进，特别是在预测类别与目标标签语义接近的情况下，从而为在分布变化下更好地评估检测器提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models</div>
<div class="meta-line">Authors: Yizhi Chen, Ahmed Hemani</div>
<div class="meta-line">First: 2026-01-14T12:52:08+00:00 · Latest: 2026-01-14T12:52:08+00:00</div>
<div class="meta-line">Comments: Accepted to DATE Late Breaking Results 2026, Verona, Italy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09451v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破性结果：Quamba-SE：状态空间模型中激活的软边量化器</div>
<div class="mono" style="margin-top:8px">我们提出了Quamba-SE，一种用于状态空间模型（SSM）激活量化的软边量化器。与现有方法使用标准INT8操作不同，Quamba-SE采用三种自适应尺度：小值的高精度、正常值的标准尺度和异常值的低精度。这保留了异常值信息，而不是硬剪切，同时保持其他值的精度。我们在Mamba-130M上评估了6个零样本基准。结果表明，Quamba-SE在各个基准上始终优于Quamba，在单个基准上最高提高了+2.68%，在6个数据集的平均准确率上提高了+0.83%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for improved activation quantization in State Space Models (SSMs) to enhance performance while preserving important data characteristics. The authors introduce Quamba-SE, a soft-edge quantizer that utilizes three adaptive scales for quantization: high-precision for small values, standard scale for normal values, and low-precision for outliers, which allows for better handling of outlier information compared to traditional hard clipping methods. Experimental evaluations on the Mamba-130M model across six zero-shot benchmarks demonstrate that Quamba-SE outperforms the previous Quamba method, achieving improvements of up to 2.68% on individual benchmarks and an average accuracy increase of 0.83% across all datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决依赖于标准INT8操作的现有方法的局限性，来改善状态空间模型（SSM）中的激活量化。作者提出了Quamba-SE，这是一种软边量化器，利用三种自适应尺度来处理不同的数值范围，保留异常值信息，同时保持正常值的精度。在对Mamba-130M模型进行的六个零样本基准测试的实验评估中，Quamba-SE始终优于其前身Quamba，在单个基准测试中提高了最多2.68%，在所有数据集的平均准确率上提高了0.83%。</div>
</details>
</div>
<div class="card">
<div class="title">Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</div>
<div class="meta-line">Authors: Jiachen Li, Xiaojin Gong</div>
<div class="meta-line">First: 2023-10-26T08:12:53+00:00 · Latest: 2026-01-14T09:17:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.17218v3">Abs</a> · <a href="https://arxiv.org/pdf/2310.17218v3">PDF</a> · <a href="https://github.com/RikoLi/PCL-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原型对比学习的CLIP微调用于物体重识别</div>
<div class="mono" style="margin-top:8px">本研究旨在将大规模预训练的视觉-语言模型（如对比语言-图像预训练（CLIP））适应于提高各种监督设置下的物体重识别（Re-ID）性能。尽管提示学习使得名为CLIP-ReID的近期工作取得了良好的性能，但由于ReID任务中缺乏语义标签，其基本机制和提示学习的必要性仍不清楚。在本研究中，我们首先分析了提示学习在CLIP-ReID中的作用，并识别其局限性。基于我们的调查，我们提出了一种简单而有效的方法来适应CLIP用于监督物体Re-ID。我们的方法直接使用原型对比学习（PCL）损失微调CLIP的图像编码器，消除了对提示学习的需求。在人和车的Re-ID数据集上的实验结果表明，我们的方法与CLIP-ReID相比具有竞争力。此外，我们将基于PCL的CLIP微调方法扩展到无监督场景，在该场景中我们实现了最先进的性能。代码可在https://github.com/RikoLi/PCL-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adaptation of large-scale pre-trained vision-language models, specifically CLIP, to improve object re-identification (Re-ID) performance across different supervision settings. The authors analyze the limitations of prompt learning in existing methods like CLIP-ReID and propose a novel approach that fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, thereby eliminating the need for prompt learning. Experimental results on person and vehicle Re-ID datasets show that their method is competitive with CLIP-ReID and achieves state-of-the-art performance in unsupervised scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用大规模预训练的视觉-语言模型（如CLIP）来提高物体重识别（Re-ID）的性能，特别关注现有方法中提示学习的角色不明确的问题。作者分析了CLIP-ReID中提示学习的局限性，并提出了一种新方法，通过原型对比学习（PCL）损失直接微调CLIP的图像编码器，从而消除了对提示的需求。在对行人和车辆Re-ID数据集的实验结果表明，该方法与CLIP-ReID具有竞争力，并在无监督场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion</div>
<div class="meta-line">Authors: Jialu Li, Taiyan Zhou</div>
<div class="meta-line">First: 2026-01-14T06:38:12+00:00 · Latest: 2026-01-14T06:38:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpikeVAEDiff：基于神经脉冲的自然视觉场景重建通过VD-VAE和多功能扩散</div>
<div class="mono" style="margin-top:8px">从神经活动重建自然视觉场景是神经科学和计算机视觉中的一个关键挑战。我们提出了SpikeVAEDiff，一个新颖的两阶段框架，结合了非常深的变分自编码器（VDVAE）和多功能扩散模型，从神经脉冲数据生成高分辨率和语义丰富的图像重建。在第一阶段，VDVAE通过将神经脉冲信号映射到潜在表示，生成低分辨率的初步重建。在第二阶段，回归模型将神经脉冲信号映射到CLIP-Vision和CLIP-Text特征，使多功能扩散能够通过图像到图像生成来细化图像。我们在艾伦视觉编码-神经像素数据集上评估了我们的方法，并分析了不同的脑区。我们的结果表明，VISI区域表现出最显著的激活，并在重建质量中发挥关键作用。我们展示了成功和不成功的重建示例，反映了解码神经活动的挑战。与基于fMRI的方法相比，脉冲数据提供了更优越的时间和空间分辨率。我们进一步验证了VDVAE模型的有效性，并进行消融研究，表明来自特定脑区的数据显著提高了重建性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing natural visual scenes from neural activity, which is significant in both neuroscience and computer vision. The authors introduce SpikeVAEDiff, a two-stage framework that integrates a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to achieve high-resolution image reconstructions from neural spike data. Experimental results on the Allen Visual Coding-Neuropixels dataset reveal that the VISI region shows the highest activation and is crucial for reconstruction quality, with the study highlighting both successful and unsuccessful reconstructions and demonstrating that spike data outperforms fMRI in terms of temporal and spatial resolution, while also confirming the VDVAE model&#x27;s effectiveness through ablation studies focused on specific brain regions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从神经活动重建自然视觉场景的挑战，这在神经科学和计算机视觉中都具有重要意义。作者提出了SpikeVAEDiff，这是一种将非常深的变分自编码器（VDVAE）和多功能扩散模型结合的两阶段框架，以实现从神经脉冲数据中获得高分辨率图像重建。在Allen视觉编码-神经像素数据集上的实验结果表明，VISI脑区对重建质量至关重要，该方法展示了成功和不成功的重建，突显了解码神经活动的复杂性，并显示脉冲数据在时间和空间分辨率上优于fMRI。</div>
</details>
</div>
<div class="card">
<div class="title">SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</div>
<div class="meta-line">Authors: Youngmin Kim, Giyeong Oh, Kwangsoo Youm, Youngjae Yu</div>
<div class="meta-line">First: 2025-07-14T11:33:47+00:00 · Latest: 2026-01-14T06:25:00+00:00</div>
<div class="meta-line">Comments: Accepted to Automation in Construction. Our project page: https://winston1214.github.io/SlumpGuard/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10171v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.10171v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://winston1214.github.io/SlumpGuard/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SlumpGuard：基于AI的实时自动混凝土坍落度预测系统通过视频分析</div>
<div class="mono" style="margin-top:8px">混凝土的可加工性对建筑质量至关重要，坍落度测试是最广泛使用的现场评估方法。然而，传统的坍落度测试是手动的，耗时且高度依赖操作员，不适合在浇筑过程中进行连续或实时监测。为了解决这些局限性，我们提出了SlumpGuard，一个基于AI的视觉系统，通过单个固定摄像头分析搅拌车漏斗的自然排放流。该系统执行自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，实现了无需传感器、硬件安装或人工干预的质量监测。我们介绍了系统设计，构建了一个包含6000多个视频片段的现场复制数据集，并报告了广泛的评估，证明了在多种现场条件下可靠的漏斗定位、准确的浇筑检测和稳健的坍落度预测。专家研究进一步揭示了人类视觉估计的显著分歧，突显了自动评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the assessment of concrete workability, which is crucial for construction quality, by addressing the limitations of traditional manual slump testing. The authors developed SlumpGuard, an AI-powered vision system that utilizes a single fixed camera to analyze the discharge flow from a mixer-truck chute, enabling automatic detection of the chute, identification of pouring events, and classification of slump via video analysis. Experimental results demonstrate that SlumpGuard achieves reliable chute localization, accurate pouring detection, and robust slump prediction across various field conditions, while an expert study indicates significant discrepancies in human visual estimates, underscoring the necessity for automated assessment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决传统人工坍落度测试的局限性，改善混凝土可加工性的评估，这对建筑质量至关重要。作者开发了SlumpGuard，这是一种利用单个固定摄像头分析搅拌车漏斗排放流的AI视觉系统，能够实现自动漏斗检测、浇筑事件识别和基于视频的坍落度分类，无需额外传感器或人工干预。实验结果表明，SlumpGuard能够可靠地定位漏斗、准确检测浇筑事件，并在各种现场条件下预测坍落度，而专家研究显示人类视觉估计存在显著差异，强调了自动评估方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning</div>
<div class="meta-line">Authors: Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</div>
<div class="meta-line">First: 2025-09-28T09:35:37+00:00 · Latest: 2026-01-14T06:01:44+00:00</div>
<div class="meta-line">Comments: The code is available at \url{https://github.com/xiaojieli0903/GenViewPlusPlus}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23770v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23770v2">PDF</a> · <a href="https://github.com/xiaojieli0903/GenViewPlusPlus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair&#x27;s semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenView++：统一自适应视图生成与质量驱动监督的对比表示学习</div>
<div class="mono" style="margin-top:8px">对比学习的成功依赖于高质量正样本对的构建和利用。然而，当前方法在两个方面面临关键限制：在构建方面，手工制作和生成增强往往缺乏多样性并存在语义损坏的风险；在学习方面，缺乏质量评估机制导致次优监督，所有样本对被平等对待。为了解决这些挑战，我们提出了GenView++，一个统一框架，通过引入两个协同创新来解决这两个方面。为了改善样本对构建，GenView++引入了一种多源自适应视图生成机制，通过动态调节生成参数，合成多样但语义一致的视图，涵盖图像条件、文本条件和图像-文本条件策略。其次，质量驱动的对比学习机制评估每个样本对的语义对齐和多样性，动态重新加权其训练贡献，优先考虑高质量样本对，同时抑制冗余或不对齐的样本对。大量实验表明，GenView++在视觉和视觉-语言任务中均表现出色。在视觉表示学习中，它在ImageNet线性分类上提高了MoCov2的性能，提升幅度为+2.5%。在视觉-语言学习中，它在十个数据集上将平均零-shot分类准确率提高了+12.31%（相较于CLIP）和+5.31%（相较于SLIP），并进一步将Flickr30k文本检索的R@5提高了+3.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of contrastive learning by addressing the limitations in the construction of positive pairs and the lack of quality assessment in current methods. The authors propose GenView++, a unified framework that incorporates a multi-source adaptive view generation mechanism to create diverse and semantically coherent views, alongside a quality-driven contrastive learning mechanism that dynamically reweights training contributions based on semantic alignment and diversity. Experimental results indicate that GenView++ significantly improves performance, achieving a +2.5% increase in MoCov2 on ImageNet linear classification and enhancing zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, while also improving Flickr30k text retrieval R@5 by +3.2%.</div>
<div class="mono" style="margin-top:8px">本研究解决了当前对比学习方法的局限性，这些方法在构建高质量正对时面临困难，并且缺乏质量评估机制进行监督。作者提出了GenView++，一个统一框架，通过多源自适应视图生成机制增强对对的构建，能够合成多样且语义一致的视图。此外，引入了一种基于质量的对比学习机制，评估每对的语义对齐和多样性，并动态调整其训练贡献。实验结果表明，GenView++在ImageNet线性分类上提高了MoCov2的性能2.5%，在十个数据集上使零-shot分类准确率较CLIP提高了12.31%，较SLIP提高了5.31%，同时在Flickr30k文本检索R@5上提高了3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-14T04:42:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0\%的图像-AUROC和92.2\%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that do not effectively balance semantic generalization and structural discriminability. The authors propose a novel method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through the Hierarchical Semantic-Visual Synergy (HSVS) mechanism and utilizes a Vision-Conditioned Prompt Generator (VCPG) for dynamic prompt generation. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临局限，无法有效平衡全局语义泛化与细致结构可区分性。作者提出了一种新方法，称为协同语义-视觉提示（SSVP），通过层次化语义-视觉协同（HSVS）机制整合多样的视觉编码，将DINOv3的多尺度结构先验与CLIP语义空间结合。实验结果表明，SSVP在七个工业基准测试中显著提高了性能，在MVTec-AD上实现了93.0%的图像AUROC和92.2%的像素AUROC，超越了现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives</div>
<div class="meta-line">Authors: Wisdom O. Ikezogwo, Kevin Zhang, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Linda Shapiro, Ranjay Krishna</div>
<div class="meta-line">First: 2025-01-07T23:32:05+00:00 · Latest: 2026-01-14T03:02:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.04184v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.04184v3">PDF</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data">Code1</a> · <a href="https://huggingface.co/datasets/wisdomik/MedicalNarratives">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to $\textit{think-aloud}$ studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial and temporal association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. $\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data]}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedicalNarratives：将医学视觉与语言连接到本地化叙事</div>
<div class="mono" style="margin-top:8px">多模态模型对数据需求量大。虽然自然图像的数据集丰富，但医学图像数据集无法享受同样的奢侈。为了大规模实现医学图像的表示学习，我们转向YouTube，这是一个拥有大量开源医学教学视频的平台。我们整理了MedicalNarratives，一个包含470万对医学图像-文本的数据库，其中100万样本包含以空间轨迹（和边界框）形式的密集注释，以及118K个以轨迹事件为中心的视频（带有对齐文本），使得超越单帧的时空基础成为可能。类似于$\textit{think-aloud}$研究，讲师在相关图像区域上悬停鼠标光标时进行讲解，MedicalNarratives中的100万张图像包含图像像素中的本地化鼠标轨迹，创建了文本与像素之间的空间和时间关联。为了评估MedicalNarratives的实用性，我们使用跨越12个医学领域的数据集训练了GenMedClip，采用类似CLIP的目标。GenMedClip在新构建的医学影像基准上，在所有12个领域的表现超过了之前的最先进模型。$\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[数据]}$</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the scarcity of large-scale medical image datasets compared to natural images, which hinders the development of multi-modal models. To address this, the authors created MedicalNarratives, a dataset comprising 4.7 million medical image-text pairs sourced from YouTube, including 1 million samples with detailed spatial annotations and 118,000 videos that provide spatiotemporal grounding. The key experimental finding is that the model GenMedClip, trained with a CLIP-like objective on this dataset across 12 medical domains, significantly outperforms existing state-of-the-art models on a newly established medical imaging benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于与自然图像数据集的丰富性相比，医学图像数据集的稀缺性阻碍了医学应用中多模态模型的发展。为了解决这一问题，作者创建了MedicalNarratives数据集，该数据集由来自YouTube的470万对医学图像-文本对组成，其中包括100万条带有详细空间注释的样本和118,000个提供时空基础的视频。实验结果表明，基于该数据集训练的GenMedClip模型在新构建的基准测试中，在所有12个医学领域的表现显著优于现有的最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models</div>
<div class="meta-line">Authors: Ganxi Xu, Zhao-Rong Lai, Yuting Tang, Yonghao Song, Guoxu Zhou, Boyu wang, Jian Zhu, Jinyi Long</div>
<div class="meta-line">First: 2025-08-31T10:29:58+00:00 · Latest: 2026-01-14T01:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.00787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M/EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present a novel image-to-brain signal framework that generates M/EEG from images by leveraging the diffusion transformer architecture enhanced with cross-attention mechanisms. Specifically, we employ a diffusion transformer (DiT) architecture based on denoising diffusion implicit models (DDIM) to achieve brain signal generation. To realize the goal of image-to-brain signal conversion, we use cross-attention mechanisms to align brain signal embeddings with CLIP image embeddings. Moreover, we leverage large language models (LLMs) to generate image captions, and concatenate the resulting CLIP text embeddings with CLIP image embeddings to form unified embeddings for cross-attention alignment, enabling our model to capture core semantic information. Moreover, to capture core semantic information, we use large language models (LLMs) to generate descriptive and semantically accurate captions for images. Furthermore, we introduce a learnable spatio-temporal position encoding that combines brain region embeddings with temporal embeddings to capture both spatial and temporal characteristics of brain signals. We evaluate the framework on two multimodal benchmark datasets (THINGS-EEG2 and THINGS-MEG) and demonstrate that it generates biologically plausible brain signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP引导的多模态扩散模型的图像到脑信号生成用于视觉假体</div>
<div class="mono" style="margin-top:8px">视觉假体在恢复盲人视力方面具有巨大潜力。尽管研究人员成功利用M/EEG信号在视觉假体的脑解码阶段引发视觉感知，但在脑编码阶段将图像转换为M/EEG信号的互补过程仍然未被充分探索，阻碍了完整功能管道的形成。在本研究中，我们提出了一种新颖的图像到脑信号框架，通过利用增强了交叉注意机制的扩散变换器架构从图像生成M/EEG信号。具体而言，我们采用基于去噪扩散隐式模型（DDIM）的扩散变换器（DiT）架构来实现脑信号生成。为了实现图像到脑信号转换的目标，我们使用交叉注意机制将脑信号嵌入与CLIP图像嵌入对齐。此外，我们利用大型语言模型（LLMs）生成图像标题，并将生成的CLIP文本嵌入与CLIP图像嵌入连接，形成统一的嵌入以进行交叉注意对齐，使我们的模型能够捕捉核心语义信息。此外，为了捕捉核心语义信息，我们使用大型语言模型（LLMs）为图像生成描述性和语义准确的标题。此外，我们引入了一种可学习的时空位置编码，将脑区嵌入与时间嵌入结合，以捕捉脑信号的空间和时间特征。我们在两个多模态基准数据集（THINGS-EEG2和THINGS-MEG）上评估了该框架，并证明其生成生物学上合理的脑信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap in converting images into M/EEG signals for visual prostheses, which is crucial for restoring vision in blind individuals. The authors propose a novel framework that utilizes a diffusion transformer architecture enhanced with cross-attention mechanisms to generate brain signals from images. Experimental results on two multimodal benchmark datasets, THINGS-EEG2 and THINGS-MEG, show that the framework successfully generates biologically plausible brain signals, thereby advancing the development of a complete functional pipeline for visual prostheses.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于填补将图像转换为M/EEG信号的空白，这对于恢复盲人视力至关重要。作者提出了一种新颖的框架，利用增强的扩散变换器架构和交叉注意机制，从图像生成M/EEG信号。对两个多模态基准数据集THINGS-EEG2和THINGS-MEG的实验结果表明，该方法成功生成生物学上合理的脑信号，从而推动了视觉假体的功能管道发展。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型快速发展，但数据在影响运动方面的作用仍不清楚。我们提出了Motive（视频生成的运动归因），这是一个以运动为中心的基于梯度的数据归因框架，能够扩展到现代大型高质量视频数据集和模型。我们利用此框架研究哪些微调片段能改善或恶化时间动态。Motive通过运动加权损失掩码将时间动态与静态外观隔离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型中，Motive识别出强烈影响运动的片段，并指导数据策划，以改善时间一致性和物理合理性。使用Motive选择的高影响数据，我们的方法在VBench上提高了运动平滑度和动态程度，与预训练基础模型相比，获得了74.1%的人工偏好胜率。据我们所知，这是第一个在视频生成模型中归因于运动而非视觉外观的框架，并利用它来策划微调数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand how data influences motion in video generation models, which remains poorly explored. The authors introduce Motive, a motion-centric, gradient-based data attribution framework designed to analyze the impact of fine-tuning clips on temporal dynamics in large video datasets. Experimental results demonstrate that Motive effectively isolates motion dynamics from static appearance, leading to improved motion smoothness and dynamic degree in text-to-video models, achieving a 74.1% human preference win rate compared to the pretrained base model, marking a significant advancement in motion attribution for video generation.</div>
<div class="mono" style="margin-top:8px">本研究解决了数据如何影响视频生成模型中的运动这一问题的有限理解。作者提出了Motive，这是一个以运动为中心的基于梯度的数据归因框架，旨在分析大型视频数据集和模型。通过应用运动加权损失掩码，Motive有效地将时间动态与静态外观分离，从而识别出对运动有显著影响的剪辑。结果表明，使用Motive策划的数据可以提高运动的平滑度和动态程度，相较于预训练的基础模型，获得了74.1%的人工偏好胜率，标志着视频生成中运动归因的新方法。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：用于行人重识别的视频超分辨率</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹质量通常被视为事后考虑，绝大多数研究集中于对基础模型的架构修改。这些方法忽视了一个重要的限制，在现实世界的困难场景中部署ReID系统时面临挑战。本文介绍了S3-CLIP，一种基于视频超分辨率的CLIP-ReID框架，旨在2026年WACV的VReID-XFD挑战中开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，适应视频基础的行人重识别设置。据我们所知，这项工作代表了首次系统性研究视频超分辨率作为提高行人ReID轨迹质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法在基线性能上具有竞争力，在空中到地面场景中实现了37.52%的mAP，在地面到空中场景中实现了29.16%的mAP。在地面到空中设置中，S3-CLIP在排名准确性上取得了显著提升，Rank-1、Rank-5和Rank-10的性能分别提高了11.24%、13.48%和17.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the often-overlooked quality of tracklets in person re-identification (ReID) systems, which poses challenges in real-world scenarios. The authors introduce S3-CLIP, a video super-resolution-based framework that integrates advancements in super-resolution networks with task-driven pipelines specifically for video-based ReID. Experimental results indicate that S3-CLIP achieves competitive performance, with 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios, and shows significant improvements in ranking accuracy in the ground-to-aerial setting, enhancing Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人脸重识别（ReID）系统中常被忽视的轨迹质量问题，这可能会影响其在现实场景中的有效性。作者提出了S3-CLIP，这是一种将视频超分辨率技术与CLIP-ReID相结合的新框架，旨在提高轨迹质量，特别是针对VReID-XFD挑战。实验结果表明，S3-CLIP在空中到地面和地面到空中条件下分别达到了37.52%和29.16%的平均精度（mAP），并在地面到空中的设置中显著提高了排名准确性，Rank-1、Rank-5和Rank-10的表现分别提升了11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Visual World with Artificial Intelligence: A Roadmap</div>
<div class="meta-line">Authors: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu</div>
<div class="meta-line">First: 2025-11-11T18:59:50+00:00 · Latest: 2026-01-13T15:42:01+00:00</div>
<div class="meta-line">Comments: Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08585v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08585v2">PDF</a> · <a href="https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://world-model-roadmap.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a &quot;window&quot; into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用人工智能模拟视觉世界：路线图</div>
<div class="mono" style="margin-top:8px">视频生成的格局正在发生变化，从专注于生成视觉吸引人的片段转向构建支持交互并保持物理合理性的虚拟环境。这些发展指向视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐式世界模型，模拟支配真实或想象世界的物理动态、代理-环境交互和任务规划。本调查提供了这一演变的系统概述，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型和视频渲染器。世界模型编码了关于世界的结构化知识，包括物理法则、交互动态和代理行为。它作为潜在的模拟引擎，使得连贯的视觉推理、长期时间一致性和目标驱动的规划成为可能。视频渲染器将这种潜在模拟转化为现实的视觉观察，有效地将视频作为“窗口”展示模拟世界。我们通过四个世代追踪视频生成的进展，其中核心能力逐步提升，最终形成一个建立在视频生成模型之上的世界模型，体现内在的物理合理性、实时多模态交互和跨多个时空尺度的规划能力。对于每一代，我们定义其核心特征，突出代表性作品，并考察其应用领域，如机器人技术、自动驾驶和互动游戏。最后，我们讨论下一代世界模型的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作的最新列表可在此链接中查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to advance video generation from merely creating visually appealing clips to developing interactive virtual environments that adhere to physical plausibility. The authors propose a systematic overview of video foundation models, which integrate an implicit world model that encodes structured knowledge about physical laws and agent interactions, along with a video renderer that generates realistic visual outputs. Key findings indicate that the evolution of video generation can be categorized into four generations, each enhancing capabilities such as coherent visual reasoning and planning, ultimately leading to a model that supports real-time interaction and spans multiple spatiotemporal scales, with applications in fields like robotics and gaming.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于推动视频生成从创建视觉吸引的剪辑转向开发保持物理合理性的互动虚拟环境。作者提出了视频基础模型的系统概述，该模型由隐式世界模型（编码关于物理法则和交互的结构化知识）和视频渲染器（生成逼真的视觉输出）组成。研究结果表明，视频生成能力经历了四个世代的进步，最终形成一个支持实时互动和跨多种时空尺度规划的模型，应用于机器人技术和自动驾驶等领域。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts</div>
<div class="meta-line">Authors: Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Loppi, Antti Honkela</div>
<div class="meta-line">First: 2024-06-25T06:04:58+00:00 · Latest: 2026-01-13T15:13:42+00:00</div>
<div class="meta-line">Comments: 19 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.17298v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.17298v3">PDF</a> · <a href="https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无捷径的差分隐私深度学习的高效可扩展实现</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DP-SGD）是基于差分隐私（DP）训练机器学习模型的标准算法。最常见的DP-SGD隐私会计依赖于泊松子采样以确保理论上的DP保证。使用泊松子采样实现计算高效的DP-SGD并非易事，这导致许多实现通过使用计算更快的子采样走捷径。我们通过实现和基准测试使用正确的泊松子采样的高效方法来量化在DP下训练深度学习模型的计算成本。我们发现，使用PyTorch中Opacus的DP-SGD的天真实现，其吞吐量比SGD低2.6到8倍。然而，像Ghost Clipping这样的高效梯度裁剪实现可以大致将此成本减半。我们提出了一种使用JAX的DP-SGD替代计算高效实现，采用泊松子采样，并与基于PyTorch的高效裁剪优化表现相当。我们研究了使用多达80个GPU的扩展行为，发现DP-SGD的扩展性优于SGD。我们在https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL分享我们的库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenges of implementing differentially private stochastic gradient descent (DP-SGD) efficiently while maintaining theoretical privacy guarantees through Poisson subsampling. The authors benchmark various methods and quantify the computational costs associated with training deep learning models under differential privacy. Their findings reveal that the naive implementation of DP-SGD with Opacus in PyTorch has a throughput significantly lower than standard SGD, but efficient gradient clipping methods like Ghost Clipping can reduce this cost by approximately half. Additionally, they propose a new implementation of DP-SGD using JAX that leverages Poisson subsampling and demonstrates comparable performance to optimized PyTorch methods, with improved scalability when utilizing up to 80 GPUs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高差分隐私随机梯度下降（DP-SGD）在训练机器学习模型时的效率，同时遵循理论隐私保证。作者实现并基准测试了各种方法，重点关注正确应用泊松子采样，以量化与DP训练相关的计算成本。他们的研究结果表明，使用PyTorch中Opacus的DP-SGD简单实现的吞吐量显著低于标准SGD，但使用像Ghost Clipping这样的高效梯度裁剪方法可以将这一成本减少约一半。此外，他们提出了一种使用JAX的计算高效的DP-SGD实现，其性能与优化后的PyTorch版本相当，并证明DP-SGD在使用多达80个GPU时的扩展性优于SGD。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</div>
<div class="meta-line">Authors: Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</div>
<div class="meta-line">First: 2026-01-13T13:42:05+00:00 · Latest: 2026-01-13T13:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08557v1">PDF</a> · <a href="https://github.com/Simula/HEDGE#videohedge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHEDGE：基于熵的视觉语言模型视频幻觉检测框架，通过语义聚类和时空扰动</div>
<div class="mono" style="margin-top:8px">在视频能力的视觉语言模型（Video-VLMs）中，幻觉现象仍然频繁且置信度高，而现有的不确定性度量往往无法与正确性对齐。我们提出了VideoHEDGE，这是一个用于视频问答中幻觉检测的模块化框架，将基于熵的可靠性估计从图像扩展到时间结构化输入。给定一个视频-问题对，VideoHEDGE从干净片段和光度及时空扰动的变体中提取基线答案和多个高温生成，然后使用基于自然语言推理（NLI）或嵌入的方法将结果文本输出聚类为语义假设。聚类级别的概率质量产生三个可靠性评分：语义熵（SE）、RadFlag和视觉增强语义熵（VASE）。我们在SoccerChat基准上评估VideoHEDGE，使用LLM作为评判者获得二元幻觉标签。在三个7B Video-VLM（Qwen2-VL、Qwen2.5-VL和一个SoccerChat微调模型）中，VASE始终实现最高的ROC-AUC，尤其是在较大的失真预算下，而SE和RadFlag往往接近随机。我们进一步表明，基于嵌入的聚类在检测性能上与基于NLI的聚类相匹配，但计算成本显著较低，并且领域微调减少了幻觉频率，但在校准方面仅带来了适度的改善。hedge-bench PyPI库支持可重复和可扩展的基准测试，完整代码和实验资源可在https://github.com/Simula/HEDGE#videohedge获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the frequent and high-confidence hallucinations in video-capable vision-language models (Video-VLMs), as existing uncertainty metrics often do not correlate with correctness. The authors introduce VideoHEDGE, a modular framework for hallucination detection that utilizes entropy-based reliability estimation for video question answering by generating baseline answers and high-temperature outputs from both clean and perturbed video clips, which are then clustered into semantic hypotheses. Experimental results on the SoccerChat benchmark demonstrate that the Vision-Amplified Semantic Entropy (VASE) score outperforms other reliability metrics in detecting hallucinations across three different Video-VLMs, particularly under larger distortion budgets, while also showing that embedding-based clustering can achieve similar performance to Natural Language Inference-based clustering at a lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频能力视觉语言模型（Video-VLMs）中频繁且高置信度的幻觉问题，而现有的不确定性度量往往难以准确评估。作者提出了VideoHEDGE，这是一个用于视频问答中检测幻觉的模块化框架，通过将基于熵的可靠性估计扩展到时间结构输入。对SoccerChat基准的实验结果表明，视觉增强语义熵（VASE）得分在较大失真预算下始终优于其他可靠性度量，同时还显示嵌入式聚类能够以较低的计算成本实现与基于自然语言推理的聚类相似的检测性能。</div>
</details>
</div>
<div class="card">
<div class="title">Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning</div>
<div class="meta-line">Authors: Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano</div>
<div class="meta-line">First: 2025-07-18T17:59:55+00:00 · Latest: 2026-01-13T13:22:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14137v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14137v3">PDF</a> · <a href="https://github.com/valeoai/Franca">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Franca：用于可扩展视觉表示学习的嵌套俄罗斯套娃聚类</div>
<div class="mono" style="margin-top:8px">我们介绍Franca（发音为Fran-ka）：一个免费的开源视觉基础模型，数据、代码和权重完全开放，性能与许多最先进的专有模型（如DINOv2、CLIP、SigLIPv2等）相匹配，并在许多情况下超越它们。我们的方法基于受Web-SSL启发的透明训练流程，使用公开可用的数据：ImageNet-21K和ReLAION-2B的一个子集。除了模型发布外，我们还解决了SSL聚类方法中的关键局限性。现代模型依赖于通过像Sinkhorn-Knopp这样的聚类算法将图像特征分配给大型代码本，但未能考虑聚类语义中的固有模糊性。为了解决这个问题，我们引入了一种基于嵌套俄罗斯套娃表示的参数高效多头聚类投影器。该设计逐步将特征细化为越来越细粒度的聚类，而不增加模型大小，从而实现性能和内存效率。此外，我们提出了一种新颖的位置信息解耦策略，明确消除密集表示中的位置偏差，从而改善语义内容的编码。这在多个下游基准测试中带来了持续的提升，证明了更清晰特征空间的实用性。我们的贡献为透明、高性能的视觉模型建立了新的标准，并为更可重复和可推广的基础模型在更广泛的AI社区中开辟了道路。代码和模型检查点可在https://github.com/valeoai/Franca获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a fully open-source vision foundation model that can compete with and often exceed the performance of leading proprietary models. The authors introduce Franca, which employs a transparent training pipeline utilizing publicly available datasets such as ImageNet-21K and a subset of ReLAION-2B. Key experimental findings indicate that Franca&#x27;s innovative multi-head clustering projector, based on nested Matryoshka representations, effectively refines image features into finer clusters while maintaining model efficiency, and the novel positional disentanglement strategy enhances semantic content encoding, resulting in improved performance across various downstream benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个完全开源的视觉基础模型，该模型在性能上与领先的专有模型相匹配或超越，同时解决自监督学习（SSL）聚类方法的局限性。作者开发了Franca，采用透明的训练流程，并利用包括ImageNet-21K和ReLAION-2B子集在内的公开数据集。关键实验结果表明，引入基于嵌套马特里奥什卡表示的参数高效多头聚类投影器，可以在不增加模型大小的情况下，逐步将特征细化为更精细的聚类，从而在多个下游基准测试中提高性能和内存效率。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Autoregressive Generation</div>
<div class="meta-line">Authors: Stepan Maschan, Haoxuan Qu, Jun Liu</div>
<div class="meta-line">First: 2026-01-06T17:07:27+00:00 · Latest: 2026-01-13T11:19:48+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03184v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去中心化自回归生成</div>
<div class="mono" style="margin-top:8px">我们对自回归生成的去中心化进行了理论分析。我们通过将概率生成速度表示为专家流的线性组合，定义了去中心化离散流匹配目标。我们还进行了实验，展示了在多模态语言模型的不同基准测试中，去中心化和中心化训练设置之间的等价性。具体而言，我们比较了两种不同的范式：LLaVA和InternVL 2.5-1B，后者在指令调优阶段使用固定的CLIP视觉编码器并进行全参数微调（ViT+MLP+LLM）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to analyze the decentralization of autoregressive generation in multimodal language models. The authors introduce the Decentralized Discrete Flow Matching objective, which represents probability generating velocity as a linear combination of expert flows. Experimental results show that decentralized and centralized training settings yield equivalent performance across various benchmarks when comparing the LLaVA and InternVL 2.5-1B models, which utilize a fixed CLIP vision encoder and undergo full-parameter fine-tuning during instruction tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于分析多模态语言模型中自回归生成的去中心化。作者提出了去中心化离散流匹配目标，该目标将概率生成速度表示为专家流的线性组合。实验结果表明，在比较使用固定CLIP视觉编码器并在指令调优阶段进行全参数微调的LLaVA和InternVL 2.5-1B模型时，去中心化和中心化训练设置在各种基准测试中表现出等效的性能。</div>
</details>
</div>
<div class="card">
<div class="title">MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</div>
<div class="meta-line">Authors: Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra, Biplab Banerjee</div>
<div class="meta-line">First: 2026-01-13T10:44:37+00:00 · Latest: 2026-01-13T10:44:37+00:00</div>
<div class="meta-line">Comments: Accepted at InGARSS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08420v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08420v1">PDF</a> · <a href="https://github.com/AdityaChaudhary2913/CLIP_HSI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMLGNet：使用CLIP进行遥感数据的跨模态对齐</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的多模态框架——多模态语言引导网络（MMLGNet），旨在使用视觉语言模型（如CLIP）将异构遥感模态（如高光谱成像（HSI）和激光雷达（LiDAR））与自然语言语义对齐。随着多模态地球观测数据的日益丰富，迫切需要有效融合光谱、空间和几何信息的方法，同时实现语义层面的理解。MMLGNet采用特定模态的编码器，通过双向对比学习在共享潜在空间中将视觉特征与手工制作的文本嵌入对齐。受到CLIP训练范式的启发，我们的方法弥合了高维遥感数据与语言引导解释之间的差距。值得注意的是，MMLGNet在简单的基于CNN的编码器上表现出色，在两个基准数据集上超越了几种已建立的仅视觉多模态方法，展示了语言监督的显著优势。代码可在https://github.com/AdityaChaudhary2913/CLIP_HSI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of aligning heterogeneous remote sensing modalities, such as Hyperspectral Imaging and LiDAR, with natural language semantics due to the increasing availability of multimodal Earth observation data. The proposed Multimodal Language-Guided Network (MMLGNet) utilizes modality-specific encoders and implements bi-directional contrastive learning to align visual features with textual embeddings in a shared latent space. Experimental results demonstrate that MMLGNet, using simple CNN-based encoders, significantly outperforms established multimodal visual-only methods on two benchmark datasets, highlighting the advantages of incorporating language supervision.</div>
<div class="mono" style="margin-top:8px">本研究解决了将异构遥感模态（如高光谱成像和激光雷达）与自然语言语义对齐的挑战，原因在于多模态地球观测数据的日益增加。作者提出了一种新颖的框架，称为多模态语言引导网络（MMLGNet），该框架利用特定模态的编码器和双向对比学习，将视觉特征与共享潜在空间中的文本嵌入对齐。实验结果表明，MMLGNet使用简单的CNN编码器在两个基准数据集上超越了几种已建立的多模态视觉方法，突显了在遥感数据解释中引入语言监督的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care</div>
<div class="meta-line">Authors: Jorge García-Torres, Øyvind Meinich-Bache, Sara Brunner, Siren Rettedal, Vilde Kolstad, Kjersti Engan</div>
<div class="meta-line">First: 2025-03-05T07:52:52+00:00 · Latest: 2026-01-13T10:18:52+00:00</div>
<div class="meta-line">Comments: This work has been accepted at IEEE 25th International Conference on Digital Signal Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03244v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03244v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Around 10% of newborns require some help to initiate breathing, and 5\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双流热成像融合技术在新生儿护理中增强出生时间检测</div>
<div class="mono" style="margin-top:8px">约10%的新生儿需要帮助以启动呼吸，5%需要通气支持。准确的出生时间（ToB）记录对于优化新生儿护理至关重要，因为及时干预对适当复苏至关重要。然而，目前记录ToB的临床方法往往依赖手动过程，容易出现不准确。在本研究中，我们提出了一种新颖的双流融合系统，结合图像和视频分析的优势，准确检测产房和手术室的热成像记录中的ToB。通过整合静态和动态流，我们的方法捕捉到更丰富的与出生相关的时空特征，从而实现更强大和精确的ToB估计。我们证明了数据模态之间的协同作用在性能上优于单流方法。我们的系统在短视频片段中检测出生的精确度达到95.7%，召回率为84.8%。此外，借助评分聚合模块，它成功识别了100%的测试案例，较手动标注的中位绝对误差为2秒，绝对平均偏差为4.5秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is the need for accurate Time of Birth (ToB) documentation in neonatal care, as timely interventions are critical for newborns requiring assistance. The authors developed a two-stream fusion system that integrates image and video analysis to enhance ToB detection from thermal recordings in delivery rooms and operating theaters. The experimental results show that this method achieves 95.7% precision and 84.8% recall in detecting birth events within short video clips, and it identifies ToB in 100% of test cases with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于新生儿护理中对准确记录出生时间（ToB）的需求，因为及时干预对需要呼吸支持的新生儿至关重要。作者开发了一种双流融合系统，结合图像和视频分析，以增强从分娩室和手术室的热成像记录中检测ToB的能力。实验结果表明，该方法在短视频片段中检测出生事件的精确度达到95.7%，召回率为84.8%，并且在所有测试案例中成功识别ToB，较手动标注的中位绝对误差仅为2秒。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment</div>
<div class="meta-line">Authors: Hua Ye, Hang Ding, Siyuan Chen, Yiyang Jiang, Changyuan Zhang, Xuan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-11T16:15:15+00:00 · Latest: 2026-01-13T06:56:09+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08399v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过不对齐进行对齐：面向边界的多模态对齐课程学习</div>
<div class="mono" style="margin-top:8px">大多数多模态模型将每个负样本视为相同，忽略了与正样本仅在细节上有所不同的模糊负样本。我们提出了边界感知课程学习与局部注意力（BACL），这是一个轻量级的附加模块，将这些边界案例转化为课程信号。边界感知负样本采样器逐步提高难度，而对比局部注意力损失则突出不匹配发生的地方。这两个模块都是完全可微分的，并且可以与任何现成的双编码器配合使用。理论预测错误率快速下降至O(1/n)；实践表明在CLIP上提高了高达32%的R@1，并在四个大规模基准上达到了新的SOTA，且无需额外标签。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in multimodal models of treating all negative pairs uniformly, which overlooks the subtle differences in ambiguous negatives. The authors introduce Boundary-Aware Curriculum with Local Attention (BACL), a method that incorporates a Boundary-aware Negative Sampler to progressively increase the difficulty of negative samples and a Contrastive Local Attention loss to identify specific mismatches. Experimental results demonstrate that BACL achieves up to a 32% improvement in R@1 over CLIP and sets new state-of-the-art performance on four large-scale benchmarks, all without requiring additional labels.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于观察到大多数多模态模型对所有负样本的处理方式相同，忽视了边界案例中细微的差异。为了解决这个问题，作者提出了边界感知课程学习与局部注意力（BACL），该方法通过边界感知负样本生成器逐步增加负样本的难度，并通过对比局部注意力损失强调不匹配的具体区域。实验结果表明，BACL在R@1上比CLIP提高了32%，并在四个大规模基准测试中创造了新的最先进性能，且无需额外标签。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向预测的深度强化学习在奶牛场高效电力负荷调度中的应用</div>
<div class="mono" style="margin-top:8px">奶牛养殖是一个能源密集型行业，严重依赖电网电力。随着可再生能源的不断整合，可持续能源管理已成为减少对电网依赖和支持联合国可持续发展目标7（可负担和清洁能源）的关键。然而，可再生能源的间歇性特征在实时平衡供需方面带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面显示出良好前景。然而，大多数基于RL的调度方法假设对未来价格或发电有完全的了解，这在动态环境中是不现实的。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，通常导致在可变电价下训练不稳定。为了解决这些挑战，本研究提出了一种深度强化学习框架，用于奶牛场的高效负荷调度，重点关注电池储存和水加热，并考虑现实的操作约束。所提出的面向预测的PPO结合了基于日时和月份的残差校准的短期需求和可再生发电预测，而PID KL PPO变体则采用比例-积分-微分控制器自适应调节KL散度，以实现稳定的策略更新。该方法在真实奶牛场数据上训练，电力成本比PPO低1%，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO将电网进口减少了13.1%，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance energy management in dairy farming, an energy-intensive sector, by addressing the challenges posed by the intermittent nature of renewable energy sources. The study introduces a Deep Reinforcement Learning framework, specifically the Forecast Aware PPO, which utilizes short-term forecasts of demand and renewable generation to improve load scheduling while adhering to operational constraints. Experimental results indicate that this method achieves up to 1% lower electricity costs compared to traditional PPO, 4.8% lower than DQN, and 1.5% lower than SAC, while also reducing grid imports by 13.1% for battery scheduling, demonstrating its effectiveness in sustainable energy management for dairy farms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善奶牛养殖业的能源管理，该行业是一个能源密集型领域，面临可再生能源来源间歇性带来的挑战。研究提出了一种深度强化学习框架，特别是预测感知PPO，利用需求和可再生发电的短期预测来改善负载调度，同时解决传统强化学习方法假设完全了解未来条件的局限性。实验结果表明，所提出的方法在电力成本上比标准PPO低1%，比DQN低4.8%，比SAC低1.5%，同时在电池调度中减少了13.1%的电网进口，证明了其在奶牛养殖业可持续能源管理中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</div>
<div class="meta-line">Authors: Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi</div>
<div class="meta-line">First: 2025-12-28T18:24:19+00:00 · Latest: 2026-01-12T08:31:50+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23035v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23035v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xavierjiezou.github.io/Co2S/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过共同引导和共同融合实现稳定的半监督遥感分割</div>
<div class="mono" style="margin-top:8px">半监督遥感（RS）图像语义分割为减轻全面标注的负担提供了有前景的解决方案，但它在根本上面临伪标签漂移的问题，即确认偏差导致训练过程中错误的积累。在这项工作中，我们提出了Co2S，一个稳定的半监督RS分割框架，协同融合来自视觉-语言模型和自监督模型的先验信息。具体而言，我们构建了一个异构双学生架构，由两个不同的基于ViT的视觉基础模型组成，分别初始化为预训练的CLIP和DINOv3，以减轻错误积累和伪标签漂移。为了有效地结合这些不同的先验信息，引入了一种显式-隐式语义共同引导机制，利用文本嵌入和可学习查询分别提供显式和隐式的类别级引导，从而共同增强语义一致性。此外，开发了一种全局-局部特征协同融合策略，有效地将CLIP捕获的全局上下文信息与DINOv3生成的局部细节融合，使模型能够生成高度精确的分割结果。在六个流行数据集上的广泛实验表明，所提方法的优越性，在各种划分协议和不同场景中始终实现领先的性能。项目页面可访问 https://xavierjiezou.github.io/Co2S/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of pseudo-label drift in semi-supervised remote sensing image semantic segmentation, which complicates the annotation process. The authors propose a framework called Co2S that integrates priors from vision-language models and self-supervised models through a dual-student architecture using ViT-based models initialized with CLIP and DINOv3. Experimental results across six datasets indicate that Co2S effectively mitigates error accumulation and achieves superior segmentation performance compared to existing methods, demonstrating its robustness across various scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决半监督遥感图像语义分割中的伪标签漂移问题，该问题会因确认偏差导致训练过程中的错误累积。作者提出了Co2S，一个新颖的框架，通过异构双学生架构整合来自视觉-语言模型和自监督模型的先验知识，使用基于ViT的模型初始化CLIP和DINOv3。对六个流行数据集的实验结果表明，Co2S有效减轻了错误累积，并在各种协议和场景中实现了优越的分割性能。</div>
</details>
</div>
<div class="card">
<div class="title">Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</div>
<div class="meta-line">Authors: Yuanyang Yin, Yufan Deng, Shenghai Yuan, Kaipeng Zhang, Xiao Yang, Feng Zhao</div>
<div class="meta-line">First: 2026-01-12T07:48:26+00:00 · Latest: 2026-01-12T07:48:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07287v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model&#x27;s learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦引导：从视频扩散模型中的语义弱层解锁可控性</div>
<div class="mono" style="margin-top:8px">图像到视频（I2V）生成任务旨在从参考图像和文本提示合成视频。这要求扩散模型在去噪过程中调和高频视觉约束和低频文本引导。然而，现有的I2V模型优先考虑视觉一致性，如何有效结合这两种引导以确保强烈遵循文本提示仍然未被充分探索。在本研究中，我们观察到在基于扩散变换器（DiT）的I2V模型中，某些中间层表现出弱语义响应（称为语义弱层），这通过文本-视觉相似度的可测量下降得以体现。我们将此归因于一种称为条件隔离的现象，其中对视觉特征的注意力部分脱离了文本引导，过度依赖学习到的视觉先验。为了解决这个问题，我们提出了聚焦引导（FG），它增强了来自语义弱层的可控性。FG包括两个机制：（1）细粒度语义引导（FSG）利用CLIP识别参考帧中的关键区域，并将其作为锚点来引导语义弱层。（2）注意力缓存将语义响应层的注意力图转移到语义弱层，注入显式语义信号，减轻它们对模型学习到的视觉先验的过度依赖，从而增强对文本指令的遵循。为了进一步验证我们的方法并解决这一方向缺乏评估的问题，我们引入了一个基准，用于评估I2V模型中的指令遵循。在这个基准上，聚焦引导证明了其有效性和通用性，将Wan2.1-I2V的总分提高到0.7250（+3.97\%），并将基于MMDiT的HunyuanVideo-I2V提升到0.5571（+7.44\%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adherence of Image-to-Video (I2V) generation models to text prompts, as existing models struggle to effectively couple visual and textual guidance. The authors propose a method called Focal Guidance (FG), which enhances controllability from Semantic-Weak Layers in Diffusion Transformer-based I2V models by implementing Fine-grained Semantic Guidance and Attention Cache mechanisms. Experimental results demonstrate that FG significantly improves performance on a newly introduced benchmark for instruction following in I2V models, achieving a total score increase of 3.97% on Wan2.1-I2V and 7.44% on MMDiT-based HunyuanVideo-I2V.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像到视频（I2V）生成模型对文本提示的遵循程度，因为现有方法在有效整合视觉和文本指导方面存在困难。作者提出了一种名为Focal Guidance（FG）的方法，通过使用细粒度语义指导来锚定参考帧中的关键区域，并利用注意力缓存将语义响应层的注意力图转移到语义弱层，从而增强对语义弱层的控制。实验结果表明，Focal Guidance显著改善了指令遵循能力，在Wan2.1-I2V基准上取得了0.7250的分数，提升了3.97%，在基于MMDiT的HunyuanVideo-I2V上取得了0.5571的分数，提升了7.44%。</div>
</details>
</div>
<div class="card">
<div class="title">VENUS: Visual Editing with Noise Inversion Using Scene Graphs</div>
<div class="meta-line">Authors: Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</div>
<div class="meta-line">First: 2026-01-12T05:24:58+00:00 · Latest: 2026-01-12T05:24:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07219v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VENUS：基于场景图的噪声反转视觉编辑</div>
<div class="mono" style="margin-top:8px">最先进的基于文本的图像编辑模型常常难以平衡背景保留与语义一致性，导致合成全新图像或输出未能实现预期编辑。相比之下，基于场景图的图像编辑通过提供语义实体及其关系的结构化表示来解决这一限制，从而提供更好的可控性。然而，现有的场景图编辑方法通常依赖于模型微调，这会产生高计算成本并限制可扩展性。为此，我们提出了VENUS（基于场景图的噪声反转视觉编辑），这是一个无训练的场景图引导图像编辑框架。具体而言，VENUS采用分离提示条件策略，将编辑的目标对象与其背景上下文解耦，同时利用噪声反转在未编辑区域保持保真度。此外，我们提出的方法将从多模态大语言模型中提取的场景图与扩散骨干网络集成，无需任何额外训练。实证结果表明，VENUS在PIE-Bench上显著提高了背景保留和语义对齐，将PSNR从22.45提高到24.80，SSIM从0.79提高到0.84，并将LPIPS从0.100降低到0.070，相较于最先进的场景图编辑模型（SGEdit）。此外，VENUS在CLIP相似性测量下增强了语义一致性（24.97对比24.19）。在EditVal上，VENUS以0.87的DINO分数实现了最高的保真度，并且显著将每张图像的运行时间从6-10分钟减少到仅20-30秒。除了基于场景图的编辑，VENUS还超越了强大的基于文本的编辑基线，如LEDIT++和P2P+DirInv，从而在两种范式中均展示了一致的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing text-based image editing models, which often fail to maintain background preservation and semantic consistency. The authors introduce VENUS, a training-free framework for scene graph-guided image editing that utilizes a split prompt conditioning strategy and noise inversion to enhance controllability and fidelity. Experimental results show that VENUS significantly improves background preservation and semantic alignment on PIE-Bench, with PSNR increasing from 22.45 to 24.80 and SSIM from 0.79 to 0.84, while also reducing LPIPS from 0.100 to 0.070 compared to the state-of-the-art model SGEdit, and achieving the highest fidelity on EditVal with a DINO score of 0.87, all while decreasing per-image runtime from 6-10 minutes to 20-30 seconds.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有基于文本的图像编辑模型在背景保留和语义一致性方面的局限性。作者提出了VENUS，这是一种无训练的场景图引导图像编辑框架，利用分离提示条件策略和噪声反演来增强可控性和保真度。实验结果表明，VENUS显著改善了背景保留和语义对齐，与最先进的模型SGEdit相比，PSNR从22.45提高到24.80，SSIM从0.79提高到0.84，同时将每张图像的运行时间从6-10分钟减少到仅20-30秒。</div>
</details>
</div>
<div class="card">
<div class="title">CLIMP: Contrastive Language-Image Mamba Pretraining</div>
<div class="meta-line">Authors: Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes</div>
<div class="meta-line">First: 2026-01-11T12:31:55+00:00 · Latest: 2026-01-11T12:31:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06891v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI&#x27;s CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP&#x27;s fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIMP：对比语言-图像Mamba预训练</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）依赖于视觉变换器，其注意机制容易受到虚假相关性的影响，并且与分辨率呈二次方关系。为了解决这些限制，我们提出了CLIMP，这是第一个完全基于Mamba的对比视觉-语言模型，替换了视觉和文本编码器。新架构在视觉和语言中编码序列结构，VMamba捕捉视觉空间归纳偏差，减少对虚假相关性的依赖，并生成有利于跨模态检索和分布外鲁棒性的嵌入空间，在ImageNet-O上超越OpenAI的CLIP-ViT-B 7.5%。CLIMP自然支持可变输入分辨率，无需位置编码插值或专门训练，在16倍训练分辨率下实现高达6.6%的检索准确率，同时使用5倍更少的内存和1.8倍更少的FLOPs。自回归文本编码器进一步克服了CLIP的固定上下文限制，使得密集标注检索成为可能。我们的研究结果表明，Mamba在视觉-语言学习中表现出有利特性，使其成为基于变换器的CLIP的有力替代品。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing Vision Transformers in contrastive language-image pre-training, particularly their susceptibility to spurious correlations and quadratic scaling with resolution. The authors introduce CLIMP, a novel contrastive vision-language model that utilizes Mamba architecture for both vision and text encoding, which captures sequential structures and reduces reliance on spurious correlations. Experimental results demonstrate that CLIMP outperforms OpenAI&#x27;s CLIP-ViT-B by 7.5% on ImageNet-O, achieves up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs, and supports dense captioning retrieval through its autoregressive text encoder.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善CLIP模型的局限性，特别是其对虚假相关性的敏感性以及与视觉变换器相关的高计算成本。作者提出了CLIMP，这是一种新颖的对比视觉-语言模型，利用Mamba架构作为视觉和文本编码器，从而更好地编码序列结构。实验结果表明，CLIMP在ImageNet-O上超越了OpenAI的CLIP-ViT-B 7.5%，在16倍训练分辨率下实现了高达6.6%的检索准确率，同时使用了5倍更少的内存和1.8倍更少的FLOPs，并有效支持密集的标题检索，表明Mamba在视觉-语言学习任务中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Continual Learning for Privacy-Preserving Hospital Imaging Classification</div>
<div class="meta-line">Authors: Anay Sinhal, Arpana Sinhal, Amit Sinhal</div>
<div class="meta-line">First: 2026-01-11T01:28:34+00:00 · Latest: 2026-01-11T01:28:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06742v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06742v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning models for radiology interpretation increasingly rely on multi-institutional data, yet privacy regulations and distribution shift across hospitals limit central data pooling. Federated learning (FL) allows hospitals to collaboratively train models without sharing raw images, but current FL algorithms typically assume a static data distribution. In practice, hospitals experience continual evolution in case mix, annotation protocols, and imaging devices, which leads to catastrophic forgetting when models are updated sequentially. Federated continual learning (FCL) aims to reconcile these challenges but existing methods either ignore the stringent privacy constraints of healthcare or rely on replay buffers and public surrogate datasets that are difficult to justify in clinical settings. We study FCL for chest radiography classification in a setting where hospitals are clients that receive temporally evolving streams of cases and labels. We introduce DP-FedEPC (Differentially Private Federated Elastic Prototype Consolidation), a method that combines elastic weight consolidation (EWC), prototype-based rehearsal, and client-side differential privacy within a standard FedAvg framework. EWC constrains updates along parameters deemed important for previous tasks, while a memory of latent prototypes preserves class structure without storing raw images. Differentially private stochastic gradient descent (DP-SGD) at each client adds calibrated Gaussian noise to clipped gradients, providing formal privacy guarantees for individual radiographs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>隐私保护医院影像分类的联邦持续学习</div>
<div class="mono" style="margin-top:8px">放射学解读的深度学习模型越来越依赖于多机构数据，但隐私法规和医院间的分布变化限制了中央数据汇聚。联邦学习（FL）允许医院在不共享原始图像的情况下协作训练模型，但当前的FL算法通常假设数据分布是静态的。在实践中，医院经历病例组合、注释协议和影像设备的持续演变，这导致模型在顺序更新时出现灾难性遗忘。联邦持续学习（FCL）旨在调和这些挑战，但现有方法要么忽视医疗保健的严格隐私约束，要么依赖于重放缓冲区和难以在临床环境中证明的公共替代数据集。我们研究了在医院作为客户接收时间演变的病例和标签流的环境中进行胸部X光分类的FCL。我们引入了DP-FedEPC（差分隐私联邦弹性原型整合），这是一种结合了弹性权重整合（EWC）、基于原型的排练和客户端差分隐私的标准FedAvg框架的方法。EWC限制了对被认为对先前任务重要的参数的更新，而潜在原型的记忆在不存储原始图像的情况下保留了类别结构。每个客户端的差分隐私随机梯度下降（DP-SGD）向裁剪的梯度添加了经过校准的高斯噪声，为单个X光片提供了正式的隐私保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of privacy regulations and distribution shifts in multi-institutional data for radiology interpretation, which hinder the effective use of centralized data. The authors propose a novel method called DP-FedEPC, which integrates elastic weight consolidation, prototype-based rehearsal, and client-side differential privacy within a federated learning framework to facilitate continual learning in hospitals. Experimental results demonstrate that this approach effectively mitigates catastrophic forgetting while ensuring privacy, allowing hospitals to collaboratively classify chest radiographs using temporally evolving case streams without compromising patient data security.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多机构医院环境中隐私法规和数据分布变化带来的挑战，这些问题妨碍了深度学习模型在放射学解读中的有效应用。作者提出了一种新方法DP-FedEPC，该方法在联邦学习框架内结合了差分隐私、弹性权重整合和基于原型的重演，以实现持续学习而不妨碍患者隐私。实验结果表明，DP-FedEPC有效减轻了灾难性遗忘，同时在胸部放射影像分类任务中保持了类别结构，并确保了对单个放射影像的正式隐私保证。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</div>
<div class="meta-line">Authors: Renren Jin, Pengzhi Gao, Yuqi Ren, Zhuowen Han, Tongxuan Zhang, Wuwei Huang, Wei Liu, Jian Luan, Deyi Xiong</div>
<div class="meta-line">First: 2025-11-08T12:50:41+00:00 · Latest: 2026-01-10T08:58:33+00:00</div>
<div class="meta-line">Comments: 22 pages, 25 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05993v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05993v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has emerged as a prominent paradigm for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, leading to premature convergence to suboptimal local minima and hindering further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To bridge this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our results identify three key factors that influence entropy: the clipping thresholds in the optimization objective, the number of off-policy updates, and the diversity of the training data. Furthermore, through both theoretical analysis and empirical validation, we demonstrate that tokens with positive advantages are the primary drivers of entropy collapse. Motivated by this insight, we propose Positive-Advantage Reweighting, a simple yet effective approach that regulates model entropy by adjusting the loss weights assigned to tokens with positive advantages during RLVR training, while maintaining competitive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视大推理模型中的强化学习熵</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习（RLVR）已成为增强大型语言模型（LLMs）推理能力的一个重要范式。然而，LLMs的熵在RLVR训练过程中通常会崩溃，导致过早收敛到次优局部最小值，阻碍进一步的性能提升。尽管已经提出了各种方法来减轻熵崩溃，但对RLVR中熵的全面研究仍然缺乏。为填补这一空白，我们进行广泛实验，调查使用RLVR训练的LLMs的熵动态，并分析模型熵与响应多样性、校准和在各种基准上的性能之间的关系。我们的结果识别出影响熵的三个关键因素：优化目标中的裁剪阈值、离策略更新的数量以及训练数据的多样性。此外，通过理论分析和实证验证，我们证明了具有正优势的标记是熵崩溃的主要驱动因素。基于这一见解，我们提出了正优势重加权，这是一种简单而有效的方法，通过调整在RLVR训练过程中分配给具有正优势的标记的损失权重来调节模型熵，同时保持竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of entropy collapse in reinforcement learning with verifiable rewards (RLVR), which negatively impacts the performance of large language models (LLMs) by causing premature convergence to suboptimal solutions. The authors conduct extensive experiments to analyze the dynamics of entropy in LLMs during RLVR training and its relationship with response diversity, calibration, and overall performance. They identify three main factors affecting entropy: clipping thresholds in the optimization objective, the number of off-policy updates, and training data diversity, and propose a new method called Positive-Advantage Reweighting to mitigate entropy collapse by adjusting loss weights for tokens with positive advantages, while still achieving competitive performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在可验证奖励的强化学习（RLVR）中，熵崩溃所带来的挑战，这可能会阻碍大型语言模型（LLMs）的推理能力。作者进行了广泛的实验，分析了RLVR训练中LLMs的熵动态及其与响应多样性、校准和各种基准性能之间的关系。研究结果表明，熵受剪切阈值、离线更新次数和训练数据多样性的影响，而具有正优势的标记是熵崩溃的主要因素。为了解决这个问题，他们提出了正优势重加权的方法，通过调整这些标记的损失权重来调节模型熵，同时保持性能。</div>
</details>
</div>
<div class="card">
<div class="title">FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching</div>
<div class="meta-line">Authors: Hongyaoxing Gul, Lijuan Hu, Shuzi Niu, Fangfang Liu</div>
<div class="meta-line">First: 2026-01-09T10:06:45+00:00 · Latest: 2026-01-09T10:06:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLRQ：基于灵活低秩矩阵草图的快速LLM量化</div>
<div class="mono" style="margin-top:8px">传统的后训练量化（PTQ）被认为是减少模型大小和加速大规模语言模型（LLM）推理的有效方法。然而，现有的低秩PTQ方法需要昂贵的微调来确定适用于大型模型中多样数据和层的折中秩，未能充分发挥其潜力。此外，当前基于SVD的低秩近似增加了计算开销。在本研究中，我们全面分析了在代表性模型中不同层次的低秩近似的有效性变化。因此，我们引入了灵活低秩量化（FLRQ），这是一种新颖的解决方案，旨在快速识别准确性最优的秩并将其聚合以实现最小存储组合。FLRQ包含两个强大的组件，基于Rank1-Sketch的灵活秩选择（R1-FLR）和剪切下的最佳低秩近似（BLC）。R1-FLR应用带有高斯投影的R1-Sketch进行快速低秩近似，使每层的异常值感知秩提取成为可能。同时，BLC旨在通过迭代方法在缩放和剪切策略下最小化低秩量化误差。FLRQ在全面实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面均实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of post-training quantization (PTQ) for large-scale language models (LLMs), which traditionally require extensive fine-tuning and incur high computational costs. The authors propose a novel method called Flexible Low-Rank Quantization (FLRQ), which includes Rank1-Sketch-based Flexible Rank Selection (R1-FLR) for rapid low-rank approximation and Best Low-rank Approximation under Clipping (BLC) to minimize quantization errors. Experimental results show that FLRQ achieves state-of-the-art performance in both quantization quality and algorithm efficiency, effectively identifying optimal ranks for various layers without the need for costly fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大规模语言模型（LLM）后训练量化（PTQ）的效率，解决现有低秩方法需要大量微调和高计算成本的局限性。作者提出了一种新方法，称为灵活低秩量化（FLRQ），其中包括基于Rank1-Sketch的灵活秩选择（R1-FLR），用于快速低秩近似，以及在剪切下的最佳低秩近似（BLC），以最小化量化误差。实验结果表明，FLRQ在量化质量和算法效率方面达到了最先进的性能，有效识别最佳秩并减少各种模型层的存储需求。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260115_0332.html">20260115_0332</a>
<a href="archive/20260114_0332.html">20260114_0332</a>
<a href="archive/20260113_0331.html">20260113_0331</a>
<a href="archive/20260112_0325.html">20260112_0325</a>
<a href="archive/20260111_0325.html">20260111_0325</a>
<a href="archive/20260110_0330.html">20260110_0330</a>
<a href="archive/20260109_0330.html">20260109_0330</a>
<a href="archive/20260108_0332.html">20260108_0332</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_1857.html">20260106_1857</a>
<a href="archive/20260106_1846.html">20260106_1846</a>
<a href="archive/20260106_0330.html">20260106_0330</a>
<a href="archive/20260105_0325.html">20260105_0325</a>
<a href="archive/20260104_2229.html">20260104_2229</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
